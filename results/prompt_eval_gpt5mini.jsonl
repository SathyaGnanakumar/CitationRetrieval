{"example_id": "blind_964", "prompt": "You are an expert citation retrieval system as described in the paper \"Multi-Agent System for Reliable Citation Retrieval\".\nYour goal is to autonomously retrieve, verify, and recommend academic references given a query or document excerpt.\n\nTask:\nGiven a citation context from a scientific paper (where a citation is missing), identify the correct paper from a list of candidates.\nAnalyze the context to understand the specific claim, method, or result being cited.\nThen, evaluate each candidate paper to see if it matches the context.\nFinally, select the best matching paper.\n\nCitation Context:\nn backbones with language models with intermediate networks and billions of data pairs, exhibiting superior results on vision-language tasks. OFA, Uni-Perceivers, and Unified-IO[CITATION] also introduce unified training architectures for different modalities with competitive performance to uni-modal methods. Recently, inspired by the powerful GPT-4, many efforts have been d\n\nCandidate Papers:\n1. Title: Microsoft coco captions: Data collection and evaluation server\n   Abstract: Microsoft COCO Captions: Data Collection and Evaluation Server: In this paper we describe the Microsoft COCO Caption dataset and evaluation server. When completed, the dataset will contain over one and a half million captions describing over 330,000 images. For the training and validation images, five independent human generated captions will be provided. To ensure consistency in evaluation of automatic caption generation algorithms, an evaluation server is used. The evaluation server receives candidate captions and scores them using several popular metrics, including BLEU, METEOR, ROUGE and CIDEr. Instructions for using the evaluation server are provided.\n\n2. Title: Photorealistic Text-to-Image Diffusion Models with Deep Language Understanding\n   Abstract: Photorealistic Text-to-Image Diffusion Models with Deep Language Understanding: We present Imagen, a text-to-image diffusion model with an unprecedented degree of photorealism and a deep level of language understanding. Imagen builds on the power of large transformer language models in understanding text and hinges on the strength of diffusion models in high-fidelity image generation. Our key discovery is that generic large language models (e.g. T5), pretrained on text-only corpora, are surprisingly effective at encoding text for image synthesis: increasing the size of the language model in Imagen boosts both sample fidelity and image-text alignment much more than increasing the size of the image diffusion model. Imagen achieves a new state-of-the-art FID score of 7.27 on the COCO dataset, without ever training on COCO, and human raters find Imagen samples to be on par with the COCO data itself in image-text alignment. To assess text-to-image models in greater depth, we introduce DrawBench, a comprehensive and challenging benchmark for text-to-image models. With DrawBench, we compare Imagen with recent methods including VQ-GAN+CLIP, Latent Diffusion Models, and DALL-E 2, and find that human raters prefer Imagen over other models in side-by-side comparisons, both in terms of sample quality and image-text alignment. See https://imagen.research.google/ for an overview of the results.\n\n3. Title: Unified-io: A unified model for vision, language, and multi-modal tasks\n   Abstract: Unified-IO: A Unified Model for Vision, Language, and Multi-Modal Tasks: We propose Unified-IO, a model that performs a large variety of AI tasks spanning classical computer vision tasks, including pose estimation, object detection, depth estimation and image generation, vision-and-language tasks such as region captioning and referring expression, to natural language processing tasks such as question answering and paraphrasing. Developing a single unified model for such a large variety of tasks poses unique challenges due to the heterogeneous inputs and outputs pertaining to each task, including RGB images, per-pixel maps, binary masks, bounding boxes, and language. We achieve this unification by homogenizing every supported input and output into a sequence of discrete vocabulary tokens. This common representation across all tasks allows us to train a single transformer-based architecture, jointly on over 90 diverse datasets in the vision and language fields. Unified-IO is the first model capable of performing all 7 tasks on the GRIT benchmark and produces strong results across 16 diverse benchmarks like NYUv2-Depth, ImageNet, VQA2.0, OK-VQA, Swig, VizWizGround, BoolQ, and SciTail, with no task-specific fine-tuning. Code and demos for Unified-IO are available at: https://unified-io.allenai.org.\n\n4. Title: Ofa: Unifying architectures, tasks, and modalities through a simple sequence-to-sequence learning framework\n   Abstract: OFA: Unifying Architectures, Tasks, and Modalities Through a Simple Sequence-to-Sequence Learning Framework: In this work, we pursue a unified paradigm for multimodal pretraining to break the scaffolds of complex task/modality-specific customization. We propose OFA, a Task-Agnostic and Modality-Agnostic framework that supports Task Comprehensiveness. OFA unifies a diverse set of cross-modal and unimodal tasks, including image generation, visual grounding, image captioning, image classification, language modeling, etc., in a simple sequence-to-sequence learning framework. OFA follows the instruction-based learning in both pretraining and finetuning stages, requiring no extra task-specific layers for downstream tasks. In comparison with the recent state-of-the-art vision & language models that rely on extremely large cross-modal datasets, OFA is pretrained on only 20M publicly available image-text pairs. Despite its simplicity and relatively small-scale training data, OFA achieves new SOTAs in a series of cross-modal tasks while attaining highly competitive performances on uni-modal tasks. Our further analysis indicates that OFA can also effectively transfer to unseen tasks and unseen domains. Our code and models are publicly available at https://github.com/OFA-Sys/OFA.\n\n5. Title: High-Resolution Image Synthesis with Latent Diffusion Models\n   Abstract: High-Resolution Image Synthesis with Latent Diffusion Models: By decomposing the image formation process into a sequential application of denoising autoencoders, diffusion models (DMs) achieve state-of-the-art synthesis results on image data and beyond. Additionally, their formulation allows for a guiding mechanism to control the image generation process without retraining. However, since these models typically operate directly in pixel space, optimization of powerful DMs often consumes hundreds of GPU days and inference is expensive due to sequential evaluations. To enable DM training on limited computational resources while retaining their quality and flexibility, we apply them in the latent space of powerful pretrained autoencoders. In contrast to previous work, training diffusion models on such a representation allows for the first time to reach a near-optimal point between complexity reduction and detail preservation, greatly boosting visual fidelity. By introducing cross-attention layers into the model architecture, we turn diffusion models into powerful and flexible generators for general conditioning inputs such as text or bounding boxes and high-resolution synthesis becomes possible in a convolutional manner. Our latent diffusion models (LDMs) achieve a new state of the art for image inpainting and highly competitive performance on various tasks, including unconditional image generation, semantic scene synthesis, and super-resolution, while significantly reducing computational requirements compared to pixel-based DMs. Code is available at https://github.com/CompVis/latent-diffusion .\n\n6. Title: Generative adversarial networks\n   Abstract: Generative Adversarial Networks: We propose a new framework for estimating generative models via an adversarial process, in which we simultaneously train two models: a generative model G that captures the data distribution, and a discriminative model D that estimates the probability that a sample came from the training data rather than G. The training procedure for G is to maximize the probability of D making a mistake. This framework corresponds to a minimax two-player game. In the space of arbitrary functions G and D, a unique solution exists, with G recovering the training data distribution and D equal to 1/2 everywhere. In the case where G and D are defined by multilayer perceptrons, the entire system can be trained with backpropagation. There is no need for any Markov chains or unrolled approximate inference networks during either training or generation of samples. Experiments demonstrate the potential of the framework through qualitative and quantitative evaluation of the generated samples.\n\n7. Title: Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models\n   Abstract: BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models: The cost of vision-and-language pre-training has become increasingly prohibitive due to end-to-end training of large-scale models. This paper proposes BLIP-2, a generic and efficient pre-training strategy that bootstraps vision-language pre-training from off-the-shelf frozen pre-trained image encoders and frozen large language models. BLIP-2 bridges the modality gap with a lightweight Querying Transformer, which is pre-trained in two stages. The first stage bootstraps vision-language representation learning from a frozen image encoder. The second stage bootstraps vision-to-language generative learning from a frozen language model. BLIP-2 achieves state-of-the-art performance on various vision-language tasks, despite having significantly fewer trainable parameters than existing methods. For example, our model outperforms Flamingo80B by 8.7% on zero-shot VQAv2 with 54x fewer trainable parameters. We also demonstrate the model's emerging capabilities of zero-shot image-to-text generation that can follow natural language instructions.\n\n8. Title: CGOF++: Controllable 3D Face Synthesis with Conditional Generative Occupancy Fields\n   Abstract: CGOF++: Controllable 3D Face Synthesis with Conditional Generative Occupancy Fields: Capitalizing on the recent advances in image generation models, existing controllable face image synthesis methods are able to generate high-fidelity images with some levels of controllability, e.g., controlling the shapes, expressions, textures, and poses of the generated face images. However, previous methods focus on controllable 2D image generative models, which are prone to producing inconsistent face images under large expression and pose changes. In this paper, we propose a new NeRF-based conditional 3D face synthesis framework, which enables 3D controllability over the generated face images by imposing explicit 3D conditions from 3D face priors. At its core is a conditional Generative Occupancy Field (cGOF++) that effectively enforces the shape of the generated face to conform to a given 3D Morphable Model (3DMM) mesh, built on top of EG3D [1], a recent tri-plane-based generative model. To achieve accurate control over fine-grained 3D face shapes of the synthesized images, we additionally incorporate a 3D landmark loss as well as a volume warping loss into our synthesis framework. Experiments validate the effectiveness of the proposed method, which is able to generate high-fidelity face images and shows more precise 3D controllability than state-of-the-art 2D-based controllable face synthesis methods.\n\n9. Title: Hierarchical Text-Conditional Image Generation with CLIP Latents\n   Abstract: Hierarchical Text-Conditional Image Generation with CLIP Latents: Contrastive models like CLIP have been shown to learn robust representations of images that capture both semantics and style. To leverage these representations for image generation, we propose a two-stage model: a prior that generates a CLIP image embedding given a text caption, and a decoder that generates an image conditioned on the image embedding. We show that explicitly generating image representations improves image diversity with minimal loss in photorealism and caption similarity. Our decoders conditioned on image representations can also produce variations of an image that preserve both its semantics and style, while varying the non-essential details absent from the image representation. Moreover, the joint embedding space of CLIP enables language-guided image manipulations in a zero-shot fashion. We use diffusion models for the decoder and experiment with both autoregressive and diffusion models for the prior, finding that the latter are computationally more efficient and produce higher-quality samples.\n\n10. Title: Inverting generative adversarial renderer for face reconstruction\n   Abstract: Inverting Generative Adversarial Renderer for Face Reconstruction: Given a monocular face image as input, 3D face geometry reconstruction aims to recover a corresponding 3D face mesh. Recently, both optimization-based and learning-based face reconstruction methods have taken advantage of the emerging differentiable renderer and shown promising results. However, the differentiable renderer, mainly based on graphics rules, simplifies the realistic mechanism of the illumination, reflection, \\etc, of the real world, thus cannot produce realistic images. This brings a lot of domain-shift noise to the optimization or training process. In this work, we introduce a novel Generative Adversarial Renderer (GAR) and propose to tailor its inverted version to the general fitting pipeline, to tackle the above problem. Specifically, the carefully designed neural renderer takes a face normal map and a latent code representing other factors as inputs and renders a realistic face image. Since the GAR learns to model the complicated real-world image, instead of relying on the simplified graphics rules, it is capable of producing realistic images, which essentially inhibits the domain-shift noise in training and optimization. Equipped with the elaborated GAR, we further proposed a novel approach to predict 3D face parameters, in which we first obtain fine initial parameters via Renderer Inverting and then refine it with gradient-based optimizers. Extensive experiments have been conducted to demonstrate the effectiveness of the proposed generative adversarial renderer and the novel optimization-based face reconstruction framework. Our method achieves state-of-the-art performances on multiple face reconstruction datasets.\n\nThink step-by-step about which candidate best matches the context. Summarize your reasoning, then provide the exact title that should fill the citation. Respond in JSON with fields:\n{\n  \"reasoning\": \"<your step-by-step analysis>\",\n  \"selected_title\": \"<exact candidate title>\"\n}", "response": {"reasoning": "The citation in context refers to Unified-IO as one of several works that introduce unified training architectures across modalities (vision, language, multi-modal) with competitive performance. Candidate 3's title and abstract (", ") explicitly describe Unified-IO as a unified model for vision, language, and multi-modal tasks, trained jointly on diverse datasets and designed to homogenize inputs/outputs for a single transformer architecture â€” matching the claim in the context. Other candidates (e.g., OFA) are different papers already mentioned separately, and the rest are unrelated to unified multi-modal architectures. Therefore Candidate 3 is the correct match for the citation.\n\nSelected paper: Candidate 3 matches the described claim and content exactly.\n\n(Note: I selected based on title and abstract content that directly supports the statement about Unified-IO introducing a unified training architecture for multiple modalities.)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n (redacted long spacing to preserve JSON formatting.)\n\nSelected_title: \"Unified-io: A unified model for vision, language, and multi-modal tasks\"": "Unified-io: A unified model for vision, language, and multi-modal tasks"}, "selected_title": "", "positive_title": "Unified-io: A unified model for vision, language, and multi-modal tasks", "is_correct": false, "reasoning": "The citation in context refers to Unified-IO as one of several works that introduce unified training architectures across modalities (vision, language, multi-modal) with competitive performance. Candidate 3's title and abstract ("}
{"example_id": "blind_15855", "prompt": "You are an expert citation retrieval system as described in the paper \"Multi-Agent System for Reliable Citation Retrieval\".\nYour goal is to autonomously retrieve, verify, and recommend academic references given a query or document excerpt.\n\nTask:\nGiven a citation context from a scientific paper (where a citation is missing), identify the correct paper from a list of candidates.\nAnalyze the context to understand the specific claim, method, or result being cited.\nThen, evaluate each candidate paper to see if it matches the context.\nFinally, select the best matching paper.\n\nCitation Context:\nce speed and energy efficiency on the edge, weights and activations of neural networks need to be quantized to low precision. Prior works use the same QBN for all layers[CITATION], which would introduce a huge decrease in accuracy as bit goes down. A possible solution here is to use mixed-precision quantization, where higher precision is used for more sensitive lay\n\nCandidate Papers:\n1. Title: Towards accurate binary convolutional neural network\n   Abstract: Towards Accurate Binary Convolutional Neural Network: We introduce a novel scheme to train binary convolutional neural networks (CNNs) -- CNNs with weights and activations constrained to {-1,+1} at run-time. It has been known that using binary weights and activations drastically reduce memory size and accesses, and can replace arithmetic operations with more efficient bitwise operations, leading to much faster test-time inference and lower power consumption. However, previous works on binarizing CNNs usually result in severe prediction accuracy degradation. In this paper, we address this issue with two major innovations: (1) approximating full-precision weights with the linear combination of multiple binary weight bases; (2) employing multiple binary activations to alleviate information loss. The implementation of the resulting binary CNN, denoted as ABC-Net, is shown to achieve much closer performance to its full-precision counterpart, and even reach the comparable prediction accuracy on ImageNet and forest trail datasets, given adequate binary weight bases and activations.\n\n2. Title: Dorefa-net: Training low bitwidth convolutional neural networks with low bitwidth gradients\n   Abstract: DoReFa-Net: Training Low Bitwidth Convolutional Neural Networks with Low Bitwidth Gradients: We propose DoReFa-Net, a method to train convolutional neural networks that have low bitwidth weights and activations using low bitwidth parameter gradients. In particular, during backward pass, parameter gradients are stochastically quantized to low bitwidth numbers before being propagated to convolutional layers. As convolutions during forward/backward passes can now operate on low bitwidth weights and activations/gradients respectively, DoReFa-Net can use bit convolution kernels to accelerate both training and inference. Moreover, as bit convolutions can be efficiently implemented on CPU, FPGA, ASIC and GPU, DoReFa-Net opens the way to accelerate training of low bitwidth neural network on these hardware. Our experiments on SVHN and ImageNet datasets prove that DoReFa-Net can achieve comparable prediction accuracy as 32-bit counterparts. For example, a DoReFa-Net derived from AlexNet that has 1-bit weights, 2-bit activations, can be trained from scratch using 6-bit gradients to get 46.1\\% top-1 accuracy on ImageNet validation set. The DoReFa-Net AlexNet model is released publicly.\n\n3. Title: Mobilenetv2: Inverted residuals and linear bottlenecks\n   Abstract: MobileNetV2: Inverted Residuals and Linear Bottlenecks: In this paper we describe a new mobile architecture, MobileNetV2, that improves the state of the art performance of mobile models on multiple tasks and benchmarks as well as across a spectrum of different model sizes. We also describe efficient ways of applying these mobile models to object detection in a novel framework we call SSDLite. Additionally, we demonstrate how to build mobile semantic segmentation models through a reduced form of DeepLabv3 which we call Mobile DeepLabv3.   The MobileNetV2 architecture is based on an inverted residual structure where the input and output of the residual block are thin bottleneck layers opposite to traditional residual models which use expanded representations in the input an MobileNetV2 uses lightweight depthwise convolutions to filter features in the intermediate expansion layer. Additionally, we find that it is important to remove non-linearities in the narrow layers in order to maintain representational power. We demonstrate that this improves performance and provide an intuition that led to this design. Finally, our approach allows decoupling of the input/output domains from the expressiveness of the transformation, which provides a convenient framework for further analysis. We measure our performance on Imagenet classification, COCO object detection, VOC image segmentation. We evaluate the trade-offs between accuracy, and number of operations measured by multiply-adds (MAdd), as well as the number of parameters\n\n4. Title: Deep compression: Compressing deep neural networks with pruning, trained quantization and huffman coding\n   Abstract: Deep Compression: Compressing Deep Neural Networks with Pruning, Trained Quantization and Huffman Coding: Neural networks are both computationally intensive and memory intensive, making them difficult to deploy on embedded systems with limited hardware resources. To address this limitation, we introduce \"deep compression\", a three stage pipeline: pruning, trained quantization and Huffman coding, that work together to reduce the storage requirement of neural networks by 35x to 49x without affecting their accuracy. Our method first prunes the network by learning only the important connections. Next, we quantize the weights to enforce weight sharing, finally, we apply Huffman coding. After the first two steps we retrain the network to fine tune the remaining connections and the quantized centroids. Pruning, reduces the number of connections by 9x to 13x; Quantization then reduces the number of bits that represent each connection from 32 to 5. On the ImageNet dataset, our method reduced the storage required by AlexNet by 35x, from 240MB to 6.9MB, without loss of accuracy. Our method reduced the size of VGG-16 by 49x from 552MB to 11.3MB, again with no loss of accuracy. This allows fitting the model into on-chip SRAM cache rather than off-chip DRAM memory. Our compression method also facilitates the use of complex neural networks in mobile applications where application size and download bandwidth are constrained. Benchmarked on CPU, GPU and mobile GPU, compressed network has 3x to 4x layerwise speedup and 3x to 7x better energy efficiency.\n\n5. Title: Xnor-net: Imagenet classification using binary convolutional neural networks\n   Abstract: XNOR-Net: ImageNet Classification Using Binary Convolutional Neural Networks: We propose two efficient approximations to standard convolutional neural networks: Binary-Weight-Networks and XNOR-Networks. In Binary-Weight-Networks, the filters are approximated with binary values resulting in 32x memory saving. In XNOR-Networks, both the filters and the input to convolutional layers are binary. XNOR-Networks approximate convolutions using primarily binary operations. This results in 58x faster convolutional operations and 32x memory savings. XNOR-Nets offer the possibility of running state-of-the-art networks on CPUs (rather than GPUs) in real-time. Our binary networks are simple, accurate, efficient, and work on challenging visual tasks. We evaluate our approach on the ImageNet classification task. The classification accuracy with a Binary-Weight-Network version of AlexNet is only 2.9% less than the full-precision AlexNet (in top-1 measure). We compare our method with recent network binarization methods, BinaryConnect and BinaryNets, and outperform these methods by large margins on ImageNet, more than 16% in top-1 accuracy.\n\n6. Title: Hawq-v2: Hessian aware trace-weighted quantization of neural networks\n   Abstract: HAWQ-V2: Hessian Aware trace-Weighted Quantization of Neural Networks: Quantization is an effective method for reducing memory footprint and inference time of Neural Networks, e.g., for efficient inference in the cloud, especially at the edge. However, ultra low precision quantization could lead to significant degradation in model generalization. A promising method to address this is to perform mixed-precision quantization, where more sensitive layers are kept at higher precision. However, the search space for a mixed-precision quantization is exponential in the number of layers. Recent work has proposed HAWQ, a novel Hessian based framework, with the aim of reducing this exponential search space by using second-order information. While promising, this prior work has three major limitations: (i) HAWQV1 only uses the top Hessian eigenvalue as a measure of sensitivity and do not consider the rest of the Hessian spectrum; (ii) HAWQV1 approach only provides relative sensitivity of different layers and therefore requires a manual selection of the mixed-precision setting; and (iii) HAWQV1 does not consider mixed-precision activation quantization. Here, we present HAWQV2 which addresses these shortcomings. For (i), we perform a theoretical analysis showing that a better sensitivity metric is to compute the average of all of the Hessian eigenvalues. For (ii), we develop a Pareto frontier based method for selecting the exact bit precision of different layers without any manual selection. For (iii), we extend the Hessian analysis to mixed-precision activation quantization. We have found this to be very beneficial for object detection. We show that HAWQV2 achieves new state-of-the-art results for a wide range of tasks.\n\n7. Title: Hawq: Hessian aware quantization of neural networks with mixed-precision\n   Abstract: HAWQ: Hessian AWare Quantization of Neural Networks with Mixed-Precision: Model size and inference speed/power have become a major challenge in the deployment of Neural Networks for many applications. A promising approach to address these problems is quantization. However, uniformly quantizing a model to ultra low precision leads to significant accuracy degradation. A novel solution for this is to use mixed-precision quantization, as some parts of the network may allow lower precision as compared to other layers. However, there is no systematic way to determine the precision of different layers. A brute force approach is not feasible for deep networks, as the search space for mixed-precision is exponential in the number of layers. Another challenge is a similar factorial complexity for determining block-wise fine-tuning order when quantizing the model to a target precision. Here, we introduce Hessian AWare Quantization (HAWQ), a novel second-order quantization method to address these problems. HAWQ allows for the automatic selection of the relative quantization precision of each layer, based on the layer's Hessian spectrum. Moreover, HAWQ provides a deterministic fine-tuning order for quantizing layers, based on second-order information. We show the results of our method on Cifar-10 using ResNet20, and on ImageNet using Inception-V3, ResNet50 and SqueezeNext models. Comparing HAWQ with state-of-the-art shows that we can achieve similar/better accuracy with $8\\times$ activation compression ratio on ResNet20, as compared to DNAS~\\cite{wu2018mixed}, and up to $1\\%$ higher accuracy with up to $14\\%$ smaller models on ResNet50 and Inception-V3, compared to recently proposed methods of RVQuant~\\cite{park2018value} and HAQ~\\cite{wang2018haq}. Furthermore, we show that we can quantize SqueezeNext to just 1MB model size while achieving above $68\\%$ top1 accuracy on ImageNet.\n\n8. Title: Autoq: Automated kernel-wise neural network quantization\n   Abstract: AutoQ: Automated Kernel-Wise Neural Network Quantization: Network quantization is one of the most hardware friendly techniques to enable the deployment of convolutional neural networks (CNNs) on low-power mobile devices. Recent network quantization techniques quantize each weight kernel in a convolutional layer independently for higher inference accuracy, since the weight kernels in a layer exhibit different variances and hence have different amounts of redundancy. The quantization bitwidth or bit number (QBN) directly decides the inference accuracy, latency, energy and hardware overhead. To effectively reduce the redundancy and accelerate CNN inferences, various weight kernels should be quantized with different QBNs. However, prior works use only one QBN to quantize each convolutional layer or the entire CNN, because the design space of searching a QBN for each weight kernel is too large. The hand-crafted heuristic of the kernel-wise QBN search is so sophisticated that domain experts can obtain only sub-optimal results. It is difficult for even deep reinforcement learning (DRL) Deep Deterministic Policy Gradient (DDPG)-based agents to find a kernel-wise QBN configuration that can achieve reasonable inference accuracy. In this paper, we propose a hierarchical-DRL-based kernel-wise network quantization technique, AutoQ, to automatically search a QBN for each weight kernel, and choose another QBN for each activation layer. Compared to the models quantized by the state-of-the-art DRL-based schemes, on average, the same models quantized by AutoQ reduce the inference latency by 54.06\\%, and decrease the inference energy consumption by 50.69\\%, while achieving the same inference accuracy.\n\n9. Title: Quantization and training of neural networks for efficient integer-arithmetic-only inference\n   Abstract: Quantization and Training of Neural Networks for Efficient Integer-Arithmetic-Only Inference: The rising popularity of intelligent mobile devices and the daunting computational cost of deep learning-based models call for efficient and accurate on-device inference schemes. We propose a quantization scheme that allows inference to be carried out using integer-only arithmetic, which can be implemented more efficiently than floating point inference on commonly available integer-only hardware. We also co-design a training procedure to preserve end-to-end model accuracy post quantization. As a result, the proposed quantization scheme improves the tradeoff between accuracy and on-device latency. The improvements are significant even on MobileNets, a model family known for run-time efficiency, and are demonstrated in ImageNet classification and COCO detection on popular CPUs.\n\n10. Title: Designing neural network architectures using reinforcement learning\n   Abstract: Designing Neural Network Architectures using Reinforcement Learning: At present, designing convolutional neural network (CNN) architectures requires both human expertise and labor. New architectures are handcrafted by careful experimentation or modified from a handful of existing networks. We introduce MetaQNN, a meta-modeling algorithm based on reinforcement learning to automatically generate high-performing CNN architectures for a given learning task. The learning agent is trained to sequentially choose CNN layers using $Q$-learning with an $\\epsilon$-greedy exploration strategy and experience replay. The agent explores a large but finite space of possible architectures and iteratively discovers designs with improved performance on the learning task. On image classification benchmarks, the agent-designed networks (consisting of only standard convolution, pooling, and fully-connected layers) beat existing networks designed with the same layer types and are competitive against the state-of-the-art methods that use more complex layer types. We also outperform existing meta-modeling approaches for network design on image classification tasks.\n\nThink step-by-step about which candidate best matches the context. Summarize your reasoning, then provide the exact title that should fill the citation. Respond in JSON with fields:\n{\n  \"reasoning\": \"<your step-by-step analysis>\",\n  \"selected_title\": \"<exact candidate title>\"\n}", "response": {"reasoning": "The context says prior works use the same QBN (quantization bit number) for all layers, causing large accuracy drops at low bits, and suggests mixed-precision as a solution. I checked candidates for a paper that (a) discusses uniform/single-QBN quantization as a limitation and (b) proposes per-kernel or mixed-precision quantization. Candidates 6 and 7 (HAWQ/HAWQ-V2) focus on mixed-precision using Hessian-based sensitivity, but they frame the problem in terms of selecting layer-wise precision via second-order information rather than explicitly criticizing prior works for using a single QBN. Candidate 8 (AutoQ) explicitly states that prior works use only one QBN per convolutional layer or the entire CNN and presents automated kernel-wise (and activation) QBN selection to address accuracy/efficiency trade-offs. That matches the cited claim about prior uniform-QBN approaches causing accuracy loss and the proposed mixed-precision solution. Therefore the best match is AutoQ.", "selected_title": "Autoq: Automated kernel-wise neural network quantization"}, "selected_title": "Autoq: Automated kernel-wise neural network quantization", "positive_title": "Quantization and training of neural networks for efficient integer-arithmetic-only inference", "is_correct": false, "reasoning": "The context says prior works use the same QBN (quantization bit number) for all layers, causing large accuracy drops at low bits, and suggests mixed-precision as a solution. I checked candidates for a paper that (a) discusses uniform/single-QBN quantization as a limitation and (b) proposes per-kernel or mixed-precision quantization. Candidates 6 and 7 (HAWQ/HAWQ-V2) focus on mixed-precision using Hessian-based sensitivity, but they frame the problem in terms of selecting layer-wise precision via second-order information rather than explicitly criticizing prior works for using a single QBN. Candidate 8 (AutoQ) explicitly states that prior works use only one QBN per convolutional layer or the entire CNN and presents automated kernel-wise (and activation) QBN selection to address accuracy/efficiency trade-offs. That matches the cited claim about prior uniform-QBN approaches causing accuracy loss and the proposed mixed-precision solution. Therefore the best match is AutoQ."}
{"example_id": "blind_7759", "prompt": "You are an expert citation retrieval system as described in the paper \"Multi-Agent System for Reliable Citation Retrieval\".\nYour goal is to autonomously retrieve, verify, and recommend academic references given a query or document excerpt.\n\nTask:\nGiven a citation context from a scientific paper (where a citation is missing), identify the correct paper from a list of candidates.\nAnalyze the context to understand the specific claim, method, or result being cited.\nThen, evaluate each candidate paper to see if it matches the context.\nFinally, select the best matching paper.\n\nCitation Context:\nea} \\end{center} \\end{figure} RGB and depth data have been used for semantic and instance segmentation for some time, but there are only few works on panoptic segmentation exploiting both modalities.[CITATION] still apply RGB images for panoptic segmentation, using depth only to estimate camera poses and to generate a 3D map. process depth and colour in two separate branches of a deep neural netw\n\nCandidate Papers:\n1. Title: Cmt-deeplab: Clustering mask transformers for panoptic segmentation\n   Abstract: CMT-DeepLab: Clustering Mask Transformers for Panoptic Segmentation: We propose Clustering Mask Transformer (CMT-DeepLab), a transformer-based framework for panoptic segmentation designed around clustering. It rethinks the existing transformer architectures used in segmentation and detection; CMT-DeepLab considers the object queries as cluster centers, which fill the role of grouping the pixels when applied to segmentation. The clustering is computed with an alternating procedure, by first assigning pixels to the clusters by their feature affinity, and then updating the cluster centers and pixel features. Together, these operations comprise the Clustering Mask Transformer (CMT) layer, which produces cross-attention that is denser and more consistent with the final segmentation task. CMT-DeepLab improves the performance over prior art significantly by 4.4% PQ, achieving a new state-of-the-art of 55.7% PQ on the COCO test-dev set.\n\n2. Title: Panoptic feature pyramid networks\n   Abstract: Panoptic Feature Pyramid Networks: The recently introduced panoptic segmentation task has renewed our community's interest in unifying the tasks of instance segmentation (for thing classes) and semantic segmentation (for stuff classes). However, current state-of-the-art methods for this joint task use separate and dissimilar networks for instance and semantic segmentation, without performing any shared computation. In this work, we aim to unify these methods at the architectural level, designing a single network for both tasks. Our approach is to endow Mask R-CNN, a popular instance segmentation method, with a semantic segmentation branch using a shared Feature Pyramid Network (FPN) backbone. Surprisingly, this simple baseline not only remains effective for instance segmentation, but also yields a lightweight, top-performing method for semantic segmentation. In this work, we perform a detailed study of this minimally extended version of Mask R-CNN with FPN, which we refer to as Panoptic FPN, and show it is a robust and accurate baseline for both tasks. Given its effectiveness and conceptual simplicity, we hope our method can serve as a strong baseline and aid future research in panoptic segmentation.\n\n3. Title: Attention-guided unified network for panoptic segmentation\n   Abstract: Attention-guided Unified Network for Panoptic Segmentation: This paper studies panoptic segmentation, a recently proposed task which segments foreground (FG) objects at the instance level as well as background (BG) contents at the semantic level. Existing methods mostly dealt with these two problems separately, but in this paper, we reveal the underlying relationship between them, in particular, FG objects provide complementary cues to assist BG understanding. Our approach, named the Attention-guided Unified Network (AUNet), is a unified framework with two branches for FG and BG segmentation simultaneously. Two sources of attentions are added to the BG branch, namely, RPN and FG segmentation mask to provide object-level and pixel-level attentions, respectively. Our approach is generalized to different backbones with consistent accuracy gain in both FG and BG segmentation, and also sets new state-of-the-arts both in the MS-COCO (46.5% PQ) and Cityscapes (59.0% PQ) benchmarks.\n\n4. Title: Efficient multi-task rgb-d scene analysis for indoor environments\n   Abstract: Efficient Multi-Task RGB-D Scene Analysis for Indoor Environments: Semantic scene understanding is essential for mobile agents acting in various environments. Although semantic segmentation already provides a lot of information, details about individual objects as well as the general scene are missing but required for many real-world applications. However, solving multiple tasks separately is expensive and cannot be accomplished in real time given limited computing and battery capabilities on a mobile platform. In this paper, we propose an efficient multi-task approach for RGB-D scene analysis~(EMSANet) that simultaneously performs semantic and instance segmentation~(panoptic segmentation), instance orientation estimation, and scene classification. We show that all tasks can be accomplished using a single neural network in real time on a mobile platform without diminishing performance - by contrast, the individual tasks are able to benefit from each other. In order to evaluate our multi-task approach, we extend the annotations of the common RGB-D indoor datasets NYUv2 and SUNRGB-D for instance segmentation and orientation estimation. To the best of our knowledge, we are the first to provide results in such a comprehensive multi-task setting for indoor scene analysis on NYUv2 and SUNRGB-D.\n\n5. Title: Panopticfusion: Online volumetric semantic mapping at the level of stuff and things\n   Abstract: PanopticFusion: Online Volumetric Semantic Mapping at the Level of Stuff and Things: We propose PanopticFusion, a novel online volumetric semantic mapping system at the level of stuff and things. In contrast to previous semantic mapping systems, PanopticFusion is able to densely predict class labels of a background region (stuff) and individually segment arbitrary foreground objects (things). In addition, our system has the capability to reconstruct a large-scale scene and extract a labeled mesh thanks to its use of a spatially hashed volumetric map representation. Our system first predicts pixel-wise panoptic labels (class labels for stuff regions and instance IDs for thing regions) for incoming RGB frames by fusing 2D semantic and instance segmentation outputs. The predicted panoptic labels are integrated into the volumetric map together with depth measurements while keeping the consistency of the instance IDs, which could vary frame to frame, by referring to the 3D map at that moment. In addition, we construct a fully connected conditional random field (CRF) model with respect to panoptic labels for map regularization. For online CRF inference, we propose a novel unary potential approximation and a map division strategy.   We evaluated the performance of our system on the ScanNet (v2) dataset. PanopticFusion outperformed or compared with state-of-the-art offline 3D DNN methods in both semantic and instance segmentation benchmarks. Also, we demonstrate a promising augmented reality application using a 3D panoptic map generated by the proposed system.\n\n6. Title: Scenegraphfusion: Incremental 3d scene graph prediction from rgb-d sequences\n   Abstract: SceneGraphFusion: Incremental 3D Scene Graph Prediction from RGB-D Sequences: Scene graphs are a compact and explicit representation successfully used in a variety of 2D scene understanding tasks. This work proposes a method to incrementally build up semantic scene graphs from a 3D environment given a sequence of RGB-D frames. To this end, we aggregate PointNet features from primitive scene components by means of a graph neural network. We also propose a novel attention mechanism well suited for partial and missing graph data present in such an incremental reconstruction scenario. Although our proposed method is designed to run on submaps of the scene, we show it also transfers to entire 3D scenes. Experiments show that our approach outperforms 3D scene graph prediction methods by a large margin and its accuracy is on par with other 3D semantic and panoptic segmentation methods while running at 35 Hz.\n\n7. Title: Fully convolutional networks for panoptic segmentation\n   Abstract: Fully Convolutional Networks for Panoptic Segmentation: In this paper, we present a conceptually simple, strong, and efficient framework for panoptic segmentation, called Panoptic FCN. Our approach aims to represent and predict foreground things and background stuff in a unified fully convolutional pipeline. In particular, Panoptic FCN encodes each object instance or stuff category into a specific kernel weight with the proposed kernel generator and produces the prediction by convolving the high-resolution feature directly. With this approach, instance-aware and semantically consistent properties for things and stuff can be respectively satisfied in a simple generate-kernel-then-segment workflow. Without extra boxes for localization or instance separation, the proposed approach outperforms previous box-based and -free models with high efficiency on COCO, Cityscapes, and Mapillary Vistas datasets with single scale input. Our code is made publicly available at https://github.com/Jia-Research-Lab/PanopticFCN.\n\n8. Title: Intra-batch supervision for panoptic segmentation on high-resolution images\n   Abstract: Intra-Batch Supervision for Panoptic Segmentation on High-Resolution Images: Unified panoptic segmentation methods are achieving state-of-the-art results on several datasets. To achieve these results on high-resolution datasets, these methods apply crop-based training. In this work, we find that, although crop-based training is advantageous in general, it also has a harmful side-effect. Specifically, it limits the ability of unified networks to discriminate between large object instances, causing them to make predictions that are confused between multiple instances. To solve this, we propose Intra-Batch Supervision (IBS), which improves a network's ability to discriminate between instances by introducing additional supervision using multiple images from the same batch. We show that, with our IBS, we successfully address the confusion problem and consistently improve the performance of unified networks. For the high-resolution Cityscapes and Mapillary Vistas datasets, we achieve improvements of up to +2.5 on the Panoptic Quality for thing classes, and even more considerable gains of up to +5.8 on both the pixel accuracy and pixel precision, which we identify as better metrics to capture the confusion problem.\n\nThink step-by-step about which candidate best matches the context. Summarize your reasoning, then provide the exact title that should fill the citation. Respond in JSON with fields:\n{\n  \"reasoning\": \"<your step-by-step analysis>\",\n  \"selected_title\": \"<exact candidate title>\"\n}", "response": {"reasoning": "The context states that RGB and depth have been used for segmentation but few works exploit both for panoptic segmentation, and that the cited work still applies RGB images for panoptic segmentation while using depth only to estimate camera poses and generate a 3D map. I evaluated candidates for that specific behavior: \n- Candidates 1,2,3,7,8 are general panoptic segmentation methods on RGB images (no emphasis on depth or mapping).\n- Candidate 4 (Efficient multi-task RGB-D scene analysis) explicitly targets RGB-D multi-task panoptic segmentation, but its focus is on a unified multi-task network operating on RGB-D for indoor scenes, not on using depth only for mapping.\n- Candidate 6 (SceneGraphFusion) uses RGB-D for incremental 3D scene graphs and mentions 3D/panoptic methods, but its emphasis is on scene graph prediction rather than using RGB for panoptic segmentation and depth for mapping.\n- Candidate 5 (PanopticFusion) describes predicting pixel-wise panoptic labels from incoming RGB frames and integrating those labels into a volumetric map together with depth measurements; it explicitly uses depth for map reconstruction/integration while performing panoptic segmentation from RGB frames. This matches the context sentence that the cited work still applies RGB images for panoptic segmentation and uses depth only to estimate camera poses and generate a 3D map. \nTherefore the best match is PanopticFusion.", "selected_title": "Panopticfusion: Online volumetric semantic mapping at the level of stuff and things"}, "selected_title": "Panopticfusion: Online volumetric semantic mapping at the level of stuff and things", "positive_title": "Panopticfusion: Online volumetric semantic mapping at the level of stuff and things", "is_correct": true, "reasoning": "The context states that RGB and depth have been used for segmentation but few works exploit both for panoptic segmentation, and that the cited work still applies RGB images for panoptic segmentation while using depth only to estimate camera poses and generate a 3D map. I evaluated candidates for that specific behavior: \n- Candidates 1,2,3,7,8 are general panoptic segmentation methods on RGB images (no emphasis on depth or mapping).\n- Candidate 4 (Efficient multi-task RGB-D scene analysis) explicitly targets RGB-D multi-task panoptic segmentation, but its focus is on a unified multi-task network operating on RGB-D for indoor scenes, not on using depth only for mapping.\n- Candidate 6 (SceneGraphFusion) uses RGB-D for incremental 3D scene graphs and mentions 3D/panoptic methods, but its emphasis is on scene graph prediction rather than using RGB for panoptic segmentation and depth for mapping.\n- Candidate 5 (PanopticFusion) describes predicting pixel-wise panoptic labels from incoming RGB frames and integrating those labels into a volumetric map together with depth measurements; it explicitly uses depth for map reconstruction/integration while performing panoptic segmentation from RGB frames. This matches the context sentence that the cited work still applies RGB images for panoptic segmentation and uses depth only to estimate camera poses and generate a 3D map. \nTherefore the best match is PanopticFusion."}
{"example_id": "blind_12146", "prompt": "You are an expert citation retrieval system as described in the paper \"Multi-Agent System for Reliable Citation Retrieval\".\nYour goal is to autonomously retrieve, verify, and recommend academic references given a query or document excerpt.\n\nTask:\nGiven a citation context from a scientific paper (where a citation is missing), identify the correct paper from a list of candidates.\nAnalyze the context to understand the specific claim, method, or result being cited.\nThen, evaluate each candidate paper to see if it matches the context.\nFinally, select the best matching paper.\n\nCitation Context:\nnt using transformers. Empirical research on both formal languages and synthetic functions has uncovered an intriguing array of learning biases, but theoretical understanding is lacking. For instance,[CITATION] experimentally argued that heldout generalization is biased towards low-degree polynomials and provided empirical evidence that transformers prefer to represent functions of \\emph{low sens\n\nCandidate Papers:\n1. Title: Generalization on the Unseen, Logic Reasoning and Degree Curriculum\n   Abstract: Generalization on the Unseen, Logic Reasoning and Degree Curriculum: This paper considers the learning of logical (Boolean) functions with focus on the generalization on the unseen (GOTU) setting, a strong case of out-of-distribution generalization. This is motivated by the fact that the rich combinatorial nature of data in certain reasoning tasks (e.g., arithmetic/logic) makes representative data sampling challenging, and learning successfully under GOTU gives a first vignette of an 'extrapolating' or 'reasoning' learner. We then study how different network architectures trained by (S)GD perform under GOTU and provide both theoretical and experimental evidence that for a class of network models including instances of Transformers, random features models, and diagonal linear networks, a min-degree-interpolator is learned on the unseen. We also provide evidence that other instances with larger learning rates or mean-field networks reach leaky min-degree solutions. These findings lead to two implications: (1) we provide an explanation to the length generalization problem (e.g., Anil et al. 2022); (2) we introduce a curriculum learning algorithm called Degree-Curriculum that learns monomials more efficiently by incrementing supports.\n\n2. Title: Theoretical Limitations of Self-Attention in Neural Sequence Models\n   Abstract: Theoretical Limitations of Self-Attention in Neural Sequence Models: Transformers are emerging as the new workhorse of NLP, showing great success across tasks. Unlike LSTMs, transformers process input sequences entirely through self-attention. Previous work has suggested that the computational capabilities of self-attention to process hierarchical structures are limited. In this work, we mathematically investigate the computational power of self-attention to model formal languages. Across both soft and hard attention, we show strong theoretical limitations of the computational abilities of self-attention, finding that it cannot model periodic finite-state languages, nor hierarchical structure, unless the number of layers or heads increases with input length. These limitations seem surprising given the practical success of self-attention and the prominent role assigned to hierarchical structure in linguistics, suggesting that natural language can be approximated well with models that are too weak for the formal languages typically assumed in theoretical linguistics.\n\n3. Title: Formal language recognition by hard attention transformers: Perspectives from circuit complexity\n   Abstract: Formal Language Recognition by Hard Attention Transformers: Perspectives from Circuit Complexity: This paper analyzes three formal models of Transformer encoders that differ in the form of their self-attention mechanism: unique hard attention (UHAT); generalized unique hard attention (GUHAT), which generalizes UHAT; and averaging hard attention (AHAT). We show that UHAT and GUHAT Transformers, viewed as string acceptors, can only recognize formal languages in the complexity class AC$^0$, the class of languages recognizable by families of Boolean circuits of constant depth and polynomial size. This upper bound subsumes Hahn's (2020) results that GUHAT cannot recognize the DYCK languages or the PARITY language, since those languages are outside AC$^0$ (Furst et al., 1984). In contrast, the non-AC$^0$ languages MAJORITY and DYCK-1 are recognizable by AHAT networks, implying that AHAT can recognize languages that UHAT and GUHAT cannot.\n\n4. Title: Average-Hard Attention Transformers are Constant-Depth Uniform Threshold\nCircuits\n   Abstract: Average-Hard Attention Transformers are Constant-Depth Uniform Threshold Circuits: Transformers have emerged as a widely used neural network model for various natural language processing tasks. Previous research explored their relationship with constant-depth threshold circuits, making two assumptions: average-hard attention and logarithmic precision for internal computations relative to input length. Merrill et al. (2022) prove that average-hard attention transformers recognize languages that fall within the complexity class TC0, denoting the set of languages that can be recognized by constant-depth polynomial-size threshold circuits. Likewise, Merrill and Sabharwal (2023) show that log-precision transformers recognize languages within the class of uniform TC0. This shows that both transformer models can be simulated by constant-depth threshold circuits, with the latter being more robust due to generating a uniform circuit family. Our paper shows that the first result can be extended to yield uniform circuits as well.\n\n5. Title: Transformers as Algorithms: Generalization and Stability in In-context Learning\n   Abstract: Transformers as Algorithms: Generalization and Stability in In-context Learning: In-context learning (ICL) is a type of prompting where a transformer model operates on a sequence of (input, output) examples and performs inference on-the-fly. In this work, we formalize in-context learning as an algorithm learning problem where a transformer model implicitly constructs a hypothesis function at inference-time. We first explore the statistical aspects of this abstraction through the lens of multitask learning: We obtain generalization bounds for ICL when the input prompt is (1) a sequence of i.i.d. (input, label) pairs or (2) a trajectory arising from a dynamical system. The crux of our analysis is relating the excess risk to the stability of the algorithm implemented by the transformer. We characterize when transformer/attention architecture provably obeys the stability condition and also provide empirical verification. For generalization on unseen tasks, we identify an inductive bias phenomenon in which the transfer learning risk is governed by the task complexity and the number of MTL tasks in a highly predictable manner. Finally, we provide numerical evaluations that (1) demonstrate transformers can indeed implement near-optimal algorithms on classical regression problems with i.i.d. and dynamic data, (2) provide insights on stability, and (3) verify our theoretical predictions.\n\n6. Title: Overcoming a Theoretical Limitation of Self-Attention\n   Abstract: Overcoming a Theoretical Limitation of Self-Attention: Although transformers are remarkably effective for many tasks, there are some surprisingly easy-looking regular languages that they struggle with. Hahn shows that for languages where acceptance depends on a single input symbol, a transformer's classification decisions become less and less confident (that is, with cross-entropy approaching 1 bit per string) as input strings get longer and longer. We examine this limitation using two languages: PARITY, the language of bit strings with an odd number of 1s, and FIRST, the language of bit strings starting with a 1. We demonstrate three ways of overcoming the limitation suggested by Hahn's lemma. First, we settle an open question by constructing a transformer that recognizes PARITY with perfect accuracy, and similarly for FIRST. Second, we use layer normalization to bring the cross-entropy of both models arbitrarily close to zero. Third, when transformers need to focus on a single position, as for FIRST, we find that they can fail to generalize to longer strings; we offer a simple remedy to this problem that also improves length generalization in machine translation.\n\n7. Title: Thinking like transformers\n   Abstract: Thinking Like Transformers: What is the computational model behind a Transformer? Where recurrent neural networks have direct parallels in finite state machines, allowing clear discussion and thought around architecture variants or trained models, Transformers have no such familiar parallel. In this paper we aim to change that, proposing a computational model for the transformer-encoder in the form of a programming language. We map the basic components of a transformer-encoder -- attention and feed-forward computation -- into simple primitives, around which we form a programming language: the Restricted Access Sequence Processing Language (RASP). We show how RASP can be used to program solutions to tasks that could conceivably be learned by a Transformer, and how a Transformer can be trained to mimic a RASP solution. In particular, we provide RASP programs for histograms, sorting, and Dyck-languages. We further use our model to relate their difficulty in terms of the number of required layers and attention heads: analyzing a RASP program implies a maximum number of heads and layers necessary to encode a task in a transformer. Finally, we see how insights gained from our abstraction might be used to explain phenomena seen in recent works.\n\n8. Title: Inductive biases and variable creation in self-attention mechanisms\n   Abstract: Inductive Biases and Variable Creation in Self-Attention Mechanisms: Self-attention, an architectural motif designed to model long-range interactions in sequential data, has driven numerous recent breakthroughs in natural language processing and beyond. This work provides a theoretical analysis of the inductive biases of self-attention modules. Our focus is to rigorously establish which functions and long-range dependencies self-attention blocks prefer to represent. Our main result shows that bounded-norm Transformer networks \"create sparse variables\": a single self-attention head can represent a sparse function of the input sequence, with sample complexity scaling only logarithmically with the context length. To support our analysis, we present synthetic experiments to probe the sample complexity of learning sparse Boolean functions with Transformers.\n\nThink step-by-step about which candidate best matches the context. Summarize your reasoning, then provide the exact title that should fill the citation. Respond in JSON with fields:\n{\n  \"reasoning\": \"<your step-by-step analysis>\",\n  \"selected_title\": \"<exact candidate title>\"\n}", "response": {"reasoning": "1) Parse the citation context: the missing citation is invoked to support the claim that heldout generalization is biased toward low-degree polynomials and that transformers empirically prefer to represent functions of low degree/sensitivity. The context links empirical work on formal languages and synthetic functions and laments lack of theoretical understanding. 2) Extract key phrases to match: â€œheldout generalizationâ€, â€œbiased towards low-degree polynomialsâ€, â€œtransformers prefer to represent functions of low degree / low sensitivityâ€, and connections to formal/Boolean functions and degree-based inductive bias. 3) Compare candidates: \n   - Candidate 1 explicitly studies learning of logical (Boolean) functions under GOTU (heldout generalization), provides theoretical and experimental evidence that Transformers and related models learn a min-degree interpolator (i.e., bias toward low-degree polynomials/monomials), and introduces a Degree-Curriculum â€” this directly matches the claim. \n   - Candidate 8 discusses inductive biases and sparse variable creation but emphasizes sparsity/sample complexity rather than low-degree polynomial bias. \n   - Candidates 2â€“7 focus on formal language recognition, computational limits of self-attention, or in-context learning stability; none explicitly claim empirical evidence that transformers prefer low-degree polynomial representations. 4) Conclusion: Candidate 1 most precisely matches the cited empirical/theoretical claim about heldout generalization bias toward low-degree polynomials and transformersâ€™ preference for low-degree functions.", "selected_title": "Generalization on the Unseen, Logic Reasoning and Degree Curriculum"}, "selected_title": "Generalization on the Unseen, Logic Reasoning and Degree Curriculum", "positive_title": "Generalization on the Unseen, Logic Reasoning and Degree Curriculum", "is_correct": true, "reasoning": "1) Parse the citation context: the missing citation is invoked to support the claim that heldout generalization is biased toward low-degree polynomials and that transformers empirically prefer to represent functions of low degree/sensitivity. The context links empirical work on formal languages and synthetic functions and laments lack of theoretical understanding. 2) Extract key phrases to match: â€œheldout generalizationâ€, â€œbiased towards low-degree polynomialsâ€, â€œtransformers prefer to represent functions of low degree / low sensitivityâ€, and connections to formal/Boolean functions and degree-based inductive bias. 3) Compare candidates: \n   - Candidate 1 explicitly studies learning of logical (Boolean) functions under GOTU (heldout generalization), provides theoretical and experimental evidence that Transformers and related models learn a min-degree interpolator (i.e., bias toward low-degree polynomials/monomials), and introduces a Degree-Curriculum â€” this directly matches the claim. \n   - Candidate 8 discusses inductive biases and sparse variable creation but emphasizes sparsity/sample complexity rather than low-degree polynomial bias. \n   - Candidates 2â€“7 focus on formal language recognition, computational limits of self-attention, or in-context learning stability; none explicitly claim empirical evidence that transformers prefer low-degree polynomial representations. 4) Conclusion: Candidate 1 most precisely matches the cited empirical/theoretical claim about heldout generalization bias toward low-degree polynomials and transformersâ€™ preference for low-degree functions."}
{"example_id": "blind_17016", "prompt": "You are an expert citation retrieval system as described in the paper \"Multi-Agent System for Reliable Citation Retrieval\".\nYour goal is to autonomously retrieve, verify, and recommend academic references given a query or document excerpt.\n\nTask:\nGiven a citation context from a scientific paper (where a citation is missing), identify the correct paper from a list of candidates.\nAnalyze the context to understand the specific claim, method, or result being cited.\nThen, evaluate each candidate paper to see if it matches the context.\nFinally, select the best matching paper.\n\nCitation Context:\nodels, the Pretraining-Finetuning paradigm takes a more dominant role compared to training from scratch across various tasks, including computer vision, natural language processing[CITATION], and speech recognition. Under the paradigm of Pretraining-Finetuing, advanced parameter-efficient finetuning~(PEFT) methods have emerged. Compared to full finetuning, PEFT method\n\nCandidate Papers:\n1. Title: An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale\n   Abstract: An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale: While the Transformer architecture has become the de-facto standard for natural language processing tasks, its applications to computer vision remain limited. In vision, attention is either applied in conjunction with convolutional networks, or used to replace certain components of convolutional networks while keeping their overall structure in place. We show that this reliance on CNNs is not necessary and a pure transformer applied directly to sequences of image patches can perform very well on image classification tasks. When pre-trained on large amounts of data and transferred to multiple mid-sized or small image recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision Transformer (ViT) attains excellent results compared to state-of-the-art convolutional networks while requiring substantially fewer computational resources to train.\n\n2. Title: Fine-Tuning can Distort Pretrained Features and Underperform Out-of-Distribution\n   Abstract: Fine-Tuning can Distort Pretrained Features and Underperform Out-of-Distribution: When transferring a pretrained model to a downstream task, two popular methods are full fine-tuning (updating all the model parameters) and linear probing (updating only the last linear layer -- the \"head\"). It is well known that fine-tuning leads to better accuracy in-distribution (ID). However, in this paper, we find that fine-tuning can achieve worse accuracy than linear probing out-of-distribution (OOD) when the pretrained features are good and the distribution shift is large. On 10 distribution shift datasets (Breeds-Living17, Breeds-Entity30, DomainNet, CIFAR $\\to$ STL, CIFAR10.1, FMoW, ImageNetV2, ImageNet-R, ImageNet-A, ImageNet-Sketch), fine-tuning obtains on average 2% higher accuracy ID but 7% lower accuracy OOD than linear probing. We show theoretically that this tradeoff between ID and OOD accuracy arises even in a simple setting: fine-tuning overparameterized two-layer linear networks. We prove that the OOD error of fine-tuning is high when we initialize with a fixed or random head -- this is because while fine-tuning learns the head, the lower layers of the neural network change simultaneously and distort the pretrained features. Our analysis suggests that the easy two-step strategy of linear probing then full fine-tuning (LP-FT), sometimes used as a fine-tuning heuristic, combines the benefits of both fine-tuning and linear probing. Empirically, LP-FT outperforms both fine-tuning and linear probing on the above datasets (1% better ID, 10% better OOD than full fine-tuning).\n\n3. Title: TWINS: A Fine-Tuning Framework for Improved Transferability of Adversarial Robustness and Generalization\n   Abstract: TWINS: A Fine-Tuning Framework for Improved Transferability of Adversarial Robustness and Generalization: Recent years have seen the ever-increasing importance of pre-trained models and their downstream training in deep learning research and applications. At the same time, the defense for adversarial examples has been mainly investigated in the context of training from random initialization on simple classification tasks. To better exploit the potential of pre-trained models in adversarial robustness, this paper focuses on the fine-tuning of an adversarially pre-trained model in various classification tasks. Existing research has shown that since the robust pre-trained model has already learned a robust feature extractor, the crucial question is how to maintain the robustness in the pre-trained model when learning the downstream task. We study the model-based and data-based approaches for this goal and find that the two common approaches cannot achieve the objective of improving both generalization and adversarial robustness. Thus, we propose a novel statistics-based approach, Two-WIng NormliSation (TWINS) fine-tuning framework, which consists of two neural networks where one of them keeps the population means and variances of pre-training data in the batch normalization layers. Besides the robust information transfer, TWINS increases the effective learning rate without hurting the training stability since the relationship between a weight norm and its gradient norm in standard batch normalization layer is broken, resulting in a faster escape from the sub-optimal initialization and alleviating the robust overfitting. Finally, TWINS is shown to be effective on a wide range of image classification datasets in terms of both generalization and robustness. Our code is available at https://github.com/ziquanliu/CVPR2023-TWINS.\n\n4. Title: Adversarial robustness: From self-supervised pre-training to fine-tuning\n   Abstract: Adversarial Robustness: From Self-Supervised Pre-Training to Fine-Tuning: Pretrained models from self-supervision are prevalently used in fine-tuning downstream tasks faster or for better accuracy. However, gaining robustness from pretraining is left unexplored. We introduce adversarial training into self-supervision, to provide general-purpose robust pre-trained models for the first time. We find these robust pre-trained models can benefit the subsequent fine-tuning in two ways: i) boosting final model robustness; ii) saving the computation cost, if proceeding towards adversarial fine-tuning. We conduct extensive experiments to demonstrate that the proposed framework achieves large performance margins (eg, 3.83% on robust accuracy and 1.3% on standard accuracy, on the CIFAR-10 dataset), compared with the conventional end-to-end adversarial training baseline. Moreover, we find that different self-supervised pre-trained models have a diverse adversarial vulnerability. It inspires us to ensemble several pretraining tasks, which boosts robustness more. Our ensemble strategy contributes to a further improvement of 3.59% on robust accuracy, while maintaining a slightly higher standard accuracy on CIFAR-10. Our codes are available at https://github.com/TAMU-VITA/Adv-SS-Pretraining.\n\n5. Title: Parameter-efficient transfer learning for NLP\n   Abstract: Parameter-Efficient Transfer Learning for NLP: Fine-tuning large pre-trained models is an effective transfer mechanism in NLP. However, in the presence of many downstream tasks, fine-tuning is parameter inefficient: an entire new model is required for every task. As an alternative, we propose transfer with adapter modules. Adapter modules yield a compact and extensible model; they add only a few trainable parameters per task, and new tasks can be added without revisiting previous ones. The parameters of the original network remain fixed, yielding a high degree of parameter sharing. To demonstrate adapter's effectiveness, we transfer the recently proposed BERT Transformer model to 26 diverse text classification tasks, including the GLUE benchmark. Adapters attain near state-of-the-art performance, whilst adding only a few parameters per task. On GLUE, we attain within 0.4% of the performance of full fine-tuning, adding only 3.6% parameters per task. By contrast, fine-tuning trains 100% of the parameters per task.\n\n6. Title: AutoLoRa: A Parameter-Free Automated Robust Fine-Tuning Framework\n   Abstract: AutoLoRa: A Parameter-Free Automated Robust Fine-Tuning Framework: Robust Fine-Tuning (RFT) is a low-cost strategy to obtain adversarial robustness in downstream applications, without requiring a lot of computational resources and collecting significant amounts of data. This paper uncovers an issue with the existing RFT, where optimizing both adversarial and natural objectives through the feature extractor (FE) yields significantly divergent gradient directions. This divergence introduces instability in the optimization process, thereby hindering the attainment of adversarial robustness and rendering RFT highly sensitive to hyperparameters. To mitigate this issue, we propose a low-rank (LoRa) branch that disentangles RFT into two distinct components: optimizing natural objectives via the LoRa branch and adversarial objectives via the FE. Besides, we introduce heuristic strategies for automating the scheduling of the learning rate and the scalars of loss terms. Extensive empirical evaluations demonstrate that our proposed automated RFT disentangled via the LoRa branch (AutoLoRa) achieves new state-of-the-art results across a range of downstream tasks. AutoLoRa holds significant practical utility, as it automatically converts a pre-trained FE into an adversarially robust model for downstream tasks without the need for searching hyperparameters.\n\n7. Title: Robust pre-training by adversarial contrastive learning\n   Abstract: Robust Pre-Training by Adversarial Contrastive Learning: Recent work has shown that, when integrated with adversarial training, self-supervised pre-training can lead to state-of-the-art robustness In this work, we improve robustness-aware self-supervised pre-training by learning representations that are consistent under both data augmentations and adversarial perturbations. Our approach leverages a recent contrastive learning framework, which learns representations by maximizing feature consistency under differently augmented views. This fits particularly well with the goal of adversarial robustness, as one cause of adversarial fragility is the lack of feature invariance, i.e., small input perturbations can result in undesirable large changes in features or even predicted labels. We explore various options to formulate the contrastive task, and demonstrate that by injecting adversarial perturbations, contrastive pre-training can lead to models that are both label-efficient and robust. We empirically evaluate the proposed Adversarial Contrastive Learning (ACL) and show it can consistently outperform existing methods. For example on the CIFAR-10 dataset, ACL outperforms the previous state-of-the-art unsupervised robust pre-training approach by 2.99% on robust accuracy and 2.14% on standard accuracy. We further demonstrate that ACL pre-training can improve semi-supervised adversarial training, even when only a few labeled examples are available. Our codes and pre-trained models have been released at: https://github.com/VITA-Group/Adversarial-Contrastive-Learning.\n\n8. Title: Attention is all you need\n   Abstract: Attention Is All You Need: The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.\n\n9. Title: Visual prompt tuning\n   Abstract: Visual Prompt Tuning: The current modus operandi in adapting pre-trained models involves updating all the backbone parameters, ie, full fine-tuning. This paper introduces Visual Prompt Tuning (VPT) as an efficient and effective alternative to full fine-tuning for large-scale Transformer models in vision. Taking inspiration from recent advances in efficiently tuning large language models, VPT introduces only a small amount (less than 1% of model parameters) of trainable parameters in the input space while keeping the model backbone frozen. Via extensive experiments on a wide variety of downstream recognition tasks, we show that VPT achieves significant performance gains compared to other parameter efficient tuning protocols. Most importantly, VPT even outperforms full fine-tuning in many cases across model capacities and training data scales, while reducing per-task storage cost.\n\nThink step-by-step about which candidate best matches the context. Summarize your reasoning, then provide the exact title that should fill the citation. Respond in JSON with fields:\n{\n  \"reasoning\": \"<your step-by-step analysis>\",\n  \"selected_title\": \"<exact candidate title>\"\n}", "response": {"reasoning": "The citation occurs directly after â€œnatural language processing,â€ and the subsequent sentence immediately introduces parameter-efficient finetuning (PEFT) methods. I therefore looked for a candidate that (a) specifically discusses pretrainingâ†’finetuning in NLP and (b) introduces or motivates parameter-efficient transfer methods. Candidates considered: (1) ViT (vision pretrainingâ†’transfer) â€” relevant to CV but not specifically NLP; (2) Fine-Tuning can Distort... â€” about fine-tuning vs linear probing and OOD, not a general NLP pretraining reference; (3,4,6,7) focus on adversarial robustness or robust pretraining â€” not a general NLP/PEFT citation; (8) Attention Is All You Need â€” foundational for Transformers in NLP but not focused on the pretrainingâ€“finetuning transfer paradigm or PEFT; (9) Visual Prompt Tuning â€” a PEFT method in vision, not an NLP citation. Candidate (5), â€œParameter-Efficient Transfer Learning for NLP,â€ explicitly frames fine-tuning large pre-trained models in NLP and introduces adapter modules as a parameter-efficient alternative to full fine-tuning. It directly supports the statement about the pretrainingâ€“finetuning paradigm in NLP and the emergence of PEFT methods. Therefore candidate 5 is the best match for the citation.", "selected_title": "Parameter-efficient transfer learning for NLP"}, "selected_title": "Parameter-efficient transfer learning for NLP", "positive_title": "Attention is all you need", "is_correct": false, "reasoning": "The citation occurs directly after â€œnatural language processing,â€ and the subsequent sentence immediately introduces parameter-efficient finetuning (PEFT) methods. I therefore looked for a candidate that (a) specifically discusses pretrainingâ†’finetuning in NLP and (b) introduces or motivates parameter-efficient transfer methods. Candidates considered: (1) ViT (vision pretrainingâ†’transfer) â€” relevant to CV but not specifically NLP; (2) Fine-Tuning can Distort... â€” about fine-tuning vs linear probing and OOD, not a general NLP pretraining reference; (3,4,6,7) focus on adversarial robustness or robust pretraining â€” not a general NLP/PEFT citation; (8) Attention Is All You Need â€” foundational for Transformers in NLP but not focused on the pretrainingâ€“finetuning transfer paradigm or PEFT; (9) Visual Prompt Tuning â€” a PEFT method in vision, not an NLP citation. Candidate (5), â€œParameter-Efficient Transfer Learning for NLP,â€ explicitly frames fine-tuning large pre-trained models in NLP and introduces adapter modules as a parameter-efficient alternative to full fine-tuning. It directly supports the statement about the pretrainingâ€“finetuning paradigm in NLP and the emergence of PEFT methods. Therefore candidate 5 is the best match for the citation."}
{"example_id": "blind_5213", "prompt": "You are an expert citation retrieval system as described in the paper \"Multi-Agent System for Reliable Citation Retrieval\".\nYour goal is to autonomously retrieve, verify, and recommend academic references given a query or document excerpt.\n\nTask:\nGiven a citation context from a scientific paper (where a citation is missing), identify the correct paper from a list of candidates.\nAnalyze the context to understand the specific claim, method, or result being cited.\nThen, evaluate each candidate paper to see if it matches the context.\nFinally, select the best matching paper.\n\nCitation Context:\nrd in Natural Language Processing (NLP), recent literature commences introducing transformer-like networks into various computer vision tasks, including image recognition, object detection[CITATION], segmentation, visual question answering (VQA). The Vision Transformer (ViT) directly feeds image patches into a transformer for image classification, which removes\n\nCandidate Papers:\n1. Title: One-shot object detection with co-attention and co-excitation\n   Abstract: One-Shot Object Detection with Co-Attention and Co-Excitation: This paper aims to tackle the challenging problem of one-shot object detection. Given a query image patch whose class label is not included in the training data, the goal of the task is to detect all instances of the same class in a target image. To this end, we develop a novel {\\em co-attention and co-excitation} (CoAE) framework that makes contributions in three key technical aspects. First, we propose to use the non-local operation to explore the co-attention embodied in each query-target pair and yield region proposals accounting for the one-shot situation. Second, we formulate a squeeze-and-co-excitation scheme that can adaptively emphasize correlated feature channels to help uncover relevant proposals and eventually the target objects. Third, we design a margin-based ranking loss for implicitly learning a metric to predict the similarity of a region proposal to the underlying query, no matter its class label is seen or unseen in training. The resulting model is therefore a two-stage detector that yields a strong baseline on both VOC and MS-COCO under one-shot setting of detecting objects from both seen and never-seen classes. Codes are available at https://github.com/timy90022/One-Shot-Object-Detection.\n\n2. Title: Lstd: A low-shot transfer detector for object detection\n   Abstract: LSTD: A Low-Shot Transfer Detector for Object Detection: Recent advances in object detection are mainly driven by deep learning with large-scale detection benchmarks. However, the fully-annotated training set is often limited for a target detection task, which may deteriorate the performance of deep detectors. To address this challenge, we propose a novel low-shot transfer detector (LSTD) in this paper, where we leverage rich source-domain knowledge to construct an effective target-domain detector with very few training examples. The main contributions are described as follows. First, we design a flexible deep architecture of LSTD to alleviate transfer difficulties in low-shot detection. This architecture can integrate the advantages of both SSD and Faster RCNN in a unified deep framework. Second, we introduce a novel regularized transfer learning framework for low-shot detection, where the transfer knowledge (TK) and background depression (BD) regularizations are proposed to leverage object knowledge respectively from source and target domains, in order to further enhance fine-tuning with a few target images. Finally, we examine our LSTD on a number of challenging low-shot detection experiments, where LSTD outperforms other state-of-the-art approaches. The results demonstrate that LSTD is a preferable deep detector for low-shot scenarios.\n\n3. Title: Faster R-CNN: Towards real-time object detection with region proposal networks\n   Abstract: Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks: State-of-the-art object detection networks depend on region proposal algorithms to hypothesize object locations. Advances like SPPnet and Fast R-CNN have reduced the running time of these detection networks, exposing region proposal computation as a bottleneck. In this work, we introduce a Region Proposal Network (RPN) that shares full-image convolutional features with the detection network, thus enabling nearly cost-free region proposals. An RPN is a fully convolutional network that simultaneously predicts object bounds and objectness scores at each position. The RPN is trained end-to-end to generate high-quality region proposals, which are used by Fast R-CNN for detection. We further merge RPN and Fast R-CNN into a single network by sharing their convolutional features---using the recently popular terminology of neural networks with 'attention' mechanisms, the RPN component tells the unified network where to look. For the very deep VGG-16 model, our detection system has a frame rate of 5fps (including all steps) on a GPU, while achieving state-of-the-art object detection accuracy on PASCAL VOC 2007, 2012, and MS COCO datasets with only 300 proposals per image. In ILSVRC and COCO 2015 competitions, Faster R-CNN and RPN are the foundations of the 1st-place winning entries in several tracks. Code has been made publicly available.\n\n4. Title: Os2d: One-stage one-shot object detection by matching anchor features\n   Abstract: OS2D: One-Stage One-Shot Object Detection by Matching Anchor Features: In this paper, we consider the task of one-shot object detection, which consists in detecting objects defined by a single demonstration. Differently from the standard object detection, the classes of objects used for training and testing do not overlap. We build the one-stage system that performs localization and recognition jointly. We use dense correlation matching of learned local features to find correspondences, a feed-forward geometric transformation model to align features and bilinear resampling of the correlation tensor to compute the detection score of the aligned features. All the components are differentiable, which allows end-to-end training. Experimental evaluation on several challenging domains (retail products, 3D objects, buildings and logos) shows that our method can detect unseen classes (e.g., toothpaste when trained on groceries) and outperforms several baselines by a significant margin. Our code is available online: https://github.com/aosokin/os2d .\n\n5. Title: End-to-end object detection with transformers\n   Abstract: End-to-End Object Detection with Transformers: We present a new method that views object detection as a direct set prediction problem. Our approach streamlines the detection pipeline, effectively removing the need for many hand-designed components like a non-maximum suppression procedure or anchor generation that explicitly encode our prior knowledge about the task. The main ingredients of the new framework, called DEtection TRansformer or DETR, are a set-based global loss that forces unique predictions via bipartite matching, and a transformer encoder-decoder architecture. Given a fixed small set of learned object queries, DETR reasons about the relations of the objects and the global image context to directly output the final set of predictions in parallel. The new model is conceptually simple and does not require a specialized library, unlike many other modern detectors. DETR demonstrates accuracy and run-time performance on par with the well-established and highly-optimized Faster RCNN baseline on the challenging COCO object detection dataset. Moreover, DETR can be easily generalized to produce panoptic segmentation in a unified manner. We show that it significantly outperforms competitive baselines. Training code and pretrained models are available at https://github.com/facebookresearch/detr.\n\n6. Title: An image is worth 16x16 words: Transformers for image recognition at scale\n   Abstract: An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale: While the Transformer architecture has become the de-facto standard for natural language processing tasks, its applications to computer vision remain limited. In vision, attention is either applied in conjunction with convolutional networks, or used to replace certain components of convolutional networks while keeping their overall structure in place. We show that this reliance on CNNs is not necessary and a pure transformer applied directly to sequences of image patches can perform very well on image classification tasks. When pre-trained on large amounts of data and transferred to multiple mid-sized or small image recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision Transformer (ViT) attains excellent results compared to state-of-the-art convolutional networks while requiring substantially fewer computational resources to train.\n\n7. Title: LXMERT: Learning cross-modality encoder representations from transformers\n   Abstract: LXMERT: Learning Cross-Modality Encoder Representations from Transformers: Vision-and-language reasoning requires an understanding of visual concepts, language semantics, and, most importantly, the alignment and relationships between these two modalities. We thus propose the LXMERT (Learning Cross-Modality Encoder Representations from Transformers) framework to learn these vision-and-language connections. In LXMERT, we build a large-scale Transformer model that consists of three encoders: an object relationship encoder, a language encoder, and a cross-modality encoder. Next, to endow our model with the capability of connecting vision and language semantics, we pre-train the model with large amounts of image-and-sentence pairs, via five diverse representative pre-training tasks: masked language modeling, masked object prediction (feature regression and label classification), cross-modality matching, and image question answering. These tasks help in learning both intra-modality and cross-modality relationships. After fine-tuning from our pre-trained parameters, our model achieves the state-of-the-art results on two visual question answering datasets (i.e., VQA and GQA). We also show the generalizability of our pre-trained cross-modality model by adapting it to a challenging visual-reasoning task, NLVR2, and improve the previous best result by 22% absolute (54% to 76%). Lastly, we demonstrate detailed ablation studies to prove that both our novel model components and pre-training strategies significantly contribute to our strong results; and also present several attention visualizations for the different encoders. Code and pre-trained models publicly available at: https://github.com/airsplay/lxmert\n\n8. Title: Cross-modal self-attention network for referring image segmentation\n   Abstract: Cross-Modal Self-Attention Network for Referring Image Segmentation: We consider the problem of referring image segmentation. Given an input image and a natural language expression, the goal is to segment the object referred by the language expression in the image. Existing works in this area treat the language expression and the input image separately in their representations. They do not sufficiently capture long-range correlations between these two modalities. In this paper, we propose a cross-modal self-attention (CMSA) module that effectively captures the long-range dependencies between linguistic and visual features. Our model can adaptively focus on informative words in the referring expression and important regions in the input image. In addition, we propose a gated multi-level fusion module to selectively integrate self-attentive cross-modal features corresponding to different levels in the image. This module controls the information flow of features at different levels. We validate the proposed approach on four evaluation datasets. Our proposed approach consistently outperforms existing state-of-the-art methods.\n\nThink step-by-step about which candidate best matches the context. Summarize your reasoning, then provide the exact title that should fill the citation. Respond in JSON with fields:\n{\n  \"reasoning\": \"<your step-by-step analysis>\",\n  \"selected_title\": \"<exact candidate title>\"\n}", "response": {"reasoning": "The citation follows a list of CV tasks where transformer-like networks have been introduced and is placed specifically after â€œobject detectionâ€. I therefore need a candidate that introduces or applies transformer architectures to object detection. Candidate 5 (DETR) explicitly presents an end-to-end object detection method based on a transformer encoder-decoder and frames detection as set prediction â€” a direct application of transformers to object detection. Other candidates do not match: #6 (ViT) addresses image classification, #7 (LXMERT) addresses vision-and-language / VQA, #8 focuses on referring segmentation with cross-modal attention, and #3 (Faster R-CNN) is a convolutional region-proposal based detector without transformers. One-shot and low-shot detectors (1,2,4) are not transformer-based general detectors. Thus the best match for the object-detection transformer citation is DETR.", "selected_title": "End-to-End Object Detection with Transformers"}, "selected_title": "End-to-End Object Detection with Transformers", "positive_title": "End-to-end object detection with transformers", "is_correct": true, "reasoning": "The citation follows a list of CV tasks where transformer-like networks have been introduced and is placed specifically after â€œobject detectionâ€. I therefore need a candidate that introduces or applies transformer architectures to object detection. Candidate 5 (DETR) explicitly presents an end-to-end object detection method based on a transformer encoder-decoder and frames detection as set prediction â€” a direct application of transformers to object detection. Other candidates do not match: #6 (ViT) addresses image classification, #7 (LXMERT) addresses vision-and-language / VQA, #8 focuses on referring segmentation with cross-modal attention, and #3 (Faster R-CNN) is a convolutional region-proposal based detector without transformers. One-shot and low-shot detectors (1,2,4) are not transformer-based general detectors. Thus the best match for the object-detection transformer citation is DETR."}
{"example_id": "blind_9272", "prompt": "You are an expert citation retrieval system as described in the paper \"Multi-Agent System for Reliable Citation Retrieval\".\nYour goal is to autonomously retrieve, verify, and recommend academic references given a query or document excerpt.\n\nTask:\nGiven a citation context from a scientific paper (where a citation is missing), identify the correct paper from a list of candidates.\nAnalyze the context to understand the specific claim, method, or result being cited.\nThen, evaluate each candidate paper to see if it matches the context.\nFinally, select the best matching paper.\n\nCitation Context:\numan feedback has been explored extensively, often being used in robotics tasks to extrapolate reward signal beyond an initial preference set. Recent work in NLP has explored implementations[CITATION], objectives, and even alternatives for RLHF, but have generally overlooked or dismissed length increases. Our work is largely orthogonal to these directions, using the issue of\n\nCandidate Papers:\n1. Title: Fine-Grained Human Feedback Gives Better Rewards for Language Model Training\n   Abstract: Fine-Grained Human Feedback Gives Better Rewards for Language Model Training: Language models (LMs) often exhibit undesirable text generation behaviors, including generating false, toxic, or irrelevant outputs. Reinforcement learning from human feedback (RLHF) - where human preference judgments on LM outputs are transformed into a learning signal - has recently shown promise in addressing these issues. However, such holistic feedback conveys limited information on long text outputs; it does not indicate which aspects of the outputs influenced user preference; e.g., which parts contain what type(s) of errors. In this paper, we use fine-grained human feedback (e.g., which sentence is false, which sub-sentence is irrelevant) as an explicit training signal. We introduce Fine-Grained RLHF, a framework that enables training and learning from reward functions that are fine-grained in two respects: (1) density, providing a reward after every segment (e.g., a sentence) is generated; and (2) incorporating multiple reward models associated with different feedback types (e.g., factual incorrectness, irrelevance, and information incompleteness). We conduct experiments on detoxification and long-form question answering to illustrate how learning with such reward functions leads to improved performance, supported by both automatic and human evaluation. Additionally, we show that LM behaviors can be customized using different combinations of fine-grained reward models. We release all data, collected human feedback, and codes at https://FineGrainedRLHF.github.io.\n\n2. Title: Secrets of RLHF in Large Language Models Part I: PPO\n   Abstract: Secrets of RLHF in Large Language Models Part I: PPO: Large language models (LLMs) have formulated a blueprint for the advancement of artificial general intelligence. Its primary objective is to function as a human-centric (helpful, honest, and harmless) assistant. Alignment with humans assumes paramount significance, and reinforcement learning with human feedback (RLHF) emerges as the pivotal technological paradigm underpinning this pursuit. Current technical routes usually include \\textbf{reward models} to measure human preferences, \\textbf{Proximal Policy Optimization} (PPO) to optimize policy model outputs, and \\textbf{process supervision} to improve step-by-step reasoning capabilities. However, due to the challenges of reward design, environment interaction, and agent training, coupled with huge trial and error cost of large language models, there is a significant barrier for AI researchers to motivate the development of technical alignment and safe landing of LLMs. The stable training of RLHF has still been a puzzle. In the first report, we dissect the framework of RLHF, re-evaluate the inner workings of PPO, and explore how the parts comprising PPO algorithms impact policy agent training. We identify policy constraints being the key factor for the effective implementation of the PPO algorithm. Therefore, we explore the PPO-max, an advanced version of PPO algorithm, to efficiently improve the training stability of the policy model. Based on our main results, we perform a comprehensive analysis of RLHF abilities compared with SFT models and ChatGPT. The absence of open-source implementations has posed significant challenges to the investigation of LLMs alignment. Therefore, we are eager to release technical reports, reward models and PPO codes, aiming to make modest contributions to the advancement of LLMs.\n\n3. Title: Defining and Characterizing Reward Hacking\n   Abstract: Defining and Characterizing Reward Hacking: We provide the first formal definition of reward hacking, a phenomenon where optimizing an imperfect proxy reward function, $\\mathcal{\\tilde{R}}$, leads to poor performance according to the true reward function, $\\mathcal{R}$. We say that a proxy is unhackable if increasing the expected proxy return can never decrease the expected true return. Intuitively, it might be possible to create an unhackable proxy by leaving some terms out of the reward function (making it \"narrower\") or overlooking fine-grained distinctions between roughly equivalent outcomes, but we show this is usually not the case. A key insight is that the linearity of reward (in state-action visit counts) makes unhackability a very strong condition. In particular, for the set of all stochastic policies, two reward functions can only be unhackable if one of them is constant. We thus turn our attention to deterministic policies and finite sets of stochastic policies, where non-trivial unhackable pairs always exist, and establish necessary and sufficient conditions for the existence of simplifications, an important special case of unhackability. Our results reveal a tension between using reward functions to specify narrow tasks and aligning AI systems with human values.\n\n4. Title: Learning to summarize from human feedback\n   Abstract: Learning to summarize from human feedback: As language models become more powerful, training and evaluation are increasingly bottlenecked by the data and metrics used for a particular task. For example, summarization models are often trained to predict human reference summaries and evaluated using ROUGE, but both of these metrics are rough proxies for what we really care about -- summary quality. In this work, we show that it is possible to significantly improve summary quality by training a model to optimize for human preferences. We collect a large, high-quality dataset of human comparisons between summaries, train a model to predict the human-preferred summary, and use that model as a reward function to fine-tune a summarization policy using reinforcement learning. We apply our method to a version of the TL;DR dataset of Reddit posts and find that our models significantly outperform both human reference summaries and much larger models fine-tuned with supervised learning alone. Our models also transfer to CNN/DM news articles, producing summaries nearly as good as the human reference without any news-specific fine-tuning. We conduct extensive analyses to understand our human feedback dataset and fine-tuned models We establish that our reward model generalizes to new datasets, and that optimizing our reward model results in better summaries than optimizing ROUGE according to humans. We hope the evidence from our paper motivates machine learning researchers to pay closer attention to how their training loss affects the model behavior they actually want.\n\n5. Title: Aligning Robot and Human Representations\n   Abstract: Aligning Robot and Human Representations: To act in the world, robots rely on a representation of salient task aspects: for example, to carry a coffee mug, a robot may consider movement efficiency or mug orientation in its behavior. However, if we want robots to act for and with people, their representations must not be just functional but also reflective of what humans care about, i.e. they must be aligned. We observe that current learning approaches suffer from representation misalignment, where the robot's learned representation does not capture the human's representation. We suggest that because humans are the ultimate evaluator of robot performance, we must explicitly focus our efforts on aligning learned representations with humans, in addition to learning the downstream task. We advocate that current representation learning approaches in robotics should be studied from the perspective of how well they accomplish the objective of representation alignment. We mathematically define the problem, identify its key desiderata, and situate current methods within this formalism. We conclude by suggesting future directions for exploring open challenges.\n\n6. Title: Event Representations for Automated Story Generation with Deep Neural Nets\n   Abstract: Event Representations for Automated Story Generation with Deep Neural Nets: Automated story generation is the problem of automatically selecting a sequence of events, actions, or words that can be told as a story. We seek to develop a system that can generate stories by learning everything it needs to know from textual story corpora. To date, recurrent neural networks that learn language models at character, word, or sentence levels have had little success generating coherent stories. We explore the question of event representations that provide a mid-level of abstraction between words and sentences in order to retain the semantic information of the original data while minimizing event sparsity. We present a technique for preprocessing textual story data into event sequences. We then present a technique for automated story generation whereby we decompose the problem into the generation of successive events (event2event) and the generation of natural language sentences from events (event2sentence). We give empirical results comparing different event representations and their effects on event successor generation and the translation of events to natural language.\n\n7. Title: Controlling Output Length in Neural Encoder-Decoders\n   Abstract: Controlling Output Length in Neural Encoder-Decoders: Neural encoder-decoder models have shown great success in many sequence generation tasks. However, previous work has not investigated situations in which we would like to control the length of encoder-decoder outputs. This capability is crucial for applications such as text summarization, in which we have to generate concise summaries with a desired length. In this paper, we propose methods for controlling the output sequence length for neural encoder-decoder models: two decoding-based methods and two learning-based methods. Results show that our learning-based methods have the capability to control length without degrading summary quality in a summarization task.\n\n8. Title: SLiC-HF: Sequence Likelihood Calibration with Human Feedback\n   Abstract: SLiC-HF: Sequence Likelihood Calibration with Human Feedback: Learning from human feedback has been shown to be effective at aligning language models with human preferences. Past work has often relied on Reinforcement Learning from Human Feedback (RLHF), which optimizes the language model using reward scores assigned from a reward model trained on human preference data. In this work we show how the recently introduced Sequence Likelihood Calibration (SLiC), can also be used to effectively learn from human preferences (SLiC-HF). Furthermore, we demonstrate this can be done with human feedback data collected for a different model, similar to off-policy, offline RL data. Automatic and human evaluation experiments on the TL;DR summarization task show that SLiC-HF significantly improves supervised fine-tuning baselines. Furthermore, SLiC-HF presents a competitive alternative to the PPO RLHF implementation used in past work while being much simpler to implement, easier to tune and more computationally efficient in practice.\n\n9. Title: Reward Gaming in Conditional Text Generation\n   Abstract: Reward Gaming in Conditional Text Generation: To align conditional text generation model outputs with desired behaviors, there has been an increasing focus on training the model using reinforcement learning (RL) with reward functions learned from human annotations. Under this framework, we identify three common cases where high rewards are incorrectly assigned to undesirable patterns: noise-induced spurious correlation, naturally occurring spurious correlation, and covariate shift. We show that even though learned metrics achieve high performance on the distribution of the data used to train the reward function, the undesirable patterns may be amplified during RL training of the text generation model. While there has been discussion about reward gaming in the RL or safety community, in this discussion piece, we would like to highlight reward gaming in the natural language generation (NLG) community using concrete conditional text generation examples and discuss potential fixes and areas for future work.\n\nThink step-by-step about which candidate best matches the context. Summarize your reasoning, then provide the exact title that should fill the citation. Respond in JSON with fields:\n{\n  \"reasoning\": \"<your step-by-step analysis>\",\n  \"selected_title\": \"<exact candidate title>\"\n}", "response": {"reasoning": "Context interpretation: The sentence summarizes recent NLP work around RLHF, mentioning implementations, objectives, and even alternatives to RLHF. The citation should therefore point to an NLP paper that (a) discusses implementations or alternatives to standard RLHF (e.g., PPO), and (b) is framed as an alternative approach in the summarization/NLP setting. Evaluation of candidates:\n- (1) Fine-Grained Human Feedback: focuses on fine-grained reward modeling/objectives for RLHF (relevant to â€˜objectivesâ€™) but does not primarily frame an alternative to standard RLHF implementations.\n- (2) Secrets of RLHF Part I: PPO: deeply about PPO implementation (relevant to â€˜implementationsâ€™) but not about alternatives.\n- (3) Defining and Characterizing Reward Hacking: theoretical RL reward-hacking perspective â€“ not an NLP RLHF alternative/implementation paper.\n- (4) Learning to summarize from human feedback: foundational NLP RLHF work (shows RLHF in summarization) and is relevant, but it is an early implementation rather than an explicit alternative or discussion of alternatives.\n- (5,6) robotics/story generation papers: not relevant.\n- (7) Controlling Output Length: about length control in encoder-decoder models, not RLHF alternatives/implementations.\n- (8) SLiC-HF: explicitly proposes an alternative training method (Sequence Likelihood Calibration) to PPO-based RLHF, demonstrates it on TL;DR summarization, and positions itself as a competitive alternative to PPO RLHFâ€”matching the phrase â€œimplementations, objectives, and even alternatives for RLHF.â€\n- (9) Reward Gaming in Conditional Text Generation: discusses reward issues and gaming but is more about failure modes than alternatives.\nSelection: Candidate 8 (SLiC-HF) best matches the cited claim about recent NLP work exploring implementations and alternatives to RLHF.\n\nTherefore the citation should be: \"SLiC-HF: Sequence Likelihood Calibration with Human Feedback.\"", "selected_title": "SLiC-HF: Sequence Likelihood Calibration with Human Feedback"}, "selected_title": "SLiC-HF: Sequence Likelihood Calibration with Human Feedback", "positive_title": "Secrets of RLHF in Large Language Models Part I: PPO", "is_correct": false, "reasoning": "Context interpretation: The sentence summarizes recent NLP work around RLHF, mentioning implementations, objectives, and even alternatives to RLHF. The citation should therefore point to an NLP paper that (a) discusses implementations or alternatives to standard RLHF (e.g., PPO), and (b) is framed as an alternative approach in the summarization/NLP setting. Evaluation of candidates:\n- (1) Fine-Grained Human Feedback: focuses on fine-grained reward modeling/objectives for RLHF (relevant to â€˜objectivesâ€™) but does not primarily frame an alternative to standard RLHF implementations.\n- (2) Secrets of RLHF Part I: PPO: deeply about PPO implementation (relevant to â€˜implementationsâ€™) but not about alternatives.\n- (3) Defining and Characterizing Reward Hacking: theoretical RL reward-hacking perspective â€“ not an NLP RLHF alternative/implementation paper.\n- (4) Learning to summarize from human feedback: foundational NLP RLHF work (shows RLHF in summarization) and is relevant, but it is an early implementation rather than an explicit alternative or discussion of alternatives.\n- (5,6) robotics/story generation papers: not relevant.\n- (7) Controlling Output Length: about length control in encoder-decoder models, not RLHF alternatives/implementations.\n- (8) SLiC-HF: explicitly proposes an alternative training method (Sequence Likelihood Calibration) to PPO-based RLHF, demonstrates it on TL;DR summarization, and positions itself as a competitive alternative to PPO RLHFâ€”matching the phrase â€œimplementations, objectives, and even alternatives for RLHF.â€\n- (9) Reward Gaming in Conditional Text Generation: discusses reward issues and gaming but is more about failure modes than alternatives.\nSelection: Candidate 8 (SLiC-HF) best matches the cited claim about recent NLP work exploring implementations and alternatives to RLHF.\n\nTherefore the citation should be: \"SLiC-HF: Sequence Likelihood Calibration with Human Feedback.\""}
{"example_id": "blind_9506", "prompt": "You are an expert citation retrieval system as described in the paper \"Multi-Agent System for Reliable Citation Retrieval\".\nYour goal is to autonomously retrieve, verify, and recommend academic references given a query or document excerpt.\n\nTask:\nGiven a citation context from a scientific paper (where a citation is missing), identify the correct paper from a list of candidates.\nAnalyze the context to understand the specific claim, method, or result being cited.\nThen, evaluate each candidate paper to see if it matches the context.\nFinally, select the best matching paper.\n\nCitation Context:\nng approach for leveraging online human-object interaction videos without explicit pose estimation to improve dexterity in robotic manipulation, sharing a similar motivation as MVP and VC-1[CITATION].\n\nCandidate Papers:\n1. Title: Dexmv: Imitation learning for dexterous manipulation from human videos\n   Abstract: DexMV: Imitation Learning for Dexterous Manipulation from Human Videos: While significant progress has been made on understanding hand-object interactions in computer vision, it is still very challenging for robots to perform complex dexterous manipulation. In this paper, we propose a new platform and pipeline DexMV (Dexterous Manipulation from Videos) for imitation learning. We design a platform with: (i) a simulation system for complex dexterous manipulation tasks with a multi-finger robot hand and (ii) a computer vision system to record large-scale demonstrations of a human hand conducting the same tasks. In our novel pipeline, we extract 3D hand and object poses from videos, and propose a novel demonstration translation method to convert human motion to robot demonstrations. We then apply and benchmark multiple imitation learning algorithms with the demonstrations. We show that the demonstrations can indeed improve robot learning by a large margin and solve the complex tasks which reinforcement learning alone cannot solve. More details can be found in the project page: https://yzqin.github.io/dexmv\n\n2. Title: Rrl: Resnet as representation for reinforcement learning\n   Abstract: RRL: Resnet as representation for Reinforcement Learning: The ability to autonomously learn behaviors via direct interactions in uninstrumented environments can lead to generalist robots capable of enhancing productivity or providing care in unstructured settings like homes. Such uninstrumented settings warrant operations only using the robot's proprioceptive sensor such as onboard cameras, joint encoders, etc which can be challenging for policy learning owing to the high dimensionality and partial observability issues. We propose RRL: Resnet as representation for Reinforcement Learning -- a straightforward yet effective approach that can learn complex behaviors directly from proprioceptive inputs. RRL fuses features extracted from pre-trained Resnet into the standard reinforcement learning pipeline and delivers results comparable to learning directly from the state. In a simulated dexterous manipulation benchmark, where the state of the art methods fail to make significant progress, RRL delivers contact rich behaviors. The appeal of RRL lies in its simplicity in bringing together progress from the fields of Representation Learning, Imitation Learning, and Reinforcement Learning. Its effectiveness in learning behaviors directly from visual inputs with performance and sample efficiency matching learning directly from the state, even in complex high dimensional domains, is far from obvious.\n\n3. Title: R3m: A universal visual representation for robot manipulation\n   Abstract: R3M: A Universal Visual Representation for Robot Manipulation: We study how visual representations pre-trained on diverse human video data can enable data-efficient learning of downstream robotic manipulation tasks. Concretely, we pre-train a visual representation using the Ego4D human video dataset using a combination of time-contrastive learning, video-language alignment, and an L1 penalty to encourage sparse and compact representations. The resulting representation, R3M, can be used as a frozen perception module for downstream policy learning. Across a suite of 12 simulated robot manipulation tasks, we find that R3M improves task success by over 20% compared to training from scratch and by over 10% compared to state-of-the-art visual representations like CLIP and MoCo. Furthermore, R3M enables a Franka Emika Panda arm to learn a range of manipulation tasks in a real, cluttered apartment given just 20 demonstrations. Code and pre-trained models are available at https://tinyurl.com/robotr3m.\n\n4. Title: GNFactor: Multi-Task Real Robot Learning with Generalizable Neural Feature Fields\n   Abstract: GNFactor: Multi-Task Real Robot Learning with Generalizable Neural Feature Fields: It is a long-standing problem in robotics to develop agents capable of executing diverse manipulation tasks from visual observations in unstructured real-world environments. To achieve this goal, the robot needs to have a comprehensive understanding of the 3D structure and semantics of the scene. In this work, we present $\\textbf{GNFactor}$, a visual behavior cloning agent for multi-task robotic manipulation with $\\textbf{G}$eneralizable $\\textbf{N}$eural feature $\\textbf{F}$ields. GNFactor jointly optimizes a generalizable neural field (GNF) as a reconstruction module and a Perceiver Transformer as a decision-making module, leveraging a shared deep 3D voxel representation. To incorporate semantics in 3D, the reconstruction module utilizes a vision-language foundation model ($\\textit{e.g.}$, Stable Diffusion) to distill rich semantic information into the deep 3D voxel. We evaluate GNFactor on 3 real robot tasks and perform detailed ablations on 10 RLBench tasks with a limited number of demonstrations. We observe a substantial improvement of GNFactor over current state-of-the-art methods in seen and unseen tasks, demonstrating the strong generalization ability of GNFactor. Our project website is https://yanjieze.com/GNFactor/ .\n\n5. Title: Masked visual pre-training for motor control\n   Abstract: Masked Visual Pre-training for Motor Control: This paper shows that self-supervised visual pre-training from real-world images is effective for learning motor control tasks from pixels. We first train the visual representations by masked modeling of natural images. We then freeze the visual encoder and train neural network controllers on top with reinforcement learning. We do not perform any task-specific fine-tuning of the encoder; the same visual representations are used for all motor control tasks. To the best of our knowledge, this is the first self-supervised model to exploit real-world images at scale for motor control. To accelerate progress in learning from pixels, we contribute a benchmark suite of hand-designed tasks varying in movements, scenes, and robots. Without relying on labels, state-estimation, or expert demonstrations, we consistently outperform supervised encoders by up to 80% absolute success rate, sometimes even matching the oracle state performance. We also find that in-the-wild images, e.g., from YouTube or Egocentric videos, lead to better visual representations for various manipulation tasks than ImageNet images.\n\n6. Title: Vrl3: A data-driven framework for visual deep reinforcement learning\n   Abstract: VRL3: A Data-Driven Framework for Visual Deep Reinforcement Learning: We propose VRL3, a powerful data-driven framework with a simple design for solving challenging visual deep reinforcement learning (DRL) tasks. We analyze a number of major obstacles in taking a data-driven approach, and present a suite of design principles, novel findings, and critical insights about data-driven visual DRL. Our framework has three stages: in stage 1, we leverage non-RL datasets (e.g. ImageNet) to learn task-agnostic visual representations; in stage 2, we use offline RL data (e.g. a limited number of expert demonstrations) to convert the task-agnostic representations into more powerful task-specific representations; in stage 3, we fine-tune the agent with online RL. On a set of challenging hand manipulation tasks with sparse reward and realistic visual inputs, compared to the previous SOTA, VRL3 achieves an average of 780% better sample efficiency. And on the hardest task, VRL3 is 1220% more sample efficient (2440% when using a wider encoder) and solves the task with only 10% of the computation. These significant results clearly demonstrate the great potential of data-driven deep reinforcement learning.\n\n7. Title: Time-contrastive networks: Self-supervised learning from video\n   Abstract: Time-Contrastive Networks: Self-Supervised Learning from Video: We propose a self-supervised approach for learning representations and robotic behaviors entirely from unlabeled videos recorded from multiple viewpoints, and study how this representation can be used in two robotic imitation settings: imitating object interactions from videos of humans, and imitating human poses. Imitation of human behavior requires a viewpoint-invariant representation that captures the relationships between end-effectors (hands or robot grippers) and the environment, object attributes, and body pose. We train our representations using a metric learning loss, where multiple simultaneous viewpoints of the same observation are attracted in the embedding space, while being repelled from temporal neighbors which are often visually similar but functionally different. In other words, the model simultaneously learns to recognize what is common between different-looking images, and what is different between similar-looking images. This signal causes our model to discover attributes that do not change across viewpoint, but do change across time, while ignoring nuisance variables such as occlusions, motion blur, lighting and background. We demonstrate that this representation can be used by a robot to directly mimic human poses without an explicit correspondence, and that it can be used as a reward function within a reinforcement learning algorithm. While representations are learned from an unlabeled collection of task-related videos, robot behaviors such as pouring are learned by watching a single 3rd-person demonstration by a human. Reward functions obtained by following the human demonstrations under the learned representation enable efficient reinforcement learning that is practical for real-world robotic systems. Video results, open-source code and dataset are available at https://sermanet.github.io/imitate\n\n8. Title: Where are we in the search for an Artificial Visual Cortex for Embodied Intelligence?\n   Abstract: Where are we in the search for an Artificial Visual Cortex for Embodied Intelligence?: We present the largest and most comprehensive empirical study of pre-trained visual representations (PVRs) or visual 'foundation models' for Embodied AI. First, we curate CortexBench, consisting of 17 different tasks spanning locomotion, navigation, dexterous, and mobile manipulation. Next, we systematically evaluate existing PVRs and find that none are universally dominant. To study the effect of pre-training data size and diversity, we combine over 4,000 hours of egocentric videos from 7 different sources (over 4.3M images) and ImageNet to train different-sized vision transformers using Masked Auto-Encoding (MAE) on slices of this data. Contrary to inferences from prior work, we find that scaling dataset size and diversity does not improve performance universally (but does so on average). Our largest model, named VC-1, outperforms all prior PVRs on average but does not universally dominate either. Next, we show that task- or domain-specific adaptation of VC-1 leads to substantial gains, with VC-1 (adapted) achieving competitive or superior performance than the best known results on all of the benchmarks in CortexBench. Finally, we present real-world hardware experiments, in which VC-1 and VC-1 (adapted) outperform the strongest pre-existing PVR. Overall, this paper presents no new techniques but a rigorous systematic evaluation, a broad set of findings about PVRs (that in some cases, refute those made in narrow domains in prior work), and open-sourced code and models (that required over 10,000 GPU-hours to train) for the benefit of the research community.\n\n9. Title: Data-efficient deep reinforcement learning for dexterous manipulation\n   Abstract: Data-efficient Deep Reinforcement Learning for Dexterous Manipulation: Deep learning and reinforcement learning methods have recently been used to solve a variety of problems in continuous control domains. An obvious application of these techniques is dexterous manipulation tasks in robotics which are difficult to solve using traditional control theory or hand-engineered approaches. One example of such a task is to grasp an object and precisely stack it on another. Solving this difficult and practically relevant problem in the real world is an important long-term goal for the field of robotics. Here we take a step towards this goal by examining the problem in simulation and providing models and techniques aimed at solving it. We introduce two extensions to the Deep Deterministic Policy Gradient algorithm (DDPG), a model-free Q-learning based method, which make it significantly more data-efficient and scalable. Our results show that by making extensive use of off-policy data and replay, it is possible to find control policies that robustly grasp objects and stack them. Further, our results hint that it may soon be feasible to train successful stacking policies by collecting interactions on real robots.\n\nThink step-by-step about which candidate best matches the context. Summarize your reasoning, then provide the exact title that should fill the citation. Respond in JSON with fields:\n{\n  \"reasoning\": \"<your step-by-step analysis>\",\n  \"selected_title\": \"<exact candidate title>\"\n}", "response": {"reasoning": "The citation context refers to â€œleveraging online human-object interaction videos without explicit pose estimationâ€ and says this shares similar motivation with MVP and VC-1. I need the candidate that (1) uses in-the-wild/human video data, (2) improves robot manipulation dexterity, and (3) does not rely on explicit pose estimation. I evaluate candidates: 1) DexMV explicitly extracts 3D hand/object poses and does demonstration translation â€” so it uses explicit pose estimation (not a match). 2) RRL focuses on using ResNet features for RL from proprioceptive inputs â€” not specifically about leveraging online human videos. 3) R3M pre-trains a visual representation on Ego4D (large in-the-wild human video dataset) and uses that frozen representation to improve downstream robot manipulation without relying on explicit pose estimation â€” this directly matches the description. 4) GNFactor is about neural feature fields and 3D reconstruction with vision-language distillation â€” not primarily about leveraging online human videos. 5) Masked visual pre-training (MVP) is already mentioned in the context. 6) VRL3 is a staged data-driven RL framework using non-RL datasets and offline RL â€” not specifically online human-object videos without pose. 7) Time-contrastive networks learn from video and enable imitation from video without explicit pose, but are older/multi-view and not focused on large-scale in-the-wild human-object videos like Ego4D; R3M is a closer, more recent match. 8) VC-1 is already referenced in the context. 9) Data-efficient DRL applies DDPG extensions in simulation, not leveraging online human videos. Therefore the best match is R3M, which exactly pre-trains on large-scale human video data (Ego4D) to enable manipulation without explicit pose estimation.", "selected_title": "R3m: A universal visual representation for robot manipulation"}, "selected_title": "R3m: A universal visual representation for robot manipulation", "positive_title": "Where are we in the search for an Artificial Visual Cortex for Embodied Intelligence?", "is_correct": false, "reasoning": "The citation context refers to â€œleveraging online human-object interaction videos without explicit pose estimationâ€ and says this shares similar motivation with MVP and VC-1. I need the candidate that (1) uses in-the-wild/human video data, (2) improves robot manipulation dexterity, and (3) does not rely on explicit pose estimation. I evaluate candidates: 1) DexMV explicitly extracts 3D hand/object poses and does demonstration translation â€” so it uses explicit pose estimation (not a match). 2) RRL focuses on using ResNet features for RL from proprioceptive inputs â€” not specifically about leveraging online human videos. 3) R3M pre-trains a visual representation on Ego4D (large in-the-wild human video dataset) and uses that frozen representation to improve downstream robot manipulation without relying on explicit pose estimation â€” this directly matches the description. 4) GNFactor is about neural feature fields and 3D reconstruction with vision-language distillation â€” not primarily about leveraging online human videos. 5) Masked visual pre-training (MVP) is already mentioned in the context. 6) VRL3 is a staged data-driven RL framework using non-RL datasets and offline RL â€” not specifically online human-object videos without pose. 7) Time-contrastive networks learn from video and enable imitation from video without explicit pose, but are older/multi-view and not focused on large-scale in-the-wild human-object videos like Ego4D; R3M is a closer, more recent match. 8) VC-1 is already referenced in the context. 9) Data-efficient DRL applies DDPG extensions in simulation, not leveraging online human videos. Therefore the best match is R3M, which exactly pre-trains on large-scale human video data (Ego4D) to enable manipulation without explicit pose estimation."}
{"example_id": "blind_10513", "prompt": "You are an expert citation retrieval system as described in the paper \"Multi-Agent System for Reliable Citation Retrieval\".\nYour goal is to autonomously retrieve, verify, and recommend academic references given a query or document excerpt.\n\nTask:\nGiven a citation context from a scientific paper (where a citation is missing), identify the correct paper from a list of candidates.\nAnalyze the context to understand the specific claim, method, or result being cited.\nThen, evaluate each candidate paper to see if it matches the context.\nFinally, select the best matching paper.\n\nCitation Context:\na transformer to directly regress the parameters of 3D models given conditions. Besides, applying diffusion models to directly generate 3D assets has also been widely researched.[CITATION] lead the SOTA of current 3D generation methods by first generating high-quality 3D shapes with DiT and then producing detailed textures with material diffusion models. As mesh is a crucial component\n\nCandidate Papers:\n1. Title: Denoising diffusion probabilistic models\n   Abstract: Denoising Diffusion Probabilistic Models: We present high quality image synthesis results using diffusion probabilistic models, a class of latent variable models inspired by considerations from nonequilibrium thermodynamics. Our best results are obtained by training on a weighted variational bound designed according to a novel connection between diffusion probabilistic models and denoising score matching with Langevin dynamics, and our models naturally admit a progressive lossy decompression scheme that can be interpreted as a generalization of autoregressive decoding. On the unconditional CIFAR10 dataset, we obtain an Inception score of 9.46 and a state-of-the-art FID score of 3.17. On 256x256 LSUN, we obtain sample quality similar to ProgressiveGAN. Our implementation is available at https://github.com/hojonathanho/diffusion\n\n2. Title: CLAY: A Controllable Large-scale Generative Model for Creating High-quality 3D Assets\n   Abstract: CLAY: A Controllable Large-scale Generative Model for Creating High-quality 3D Assets: In the realm of digital creativity, our potential to craft intricate 3D worlds from imagination is often hampered by the limitations of existing digital tools, which demand extensive expertise and efforts. To narrow this disparity, we introduce CLAY, a 3D geometry and material generator designed to effortlessly transform human imagination into intricate 3D digital structures. CLAY supports classic text or image inputs as well as 3D-aware controls from diverse primitives (multi-view images, voxels, bounding boxes, point clouds, implicit representations, etc). At its core is a large-scale generative model composed of a multi-resolution Variational Autoencoder (VAE) and a minimalistic latent Diffusion Transformer (DiT), to extract rich 3D priors directly from a diverse range of 3D geometries. Specifically, it adopts neural fields to represent continuous and complete surfaces and uses a geometry generative module with pure transformer blocks in latent space. We present a progressive training scheme to train CLAY on an ultra large 3D model dataset obtained through a carefully designed processing pipeline, resulting in a 3D native geometry generator with 1.5 billion parameters. For appearance generation, CLAY sets out to produce physically-based rendering (PBR) textures by employing a multi-view material diffusion model that can generate 2K resolution textures with diffuse, roughness, and metallic modalities. We demonstrate using CLAY for a range of controllable 3D asset creations, from sketchy conceptual designs to production ready assets with intricate details. Even first time users can easily use CLAY to bring their vivid 3D imaginations to life, unleashing unlimited creativity.\n\n3. Title: PivotMesh: Generic 3D Mesh Generation via Pivot Vertices Guidance\n   Abstract: PivotMesh: Generic 3D Mesh Generation via Pivot Vertices Guidance: Generating compact and sharply detailed 3D meshes poses a significant challenge for current 3D generative models. Different from extracting dense meshes from neural representation, some recent works try to model the native mesh distribution (i.e., a set of triangles), which generates more compact results as humans crafted. However, due to the complexity and variety of mesh topology, these methods are typically limited to small datasets with specific categories and are hard to extend. In this paper, we introduce a generic and scalable mesh generation framework PivotMesh, which makes an initial attempt to extend the native mesh generation to large-scale datasets. We employ a transformer-based auto-encoder to encode meshes into discrete tokens and decode them from face level to vertex level hierarchically. Subsequently, to model the complex typology, we first learn to generate pivot vertices as coarse mesh representation and then generate the complete mesh tokens with the same auto-regressive Transformer. This reduces the difficulty compared with directly modeling the mesh distribution and further improves the model controllability. PivotMesh demonstrates its versatility by effectively learning from both small datasets like Shapenet, and large-scale datasets like Objaverse and Objaverse-xl. Extensive experiments indicate that PivotMesh can generate compact and sharp 3D meshes across various categories, highlighting its great potential for native mesh modeling.\n\n4. Title: MeshLRM: Large Reconstruction Model for High-Quality Mesh\n   Abstract: MeshLRM: Large Reconstruction Model for High-Quality Mesh: We propose MeshLRM, a novel LRM-based approach that can reconstruct a high-quality mesh from merely four input images in less than one second. Different from previous large reconstruction models (LRMs) that focus on NeRF-based reconstruction, MeshLRM incorporates differentiable mesh extraction and rendering within the LRM framework. This allows for end-to-end mesh reconstruction by fine-tuning a pre-trained NeRF LRM with mesh rendering. Moreover, we improve the LRM architecture by simplifying several complex designs in previous LRMs. MeshLRM's NeRF initialization is sequentially trained with low- and high-resolution images; this new LRM training strategy enables significantly faster convergence and thereby leads to better quality with less compute. Our approach achieves state-of-the-art mesh reconstruction from sparse-view inputs and also allows for many downstream applications, including text-to-3D and single-image-to-3D generation. Project page: https://sarahweiii.github.io/meshlrm/\n\n5. Title: PolyDiff: Generating 3D Polygonal Meshes with Diffusion Models\n   Abstract: PolyDiff: Generating 3D Polygonal Meshes with Diffusion Models: We introduce PolyDiff, the first diffusion-based approach capable of directly generating realistic and diverse 3D polygonal meshes. In contrast to methods that use alternate 3D shape representations (e.g. implicit representations), our approach is a discrete denoising diffusion probabilistic model that operates natively on the polygonal mesh data structure. This enables learning of both the geometric properties of vertices and the topological characteristics of faces. Specifically, we treat meshes as quantized triangle soups, progressively corrupted with categorical noise in the forward diffusion phase. In the reverse diffusion phase, a transformer-based denoising network is trained to revert the noising process, restoring the original mesh structure. At inference, new meshes can be generated by applying this denoising network iteratively, starting with a completely noisy triangle soup. Consequently, our model is capable of producing high-quality 3D polygonal meshes, ready for integration into downstream 3D workflows. Our extensive experimental analysis shows that PolyDiff achieves a significant advantage (avg. FID and JSD improvement of 18.2 and 5.8 respectively) over current state-of-the-art methods.\n\n6. Title: Attention is all you need\n   Abstract: Attention Is All You Need: The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.\n\n7. Title: MeshAnything: Artist-Created Mesh Generation with Autoregressive Transformers\n   Abstract: MeshAnything: Artist-Created Mesh Generation with Autoregressive Transformers: Recently, 3D assets created via reconstruction and generation have matched the quality of manually crafted assets, highlighting their potential for replacement. However, this potential is largely unrealized because these assets always need to be converted to meshes for 3D industry applications, and the meshes produced by current mesh extraction methods are significantly inferior to Artist-Created Meshes (AMs), i.e., meshes created by human artists. Specifically, current mesh extraction methods rely on dense faces and ignore geometric features, leading to inefficiencies, complicated post-processing, and lower representation quality. To address these issues, we introduce MeshAnything, a model that treats mesh extraction as a generation problem, producing AMs aligned with specified shapes. By converting 3D assets in any 3D representation into AMs, MeshAnything can be integrated with various 3D asset production methods, thereby enhancing their application across the 3D industry. The architecture of MeshAnything comprises a VQ-VAE and a shape-conditioned decoder-only transformer. We first learn a mesh vocabulary using the VQ-VAE, then train the shape-conditioned decoder-only transformer on this vocabulary for shape-conditioned autoregressive mesh generation. Our extensive experiments show that our method generates AMs with hundreds of times fewer faces, significantly improving storage, rendering, and simulation efficiencies, while achieving precision comparable to previous methods.\n\n8. Title: Lrm: Large reconstruction model for single image to 3d\n   Abstract: LRM: Large Reconstruction Model for Single Image to 3D: We propose the first Large Reconstruction Model (LRM) that predicts the 3D model of an object from a single input image within just 5 seconds. In contrast to many previous methods that are trained on small-scale datasets such as ShapeNet in a category-specific fashion, LRM adopts a highly scalable transformer-based architecture with 500 million learnable parameters to directly predict a neural radiance field (NeRF) from the input image. We train our model in an end-to-end manner on massive multi-view data containing around 1 million objects, including both synthetic renderings from Objaverse and real captures from MVImgNet. This combination of a high-capacity model and large-scale training data empowers our model to be highly generalizable and produce high-quality 3D reconstructions from various testing inputs, including real-world in-the-wild captures and images created by generative models. Video demos and interactable 3D meshes can be found on our LRM project webpage: https://yiconghong.me/LRM.\n\nThink step-by-step about which candidate best matches the context. Summarize your reasoning, then provide the exact title that should fill the citation. Respond in JSON with fields:\n{\n  \"reasoning\": \"<your step-by-step analysis>\",\n  \"selected_title\": \"<exact candidate title>\"\n}", "response": {"reasoning": "The citation context refers to recent 3D generation work that (1) uses a DiT (Diffusion Transformer) to generate high-quality 3D shapes and (2) produces detailed textures with material diffusion models. I examined candidates for these specific elements. Candidate 2 (CLAY) explicitly describes a latent Diffusion Transformer (DiT) for 3D geometry generation and a multi-view material diffusion model to produce PBR textures, matching both the DiT mention and the texture/material diffusion claim. Other candidates: (1) is a foundational diffusion paper for images (not DiT or 3D textures); (5) PolyDiff uses diffusion for meshes but focuses on polygonal mesh generation (no DiT + material diffusion textures); (3,7) focus on transformer-based mesh generation but do not describe DiT + material diffusion for textures; (4,8) are reconstruction models (LRMs) without the DiT/material diffusion pipeline; (6) is the original Transformer paper and not relevant to DiT/material diffusion for 3D assets. Therefore CLAY is the best match.", "selected_title": "CLAY: A Controllable Large-scale Generative Model for Creating High-quality 3D Assets"}, "selected_title": "CLAY: A Controllable Large-scale Generative Model for Creating High-quality 3D Assets", "positive_title": "CLAY: A Controllable Large-scale Generative Model for Creating High-quality 3D Assets", "is_correct": true, "reasoning": "The citation context refers to recent 3D generation work that (1) uses a DiT (Diffusion Transformer) to generate high-quality 3D shapes and (2) produces detailed textures with material diffusion models. I examined candidates for these specific elements. Candidate 2 (CLAY) explicitly describes a latent Diffusion Transformer (DiT) for 3D geometry generation and a multi-view material diffusion model to produce PBR textures, matching both the DiT mention and the texture/material diffusion claim. Other candidates: (1) is a foundational diffusion paper for images (not DiT or 3D textures); (5) PolyDiff uses diffusion for meshes but focuses on polygonal mesh generation (no DiT + material diffusion textures); (3,7) focus on transformer-based mesh generation but do not describe DiT + material diffusion for textures; (4,8) are reconstruction models (LRMs) without the DiT/material diffusion pipeline; (6) is the original Transformer paper and not relevant to DiT/material diffusion for 3D assets. Therefore CLAY is the best match."}
{"example_id": "blind_16646", "prompt": "You are an expert citation retrieval system as described in the paper \"Multi-Agent System for Reliable Citation Retrieval\".\nYour goal is to autonomously retrieve, verify, and recommend academic references given a query or document excerpt.\n\nTask:\nGiven a citation context from a scientific paper (where a citation is missing), identify the correct paper from a list of candidates.\nAnalyze the context to understand the specific claim, method, or result being cited.\nThen, evaluate each candidate paper to see if it matches the context.\nFinally, select the best matching paper.\n\nCitation Context:\nke a novel yet different path to achieve editing by modulating the initial noise. There are also some novel methods who offer customized text control by inverting images into textual tokens[CITATION]. \\subsection{High-Fidelity Inversion of GANs} Unlike the natural inversion capability exists in diffusion models, GANs need to do inversion by encoder, optimization\n\nCandidate Papers:\n1. Title: Paint by example: Exemplar-based image editing with diffusion models\n   Abstract: Paint by Example: Exemplar-based Image Editing with Diffusion Models: Language-guided image editing has achieved great success recently. In this paper, for the first time, we investigate exemplar-guided image editing for more precise control. We achieve this goal by leveraging self-supervised training to disentangle and re-organize the source image and the exemplar. However, the naive approach will cause obvious fusing artifacts. We carefully analyze it and propose an information bottleneck and strong augmentations to avoid the trivial solution of directly copying and pasting the exemplar image. Meanwhile, to ensure the controllability of the editing process, we design an arbitrary shape mask for the exemplar image and leverage the classifier-free guidance to increase the similarity to the exemplar image. The whole framework involves a single forward of the diffusion model without any iterative optimization. We demonstrate that our method achieves an impressive performance and enables controllable editing on in-the-wild images with high fidelity.\n\n2. Title: Null-text inversion for editing real images using guided diffusion models\n   Abstract: Null-text Inversion for Editing Real Images using Guided Diffusion Models: Recent text-guided diffusion models provide powerful image generation capabilities. Currently, a massive effort is given to enable the modification of these images using text only as means to offer intuitive and versatile editing. To edit a real image using these state-of-the-art tools, one must first invert the image with a meaningful text prompt into the pretrained model's domain. In this paper, we introduce an accurate inversion technique and thus facilitate an intuitive text-based modification of the image. Our proposed inversion consists of two novel key components: (i) Pivotal inversion for diffusion models. While current methods aim at mapping random noise samples to a single input image, we use a single pivotal noise vector for each timestamp and optimize around it. We demonstrate that a direct inversion is inadequate on its own, but does provide a good anchor for our optimization. (ii) NULL-text optimization, where we only modify the unconditional textual embedding that is used for classifier-free guidance, rather than the input text embedding. This allows for keeping both the model weights and the conditional embedding intact and hence enables applying prompt-based editing while avoiding the cumbersome tuning of the model's weights. Our Null-text inversion, based on the publicly available Stable Diffusion model, is extensively evaluated on a variety of images and prompt editing, showing high-fidelity editing of real images.\n\n3. Title: Diffusion models already have a semantic latent space\n   Abstract: Diffusion Models already have a Semantic Latent Space: Diffusion models achieve outstanding generative performance in various domains. Despite their great success, they lack semantic latent space which is essential for controlling the generative process. To address the problem, we propose asymmetric reverse process (Asyrp) which discovers the semantic latent space in frozen pretrained diffusion models. Our semantic latent space, named h-space, has nice properties for accommodating semantic image manipulation: homogeneity, linearity, robustness, and consistency across timesteps. In addition, we introduce a principled design of the generative process for versatile editing and quality boost ing by quantifiable measures: editing strength of an interval and quality deficiency at a timestep. Our method is applicable to various architectures (DDPM++, iD- DPM, and ADM) and datasets (CelebA-HQ, AFHQ-dog, LSUN-church, LSUN- bedroom, and METFACES). Project page: https://kwonminki.github.io/Asyrp/\n\n4. Title: Repaint: Inpainting using denoising diffusion probabilistic models\n   Abstract: RePaint: Inpainting using Denoising Diffusion Probabilistic Models: Free-form inpainting is the task of adding new content to an image in the regions specified by an arbitrary binary mask. Most existing approaches train for a certain distribution of masks, which limits their generalization capabilities to unseen mask types. Furthermore, training with pixel-wise and perceptual losses often leads to simple textural extensions towards the missing areas instead of semantically meaningful generation. In this work, we propose RePaint: A Denoising Diffusion Probabilistic Model (DDPM) based inpainting approach that is applicable to even extreme masks. We employ a pretrained unconditional DDPM as the generative prior. To condition the generation process, we only alter the reverse diffusion iterations by sampling the unmasked regions using the given image information. Since this technique does not modify or condition the original DDPM network itself, the model produces high-quality and diverse output images for any inpainting form. We validate our method for both faces and general-purpose image inpainting using standard and extreme masks.   RePaint outperforms state-of-the-art Autoregressive, and GAN approaches for at least five out of six mask distributions.   Github Repository: git.io/RePaint\n\n5. Title: Glide: Towards photorealistic image generation and editing with text-guided diffusion models\n   Abstract: GLIDE: Towards Photorealistic Image Generation and Editing with Text-Guided Diffusion Models: Diffusion models have recently been shown to generate high-quality synthetic images, especially when paired with a guidance technique to trade off diversity for fidelity. We explore diffusion models for the problem of text-conditional image synthesis and compare two different guidance strategies: CLIP guidance and classifier-free guidance. We find that the latter is preferred by human evaluators for both photorealism and caption similarity, and often produces photorealistic samples. Samples from a 3.5 billion parameter text-conditional diffusion model using classifier-free guidance are favored by human evaluators to those from DALL-E, even when the latter uses expensive CLIP reranking. Additionally, we find that our models can be fine-tuned to perform image inpainting, enabling powerful text-driven image editing. We train a smaller model on a filtered dataset and release the code and weights at https://github.com/openai/glide-text2im.\n\n6. Title: Imagic: Text-based real image editing with diffusion models\n   Abstract: Imagic: Text-Based Real Image Editing with Diffusion Models: Text-conditioned image editing has recently attracted considerable interest. However, most methods are currently either limited to specific editing types (e.g., object overlay, style transfer), or apply to synthetically generated images, or require multiple input images of a common object. In this paper we demonstrate, for the very first time, the ability to apply complex (e.g., non-rigid) text-guided semantic edits to a single real image. For example, we can change the posture and composition of one or multiple objects inside an image, while preserving its original characteristics. Our method can make a standing dog sit down or jump, cause a bird to spread its wings, etc. -- each within its single high-resolution natural image provided by the user. Contrary to previous work, our proposed method requires only a single input image and a target text (the desired edit). It operates on real images, and does not require any additional inputs (such as image masks or additional views of the object). Our method, which we call \"Imagic\", leverages a pre-trained text-to-image diffusion model for this task. It produces a text embedding that aligns with both the input image and the target text, while fine-tuning the diffusion model to capture the image-specific appearance. We demonstrate the quality and versatility of our method on numerous inputs from various domains, showcasing a plethora of high quality complex semantic image edits, all within a single unified framework.\n\n7. Title: Prompt-to-prompt image editing with cross attention control\n   Abstract: Prompt-to-Prompt Image Editing with Cross Attention Control: Recent large-scale text-driven synthesis models have attracted much attention thanks to their remarkable capabilities of generating highly diverse images that follow given text prompts. Such text-based synthesis methods are particularly appealing to humans who are used to verbally describe their intent. Therefore, it is only natural to extend the text-driven image synthesis to text-driven image editing. Editing is challenging for these generative models, since an innate property of an editing technique is to preserve most of the original image, while in the text-based models, even a small modification of the text prompt often leads to a completely different outcome. State-of-the-art methods mitigate this by requiring the users to provide a spatial mask to localize the edit, hence, ignoring the original structure and content within the masked region. In this paper, we pursue an intuitive prompt-to-prompt editing framework, where the edits are controlled by text only. To this end, we analyze a text-conditioned model in depth and observe that the cross-attention layers are the key to controlling the relation between the spatial layout of the image to each word in the prompt. With this observation, we present several applications which monitor the image synthesis by editing the textual prompt only. This includes localized editing by replacing a word, global editing by adding a specification, and even delicately controlling the extent to which a word is reflected in the image. We present our results over diverse images and prompts, demonstrating high-quality synthesis and fidelity to the edited prompts.\n\n8. Title: Denoising diffusion probabilistic models\n   Abstract: Denoising Diffusion Probabilistic Models: We present high quality image synthesis results using diffusion probabilistic models, a class of latent variable models inspired by considerations from nonequilibrium thermodynamics. Our best results are obtained by training on a weighted variational bound designed according to a novel connection between diffusion probabilistic models and denoising score matching with Langevin dynamics, and our models naturally admit a progressive lossy decompression scheme that can be interpreted as a generalization of autoregressive decoding. On the unconditional CIFAR10 dataset, we obtain an Inception score of 9.46 and a state-of-the-art FID score of 3.17. On 256x256 LSUN, we obtain sample quality similar to ProgressiveGAN. Our implementation is available at https://github.com/hojonathanho/diffusion\n\n9. Title: An image is worth one word: Personalizing text-to-image generation using textual inversion\n   Abstract: An Image is Worth One Word: Personalizing Text-to-Image Generation using Textual Inversion: Text-to-image models offer unprecedented freedom to guide creation through natural language. Yet, it is unclear how such freedom can be exercised to generate images of specific unique concepts, modify their appearance, or compose them in new roles and novel scenes. In other words, we ask: how can we use language-guided models to turn our cat into a painting, or imagine a new product based on our favorite toy? Here we present a simple approach that allows such creative freedom. Using only 3-5 images of a user-provided concept, like an object or a style, we learn to represent it through new \"words\" in the embedding space of a frozen text-to-image model. These \"words\" can be composed into natural language sentences, guiding personalized creation in an intuitive way. Notably, we find evidence that a single word embedding is sufficient for capturing unique and varied concepts. We compare our approach to a wide range of baselines, and demonstrate that it can more faithfully portray the concepts across a range of applications and tasks.   Our code, data and new words will be available at: https://textual-inversion.github.io\n\nThink step-by-step about which candidate best matches the context. Summarize your reasoning, then provide the exact title that should fill the citation. Respond in JSON with fields:\n{\n  \"reasoning\": \"<your step-by-step analysis>\",\n  \"selected_title\": \"<exact candidate title>\"\n}", "response": {"reasoning": "1) Parse the citation context: the sentence refers to methods that offer customized text control by \"inverting images into textual tokens\" â€” i.e., learning or deriving discrete/textual embeddings/words that represent an image or concept for use in text-conditioned editing. 2) Scan candidates for a direct match: Candidate 9, \"An image is worth one word: Personalizing text-to-image generation using textual inversion,\" explicitly proposes learning new \"words\" in the text embedding space from a few images to represent a concept â€” exactly \"inverting images into textual tokens.\" 3) Contrast with close candidates: Candidate 2 (Null-text Inversion) deals with optimizing unconditional text embeddings for inversion but not learning new textual tokens/words that represent an image concept; Candidate 6 (Imagic) aligns/fine-tunes embeddings for a single image but does not formulate learning new word tokens; others address editing, inpainting, or semantic latents but not the textual-token personalization described. 4) Therefore the best match is Candidate 9, which directly introduces textual inversion (learning one-word embeddings from images) and is the correct citation for the stated claim.", "selected_title": "An image is worth one word: Personalizing text-to-image generation using textual inversion"}, "selected_title": "An image is worth one word: Personalizing text-to-image generation using textual inversion", "positive_title": "An image is worth one word: Personalizing text-to-image generation using textual inversion", "is_correct": true, "reasoning": "1) Parse the citation context: the sentence refers to methods that offer customized text control by \"inverting images into textual tokens\" â€” i.e., learning or deriving discrete/textual embeddings/words that represent an image or concept for use in text-conditioned editing. 2) Scan candidates for a direct match: Candidate 9, \"An image is worth one word: Personalizing text-to-image generation using textual inversion,\" explicitly proposes learning new \"words\" in the text embedding space from a few images to represent a concept â€” exactly \"inverting images into textual tokens.\" 3) Contrast with close candidates: Candidate 2 (Null-text Inversion) deals with optimizing unconditional text embeddings for inversion but not learning new textual tokens/words that represent an image concept; Candidate 6 (Imagic) aligns/fine-tunes embeddings for a single image but does not formulate learning new word tokens; others address editing, inpainting, or semantic latents but not the textual-token personalization described. 4) Therefore the best match is Candidate 9, which directly introduces textual inversion (learning one-word embeddings from images) and is the correct citation for the stated claim."}
{"example_id": "blind_13939", "prompt": "You are an expert citation retrieval system as described in the paper \"Multi-Agent System for Reliable Citation Retrieval\".\nYour goal is to autonomously retrieve, verify, and recommend academic references given a query or document excerpt.\n\nTask:\nGiven a citation context from a scientific paper (where a citation is missing), identify the correct paper from a list of candidates.\nAnalyze the context to understand the specific claim, method, or result being cited.\nThen, evaluate each candidate paper to see if it matches the context.\nFinally, select the best matching paper.\n\nCitation Context:\nof work (SentenceBERT, BERT-flow, SimCSE) explored intermediate training to improve STS tasks. All of them applied pre-fine tuning on NLI datasets. More recently, EFL[CITATION] proposed a task transformation paradigm, improving single sentence tasks with less labels using rich sentence-pair datasets. \\textbf{Adapter Tuning:} Adapter tuning has emerged as a novel parameter\n\nCandidate Papers:\n1. Title: P-Tuning v2: Prompt Tuning Can Be Comparable to Fine-tuning Universally Across Scales and Tasks\n   Abstract: P-Tuning v2: Prompt Tuning Can Be Comparable to Fine-tuning Universally Across Scales and Tasks: Prompt tuning, which only tunes continuous prompts with a frozen language model, substantially reduces per-task storage and memory usage at training. However, in the context of NLU, prior work reveals that prompt tuning does not perform well for normal-sized pretrained models. We also find that existing methods of prompt tuning cannot handle hard sequence labeling tasks, indicating a lack of universality. We present a novel empirical finding that properly optimized prompt tuning can be universally effective across a wide range of model scales and NLU tasks. It matches the performance of finetuning while having only 0.1%-3% tuned parameters. Our method P-Tuning v2 is an implementation of Deep Prompt Tuning \\cite{li2021prefix,qin2021learning} optimized and adapted for NLU. Given the universality and simplicity of P-Tuning v2, we believe it can serve as an alternative to finetuning and a strong baseline for future research.Our code and data are released at https://github.com/THUDM/P-tuning-v2.\n\n2. Title: Entailment as Few-Shot Learner\n   Abstract: Entailment as Few-Shot Learner: Large pre-trained language models (LMs) have demonstrated remarkable ability as few-shot learners. However, their success hinges largely on scaling model parameters to a degree that makes it challenging to train and serve. In this paper, we propose a new approach, named as EFL, that can turn small LMs into better few-shot learners. The key idea of this approach is to reformulate potential NLP task into an entailment one, and then fine-tune the model with as little as 8 examples. We further demonstrate our proposed method can be: (i) naturally combined with an unsupervised contrastive learning-based data augmentation method; (ii) easily extended to multilingual few-shot learning. A systematic evaluation on 18 standard NLP tasks demonstrates that this approach improves the various existing SOTA few-shot learning methods by 12\\%, and yields competitive few-shot performance with 500 times larger models, such as GPT-3.\n\n3. Title: Language models are few-shot learners\n   Abstract: Language Models are Few-Shot Learners: Recent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training on a large corpus of text followed by fine-tuning on a specific task. While typically task-agnostic in architecture, this method still requires task-specific fine-tuning datasets of thousands or tens of thousands of examples. By contrast, humans can generally perform a new language task from only a few examples or from simple instructions - something which current NLP systems still largely struggle to do. Here we show that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art fine-tuning approaches. Specifically, we train GPT-3, an autoregressive language model with 175 billion parameters, 10x more than any previous non-sparse language model, and test its performance in the few-shot setting. For all tasks, GPT-3 is applied without any gradient updates or fine-tuning, with tasks and few-shot demonstrations specified purely via text interaction with the model. GPT-3 achieves strong performance on many NLP datasets, including translation, question-answering, and cloze tasks, as well as several tasks that require on-the-fly reasoning or domain adaptation, such as unscrambling words, using a novel word in a sentence, or performing 3-digit arithmetic. At the same time, we also identify some datasets where GPT-3's few-shot learning still struggles, as well as some datasets where GPT-3 faces methodological issues related to training on large web corpora. Finally, we find that GPT-3 can generate samples of news articles which human evaluators have difficulty distinguishing from articles written by humans. We discuss broader societal impacts of this finding and of GPT-3 in general.\n\n4. Title: Exploiting cloze questions for few shot text classification and natural language inference\n   Abstract: Exploiting Cloze Questions for Few Shot Text Classification and Natural Language Inference: Some NLP tasks can be solved in a fully unsupervised fashion by providing a pretrained language model with \"task descriptions\" in natural language (e.g., Radford et al., 2019). While this approach underperforms its supervised counterpart, we show in this work that the two ideas can be combined: We introduce Pattern-Exploiting Training (PET), a semi-supervised training procedure that reformulates input examples as cloze-style phrases to help language models understand a given task. These phrases are then used to assign soft labels to a large set of unlabeled examples. Finally, standard supervised training is performed on the resulting training set. For several tasks and languages, PET outperforms supervised training and strong semi-supervised approaches in low-resource settings by a large margin.\n\n5. Title: BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\n   Abstract: BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding: We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models, BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications.   BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5% (7.7% point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).\n\n6. Title: Mad-x: An adapter-based framework for multi-task cross-lingual transfer\n   Abstract: MAD-X: An Adapter-Based Framework for Multi-Task Cross-Lingual Transfer: The main goal behind state-of-the-art pre-trained multilingual models such as multilingual BERT and XLM-R is enabling and bootstrapping NLP applications in low-resource languages through zero-shot or few-shot cross-lingual transfer. However, due to limited model capacity, their transfer performance is the weakest exactly on such low-resource languages and languages unseen during pre-training. We propose MAD-X, an adapter-based framework that enables high portability and parameter-efficient transfer to arbitrary tasks and languages by learning modular language and task representations. In addition, we introduce a novel invertible adapter architecture and a strong baseline method for adapting a pre-trained multilingual model to a new language. MAD-X outperforms the state of the art in cross-lingual transfer across a representative set of typologically diverse languages on named entity recognition and causal commonsense reasoning, and achieves competitive results on question answering. Our code and adapters are available at AdapterHub.ml\n\n7. Title: Sentence-bert: Sentence embeddings using siamese bert-networks\n   Abstract: Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks: BERT (Devlin et al., 2018) and RoBERTa (Liu et al., 2019) has set a new state-of-the-art performance on sentence-pair regression tasks like semantic textual similarity (STS). However, it requires that both sentences are fed into the network, which causes a massive computational overhead: Finding the most similar pair in a collection of 10,000 sentences requires about 50 million inference computations (~65 hours) with BERT. The construction of BERT makes it unsuitable for semantic similarity search as well as for unsupervised tasks like clustering.   In this publication, we present Sentence-BERT (SBERT), a modification of the pretrained BERT network that use siamese and triplet network structures to derive semantically meaningful sentence embeddings that can be compared using cosine-similarity. This reduces the effort for finding the most similar pair from 65 hours with BERT / RoBERTa to about 5 seconds with SBERT, while maintaining the accuracy from BERT.   We evaluate SBERT and SRoBERTa on common STS tasks and transfer learning tasks, where it outperforms other state-of-the-art sentence embeddings methods.\n\n8. Title: The power of scale for parameter-efficient prompt tuning\n   Abstract: The Power of Scale for Parameter-Efficient Prompt Tuning: In this work, we explore \"prompt tuning\", a simple yet effective mechanism for learning \"soft prompts\" to condition frozen language models to perform specific downstream tasks. Unlike the discrete text prompts used by GPT-3, soft prompts are learned through backpropagation and can be tuned to incorporate signal from any number of labeled examples. Our end-to-end learned approach outperforms GPT-3's \"few-shot\" learning by a large margin. More remarkably, through ablations on model size using T5, we show that prompt tuning becomes more competitive with scale: as models exceed billions of parameters, our method \"closes the gap\" and matches the strong performance of model tuning (where all model weights are tuned). This finding is especially relevant in that large models are costly to share and serve, and the ability to reuse one frozen model for multiple downstream tasks can ease this burden. Our method can be seen as a simplification of the recently proposed \"prefix tuning\" of Li and Liang (2021), and we provide a comparison to this and other similar approaches. Finally, we show that conditioning a frozen model with soft prompts confers benefits in robustness to domain transfer, as compared to full model tuning.\n\nThink step-by-step about which candidate best matches the context. Summarize your reasoning, then provide the exact title that should fill the citation. Respond in JSON with fields:\n{\n  \"reasoning\": \"<your step-by-step analysis>\",\n  \"selected_title\": \"<exact candidate title>\"\n}", "response": {"reasoning": "The context names â€œEFLâ€ and describes a \"task transformation paradigm\" that improves single-sentence tasks by leveraging rich sentence-pair datasets and fewer labels (i.e., reformulating tasks as entailment). Candidate 2â€™s abstract explicitly introduces EFL (Entailment as Few-Shot Learner), describing reformulating NLP tasks into an entailment formulation to improve few-shot performance using sentence-pair (entailment) data. Other candidates do not match: Sentence-BERT (7) is about sentence embeddings, P-Tuning v2 (1) and prompt-tuning papers (8,4) concern prompts, GPT-3 (3) is scaling/few-shot but not entailment-task transformation, BERT (5) is the base model, and MAD-X (6) is about adapters for cross-lingual transfer. Therefore candidate 2 exactly matches the described EFL work.", "selected_title": "Entailment as Few-Shot Learner"}, "selected_title": "Entailment as Few-Shot Learner", "positive_title": "Entailment as Few-Shot Learner", "is_correct": true, "reasoning": "The context names â€œEFLâ€ and describes a \"task transformation paradigm\" that improves single-sentence tasks by leveraging rich sentence-pair datasets and fewer labels (i.e., reformulating tasks as entailment). Candidate 2â€™s abstract explicitly introduces EFL (Entailment as Few-Shot Learner), describing reformulating NLP tasks into an entailment formulation to improve few-shot performance using sentence-pair (entailment) data. Other candidates do not match: Sentence-BERT (7) is about sentence embeddings, P-Tuning v2 (1) and prompt-tuning papers (8,4) concern prompts, GPT-3 (3) is scaling/few-shot but not entailment-task transformation, BERT (5) is the base model, and MAD-X (6) is about adapters for cross-lingual transfer. Therefore candidate 2 exactly matches the described EFL work."}
{"example_id": "blind_4010", "prompt": "You are an expert citation retrieval system as described in the paper \"Multi-Agent System for Reliable Citation Retrieval\".\nYour goal is to autonomously retrieve, verify, and recommend academic references given a query or document excerpt.\n\nTask:\nGiven a citation context from a scientific paper (where a citation is missing), identify the correct paper from a list of candidates.\nAnalyze the context to understand the specific claim, method, or result being cited.\nThen, evaluate each candidate paper to see if it matches the context.\nFinally, select the best matching paper.\n\nCitation Context:\nimal bit widths. In this paper, we leverage transformer explainability to estimate the ViT layer sensitivity for allocating mixed precision for quantization. Furthermore, we adopt and improve the RepQ[CITATION] framework for efficient mixed precision PTQ. \\subsection{Explainability for Transformers} The majority of prior research on explainability has concentrated on CNNs, with a main focus on gradient me\n\nCandidate Papers:\n1. Title: Analyzing Multi-Head Self-Attention: Specialized Heads Do the Heavy Lifting, the Rest Can Be Pruned\n   Abstract: Analyzing Multi-Head Self-Attention: Specialized Heads Do the Heavy Lifting, the Rest Can Be Pruned: Multi-head self-attention is a key component of the Transformer, a state-of-the-art architecture for neural machine translation. In this work we evaluate the contribution made by individual attention heads in the encoder to the overall performance of the model and analyze the roles played by them. We find that the most important and confident heads play consistent and often linguistically-interpretable roles. When pruning heads using a method based on stochastic gates and a differentiable relaxation of the L0 penalty, we observe that specialized heads are last to be pruned. Our novel pruning method removes the vast majority of heads without seriously affecting performance. For example, on the English-Russian WMT dataset, pruning 38 out of 48 encoder heads results in a drop of only 0.15 BLEU.\n\n2. Title: Hawq-v2: Hessian aware trace-weighted quantization of neural networks\n   Abstract: HAWQ-V2: Hessian Aware trace-Weighted Quantization of Neural Networks: Quantization is an effective method for reducing memory footprint and inference time of Neural Networks, e.g., for efficient inference in the cloud, especially at the edge. However, ultra low precision quantization could lead to significant degradation in model generalization. A promising method to address this is to perform mixed-precision quantization, where more sensitive layers are kept at higher precision. However, the search space for a mixed-precision quantization is exponential in the number of layers. Recent work has proposed HAWQ, a novel Hessian based framework, with the aim of reducing this exponential search space by using second-order information. While promising, this prior work has three major limitations: (i) HAWQV1 only uses the top Hessian eigenvalue as a measure of sensitivity and do not consider the rest of the Hessian spectrum; (ii) HAWQV1 approach only provides relative sensitivity of different layers and therefore requires a manual selection of the mixed-precision setting; and (iii) HAWQV1 does not consider mixed-precision activation quantization. Here, we present HAWQV2 which addresses these shortcomings. For (i), we perform a theoretical analysis showing that a better sensitivity metric is to compute the average of all of the Hessian eigenvalues. For (ii), we develop a Pareto frontier based method for selecting the exact bit precision of different layers without any manual selection. For (iii), we extend the Hessian analysis to mixed-precision activation quantization. We have found this to be very beneficial for object detection. We show that HAWQV2 achieves new state-of-the-art results for a wide range of tasks.\n\n3. Title: Haq: Hardware-aware automated quantization with mixed precision\n   Abstract: HAQ: Hardware-Aware Automated Quantization with Mixed Precision: Model quantization is a widely used technique to compress and accelerate deep neural network (DNN) inference. Emergent DNN hardware accelerators begin to support mixed precision (1-8 bits) to further improve the computation efficiency, which raises a great challenge to find the optimal bitwidth for each layer: it requires domain experts to explore the vast design space trading off among accuracy, latency, energy, and model size, which is both time-consuming and sub-optimal. Conventional quantization algorithm ignores the different hardware architectures and quantizes all the layers in a uniform way. In this paper, we introduce the Hardware-Aware Automated Quantization (HAQ) framework which leverages the reinforcement learning to automatically determine the quantization policy, and we take the hardware accelerator's feedback in the design loop. Rather than relying on proxy signals such as FLOPs and model size, we employ a hardware simulator to generate direct feedback signals (latency and energy) to the RL agent. Compared with conventional methods, our framework is fully automated and can specialize the quantization policy for different neural network architectures and hardware architectures. Our framework effectively reduced the latency by 1.4-1.95x and the energy consumption by 1.9x with negligible loss of accuracy compared with the fixed bitwidth (8 bits) quantization. Our framework reveals that the optimal policies on different hardware architectures (i.e., edge and cloud architectures) under different resource constraints (i.e., latency, energy and model size) are drastically different. We interpreted the implication of different quantization policies, which offer insights for both neural network architecture design and hardware architecture design.\n\n4. Title: Patch-wise Mixed-Precision Quantization of Vision Transformer\n   Abstract: Patch-wise Mixed-Precision Quantization of Vision Transformer: As emerging hardware begins to support mixed bit-width arithmetic computation, mixed-precision quantization is widely used to reduce the complexity of neural networks. However, Vision Transformers (ViTs) require complex self-attention computation to guarantee the learning of powerful feature representations, which makes mixed-precision quantization of ViTs still challenging. In this paper, we propose a novel patch-wise mixed-precision quantization (PMQ) for efficient inference of ViTs. Specifically, we design a lightweight global metric, which is faster than existing methods, to measure the sensitivity of each component in ViTs to quantization errors. Moreover, we also introduce a pareto frontier approach to automatically allocate the optimal bit-precision according to the sensitivity. To further reduce the computational complexity of self-attention in inference stage, we propose a patch-wise module to reallocate bit-width of patches in each layer. Extensive experiments on the ImageNet dataset shows that our method greatly reduces the search cost and facilitates the application of mixed-precision quantization to ViTs.\n\n5. Title: Segmenter: Transformer for semantic segmentation\n   Abstract: Segmenter: Transformer for Semantic Segmentation: Image segmentation is often ambiguous at the level of individual image patches and requires contextual information to reach label consensus. In this paper we introduce Segmenter, a transformer model for semantic segmentation. In contrast to convolution-based methods, our approach allows to model global context already at the first layer and throughout the network. We build on the recent Vision Transformer (ViT) and extend it to semantic segmentation. To do so, we rely on the output embeddings corresponding to image patches and obtain class labels from these embeddings with a point-wise linear decoder or a mask transformer decoder. We leverage models pre-trained for image classification and show that we can fine-tune them on moderate sized datasets available for semantic segmentation. The linear decoder allows to obtain excellent results already, but the performance can be further improved by a mask transformer generating class masks. We conduct an extensive ablation study to show the impact of the different parameters, in particular the performance is better for large models and small patch sizes. Segmenter attains excellent results for semantic segmentation. It outperforms the state of the art on both ADE20K and Pascal Context datasets and is competitive on Cityscapes.\n\n6. Title: Repq-vit: Scale reparameterization for post-training quantization of vision transformers\n   Abstract: RepQ-ViT: Scale Reparameterization for Post-Training Quantization of Vision Transformers: Post-training quantization (PTQ), which only requires a tiny dataset for calibration without end-to-end retraining, is a light and practical model compression technique. Recently, several PTQ schemes for vision transformers (ViTs) have been presented; unfortunately, they typically suffer from non-trivial accuracy degradation, especially in low-bit cases. In this paper, we propose RepQ-ViT, a novel PTQ framework for ViTs based on quantization scale reparameterization, to address the above issues. RepQ-ViT decouples the quantization and inference processes, where the former employs complex quantizers and the latter employs scale-reparameterized simplified quantizers. This ensures both accurate quantization and efficient inference, which distinguishes it from existing approaches that sacrifice quantization performance to meet the target hardware. More specifically, we focus on two components with extreme distributions: post-LayerNorm activations with severe inter-channel variation and post-Softmax activations with power-law features, and initially apply channel-wise quantization and log$\\sqrt{2}$ quantization, respectively. Then, we reparameterize the scales to hardware-friendly layer-wise quantization and log2 quantization for inference, with only slight accuracy or computational costs. Extensive experiments are conducted on multiple vision tasks with different model variants, proving that RepQ-ViT, without hyperparameters and expensive reconstruction procedures, can outperform existing strong baselines and encouragingly improve the accuracy of 4-bit PTQ of ViTs to a usable level. Code is available at https://github.com/zkkli/RepQ-ViT.\n\n7. Title: Efficientformer: Vision transformers at mobilenet speed\n   Abstract: EfficientFormer: Vision Transformers at MobileNet Speed: Vision Transformers (ViT) have shown rapid progress in computer vision tasks, achieving promising results on various benchmarks. However, due to the massive number of parameters and model design, \\textit{e.g.}, attention mechanism, ViT-based models are generally times slower than lightweight convolutional networks. Therefore, the deployment of ViT for real-time applications is particularly challenging, especially on resource-constrained hardware such as mobile devices. Recent efforts try to reduce the computation complexity of ViT through network architecture search or hybrid design with MobileNet block, yet the inference speed is still unsatisfactory. This leads to an important question: can transformers run as fast as MobileNet while obtaining high performance? To answer this, we first revisit the network architecture and operators used in ViT-based models and identify inefficient designs. Then we introduce a dimension-consistent pure transformer (without MobileNet blocks) as a design paradigm. Finally, we perform latency-driven slimming to get a series of final models dubbed EfficientFormer. Extensive experiments show the superiority of EfficientFormer in performance and speed on mobile devices. Our fastest model, EfficientFormer-L1, achieves $79.2\\%$ top-1 accuracy on ImageNet-1K with only $1.6$ ms inference latency on iPhone 12 (compiled with CoreML), which runs as fast as MobileNetV2$\\times 1.4$ ($1.6$ ms, $74.7\\%$ top-1), and our largest model, EfficientFormer-L7, obtains $83.3\\%$ accuracy with only $7.0$ ms latency. Our work proves that properly designed transformers can reach extremely low latency on mobile devices while maintaining high performance.\n\n8. Title: Transformer interpretability beyond attention visualization\n   Abstract: Transformer Interpretability Beyond Attention Visualization: Self-attention techniques, and specifically Transformers, are dominating the field of text processing and are becoming increasingly popular in computer vision classification tasks. In order to visualize the parts of the image that led to a certain classification, existing methods either rely on the obtained attention maps or employ heuristic propagation along the attention graph. In this work, we propose a novel way to compute relevancy for Transformer networks. The method assigns local relevance based on the Deep Taylor Decomposition principle and then propagates these relevancy scores through the layers. This propagation involves attention layers and skip connections, which challenge existing methods. Our solution is based on a specific formulation that is shown to maintain the total relevancy across layers. We benchmark our method on very recent visual Transformer networks, as well as on a text classification problem, and demonstrate a clear advantage over the existing explainability methods.\n\n9. Title: Hawq: Hessian aware quantization of neural networks with mixed-precision\n   Abstract: HAWQ: Hessian AWare Quantization of Neural Networks with Mixed-Precision: Model size and inference speed/power have become a major challenge in the deployment of Neural Networks for many applications. A promising approach to address these problems is quantization. However, uniformly quantizing a model to ultra low precision leads to significant accuracy degradation. A novel solution for this is to use mixed-precision quantization, as some parts of the network may allow lower precision as compared to other layers. However, there is no systematic way to determine the precision of different layers. A brute force approach is not feasible for deep networks, as the search space for mixed-precision is exponential in the number of layers. Another challenge is a similar factorial complexity for determining block-wise fine-tuning order when quantizing the model to a target precision. Here, we introduce Hessian AWare Quantization (HAWQ), a novel second-order quantization method to address these problems. HAWQ allows for the automatic selection of the relative quantization precision of each layer, based on the layer's Hessian spectrum. Moreover, HAWQ provides a deterministic fine-tuning order for quantizing layers, based on second-order information. We show the results of our method on Cifar-10 using ResNet20, and on ImageNet using Inception-V3, ResNet50 and SqueezeNext models. Comparing HAWQ with state-of-the-art shows that we can achieve similar/better accuracy with $8\\times$ activation compression ratio on ResNet20, as compared to DNAS~\\cite{wu2018mixed}, and up to $1\\%$ higher accuracy with up to $14\\%$ smaller models on ResNet50 and Inception-V3, compared to recently proposed methods of RVQuant~\\cite{park2018value} and HAQ~\\cite{wang2018haq}. Furthermore, we show that we can quantize SqueezeNext to just 1MB model size while achieving above $68\\%$ top1 accuracy on ImageNet.\n\nThink step-by-step about which candidate best matches the context. Summarize your reasoning, then provide the exact title that should fill the citation. Respond in JSON with fields:\n{\n  \"reasoning\": \"<your step-by-step analysis>\",\n  \"selected_title\": \"<exact candidate title>\"\n}", "response": {"reasoning": "The citation appears immediately after mentioning \"RepQ\" and the surrounding sentence refers to \"efficient mixed precision PTQ\" (post-training quantization) for Vision Transformers (ViTs). Candidate 6 is explicitly titled \"RepQ-ViT: Scale Reparameterization for Post-Training Quantization of Vision Transformers\" and its abstract describes a PTQ framework for ViTs, mixed-precision concerns, and scale reparameterization â€” matching both the RepQ name and the described purpose. Other candidates either cover general mixed-precision quantization (HAQ/HAWQ, candidates 2,3,9) or ViT quantization but under different method names (candidate 4), or cover interpretability or unrelated transformer analyses (candidates 1,5,7,8). None match the RepQ name and PTQ focus as precisely as candidate 6. Therefore candidate 6 is the correct citation to fill RepQ[CITATION].", "selected_title": "Repq-vit: Scale reparameterization for post-training quantization of vision transformers"}, "selected_title": "Repq-vit: Scale reparameterization for post-training quantization of vision transformers", "positive_title": "Repq-vit: Scale reparameterization for post-training quantization of vision transformers", "is_correct": true, "reasoning": "The citation appears immediately after mentioning \"RepQ\" and the surrounding sentence refers to \"efficient mixed precision PTQ\" (post-training quantization) for Vision Transformers (ViTs). Candidate 6 is explicitly titled \"RepQ-ViT: Scale Reparameterization for Post-Training Quantization of Vision Transformers\" and its abstract describes a PTQ framework for ViTs, mixed-precision concerns, and scale reparameterization â€” matching both the RepQ name and the described purpose. Other candidates either cover general mixed-precision quantization (HAQ/HAWQ, candidates 2,3,9) or ViT quantization but under different method names (candidate 4), or cover interpretability or unrelated transformer analyses (candidates 1,5,7,8). None match the RepQ name and PTQ focus as precisely as candidate 6. Therefore candidate 6 is the correct citation to fill RepQ[CITATION]."}
{"example_id": "blind_5482", "prompt": "You are an expert citation retrieval system as described in the paper \"Multi-Agent System for Reliable Citation Retrieval\".\nYour goal is to autonomously retrieve, verify, and recommend academic references given a query or document excerpt.\n\nTask:\nGiven a citation context from a scientific paper (where a citation is missing), identify the correct paper from a list of candidates.\nAnalyze the context to understand the specific claim, method, or result being cited.\nThen, evaluate each candidate paper to see if it matches the context.\nFinally, select the best matching paper.\n\nCitation Context:\noned on given human poses. Text2Human proposes to generate human images conditioned on texts and poses. We focus on human video generation. For the human content manipulation, pose transfer[CITATION] is a popular topic. Pose transfer deals with video data. The task is to transfer the poses from the reference videos to the source video. Chan \\etal take human poses as inputs and the desi\n\nCandidate Papers:\n1. Title: Phenaki: Variable length video generation from open domain textual description\n   Abstract: Phenaki: Variable Length Video Generation From Open Domain Textual Description: We present Phenaki, a model capable of realistic video synthesis, given a sequence of textual prompts. Generating videos from text is particularly challenging due to the computational cost, limited quantities of high quality text-video data and variable length of videos. To address these issues, we introduce a new model for learning video representation which compresses the video to a small representation of discrete tokens. This tokenizer uses causal attention in time, which allows it to work with variable-length videos. To generate video tokens from text we are using a bidirectional masked transformer conditioned on pre-computed text tokens. The generated video tokens are subsequently de-tokenized to create the actual video. To address data issues, we demonstrate how joint training on a large corpus of image-text pairs as well as a smaller number of video-text examples can result in generalization beyond what is available in the video datasets. Compared to the previous video generation methods, Phenaki can generate arbitrary long videos conditioned on a sequence of prompts (i.e. time variable text or a story) in open domain. To the best of our knowledge, this is the first time a paper studies generating videos from time variable prompts. In addition, compared to the per-frame baselines, the proposed video encoder-decoder computes fewer tokens per video but results in better spatio-temporal consistency.\n\n2. Title: Liquid warping gan: A unified framework for human motion imitation, appearance transfer and novel view synthesis\n   Abstract: Liquid Warping GAN: A Unified Framework for Human Motion Imitation, Appearance Transfer and Novel View Synthesis: We tackle the human motion imitation, appearance transfer, and novel view synthesis within a unified framework, which means that the model once being trained can be used to handle all these tasks. The existing task-specific methods mainly use 2D keypoints (pose) to estimate the human body structure. However, they only expresses the position information with no abilities to characterize the personalized shape of the individual person and model the limbs rotations. In this paper, we propose to use a 3D body mesh recovery module to disentangle the pose and shape, which can not only model the joint location and rotation but also characterize the personalized body shape. To preserve the source information, such as texture, style, color, and face identity, we propose a Liquid Warping GAN with Liquid Warping Block (LWB) that propagates the source information in both image and feature spaces, and synthesizes an image with respect to the reference. Specifically, the source features are extracted by a denoising convolutional auto-encoder for characterizing the source identity well. Furthermore, our proposed method is able to support a more flexible warping from multiple sources. In addition, we build a new dataset, namely Impersonator (iPER) dataset, for the evaluation of human motion imitation, appearance transfer, and novel view synthesis. Extensive experiments demonstrate the effectiveness of our method in several aspects, such as robustness in occlusion case and preserving face identity, shape consistency and clothes details. All codes and datasets are available on https://svip-lab.github.io/project/impersonator.html\n\n3. Title: Generative adversarial networks\n   Abstract: Generative Adversarial Networks: We propose a new framework for estimating generative models via an adversarial process, in which we simultaneously train two models: a generative model G that captures the data distribution, and a discriminative model D that estimates the probability that a sample came from the training data rather than G. The training procedure for G is to maximize the probability of D making a mistake. This framework corresponds to a minimax two-player game. In the space of arbitrary functions G and D, a unique solution exists, with G recovering the training data distribution and D equal to 1/2 everywhere. In the case where G and D are defined by multilayer perceptrons, the entire system can be trained with backpropagation. There is no need for any Markov chains or unrolled approximate inference networks during either training or generation of samples. Experiments demonstrate the potential of the framework through qualitative and quantitative evaluation of the generated samples.\n\n4. Title: Make-a-video: Text-to-video generation without text-video data\n   Abstract: Make-A-Video: Text-to-Video Generation without Text-Video Data: We propose Make-A-Video -- an approach for directly translating the tremendous recent progress in Text-to-Image (T2I) generation to Text-to-Video (T2V). Our intuition is simple: learn what the world looks like and how it is described from paired text-image data, and learn how the world moves from unsupervised video footage. Make-A-Video has three advantages: (1) it accelerates training of the T2V model (it does not need to learn visual and multimodal representations from scratch), (2) it does not require paired text-video data, and (3) the generated videos inherit the vastness (diversity in aesthetic, fantastical depictions, etc.) of today's image generation models. We design a simple yet effective way to build on T2I models with novel and effective spatial-temporal modules. First, we decompose the full temporal U-Net and attention tensors and approximate them in space and time. Second, we design a spatial temporal pipeline to generate high resolution and frame rate videos with a video decoder, interpolation model and two super resolution models that can enable various applications besides T2V. In all aspects, spatial and temporal resolution, faithfulness to text, and quality, Make-A-Video sets the new state-of-the-art in text-to-video generation, as determined by both qualitative and quantitative measures.\n\n5. Title: High-resolution image synthesis with latent diffusion models\n   Abstract: High-Resolution Image Synthesis with Latent Diffusion Models: By decomposing the image formation process into a sequential application of denoising autoencoders, diffusion models (DMs) achieve state-of-the-art synthesis results on image data and beyond. Additionally, their formulation allows for a guiding mechanism to control the image generation process without retraining. However, since these models typically operate directly in pixel space, optimization of powerful DMs often consumes hundreds of GPU days and inference is expensive due to sequential evaluations. To enable DM training on limited computational resources while retaining their quality and flexibility, we apply them in the latent space of powerful pretrained autoencoders. In contrast to previous work, training diffusion models on such a representation allows for the first time to reach a near-optimal point between complexity reduction and detail preservation, greatly boosting visual fidelity. By introducing cross-attention layers into the model architecture, we turn diffusion models into powerful and flexible generators for general conditioning inputs such as text or bounding boxes and high-resolution synthesis becomes possible in a convolutional manner. Our latent diffusion models (LDMs) achieve a new state of the art for image inpainting and highly competitive performance on various tasks, including unconditional image generation, semantic scene synthesis, and super-resolution, while significantly reducing computational requirements compared to pixel-based DMs. Code is available at https://github.com/CompVis/latent-diffusion .\n\n6. Title: Photorealistic Text-to-Image Diffusion Models with Deep Language Understanding\n   Abstract: Photorealistic Text-to-Image Diffusion Models with Deep Language Understanding: We present Imagen, a text-to-image diffusion model with an unprecedented degree of photorealism and a deep level of language understanding. Imagen builds on the power of large transformer language models in understanding text and hinges on the strength of diffusion models in high-fidelity image generation. Our key discovery is that generic large language models (e.g. T5), pretrained on text-only corpora, are surprisingly effective at encoding text for image synthesis: increasing the size of the language model in Imagen boosts both sample fidelity and image-text alignment much more than increasing the size of the image diffusion model. Imagen achieves a new state-of-the-art FID score of 7.27 on the COCO dataset, without ever training on COCO, and human raters find Imagen samples to be on par with the COCO data itself in image-text alignment. To assess text-to-image models in greater depth, we introduce DrawBench, a comprehensive and challenging benchmark for text-to-image models. With DrawBench, we compare Imagen with recent methods including VQ-GAN+CLIP, Latent Diffusion Models, and DALL-E 2, and find that human raters prefer Imagen over other models in side-by-side comparisons, both in terms of sample quality and image-text alignment. See https://imagen.research.google/ for an overview of the results.\n\n7. Title: Denoising diffusion probabilistic models\n   Abstract: Denoising Diffusion Probabilistic Models: We present high quality image synthesis results using diffusion probabilistic models, a class of latent variable models inspired by considerations from nonequilibrium thermodynamics. Our best results are obtained by training on a weighted variational bound designed according to a novel connection between diffusion probabilistic models and denoising score matching with Langevin dynamics, and our models naturally admit a progressive lossy decompression scheme that can be interpreted as a generalization of autoregressive decoding. On the unconditional CIFAR10 dataset, we obtain an Inception score of 9.46 and a state-of-the-art FID score of 3.17. On 256x256 LSUN, we obtain sample quality similar to ProgressiveGAN. Our implementation is available at https://github.com/hojonathanho/diffusion\n\n8. Title: Bailando: 3D Dance Generation by Actor-Critic GPT with Choreographic Memory\n   Abstract: Bailando: 3D Dance Generation by Actor-Critic GPT with Choreographic Memory: Driving 3D characters to dance following a piece of music is highly challenging due to the spatial constraints applied to poses by choreography norms. In addition, the generated dance sequence also needs to maintain temporal coherency with different music genres. To tackle these challenges, we propose a novel music-to-dance framework, Bailando, with two powerful components: 1) a choreographic memory that learns to summarize meaningful dancing units from 3D pose sequence to a quantized codebook, 2) an actor-critic Generative Pre-trained Transformer (GPT) that composes these units to a fluent dance coherent to the music. With the learned choreographic memory, dance generation is realized on the quantized units that meet high choreography standards, such that the generated dancing sequences are confined within the spatial constraints. To achieve synchronized alignment between diverse motion tempos and music beats, we introduce an actor-critic-based reinforcement learning scheme to the GPT with a newly-designed beat-align reward function. Extensive experiments on the standard benchmark demonstrate that our proposed framework achieves state-of-the-art performance both qualitatively and quantitatively. Notably, the learned choreographic memory is shown to discover human-interpretable dancing-style poses in an unsupervised manner.\n\n9. Title: Zero-shot text-to-image generation\n   Abstract: Zero-Shot Text-to-Image Generation: Text-to-image generation has traditionally focused on finding better modeling assumptions for training on a fixed dataset. These assumptions might involve complex architectures, auxiliary losses, or side information such as object part labels or segmentation masks supplied during training. We describe a simple approach for this task based on a transformer that autoregressively models the text and image tokens as a single stream of data. With sufficient data and scale, our approach is competitive with previous domain-specific models when evaluated in a zero-shot fashion.\n\n10. Title: Neural discrete representation learning\n   Abstract: Neural Discrete Representation Learning: Learning useful representations without supervision remains a key challenge in machine learning. In this paper, we propose a simple yet powerful generative model that learns such discrete representations. Our model, the Vector Quantised-Variational AutoEncoder (VQ-VAE), differs from VAEs in two key ways: the encoder network outputs discrete, rather than continuous, codes; and the prior is learnt rather than static. In order to learn a discrete latent representation, we incorporate ideas from vector quantisation (VQ). Using the VQ method allows the model to circumvent issues of \"posterior collapse\" -- where the latents are ignored when they are paired with a powerful autoregressive decoder -- typically observed in the VAE framework. Pairing these representations with an autoregressive prior, the model can generate high quality images, videos, and speech as well as doing high quality speaker conversion and unsupervised learning of phonemes, providing further evidence of the utility of the learnt representations.\n\nThink step-by-step about which candidate best matches the context. Summarize your reasoning, then provide the exact title that should fill the citation. Respond in JSON with fields:\n{\n  \"reasoning\": \"<your step-by-step analysis>\",\n  \"selected_title\": \"<exact candidate title>\"\n}", "response": {"reasoning": "The citation context refers to pose-transfer for human content manipulation in video: transferring poses from reference videos to a source video and mentions prior work that takes human poses as inputs. I examined the candidates for papers specifically about pose transfer / human motion imitation in video. Candidate 2 (Liquid Warping GAN) explicitly addresses human motion imitation, appearance transfer, and novel view synthesis using pose (keypoints and 3D mesh) and is evaluated on video datasets (iPER) â€” directly matching the described task. Other candidates are about text-to-video or general generative models (Phenaki, Make-a-Video, diffusion/VQ/GAN papers), or about 3D dance generation from music (Bailando) which is not the general pose-transfer/video appearance-transfer described. Therefore the best match is Liquid Warping GAN.", "selected_title": "Liquid warping gan: A unified framework for human motion imitation, appearance transfer and novel view synthesis"}, "selected_title": "Liquid warping gan: A unified framework for human motion imitation, appearance transfer and novel view synthesis", "positive_title": "Liquid warping gan: A unified framework for human motion imitation, appearance transfer and novel view synthesis", "is_correct": true, "reasoning": "The citation context refers to pose-transfer for human content manipulation in video: transferring poses from reference videos to a source video and mentions prior work that takes human poses as inputs. I examined the candidates for papers specifically about pose transfer / human motion imitation in video. Candidate 2 (Liquid Warping GAN) explicitly addresses human motion imitation, appearance transfer, and novel view synthesis using pose (keypoints and 3D mesh) and is evaluated on video datasets (iPER) â€” directly matching the described task. Other candidates are about text-to-video or general generative models (Phenaki, Make-a-Video, diffusion/VQ/GAN papers), or about 3D dance generation from music (Bailando) which is not the general pose-transfer/video appearance-transfer described. Therefore the best match is Liquid Warping GAN."}
{"example_id": "blind_8838", "prompt": "You are an expert citation retrieval system as described in the paper \"Multi-Agent System for Reliable Citation Retrieval\".\nYour goal is to autonomously retrieve, verify, and recommend academic references given a query or document excerpt.\n\nTask:\nGiven a citation context from a scientific paper (where a citation is missing), identify the correct paper from a list of candidates.\nAnalyze the context to understand the specific claim, method, or result being cited.\nThen, evaluate each candidate paper to see if it matches the context.\nFinally, select the best matching paper.\n\nCitation Context:\nas the dominant architecture in numerous natural language processing (NLP) tasks. The transformative impact of the transformer has been further propelled by the emergence of models like GPT[CITATION], catapulting the popularity of this architecture to new heights. Meanwhile, the versatility of transformers extends beyond NLP, encompassing diverse domains such as vision, audio,\n\nCandidate Papers:\n1. Title: Post-training quantization for vision transformer\n   Abstract: Post-Training Quantization for Vision Transformer: Recently, transformer has achieved remarkable performance on a variety of computer vision applications. Compared with mainstream convolutional neural networks, vision transformers are often of sophisticated architectures for extracting powerful feature representations, which are more difficult to be developed on mobile devices. In this paper, we present an effective post-training quantization algorithm for reducing the memory storage and computational costs of vision transformers. Basically, the quantization task can be regarded as finding the optimal low-bit quantization intervals for weights and inputs, respectively. To preserve the functionality of the attention mechanism, we introduce a ranking loss into the conventional quantization objective that aims to keep the relative order of the self-attention results after quantization. Moreover, we thoroughly analyze the relationship between quantization loss of different layers and the feature diversity, and explore a mixed-precision quantization scheme by exploiting the nuclear norm of each attention map and output feature. The effectiveness of the proposed method is verified on several benchmark models and datasets, which outperforms the state-of-the-art post-training quantization algorithms. For instance, we can obtain an 81.29\\% top-1 accuracy using DeiT-B model on ImageNet dataset with about 8-bit quantization.\n\n2. Title: Vatt: Transformers for multimodal self-supervised learning from raw video, audio and text\n   Abstract: VATT: Transformers for Multimodal Self-Supervised Learning from Raw Video, Audio and Text: We present a framework for learning multimodal representations from unlabeled data using convolution-free Transformer architectures. Specifically, our Video-Audio-Text Transformer (VATT) takes raw signals as inputs and extracts multimodal representations that are rich enough to benefit a variety of downstream tasks. We train VATT end-to-end from scratch using multimodal contrastive losses and evaluate its performance by the downstream tasks of video action recognition, audio event classification, image classification, and text-to-video retrieval. Furthermore, we study a modality-agnostic, single-backbone Transformer by sharing weights among the three modalities. We show that the convolution-free VATT outperforms state-of-the-art ConvNet-based architectures in the downstream tasks. Especially, VATT's vision Transformer achieves the top-1 accuracy of 82.1% on Kinetics-400, 83.6% on Kinetics-600, 72.7% on Kinetics-700, and 41.1% on Moments in Time, new records while avoiding supervised pre-training. Transferring to image classification leads to 78.7% top-1 accuracy on ImageNet compared to 64.7% by training the same Transformer from scratch, showing the generalizability of our model despite the domain gap between videos and images. VATT's audio Transformer also sets a new record on waveform-based audio event recognition by achieving the mAP of 39.4% on AudioSet without any supervised pre-training. VATT's source code is publicly available.\n\n3. Title: Ptq4vit: Post-training quantization for vision transformers with twin uniform quantization\n   Abstract: PTQ4ViT: Post-training quantization for vision transformers with twin uniform quantization: Quantization is one of the most effective methods to compress neural networks, which has achieved great success on convolutional neural networks (CNNs). Recently, vision transformers have demonstrated great potential in computer vision. However, previous post-training quantization methods performed not well on vision transformer, resulting in more than 1% accuracy drop even in 8-bit quantization. Therefore, we analyze the problems of quantization on vision transformers. We observe the distributions of activation values after softmax and GELU functions are quite different from the Gaussian distribution. We also observe that common quantization metrics, such as MSE and cosine distance, are inaccurate to determine the optimal scaling factor. In this paper, we propose the twin uniform quantization method to reduce the quantization error on these activation values. And we propose to use a Hessian guided metric to evaluate different scaling factors, which improves the accuracy of calibration at a small cost. To enable the fast quantization of vision transformers, we develop an efficient framework, PTQ4ViT. Experiments show the quantized vision transformers achieve near-lossless prediction accuracy (less than 0.5% drop at 8-bit quantization) on the ImageNet classification task.\n\n4. Title: Language models are few-shot learners\n   Abstract: Language Models are Few-Shot Learners: Recent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training on a large corpus of text followed by fine-tuning on a specific task. While typically task-agnostic in architecture, this method still requires task-specific fine-tuning datasets of thousands or tens of thousands of examples. By contrast, humans can generally perform a new language task from only a few examples or from simple instructions - something which current NLP systems still largely struggle to do. Here we show that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art fine-tuning approaches. Specifically, we train GPT-3, an autoregressive language model with 175 billion parameters, 10x more than any previous non-sparse language model, and test its performance in the few-shot setting. For all tasks, GPT-3 is applied without any gradient updates or fine-tuning, with tasks and few-shot demonstrations specified purely via text interaction with the model. GPT-3 achieves strong performance on many NLP datasets, including translation, question-answering, and cloze tasks, as well as several tasks that require on-the-fly reasoning or domain adaptation, such as unscrambling words, using a novel word in a sentence, or performing 3-digit arithmetic. At the same time, we also identify some datasets where GPT-3's few-shot learning still struggles, as well as some datasets where GPT-3 faces methodological issues related to training on large web corpora. Finally, we find that GPT-3 can generate samples of news articles which human evaluators have difficulty distinguishing from articles written by humans. We discuss broader societal impacts of this finding and of GPT-3 in general.\n\n5. Title: Fp8 quantization: The power of the exponent\n   Abstract: FP8 Quantization: The Power of the Exponent: When quantizing neural networks for efficient inference, low-bit integers are the go-to format for efficiency. However, low-bit floating point numbers have an extra degree of freedom, assigning some bits to work on an exponential scale instead. This paper in-depth investigates this benefit of the floating point format for neural network inference. We detail the choices that can be made for the FP8 format, including the important choice of the number of bits for the mantissa and exponent, and show analytically in which settings these choices give better performance. Then we show how these findings translate to real networks, provide an efficient implementation for FP8 simulation, and a new algorithm that enables the learning of both the scale parameters and the number of exponent bits in the FP8 format. Our chief conclusion is that when doing post-training quantization for a wide range of networks, the FP8 format is better than INT8 in terms of accuracy, and the choice of the number of exponent bits is driven by the severity of outliers in the network. We also conduct experiments with quantization-aware training where the difference in formats disappears as the network is trained to reduce the effect of outliers.\n\n6. Title: Zeroq: A novel zero shot quantization framework\n   Abstract: ZeroQ: A Novel Zero Shot Quantization Framework: Quantization is a promising approach for reducing the inference time and memory footprint of neural networks. However, most existing quantization methods require access to the original training dataset for retraining during quantization. This is often not possible for applications with sensitive or proprietary data, e.g., due to privacy and security concerns. Existing zero-shot quantization methods use different heuristics to address this, but they result in poor performance, especially when quantizing to ultra-low precision. Here, we propose ZeroQ , a novel zero-shot quantization framework to address this. ZeroQ enables mixed-precision quantization without any access to the training or validation data. This is achieved by optimizing for a Distilled Dataset, which is engineered to match the statistics of batch normalization across different layers of the network. ZeroQ supports both uniform and mixed-precision quantization. For the latter, we introduce a novel Pareto frontier based method to automatically determine the mixed-precision bit setting for all layers, with no manual search involved. We extensively test our proposed method on a diverse set of models, including ResNet18/50/152, MobileNetV2, ShuffleNet, SqueezeNext, and InceptionV3 on ImageNet, as well as RetinaNet-ResNet50 on the Microsoft COCO dataset. In particular, we show that ZeroQ can achieve 1.71\\% higher accuracy on MobileNetV2, as compared to the recently proposed DFQ method. Importantly, ZeroQ has a very low computational overhead, and it can finish the entire quantization process in less than 30s (0.5\\% of one epoch training time of ResNet50 on ImageNet). We have open-sourced the ZeroQ framework\\footnote{https://github.com/amirgholami/ZeroQ}.\n\n7. Title: Integer or Floating Point? New Outlooks for Low-Bit Quantization on Large Language Models\n   Abstract: Integer or Floating Point? New Outlooks for Low-Bit Quantization on Large Language Models: Efficient deployment of large language models (LLMs) necessitates low-bit quantization to minimize model size and inference cost. While low-bit integer formats (e.g., INT8/INT4) have been the conventional choice, emerging low-bit floating-point formats (e.g., FP8/FP4) offer a compelling alternative and are gaining support from cutting-edge hardware, such as NVIDIA's H100 GPU. However, the superiority of low-bit INT versus FP formats for quantization on LLMs remains unclear. In this study, we conduct a comparative analysis of INT and FP quantization with the same bit-width, revealing that the optimal quantization format varies across different layers due to the complexity and diversity of tensor distribution. Consequently, we advocate the Mixture of Formats Quantization (MoFQ), which selects the optimal format on a layer-wise basis. This simple yet effective approach achieves state-of-the-art results in both weight-only (W-only) and weight-activation (WA) post-training quantization scenarios when tested on LLaMA across various tasks. In 4-bit W-only quantization, MoFQ surpasses GPTQ without complex hyperparameter tuning and with an order of magnitude faster quantization speed. While in 8-bit WA quantization, MoFQ significantly outperforms INT/FP-only methods, achieving performance close to the full precision model. Notably, MoFQ incurs no hardware overhead compared to INT/FP-only quantization, as the bit-width remains unchanged.\n\n8. Title: Smoothquant: Accurate and efficient post-training quantization for large language models\n   Abstract: SmoothQuant: Accurate and Efficient Post-Training Quantization for Large Language Models: Large language models (LLMs) show excellent performance but are compute- and memory-intensive. Quantization can reduce memory and accelerate inference. However, existing methods cannot maintain accuracy and hardware efficiency at the same time. We propose SmoothQuant, a training-free, accuracy-preserving, and general-purpose post-training quantization (PTQ) solution to enable 8-bit weight, 8-bit activation (W8A8) quantization for LLMs. Based on the fact that weights are easy to quantize while activations are not, SmoothQuant smooths the activation outliers by offline migrating the quantization difficulty from activations to weights with a mathematically equivalent transformation. SmoothQuant enables an INT8 quantization of both weights and activations for all the matrix multiplications in LLMs, including OPT, BLOOM, GLM, MT-NLG, Llama-1/2, Falcon, Mistral, and Mixtral models. We demonstrate up to 1.56x speedup and 2x memory reduction for LLMs with negligible loss in accuracy. SmoothQuant enables serving 530B LLM within a single node. Our work offers a turn-key solution that reduces hardware costs and democratizes LLMs. Code is available at https://github.com/mit-han-lab/smoothquant.\n\n9. Title: Data-Free Quantization Through Weight Equalization and Bias Correction\n   Abstract: Data-Free Quantization Through Weight Equalization and Bias Correction: We introduce a data-free quantization method for deep neural networks that does not require fine-tuning or hyperparameter selection. It achieves near-original model performance on common computer vision architectures and tasks. 8-bit fixed-point quantization is essential for efficient inference on modern deep learning hardware. However, quantizing models to run in 8-bit is a non-trivial task, frequently leading to either significant performance reduction or engineering time spent on training a network to be amenable to quantization. Our approach relies on equalizing the weight ranges in the network by making use of a scale-equivariance property of activation functions. In addition the method corrects biases in the error that are introduced during quantization. This improves quantization accuracy performance, and can be applied to many common computer vision architectures with a straight forward API call. For common architectures, such as the MobileNet family, we achieve state-of-the-art quantized model performance. We further show that the method also extends to other computer vision architectures and tasks such as semantic segmentation and object detection.\n\n10. Title: Understanding and Overcoming the Challenges of Efficient Transformer Quantization\n   Abstract: Understanding and Overcoming the Challenges of Efficient Transformer Quantization: Transformer-based architectures have become the de-facto standard models for a wide range of Natural Language Processing tasks. However, their memory footprint and high latency are prohibitive for efficient deployment and inference on resource-limited devices. In this work, we explore quantization for transformers. We show that transformers have unique quantization challenges -- namely, high dynamic activation ranges that are difficult to represent with a low bit fixed-point format. We establish that these activations contain structured outliers in the residual connections that encourage specific attention patterns, such as attending to the special separator token. To combat these challenges, we present three solutions based on post-training quantization and quantization-aware training, each with a different set of compromises for accuracy, model size, and ease of use. In particular, we introduce a novel quantization scheme -- per-embedding-group quantization. We demonstrate the effectiveness of our methods on the GLUE benchmark using BERT, establishing state-of-the-art results for post-training quantization. Finally, we show that transformer weights and embeddings can be quantized to ultra-low bit-widths, leading to significant memory savings with a minimum accuracy loss. Our source code is available at~\\url{https://github.com/qualcomm-ai-research/transformer-quantization}.\n\nThink step-by-step about which candidate best matches the context. Summarize your reasoning, then provide the exact title that should fill the citation. Respond in JSON with fields:\n{\n  \"reasoning\": \"<your step-by-step analysis>\",\n  \"selected_title\": \"<exact candidate title>\"\n}", "response": {"reasoning": "The citation context refers to transformers becoming the dominant NLP architecture and specifically credits the emergence of models like GPT for greatly increasing the architecture's popularity. This points to a seminal GPT paper describing the large-scale, high-impact GPT family (not technical quantization or vision/audio transformer papers). Evaluating the candidates: 1, 3, 5, 6, 9, 10, 7, and 8 are focused on quantization or efficient deployment of transformers/LLMsâ€”not an origin/impact paper about GPT models. Candidate 2 (VATT) is about multimodal transformers for video/audio/text and does not serve as the cited example of GPT's transformative effect. Candidate 4 is the GPT-3 paper (describing GPT-3, its scale, few-shot learning capabilities, and broad impact on NLP), which directly matches the claim. Therefore the correct citation is the GPT paper (candidate 4).", "selected_title": "Language models are few-shot learners"}, "selected_title": "Language models are few-shot learners", "positive_title": "Language models are few-shot learners", "is_correct": true, "reasoning": "The citation context refers to transformers becoming the dominant NLP architecture and specifically credits the emergence of models like GPT for greatly increasing the architecture's popularity. This points to a seminal GPT paper describing the large-scale, high-impact GPT family (not technical quantization or vision/audio transformer papers). Evaluating the candidates: 1, 3, 5, 6, 9, 10, 7, and 8 are focused on quantization or efficient deployment of transformers/LLMsâ€”not an origin/impact paper about GPT models. Candidate 2 (VATT) is about multimodal transformers for video/audio/text and does not serve as the cited example of GPT's transformative effect. Candidate 4 is the GPT-3 paper (describing GPT-3, its scale, few-shot learning capabilities, and broad impact on NLP), which directly matches the claim. Therefore the correct citation is the GPT paper (candidate 4)."}
{"example_id": "blind_8701", "prompt": "You are an expert citation retrieval system as described in the paper \"Multi-Agent System for Reliable Citation Retrieval\".\nYour goal is to autonomously retrieve, verify, and recommend academic references given a query or document excerpt.\n\nTask:\nGiven a citation context from a scientific paper (where a citation is missing), identify the correct paper from a list of candidates.\nAnalyze the context to understand the specific claim, method, or result being cited.\nThen, evaluate each candidate paper to see if it matches the context.\nFinally, select the best matching paper.\n\nCitation Context:\nained classifier; they demonstrate that a large fraction of images are misclassified. Eykholt \\etal consider physical attacks on stop sign classifiers using images cropped from video frames[CITATION]. They successfully fool classifiers using both norm bounded perturbations, and also sparse perturbations using carefully placed stickers. Stop signs attacks on object detectors are considered in<|cit\n\nCandidate Papers:\n1. Title: Robust physical-world attacks on deep learning models\n   Abstract: Robust Physical-World Attacks on Deep Learning Models: Recent studies show that the state-of-the-art deep neural networks (DNNs) are vulnerable to adversarial examples, resulting from small-magnitude perturbations added to the input. Given that that emerging physical systems are using DNNs in safety-critical situations, adversarial examples could mislead these systems and cause dangerous situations.Therefore, understanding adversarial examples in the physical world is an important step towards developing resilient learning algorithms. We propose a general attack algorithm,Robust Physical Perturbations (RP2), to generate robust visual adversarial perturbations under different physical conditions. Using the real-world case of road sign classification, we show that adversarial examples generated using RP2 achieve high targeted misclassification rates against standard-architecture road sign classifiers in the physical world under various environmental conditions, including viewpoints. Due to the current lack of a standardized testing method, we propose a two-stage evaluation methodology for robust physical adversarial examples consisting of lab and field tests. Using this methodology, we evaluate the efficacy of physical adversarial manipulations on real objects. Witha perturbation in the form of only black and white stickers,we attack a real stop sign, causing targeted misclassification in 100% of the images obtained in lab settings, and in 84.8%of the captured video frames obtained on a moving vehicle(field test) for the target classifier.\n\n2. Title: Adversarial t-shirt! evading person detectors in a physical world\n   Abstract: Adversarial T-shirt! Evading Person Detectors in A Physical World: It is known that deep neural networks (DNNs) are vulnerable to adversarial attacks. The so-called physical adversarial examples deceive DNN-based decisionmakers by attaching adversarial patches to real objects. However, most of the existing works on physical adversarial attacks focus on static objects such as glass frames, stop signs and images attached to cardboard. In this work, we proposed adversarial T-shirts, a robust physical adversarial example for evading person detectors even if it could undergo non-rigid deformation due to a moving person's pose changes. To the best of our knowledge, this is the first work that models the effect of deformation for designing physical adversarial examples with respect to-rigid objects such as T-shirts. We show that the proposed method achieves74% and 57% attack success rates in the digital and physical worlds respectively against YOLOv2. In contrast, the state-of-the-art physical attack method to fool a person detector only achieves 18% attack success rate. Furthermore, by leveraging min-max optimization, we extend our method to the ensemble attack setting against two object detectors YOLO-v2 and Faster R-CNN simultaneously.\n\n3. Title: AdvHat: Real-world adversarial attack on ArcFace Face ID system\n   Abstract: AdvHat: Real-world adversarial attack on ArcFace Face ID system: In this paper we propose a novel easily reproducible technique to attack the best public Face ID system ArcFace in different shooting conditions. To create an attack, we print the rectangular paper sticker on a common color printer and put it on the hat. The adversarial sticker is prepared with a novel algorithm for off-plane transformations of the image which imitates sticker location on the hat. Such an approach confuses the state-of-the-art public Face ID model LResNet100E-IR, ArcFace@ms1m-refine-v2 and is transferable to other Face ID models.\n\n4. Title: Adversarial examples for semantic segmentation and object detection\n   Abstract: Adversarial Examples for Semantic Segmentation and Object Detection: It has been well demonstrated that adversarial examples, i.e., natural images with visually imperceptible perturbations added, generally exist for deep networks to fail on image classification. In this paper, we extend adversarial examples to semantic segmentation and object detection which are much more difficult. Our observation is that both segmentation and detection are based on classifying multiple targets on an image (e.g., the basic target is a pixel or a receptive field in segmentation, and an object proposal in detection), which inspires us to optimize a loss function over a set of pixels/proposals for generating adversarial perturbations. Based on this idea, we propose a novel algorithm named Dense Adversary Generation (DAG), which generates a large family of adversarial examples, and applies to a wide range of state-of-the-art deep networks for segmentation and detection. We also find that the adversarial perturbations can be transferred across networks with different training data, based on different architectures, and even for different recognition tasks. In particular, the transferability across networks with the same architecture is more significant than in other cases. Besides, summing up heterogeneous perturbations often leads to better transfer performance, which provides an effective method of black-box adversarial attack.\n\n5. Title: Adversarial patch\n   Abstract: Adversarial Patch: We present a method to create universal, robust, targeted adversarial image patches in the real world. The patches are universal because they can be used to attack any scene, robust because they work under a wide variety of transformations, and targeted because they can cause a classifier to output any target class. These adversarial patches can be printed, added to any scene, photographed, and presented to image classifiers; even when the patches are small, they cause the classifiers to ignore the other items in the scene and report a chosen target class.   To reproduce the results from the paper, our code is available at https://github.com/tensorflow/cleverhans/tree/master/examples/adversarial_patch\n\n6. Title: Transferable Adversarial Attacks for Image and Video Object Detection\n   Abstract: Transferable Adversarial Attacks for Image and Video Object Detection: Adversarial examples have been demonstrated to threaten many computer vision tasks including object detection. However, the existing attacking methods for object detection have two limitations: poor transferability, which denotes that the generated adversarial examples have low success rate to attack other kinds of detection methods, and high computation cost, which means that they need more time to generate an adversarial image, and therefore are difficult to deal with the video data. To address these issues, we utilize a generative mechanism to obtain the adversarial image and video. In this way, the processing time is reduced. To enhance the transferability, we destroy the feature maps extracted from the feature network, which usually constitutes the basis of object detectors. The proposed method is based on the Generative Adversarial Network (GAN) framework, where we combine the high-level class loss and low-level feature loss to jointly train the adversarial example generator. A series of experiments conducted on PASCAL VOC and ImageNet VID datasets show that our method can efficiently generate image and video adversarial examples, and more importantly, these adversarial examples have better transferability, and thus, are able to simultaneously attack two kinds of representative object detection models: proposal based models like Faster-RCNN, and regression based models like SSD.\n\n7. Title: Fooling automated surveillance cameras: adversarial patches to attack person detection\n   Abstract: Fooling automated surveillance cameras: adversarial patches to attack person detection: Adversarial attacks on machine learning models have seen increasing interest in the past years. By making only subtle changes to the input of a convolutional neural network, the output of the network can be swayed to output a completely different result. The first attacks did this by changing pixel values of an input image slightly to fool a classifier to output the wrong class. Other approaches have tried to learn \"patches\" that can be applied to an object to fool detectors and classifiers. Some of these approaches have also shown that these attacks are feasible in the real-world, i.e. by modifying an object and filming it with a video camera. However, all of these approaches target classes that contain almost no intra-class variety (e.g. stop signs). The known structure of the object is then used to generate an adversarial patch on top of it.   In this paper, we present an approach to generate adversarial patches to targets with lots of intra-class variety, namely persons. The goal is to generate a patch that is able successfully hide a person from a person detector. An attack that could for instance be used maliciously to circumvent surveillance systems, intruders can sneak around undetected by holding a small cardboard plate in front of their body aimed towards the surveillance camera.   From our results we can see that our system is able significantly lower the accuracy of a person detector. Our approach also functions well in real-life scenarios where the patch is filmed by a camera. To the best of our knowledge we are the first to attempt this kind of attack on targets with a high level of intra-class variety like persons.\n\n8. Title: Focal loss for dense object detection\n   Abstract: Focal Loss for Dense Object Detection: The highest accuracy object detectors to date are based on a two-stage approach popularized by R-CNN, where a classifier is applied to a sparse set of candidate object locations. In contrast, one-stage detectors that are applied over a regular, dense sampling of possible object locations have the potential to be faster and simpler, but have trailed the accuracy of two-stage detectors thus far. In this paper, we investigate why this is the case. We discover that the extreme foreground-background class imbalance encountered during training of dense detectors is the central cause. We propose to address this class imbalance by reshaping the standard cross entropy loss such that it down-weights the loss assigned to well-classified examples. Our novel Focal Loss focuses training on a sparse set of hard examples and prevents the vast number of easy negatives from overwhelming the detector during training. To evaluate the effectiveness of our loss, we design and train a simple dense detector we call RetinaNet. Our results show that when trained with the focal loss, RetinaNet is able to match the speed of previous one-stage detectors while surpassing the accuracy of all existing state-of-the-art two-stage detectors. Code is at: https://github.com/facebookresearch/Detectron.\n\n9. Title: Mask r-cnn\n   Abstract: Mask R-CNN: We present a conceptually simple, flexible, and general framework for object instance segmentation. Our approach efficiently detects objects in an image while simultaneously generating a high-quality segmentation mask for each instance. The method, called Mask R-CNN, extends Faster R-CNN by adding a branch for predicting an object mask in parallel with the existing branch for bounding box recognition. Mask R-CNN is simple to train and adds only a small overhead to Faster R-CNN, running at 5 fps. Moreover, Mask R-CNN is easy to generalize to other tasks, e.g., allowing us to estimate human poses in the same framework. We show top results in all three tracks of the COCO suite of challenges, including instance segmentation, bounding-box object detection, and person keypoint detection. Without bells and whistles, Mask R-CNN outperforms all existing, single-model entries on every task, including the COCO 2016 challenge winners. We hope our simple and effective approach will serve as a solid baseline and help ease future research in instance-level recognition. Code has been made available at: https://github.com/facebookresearch/Detectron\n\n10. Title: Physical adversarial examples for object detectors\n   Abstract: Physical Adversarial Examples for Object Detectors: Deep neural networks (DNNs) are vulnerable to adversarial examples-maliciously crafted inputs that cause DNNs to make incorrect predictions. Recent work has shown that these attacks generalize to the physical domain, to create perturbations on physical objects that fool image classifiers under a variety of real-world conditions. Such attacks pose a risk to deep learning models used in safety-critical cyber-physical systems. In this work, we extend physical attacks to more challenging object detection models, a broader class of deep learning algorithms widely used to detect and label multiple objects within a scene. Improving upon a previous physical attack on image classifiers, we create perturbed physical objects that are either ignored or mislabeled by object detection models. We implement a Disappearance Attack, in which we cause a Stop sign to \"disappear\" according to the detector-either by covering thesign with an adversarial Stop sign poster, or by adding adversarial stickers onto the sign. In a video recorded in a controlled lab environment, the state-of-the-art YOLOv2 detector failed to recognize these adversarial Stop signs in over 85% of the video frames. In an outdoor experiment, YOLO was fooled by the poster and sticker attacks in 72.5% and 63.5% of the video frames respectively. We also use Faster R-CNN, a different object detection model, to demonstrate the transferability of our adversarial perturbations. The created poster perturbation is able to fool Faster R-CNN in 85.9% of the video frames in a controlled lab environment, and 40.2% of the video frames in an outdoor environment. Finally, we present preliminary results with a new Creation Attack, where in innocuous physical stickers fool a model into detecting nonexistent objects.\n\nThink step-by-step about which candidate best matches the context. Summarize your reasoning, then provide the exact title that should fill the citation. Respond in JSON with fields:\n{\n  \"reasoning\": \"<your step-by-step analysis>\",\n  \"selected_title\": \"<exact candidate title>\"\n}", "response": {"reasoning": "The context cites Eykholt et al. for work on physical attacks on stop-sign classifiers using images cropped from video frames, reporting both norm-bounded perturbations and sparse sticker-based perturbations. Candidate 1's abstract describes the RP2 method by Eykholt et al., explicit experiments on stop sign classification, lab and vehicle video-frame (field) tests, and attacks with printed stickersâ€”exactly matching the described content. Candidate 10 concerns physical attacks on object detectors (disappearance attacks) rather than the stop-sign classifier experiments described here. Other candidates address different domains (person detectors, face ID, adversarial patches) or general detection/segmentation methods. Therefore Candidate 1 is the correct citation.", "selected_title": "Robust physical-world attacks on deep learning models"}, "selected_title": "Robust physical-world attacks on deep learning models", "positive_title": "Robust physical-world attacks on deep learning models", "is_correct": true, "reasoning": "The context cites Eykholt et al. for work on physical attacks on stop-sign classifiers using images cropped from video frames, reporting both norm-bounded perturbations and sparse sticker-based perturbations. Candidate 1's abstract describes the RP2 method by Eykholt et al., explicit experiments on stop sign classification, lab and vehicle video-frame (field) tests, and attacks with printed stickersâ€”exactly matching the described content. Candidate 10 concerns physical attacks on object detectors (disappearance attacks) rather than the stop-sign classifier experiments described here. Other candidates address different domains (person detectors, face ID, adversarial patches) or general detection/segmentation methods. Therefore Candidate 1 is the correct citation."}
{"example_id": "blind_19751", "prompt": "You are an expert citation retrieval system as described in the paper \"Multi-Agent System for Reliable Citation Retrieval\".\nYour goal is to autonomously retrieve, verify, and recommend academic references given a query or document excerpt.\n\nTask:\nGiven a citation context from a scientific paper (where a citation is missing), identify the correct paper from a list of candidates.\nAnalyze the context to understand the specific claim, method, or result being cited.\nThen, evaluate each candidate paper to see if it matches the context.\nFinally, select the best matching paper.\n\nCitation Context:\nrt in controlling the size and location of the generated objects. Another very recent research direction investigates \\emph{localization-controllable} diffusion models. In particular, GLIGEN[CITATION] integrates a new adapter layer into each Transformer block of a pre-trained diffusion model to achieve size and location control. Although GLIGEN can generate objects according to a user-specified si\n\nCandidate Papers:\n1. Title: Hierarchical text-conditional image generation with clip latents\n   Abstract: Hierarchical Text-Conditional Image Generation with CLIP Latents: Contrastive models like CLIP have been shown to learn robust representations of images that capture both semantics and style. To leverage these representations for image generation, we propose a two-stage model: a prior that generates a CLIP image embedding given a text caption, and a decoder that generates an image conditioned on the image embedding. We show that explicitly generating image representations improves image diversity with minimal loss in photorealism and caption similarity. Our decoders conditioned on image representations can also produce variations of an image that preserve both its semantics and style, while varying the non-essential details absent from the image representation. Moreover, the joint embedding space of CLIP enables language-guided image manipulations in a zero-shot fashion. We use diffusion models for the decoder and experiment with both autoregressive and diffusion models for the prior, finding that the latter are computationally more efficient and produce higher-quality samples.\n\n2. Title: Denoising diffusion implicit models\n   Abstract: Denoising Diffusion Implicit Models: Denoising diffusion probabilistic models (DDPMs) have achieved high quality image generation without adversarial training, yet they require simulating a Markov chain for many steps to produce a sample. To accelerate sampling, we present denoising diffusion implicit models (DDIMs), a more efficient class of iterative implicit probabilistic models with the same training procedure as DDPMs. In DDPMs, the generative process is defined as the reverse of a Markovian diffusion process. We construct a class of non-Markovian diffusion processes that lead to the same training objective, but whose reverse process can be much faster to sample from. We empirically demonstrate that DDIMs can produce high quality samples $10 \\times$ to $50 \\times$ faster in terms of wall-clock time compared to DDPMs, allow us to trade off computation for sample quality, and can perform semantically meaningful image interpolation directly in the latent space.\n\n3. Title: T2i-adapter: Learning adapters to dig out more controllable ability for text-to-image diffusion models\n   Abstract: T2I-Adapter: Learning Adapters to Dig out More Controllable Ability for Text-to-Image Diffusion Models: The incredible generative ability of large-scale text-to-image (T2I) models has demonstrated strong power of learning complex structures and meaningful semantics. However, relying solely on text prompts cannot fully take advantage of the knowledge learned by the model, especially when flexible and accurate controlling (e.g., color and structure) is needed. In this paper, we aim to ``dig out\" the capabilities that T2I models have implicitly learned, and then explicitly use them to control the generation more granularly. Specifically, we propose to learn simple and lightweight T2I-Adapters to align internal knowledge in T2I models with external control signals, while freezing the original large T2I models. In this way, we can train various adapters according to different conditions, achieving rich control and editing effects in the color and structure of the generation results. Further, the proposed T2I-Adapters have attractive properties of practical value, such as composability and generalization ability. Extensive experiments demonstrate that our T2I-Adapter has promising generation quality and a wide range of applications.\n\n4. Title: Photorealistic text-to-image diffusion models with deep language understanding\n   Abstract: Photorealistic Text-to-Image Diffusion Models with Deep Language Understanding: We present Imagen, a text-to-image diffusion model with an unprecedented degree of photorealism and a deep level of language understanding. Imagen builds on the power of large transformer language models in understanding text and hinges on the strength of diffusion models in high-fidelity image generation. Our key discovery is that generic large language models (e.g. T5), pretrained on text-only corpora, are surprisingly effective at encoding text for image synthesis: increasing the size of the language model in Imagen boosts both sample fidelity and image-text alignment much more than increasing the size of the image diffusion model. Imagen achieves a new state-of-the-art FID score of 7.27 on the COCO dataset, without ever training on COCO, and human raters find Imagen samples to be on par with the COCO data itself in image-text alignment. To assess text-to-image models in greater depth, we introduce DrawBench, a comprehensive and challenging benchmark for text-to-image models. With DrawBench, we compare Imagen with recent methods including VQ-GAN+CLIP, Latent Diffusion Models, and DALL-E 2, and find that human raters prefer Imagen over other models in side-by-side comparisons, both in terms of sample quality and image-text alignment. See https://imagen.research.google/ for an overview of the results.\n\n5. Title: Composer: Creative and controllable image synthesis with composable conditions\n   Abstract: Composer: Creative and Controllable Image Synthesis with Composable Conditions: Recent large-scale generative models learned on big data are capable of synthesizing incredible images yet suffer from limited controllability. This work offers a new generation paradigm that allows flexible control of the output image, such as spatial layout and palette, while maintaining the synthesis quality and model creativity. With compositionality as the core idea, we first decompose an image into representative factors, and then train a diffusion model with all these factors as the conditions to recompose the input. At the inference stage, the rich intermediate representations work as composable elements, leading to a huge design space (i.e., exponentially proportional to the number of decomposed factors) for customizable content creation. It is noteworthy that our approach, which we call Composer, supports various levels of conditions, such as text description as the global information, depth map and sketch as the local guidance, color histogram for low-level details, etc. Besides improving controllability, we confirm that Composer serves as a general framework and facilitates a wide range of classical generative tasks without retraining. Code and models will be made available.\n\n6. Title: Vector-quantized image modeling with improved VQGAN\n   Abstract: Vector-quantized Image Modeling with Improved VQGAN: Pretraining language models with next-token prediction on massive text corpora has delivered phenomenal zero-shot, few-shot, transfer learning and multi-tasking capabilities on both generative and discriminative language tasks. Motivated by this success, we explore a Vector-quantized Image Modeling (VIM) approach that involves pretraining a Transformer to predict rasterized image tokens autoregressively. The discrete image tokens are encoded from a learned Vision-Transformer-based VQGAN (ViT-VQGAN). We first propose multiple improvements over vanilla VQGAN from architecture to codebook learning, yielding better efficiency and reconstruction fidelity. The improved ViT-VQGAN further improves vector-quantized image modeling tasks, including unconditional, class-conditioned image generation and unsupervised representation learning. When trained on ImageNet at \\(256\\times256\\) resolution, we achieve Inception Score (IS) of 175.1 and Fr'echet Inception Distance (FID) of 4.17, a dramatic improvement over the vanilla VQGAN, which obtains 70.6 and 17.04 for IS and FID, respectively. Based on ViT-VQGAN and unsupervised pretraining, we further evaluate the pretrained Transformer by averaging intermediate features, similar to Image GPT (iGPT). This ImageNet-pretrained VIM-L significantly beats iGPT-L on linear-probe accuracy from 60.3% to 73.2% for a similar model size. VIM-L also outperforms iGPT-XL which is trained with extra web image data and larger model size.\n\n7. Title: GLIGEN: Open-Set Grounded Text-to-Image Generation\n   Abstract: GLIGEN: Open-Set Grounded Text-to-Image Generation: Large-scale text-to-image diffusion models have made amazing advances. However, the status quo is to use text input alone, which can impede controllability. In this work, we propose GLIGEN, Grounded-Language-to-Image Generation, a novel approach that builds upon and extends the functionality of existing pre-trained text-to-image diffusion models by enabling them to also be conditioned on grounding inputs. To preserve the vast concept knowledge of the pre-trained model, we freeze all of its weights and inject the grounding information into new trainable layers via a gated mechanism. Our model achieves open-world grounded text2img generation with caption and bounding box condition inputs, and the grounding ability generalizes well to novel spatial configurations and concepts. GLIGEN's zero-shot performance on COCO and LVIS outperforms that of existing supervised layout-to-image baselines by a large margin.\n\n8. Title: Deep Unsupervised Learning using Nonequilibrium Thermodynamics\n   Abstract: Deep Unsupervised Learning using Nonequilibrium Thermodynamics: A central problem in machine learning involves modeling complex data-sets using highly flexible families of probability distributions in which learning, sampling, inference, and evaluation are still analytically or computationally tractable. Here, we develop an approach that simultaneously achieves both flexibility and tractability. The essential idea, inspired by non-equilibrium statistical physics, is to systematically and slowly destroy structure in a data distribution through an iterative forward diffusion process. We then learn a reverse diffusion process that restores structure in data, yielding a highly flexible and tractable generative model of the data. This approach allows us to rapidly learn, sample from, and evaluate probabilities in deep generative models with thousands of layers or time steps, as well as to compute conditional and posterior probabilities under the learned model. We additionally release an open source reference implementation of the algorithm.\n\nThink step-by-step about which candidate best matches the context. Summarize your reasoning, then provide the exact title that should fill the citation. Respond in JSON with fields:\n{\n  \"reasoning\": \"<your step-by-step analysis>\",\n  \"selected_title\": \"<exact candidate title>\"\n}", "response": {"reasoning": "Context interpretation: The missing citation refers to a recent work on \"localization-controllable\" diffusion models that \"integrates a new adapter layer into each Transformer block of a pre-trained diffusion model to achieve size and location control.\" Key required attributes: (a) grounded/localization control (size & location), (b) injects trainable adapter layers into a frozen pre-trained diffusion model (Transformer/U-Net blocks), and (c) supports bounding-box / grounded conditioning.\n\nEvaluation of candidates:\n1. \"Hierarchical text-conditional image generation with clip latents\" â€” focuses on a two-stage CLIP-latent approach (prior + decoder) and diversity via CLIP embeddings; not about adapter layers for spatial/size control or grounded bounding-box conditioning. Does not match.\n2. \"Denoising diffusion implicit models\" â€” about DDIM sampling speed and non-Markovian processes; unrelated to localization control or adapter injections. Does not match.\n3. \"T2i-adapter: Learning adapters to dig out more controllable ability for text-to-image diffusion models\" â€” concerns learning lightweight adapters to add controllability (color/structure) to T2I models. While it involves adapters, its focus is general controllability (color/structure) rather than explicit localization/size control via per-Transformer-block grounding or bounding-box conditioning. Likely not the cited GLIGEN paper.\n4. \"Photorealistic text-to-image diffusion models with deep language understanding\" (Imagen) â€” about using large language models for text encoding and high-fidelity generation; not about adapter layers for spatial grounding. Does not match.\n5. \"Composer: Creative and controllable image synthesis with composable conditions\" â€” focuses on composable conditions (depth, sketch, color histogram) and compositional diffusion conditioning; may enable layout control but does not describe injecting adapter layers per Transformer block into a frozen pre-trained diffusion model as described. Not a direct match.\n6. \"Vector-quantized image modeling with improved VQGAN\" â€” about VQGAN improvements and autoregressive image token modeling; unrelated to diffusion-model adapters for localization. Does not match.\n7. \"GLIGEN: Open-Set Grounded Text-to-Image Generation\" â€” explicitly describes enabling pre-trained text-to-image diffusion models to be conditioned on grounding inputs by freezing the model and injecting grounding information into new trainable layers (via a gated mechanism). It supports caption + bounding box conditions and achieves spatial/size/location control and open-world grounding. This directly matches the described claim about integrating adapter layers into Transformer blocks to achieve size and location control.\n8. \"Deep Unsupervised Learning using Nonequilibrium Thermodynamics\" â€” the original diffusion model formulation (DDPM) paper; foundational but not the recent localization-controllable adapter work described. Does not match.\n\nConclusion: Candidate 7 (GLIGEN) precisely matches the described method and claim (adapter layers injected into a frozen pre-trained diffusion model to provide size/location grounding).", "selected_title": "GLIGEN: Open-Set Grounded Text-to-Image Generation"}, "selected_title": "GLIGEN: Open-Set Grounded Text-to-Image Generation", "positive_title": "GLIGEN: Open-Set Grounded Text-to-Image Generation", "is_correct": true, "reasoning": "Context interpretation: The missing citation refers to a recent work on \"localization-controllable\" diffusion models that \"integrates a new adapter layer into each Transformer block of a pre-trained diffusion model to achieve size and location control.\" Key required attributes: (a) grounded/localization control (size & location), (b) injects trainable adapter layers into a frozen pre-trained diffusion model (Transformer/U-Net blocks), and (c) supports bounding-box / grounded conditioning.\n\nEvaluation of candidates:\n1. \"Hierarchical text-conditional image generation with clip latents\" â€” focuses on a two-stage CLIP-latent approach (prior + decoder) and diversity via CLIP embeddings; not about adapter layers for spatial/size control or grounded bounding-box conditioning. Does not match.\n2. \"Denoising diffusion implicit models\" â€” about DDIM sampling speed and non-Markovian processes; unrelated to localization control or adapter injections. Does not match.\n3. \"T2i-adapter: Learning adapters to dig out more controllable ability for text-to-image diffusion models\" â€” concerns learning lightweight adapters to add controllability (color/structure) to T2I models. While it involves adapters, its focus is general controllability (color/structure) rather than explicit localization/size control via per-Transformer-block grounding or bounding-box conditioning. Likely not the cited GLIGEN paper.\n4. \"Photorealistic text-to-image diffusion models with deep language understanding\" (Imagen) â€” about using large language models for text encoding and high-fidelity generation; not about adapter layers for spatial grounding. Does not match.\n5. \"Composer: Creative and controllable image synthesis with composable conditions\" â€” focuses on composable conditions (depth, sketch, color histogram) and compositional diffusion conditioning; may enable layout control but does not describe injecting adapter layers per Transformer block into a frozen pre-trained diffusion model as described. Not a direct match.\n6. \"Vector-quantized image modeling with improved VQGAN\" â€” about VQGAN improvements and autoregressive image token modeling; unrelated to diffusion-model adapters for localization. Does not match.\n7. \"GLIGEN: Open-Set Grounded Text-to-Image Generation\" â€” explicitly describes enabling pre-trained text-to-image diffusion models to be conditioned on grounding inputs by freezing the model and injecting grounding information into new trainable layers (via a gated mechanism). It supports caption + bounding box conditions and achieves spatial/size/location control and open-world grounding. This directly matches the described claim about integrating adapter layers into Transformer blocks to achieve size and location control.\n8. \"Deep Unsupervised Learning using Nonequilibrium Thermodynamics\" â€” the original diffusion model formulation (DDPM) paper; foundational but not the recent localization-controllable adapter work described. Does not match.\n\nConclusion: Candidate 7 (GLIGEN) precisely matches the described method and claim (adapter layers injected into a frozen pre-trained diffusion model to provide size/location grounding)."}
{"example_id": "blind_6280", "prompt": "You are an expert citation retrieval system as described in the paper \"Multi-Agent System for Reliable Citation Retrieval\".\nYour goal is to autonomously retrieve, verify, and recommend academic references given a query or document excerpt.\n\nTask:\nGiven a citation context from a scientific paper (where a citation is missing), identify the correct paper from a list of candidates.\nAnalyze the context to understand the specific claim, method, or result being cited.\nThen, evaluate each candidate paper to see if it matches the context.\nFinally, select the best matching paper.\n\nCitation Context:\nntered at the centroid of each class with a certain $\\ell_p$ radius; 2) the convexity of samples in the high dimensional latent space. We evaluate our metric on the AudioSet and Kinetic-Sounds dataset[CITATION]. The results indicate that multi-modal models are only more robust measured by class-wise metrics for a limited number of classes. We also observe the point-wise robustness of classification results\n\nCandidate Papers:\n1. Title: Certified adversarial robustness via randomized smoothing\n   Abstract: Certified Adversarial Robustness via Randomized Smoothing: We show how to turn any classifier that classifies well under Gaussian noise into a new classifier that is certifiably robust to adversarial perturbations under the $\\ell_2$ norm. This \"randomized smoothing\" technique has been proposed recently in the literature, but existing guarantees are loose. We prove a tight robustness guarantee in $\\ell_2$ norm for smoothing with Gaussian noise. We use randomized smoothing to obtain an ImageNet classifier with e.g. a certified top-1 accuracy of 49% under adversarial perturbations with $\\ell_2$ norm less than 0.5 (=127/255). No certified defense has been shown feasible on ImageNet except for smoothing. On smaller-scale datasets where competing approaches to certified $\\ell_2$ robustness are viable, smoothing delivers higher certified accuracies. Our strong empirical results suggest that randomized smoothing is a promising direction for future research into adversarially robust classification. Code and models are available at http://github.com/locuslab/smoothing.\n\n2. Title: Look, listen and learn\n   Abstract: Look, Listen and Learn: We consider the question: what can be learnt by looking at and listening to a large number of unlabelled videos? There is a valuable, but so far untapped, source of information contained in the video itself -- the correspondence between the visual and the audio streams, and we introduce a novel \"Audio-Visual Correspondence\" learning task that makes use of this. Training visual and audio networks from scratch, without any additional supervision other than the raw unconstrained videos themselves, is shown to successfully solve this task, and, more interestingly, result in good visual and audio representations. These features set the new state-of-the-art on two sound classification benchmarks, and perform on par with the state-of-the-art self-supervised approaches on ImageNet classification. We also demonstrate that the network is able to localize objects in both modalities, as well as perform fine-grained recognition tasks.\n\n3. Title: Provable defenses against adversarial examples via the convex outer adversarial polytope\n   Abstract: Provable defenses against adversarial examples via the convex outer adversarial polytope: We propose a method to learn deep ReLU-based classifiers that are provably robust against norm-bounded adversarial perturbations on the training data. For previously unseen examples, the approach is guaranteed to detect all adversarial examples, though it may flag some non-adversarial examples as well. The basic idea is to consider a convex outer approximation of the set of activations reachable through a norm-bounded perturbation, and we develop a robust optimization procedure that minimizes the worst case loss over this outer region (via a linear program). Crucially, we show that the dual problem to this linear program can be represented itself as a deep network similar to the backpropagation network, leading to very efficient optimization approaches that produce guaranteed bounds on the robust loss. The end result is that by executing a few more forward and backward passes through a slightly modified version of the original network (though possibly with much larger batch sizes), we can learn a classifier that is provably robust to any norm-bounded adversarial attack. We illustrate the approach on a number of tasks to train classifiers with robust adversarial guarantees (e.g. for MNIST, we produce a convolutional classifier that provably has less than 5.8% test error for any adversarial attack with bounded $\\ell_\\infty$ norm less than $\\epsilon = 0.1$), and code for all experiments in the paper is available at https://github.com/locuslab/convex_adversarial.\n\n4. Title: On evaluating adversarial robustness\n   Abstract: On Evaluating Adversarial Robustness: Correctly evaluating defenses against adversarial examples has proven to be extremely difficult. Despite the significant amount of recent work attempting to design defenses that withstand adaptive attacks, few have succeeded; most papers that propose defenses are quickly shown to be incorrect.   We believe a large contributing factor is the difficulty of performing security evaluations. In this paper, we discuss the methodological foundations, review commonly accepted best practices, and suggest new methods for evaluating defenses to adversarial examples. We hope that both researchers developing defenses as well as readers and reviewers who wish to understand the completeness of an evaluation consider our advice in order to avoid common pitfalls.\n\n5. Title: Towards deep learning models resistant to adversarial attacks\n   Abstract: Towards Deep Learning Models Resistant to Adversarial Attacks: Recent work has demonstrated that deep neural networks are vulnerable to adversarial examples---inputs that are almost indistinguishable from natural data and yet classified incorrectly by the network. In fact, some of the latest findings suggest that the existence of adversarial attacks may be an inherent weakness of deep learning models. To address this problem, we study the adversarial robustness of neural networks through the lens of robust optimization. This approach provides us with a broad and unifying view on much of the prior work on this topic. Its principled nature also enables us to identify methods for both training and attacking neural networks that are reliable and, in a certain sense, universal. In particular, they specify a concrete security guarantee that would protect against any adversary. These methods let us train networks with significantly improved resistance to a wide range of adversarial attacks. They also suggest the notion of security against a first-order adversary as a natural and broad security guarantee. We believe that robustness against such well-defined classes of adversaries is an important stepping stone towards fully resistant deep learning models. Code and pre-trained models are available at https://github.com/MadryLab/mnist_challenge and https://github.com/MadryLab/cifar10_challenge.\n\n6. Title: Universal adversarial perturbations\n   Abstract: Universal adversarial perturbations: Given a state-of-the-art deep neural network classifier, we show the existence of a universal (image-agnostic) and very small perturbation vector that causes natural images to be misclassified with high probability. We propose a systematic algorithm for computing universal perturbations, and show that state-of-the-art deep neural networks are highly vulnerable to such perturbations, albeit being quasi-imperceptible to the human eye. We further empirically analyze these universal perturbations and show, in particular, that they generalize very well across neural networks. The surprising existence of universal perturbations reveals important geometric correlations among the high-dimensional decision boundary of classifiers. It further outlines potential security breaches with the existence of single directions in the input space that adversaries can possibly exploit to break a classifier on most natural images.\n\n7. Title: Empir: Ensembles of mixed precision deep networks for increased robustness against adversarial attacks\n   Abstract: EMPIR: Ensembles of Mixed Precision Deep Networks for Increased Robustness against Adversarial Attacks: Ensuring robustness of Deep Neural Networks (DNNs) is crucial to their adoption in safety-critical applications such as self-driving cars, drones, and healthcare. Notably, DNNs are vulnerable to adversarial attacks in which small input perturbations can produce catastrophic misclassifications. In this work, we propose EMPIR, ensembles of quantized DNN models with different numerical precisions, as a new approach to increase robustness against adversarial attacks. EMPIR is based on the observation that quantized neural networks often demonstrate much higher robustness to adversarial attacks than full precision networks, but at the cost of a substantial loss in accuracy on the original (unperturbed) inputs. EMPIR overcomes this limitation to achieve the 'best of both worlds', i.e., the higher unperturbed accuracies of the full precision models combined with the higher robustness of the low precision models, by composing them in an ensemble. Further, as low precision DNN models have significantly lower computational and storage requirements than full precision models, EMPIR models only incur modest compute and memory overheads compared to a single full-precision model (<25% in our evaluations). We evaluate EMPIR across a suite of DNNs for 3 different image recognition tasks (MNIST, CIFAR-10 and ImageNet) and under 4 different adversarial attacks. Our results indicate that EMPIR boosts the average adversarial accuracies by 42.6%, 15.2% and 10.5% for the DNN models trained on the MNIST, CIFAR-10 and ImageNet datasets respectively, when compared to single full-precision models, without sacrificing accuracy on the unperturbed inputs.\n\n8. Title: Ucf101: A dataset of 101 human actions classes from videos in the wild\n   Abstract: UCF101: A Dataset of 101 Human Actions Classes From Videos in The Wild: We introduce UCF101 which is currently the largest dataset of human actions. It consists of 101 action classes, over 13k clips and 27 hours of video data. The database consists of realistic user uploaded videos containing camera motion and cluttered background. Additionally, we provide baseline action recognition results on this new dataset using standard bag of words approach with overall performance of 44.5%. To the best of our knowledge, UCF101 is currently the most challenging dataset of actions due to its large number of classes, large number of clips and also unconstrained nature of such clips.\n\nThink step-by-step about which candidate best matches the context. Summarize your reasoning, then provide the exact title that should fill the citation. Respond in JSON with fields:\n{\n  \"reasoning\": \"<your step-by-step analysis>\",\n  \"selected_title\": \"<exact candidate title>\"\n}", "response": {"reasoning": "The citation context mentions evaluation on AudioSet and Kinetic-Sounds and discusses multi-modal models and audio-visual classification robustness. Among the candidates, only paper 2 ('Look, listen and learn') is about learning from audio-visual video data and evaluates audio/sound classification performance. All other candidates focus on adversarial robustness or unrelated image/action datasets (UCF101) and do not fit the audio-visual/multi-modal dataset reference. Therefore the best match is candidate 2.", "selected_title": "Look, listen and learn"}, "selected_title": "Look, listen and learn", "positive_title": "Look, listen and learn", "is_correct": true, "reasoning": "The citation context mentions evaluation on AudioSet and Kinetic-Sounds and discusses multi-modal models and audio-visual classification robustness. Among the candidates, only paper 2 ('Look, listen and learn') is about learning from audio-visual video data and evaluates audio/sound classification performance. All other candidates focus on adversarial robustness or unrelated image/action datasets (UCF101) and do not fit the audio-visual/multi-modal dataset reference. Therefore the best match is candidate 2."}
{"example_id": "blind_9124", "prompt": "You are an expert citation retrieval system as described in the paper \"Multi-Agent System for Reliable Citation Retrieval\".\nYour goal is to autonomously retrieve, verify, and recommend academic references given a query or document excerpt.\n\nTask:\nGiven a citation context from a scientific paper (where a citation is missing), identify the correct paper from a list of candidates.\nAnalyze the context to understand the specific claim, method, or result being cited.\nThen, evaluate each candidate paper to see if it matches the context.\nFinally, select the best matching paper.\n\nCitation Context:\nrefore, most of the existing effective defenses are conducted in the inference phase. For example, motivated by classic detection and preprocessing defenses in the vision domain,[CITATION] explores how these methods can be directly transferred to this scenario and propose perplexity-based detection, paraphrasing as baseline defenses for GCG attack. Besides, prop\n\nCandidate Papers:\n1. Title: Certifying LLM Safety against Adversarial Prompting\n   Abstract: Certifying LLM Safety against Adversarial Prompting: Large language models (LLMs) are vulnerable to adversarial attacks that add malicious tokens to an input prompt to bypass the safety guardrails of an LLM and cause it to produce harmful content. In this work, we introduce erase-and-check, the first framework for defending against adversarial prompts with certifiable safety guarantees. Given a prompt, our procedure erases tokens individually and inspects the resulting subsequences using a safety filter. Our safety certificate guarantees that harmful prompts are not mislabeled as safe due to an adversarial attack up to a certain size. We implement the safety filter in two ways, using Llama 2 and DistilBERT, and compare the performance of erase-and-check for the two cases. We defend against three attack modes: i) adversarial suffix, where an adversarial sequence is appended at the end of a harmful prompt; ii) adversarial insertion, where the adversarial sequence is inserted anywhere in the middle of the prompt; and iii) adversarial infusion, where adversarial tokens are inserted at arbitrary positions in the prompt, not necessarily as a contiguous block. Our experimental results demonstrate that this procedure can obtain strong certified safety guarantees on harmful prompts while maintaining good empirical performance on safe prompts. Additionally, we propose three efficient empirical defenses: i) RandEC, a randomized subsampling version of erase-and-check; ii) GreedyEC, which greedily erases tokens that maximize the softmax score of the harmful class; and iii) GradEC, which uses gradient information to optimize tokens to erase. We demonstrate their effectiveness against adversarial prompts generated by the Greedy Coordinate Gradient (GCG) attack algorithm. The code for our experiments is available at https://github.com/aounon/certified-llm-safety.\n\n2. Title: Baseline Defenses for Adversarial Attacks Against Aligned Language Models\n   Abstract: Baseline Defenses for Adversarial Attacks Against Aligned Language Models: As Large Language Models quickly become ubiquitous, it becomes critical to understand their security vulnerabilities. Recent work shows that text optimizers can produce jailbreaking prompts that bypass moderation and alignment. Drawing from the rich body of work on adversarial machine learning, we approach these attacks with three questions: What threat models are practically useful in this domain? How do baseline defense techniques perform in this new domain? How does LLM security differ from computer vision?   We evaluate several baseline defense strategies against leading adversarial attacks on LLMs, discussing the various settings in which each is feasible and effective. Particularly, we look at three types of defenses: detection (perplexity based), input preprocessing (paraphrase and retokenization), and adversarial training. We discuss white-box and gray-box settings and discuss the robustness-performance trade-off for each of the defenses considered. We find that the weakness of existing discrete optimizers for text, combined with the relatively high costs of optimization, makes standard adaptive attacks more challenging for LLMs. Future research will be needed to uncover whether more powerful optimizers can be developed, or whether the strength of filtering and preprocessing defenses is greater in the LLMs domain than it has been in computer vision.\n\n3. Title: On the (Statistical) Detection of Adversarial Examples\n   Abstract: On the (Statistical) Detection of Adversarial Examples: Machine Learning (ML) models are applied in a variety of tasks such as network intrusion detection or Malware classification. Yet, these models are vulnerable to a class of malicious inputs known as adversarial examples. These are slightly perturbed inputs that are classified incorrectly by the ML model. The mitigation of these adversarial inputs remains an open problem. As a step towards understanding adversarial examples, we show that they are not drawn from the same distribution than the original data, and can thus be detected using statistical tests. Using thus knowledge, we introduce a complimentary approach to identify specific inputs that are adversarial. Specifically, we augment our ML model with an additional output, in which the model is trained to classify all adversarial inputs. We evaluate our approach on multiple adversarial example crafting methods (including the fast gradient sign and saliency map methods) with several datasets. The statistical test flags sample sets containing adversarial inputs confidently at sample sizes between 10 and 100 data points. Furthermore, our augmented model either detects adversarial examples as outliers with high accuracy (> 80%) or increases the adversary's cost - the perturbation added - by more than 150%. In this way, we show that statistical properties of adversarial examples are essential to their detection.\n\n4. Title: Gradient-based Adversarial Attacks against Text Transformers\n   Abstract: Gradient-based Adversarial Attacks against Text Transformers: We propose the first general-purpose gradient-based attack against transformer models. Instead of searching for a single adversarial example, we search for a distribution of adversarial examples parameterized by a continuous-valued matrix, hence enabling gradient-based optimization. We empirically demonstrate that our white-box attack attains state-of-the-art attack performance on a variety of natural language tasks. Furthermore, we show that a powerful black-box transfer attack, enabled by sampling from the adversarial distribution, matches or exceeds existing methods, while only requiring hard-label outputs.\n\n5. Title: Training language models to follow instructions with human feedback\n   Abstract: Training language models to follow instructions with human feedback: Making language models bigger does not inherently make them better at following a user's intent. For example, large language models can generate outputs that are untruthful, toxic, or simply not helpful to the user. In other words, these models are not aligned with their users. In this paper, we show an avenue for aligning language models with user intent on a wide range of tasks by fine-tuning with human feedback. Starting with a set of labeler-written prompts and prompts submitted through the OpenAI API, we collect a dataset of labeler demonstrations of the desired model behavior, which we use to fine-tune GPT-3 using supervised learning. We then collect a dataset of rankings of model outputs, which we use to further fine-tune this supervised model using reinforcement learning from human feedback. We call the resulting models InstructGPT. In human evaluations on our prompt distribution, outputs from the 1.3B parameter InstructGPT model are preferred to outputs from the 175B GPT-3, despite having 100x fewer parameters. Moreover, InstructGPT models show improvements in truthfulness and reductions in toxic output generation while having minimal performance regressions on public NLP datasets. Even though InstructGPT still makes simple mistakes, our results show that fine-tuning with human feedback is a promising direction for aligning language models with human intent.\n\n6. Title: Judging LLM-as-a-judge with MT-Bench and Chatbot Arena\n   Abstract: Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena: Evaluating large language model (LLM) based chat assistants is challenging due to their broad capabilities and the inadequacy of existing benchmarks in measuring human preferences. To address this, we explore using strong LLMs as judges to evaluate these models on more open-ended questions. We examine the usage and limitations of LLM-as-a-judge, including position, verbosity, and self-enhancement biases, as well as limited reasoning ability, and propose solutions to mitigate some of them. We then verify the agreement between LLM judges and human preferences by introducing two benchmarks: MT-bench, a multi-turn question set; and Chatbot Arena, a crowdsourced battle platform. Our results reveal that strong LLM judges like GPT-4 can match both controlled and crowdsourced human preferences well, achieving over 80% agreement, the same level of agreement between humans. Hence, LLM-as-a-judge is a scalable and explainable way to approximate human preferences, which are otherwise very expensive to obtain. Additionally, we show our benchmark and traditional benchmarks complement each other by evaluating several variants of LLaMA and Vicuna. The MT-bench questions, 3K expert votes, and 30K conversations with human preferences are publicly available at https://github.com/lm-sys/FastChat/tree/main/fastchat/llm_judge.\n\n7. Title: RAIN: Your Language Models Can Align Themselves without Finetuning\n   Abstract: RAIN: Your Language Models Can Align Themselves without Finetuning: Large language models (LLMs) often demonstrate inconsistencies with human preferences. Previous research typically gathered human preference data and then aligned the pre-trained models using reinforcement learning or instruction tuning, a.k.a. the finetuning step. In contrast, aligning frozen LLMs without requiring alignment data is more appealing. This work explores the potential of the latter setting. We discover that by integrating self-evaluation and rewind mechanisms, unaligned LLMs can directly produce responses consistent with human preferences via self-boosting. We introduce a novel inference method, Rewindable Auto-regressive INference (RAIN), that allows pre-trained LLMs to evaluate their own generation and use the evaluation results to guide rewind and generation for AI safety. Notably, RAIN operates without the need of extra data for model alignment and abstains from any training, gradient computation, or parameter updates. Experimental results evaluated by GPT-4 and humans demonstrate the effectiveness of RAIN: on the HH dataset, RAIN improves the harmlessness rate of LLaMA 30B from 82% of vanilla inference to 97%, while maintaining the helpfulness rate. On the TruthfulQA dataset, RAIN improves the truthfulness of the already-well-aligned LLaMA-2-chat 13B model by 5%.\n\n8. Title: Universal and Transferable Adversarial Attacks on Aligned Language Models\n   Abstract: Universal and Transferable Adversarial Attacks on Aligned Language Models: Because \"out-of-the-box\" large language models are capable of generating a great deal of objectionable content, recent work has focused on aligning these models in an attempt to prevent undesirable generation. While there has been some success at circumventing these measures -- so-called \"jailbreaks\" against LLMs -- these attacks have required significant human ingenuity and are brittle in practice. In this paper, we propose a simple and effective attack method that causes aligned language models to generate objectionable behaviors. Specifically, our approach finds a suffix that, when attached to a wide range of queries for an LLM to produce objectionable content, aims to maximize the probability that the model produces an affirmative response (rather than refusing to answer). However, instead of relying on manual engineering, our approach automatically produces these adversarial suffixes by a combination of greedy and gradient-based search techniques, and also improves over past automatic prompt generation methods.   Surprisingly, we find that the adversarial prompts generated by our approach are quite transferable, including to black-box, publicly released LLMs. Specifically, we train an adversarial attack suffix on multiple prompts (i.e., queries asking for many different types of objectionable content), as well as multiple models (in our case, Vicuna-7B and 13B). When doing so, the resulting attack suffix is able to induce objectionable content in the public interfaces to ChatGPT, Bard, and Claude, as well as open source LLMs such as LLaMA-2-Chat, Pythia, Falcon, and others. In total, this work significantly advances the state-of-the-art in adversarial attacks against aligned language models, raising important questions about how such systems can be prevented from producing objectionable information. Code is available at github.com/llm-attacks/llm-attacks.\n\n9. Title: Towards deep learning models resistant to adversarial attacks\n   Abstract: Towards Deep Learning Models Resistant to Adversarial Attacks: Recent work has demonstrated that deep neural networks are vulnerable to adversarial examples---inputs that are almost indistinguishable from natural data and yet classified incorrectly by the network. In fact, some of the latest findings suggest that the existence of adversarial attacks may be an inherent weakness of deep learning models. To address this problem, we study the adversarial robustness of neural networks through the lens of robust optimization. This approach provides us with a broad and unifying view on much of the prior work on this topic. Its principled nature also enables us to identify methods for both training and attacking neural networks that are reliable and, in a certain sense, universal. In particular, they specify a concrete security guarantee that would protect against any adversary. These methods let us train networks with significantly improved resistance to a wide range of adversarial attacks. They also suggest the notion of security against a first-order adversary as a natural and broad security guarantee. We believe that robustness against such well-defined classes of adversaries is an important stepping stone towards fully resistant deep learning models. Code and pre-trained models are available at https://github.com/MadryLab/mnist_challenge and https://github.com/MadryLab/cifar10_challenge.\n\nThink step-by-step about which candidate best matches the context. Summarize your reasoning, then provide the exact title that should fill the citation. Respond in JSON with fields:\n{\n  \"reasoning\": \"<your step-by-step analysis>\",\n  \"selected_title\": \"<exact candidate title>\"\n}", "response": {"reasoning": "The citation context describes inference-time defenses transferred from vision: specifically \"perplexity-based detection\" and \"paraphrasing as baseline defenses\" evaluated against a GCG attack. I inspected the candidates: \n- Paper 2 (Baseline Defenses for Adversarial Attacks Against Aligned Language Models) explicitly studies detection (perplexity-based) and input preprocessing (paraphrase and retokenization) as baseline defenses against adversarial prompt attacks â€” a near word-for-word match to the context. \n- Paper 1 (Certifying LLM Safety...) does evaluate defenses against the Greedy Coordinate Gradient (GCG) attack, but focuses on an erase-and-check certification framework and different empirical defenses (RandEC, GreedyEC, GradEC), not on perplexity-based detection and paraphrasing/preprocessing. \n- Papers 3, 4, 8, and 9 are about general adversarial detection/attacks or attack methods and do not describe the specific perplexity- and paraphrase-based baseline defenses transferred from vision. \nTherefore Paper 2 best matches the described claim and methods in the citation context.", "selected_title": "Baseline Defenses for Adversarial Attacks Against Aligned Language Models"}, "selected_title": "Baseline Defenses for Adversarial Attacks Against Aligned Language Models", "positive_title": "Baseline Defenses for Adversarial Attacks Against Aligned Language Models", "is_correct": true, "reasoning": "The citation context describes inference-time defenses transferred from vision: specifically \"perplexity-based detection\" and \"paraphrasing as baseline defenses\" evaluated against a GCG attack. I inspected the candidates: \n- Paper 2 (Baseline Defenses for Adversarial Attacks Against Aligned Language Models) explicitly studies detection (perplexity-based) and input preprocessing (paraphrase and retokenization) as baseline defenses against adversarial prompt attacks â€” a near word-for-word match to the context. \n- Paper 1 (Certifying LLM Safety...) does evaluate defenses against the Greedy Coordinate Gradient (GCG) attack, but focuses on an erase-and-check certification framework and different empirical defenses (RandEC, GreedyEC, GradEC), not on perplexity-based detection and paraphrasing/preprocessing. \n- Papers 3, 4, 8, and 9 are about general adversarial detection/attacks or attack methods and do not describe the specific perplexity- and paraphrase-based baseline defenses transferred from vision. \nTherefore Paper 2 best matches the described claim and methods in the citation context."}
{"example_id": "blind_2317", "prompt": "You are an expert citation retrieval system as described in the paper \"Multi-Agent System for Reliable Citation Retrieval\".\nYour goal is to autonomously retrieve, verify, and recommend academic references given a query or document excerpt.\n\nTask:\nGiven a citation context from a scientific paper (where a citation is missing), identify the correct paper from a list of candidates.\nAnalyze the context to understand the specific claim, method, or result being cited.\nThen, evaluate each candidate paper to see if it matches the context.\nFinally, select the best matching paper.\n\nCitation Context:\nis precisely due to its real-time high-quality rendering capabilities that 3DGS has been applied to various domains, including autonomous driving, content generation, and 4D dynamic scenes[CITATION], among others. Despite these advancements, 3DGS still has some drawbacks, such as the storage of Gaussians and handling multi-resolution, and so on. Several works have enhanced 3DGS by improving Gau\n\nCandidate Papers:\n1. Title: 3d gaussian splatting for real-time radiance field rendering\n   Abstract: 3D Gaussian Splatting for Real-Time Radiance Field Rendering: Radiance Field methods have recently revolutionized novel-view synthesis of scenes captured with multiple photos or videos. However, achieving high visual quality still requires neural networks that are costly to train and render, while recent faster methods inevitably trade off speed for quality. For unbounded and complete scenes (rather than isolated objects) and 1080p resolution rendering, no current method can achieve real-time display rates. We introduce three key elements that allow us to achieve state-of-the-art visual quality while maintaining competitive training times and importantly allow high-quality real-time (>= 30 fps) novel-view synthesis at 1080p resolution. First, starting from sparse points produced during camera calibration, we represent the scene with 3D Gaussians that preserve desirable properties of continuous volumetric radiance fields for scene optimization while avoiding unnecessary computation in empty space; Second, we perform interleaved optimization/density control of the 3D Gaussians, notably optimizing anisotropic covariance to achieve an accurate representation of the scene; Third, we develop a fast visibility-aware rendering algorithm that supports anisotropic splatting and both accelerates training and allows realtime rendering. We demonstrate state-of-the-art visual quality and real-time rendering on several established datasets.\n\n2. Title: Mip-nerf 360: Unbounded anti-aliased neural radiance fields\n   Abstract: Mip-NeRF 360: Unbounded Anti-Aliased Neural Radiance Fields: Though neural radiance fields (NeRF) have demonstrated impressive view synthesis results on objects and small bounded regions of space, they struggle on \"unbounded\" scenes, where the camera may point in any direction and content may exist at any distance. In this setting, existing NeRF-like models often produce blurry or low-resolution renderings (due to the unbalanced detail and scale of nearby and distant objects), are slow to train, and may exhibit artifacts due to the inherent ambiguity of the task of reconstructing a large scene from a small set of images. We present an extension of mip-NeRF (a NeRF variant that addresses sampling and aliasing) that uses a non-linear scene parameterization, online distillation, and a novel distortion-based regularizer to overcome the challenges presented by unbounded scenes. Our model, which we dub \"mip-NeRF 360\" as we target scenes in which the camera rotates 360 degrees around a point, reduces mean-squared error by 57% compared to mip-NeRF, and is able to produce realistic synthesized views and detailed depth maps for highly intricate, unbounded real-world scenes.\n\n3. Title: Mega-nerf: Scalable construction of large-scale nerfs for virtual fly-throughs\n   Abstract: Mega-NeRF: Scalable Construction of Large-Scale NeRFs for Virtual Fly-Throughs: We use neural radiance fields (NeRFs) to build interactive 3D environments from large-scale visual captures spanning buildings or even multiple city blocks collected primarily from drones. In contrast to single object scenes (on which NeRFs are traditionally evaluated), our scale poses multiple challenges including (1) the need to model thousands of images with varying lighting conditions, each of which capture only a small subset of the scene, (2) prohibitively large model capacities that make it infeasible to train on a single GPU, and (3) significant challenges for fast rendering that would enable interactive fly-throughs.   To address these challenges, we begin by analyzing visibility statistics for large-scale scenes, motivating a sparse network structure where parameters are specialized to different regions of the scene. We introduce a simple geometric clustering algorithm for data parallelism that partitions training images (or rather pixels) into different NeRF submodules that can be trained in parallel.   We evaluate our approach on existing datasets (Quad 6k and UrbanScene3D) as well as against our own drone footage, improving training speed by 3x and PSNR by 12%. We also evaluate recent NeRF fast renderers on top of Mega-NeRF and introduce a novel method that exploits temporal coherence. Our technique achieves a 40x speedup over conventional NeRF rendering while remaining within 0.8 db in PSNR quality, exceeding the fidelity of existing fast renderers.\n\n4. Title: Unisim: A neural closed-loop sensor simulator\n   Abstract: UniSim: A Neural Closed-Loop Sensor Simulator: Rigorously testing autonomy systems is essential for making safe self-driving vehicles (SDV) a reality. It requires one to generate safety critical scenarios beyond what can be collected safely in the world, as many scenarios happen rarely on public roads. To accurately evaluate performance, we need to test the SDV on these scenarios in closed-loop, where the SDV and other actors interact with each other at each timestep. Previously recorded driving logs provide a rich resource to build these new scenarios from, but for closed loop evaluation, we need to modify the sensor data based on the new scene configuration and the SDV's decisions, as actors might be added or removed and the trajectories of existing actors and the SDV will differ from the original log. In this paper, we present UniSim, a neural sensor simulator that takes a single recorded log captured by a sensor-equipped vehicle and converts it into a realistic closed-loop multi-sensor simulation. UniSim builds neural feature grids to reconstruct both the static background and dynamic actors in the scene, and composites them together to simulate LiDAR and camera data at new viewpoints, with actors added or removed and at new placements. To better handle extrapolated views, we incorporate learnable priors for dynamic objects, and leverage a convolutional network to complete unseen regions. Our experiments show UniSim can simulate realistic sensor data with small domain gap on downstream tasks. With UniSim, we demonstrate closed-loop evaluation of an autonomy system on safety-critical scenarios as if it were in the real world.\n\n5. Title: Instant neural graphics primitives with a multiresolution hash encoding\n   Abstract: Instant Neural Graphics Primitives with a Multiresolution Hash Encoding: Neural graphics primitives, parameterized by fully connected neural networks, can be costly to train and evaluate. We reduce this cost with a versatile new input encoding that permits the use of a smaller network without sacrificing quality, thus significantly reducing the number of floating point and memory access operations: a small neural network is augmented by a multiresolution hash table of trainable feature vectors whose values are optimized through stochastic gradient descent. The multiresolution structure allows the network to disambiguate hash collisions, making for a simple architecture that is trivial to parallelize on modern GPUs. We leverage this parallelism by implementing the whole system using fully-fused CUDA kernels with a focus on minimizing wasted bandwidth and compute operations. We achieve a combined speedup of several orders of magnitude, enabling training of high-quality neural graphics primitives in a matter of seconds, and rendering in tens of milliseconds at a resolution of ${1920\\!\\times\\!1080}$.\n\n6. Title: Neural point-based graphics\n   Abstract: Neural Point-Based Graphics: We present a new point-based approach for modeling the appearance of real scenes. The approach uses a raw point cloud as the geometric representation of a scene, and augments each point with a learnable neural descriptor that encodes local geometry and appearance. A deep rendering network is learned in parallel with the descriptors, so that new views of the scene can be obtained by passing the rasterizations of a point cloud from new viewpoints through this network. The input rasterizations use the learned descriptors as point pseudo-colors. We show that the proposed approach can be used for modeling complex scenes and obtaining their photorealistic views, while avoiding explicit surface estimation and meshing. In particular, compelling results are obtained for scene scanned using hand-held commodity RGB-D sensors as well as standard RGB cameras even in the presence of objects that are challenging for standard mesh-based modeling.\n\n7. Title: Monosdf: Exploring monocular geometric cues for neural implicit surface reconstruction\n   Abstract: MonoSDF: Exploring Monocular Geometric Cues for Neural Implicit Surface Reconstruction: In recent years, neural implicit surface reconstruction methods have become popular for multi-view 3D reconstruction. In contrast to traditional multi-view stereo methods, these approaches tend to produce smoother and more complete reconstructions due to the inductive smoothness bias of neural networks. State-of-the-art neural implicit methods allow for high-quality reconstructions of simple scenes from many input views. Yet, their performance drops significantly for larger and more complex scenes and scenes captured from sparse viewpoints. This is caused primarily by the inherent ambiguity in the RGB reconstruction loss that does not provide enough constraints, in particular in less-observed and textureless areas. Motivated by recent advances in the area of monocular geometry prediction, we systematically explore the utility these cues provide for improving neural implicit surface reconstruction. We demonstrate that depth and normal cues, predicted by general-purpose monocular estimators, significantly improve reconstruction quality and optimization time. Further, we analyse and investigate multiple design choices for representing neural implicit surfaces, ranging from monolithic MLP models over single-grid to multi-resolution grid representations. We observe that geometric monocular priors improve performance both for small-scale single-object as well as large-scale multi-object scenes, independent of the choice of representation.\n\n8. Title: Mip-nerf: A multiscale representation for anti-aliasing neural radiance fields\n   Abstract: Mip-NeRF: A Multiscale Representation for Anti-Aliasing Neural Radiance Fields: The rendering procedure used by neural radiance fields (NeRF) samples a scene with a single ray per pixel and may therefore produce renderings that are excessively blurred or aliased when training or testing images observe scene content at different resolutions. The straightforward solution of supersampling by rendering with multiple rays per pixel is impractical for NeRF, because rendering each ray requires querying a multilayer perceptron hundreds of times. Our solution, which we call \"mip-NeRF\" (a la \"mipmap\"), extends NeRF to represent the scene at a continuously-valued scale. By efficiently rendering anti-aliased conical frustums instead of rays, mip-NeRF reduces objectionable aliasing artifacts and significantly improves NeRF's ability to represent fine details, while also being 7% faster than NeRF and half the size. Compared to NeRF, mip-NeRF reduces average error rates by 17% on the dataset presented with NeRF and by 60% on a challenging multiscale variant of that dataset that we present. Mip-NeRF is also able to match the accuracy of a brute-force supersampled NeRF on our multiscale dataset while being 22x faster.\n\n9. Title: Spacetime gaussian feature splatting for real-time dynamic view synthesis\n   Abstract: Spacetime Gaussian Feature Splatting for Real-Time Dynamic View Synthesis: Novel view synthesis of dynamic scenes has been an intriguing yet challenging problem. Despite recent advancements, simultaneously achieving high-resolution photorealistic results, real-time rendering, and compact storage remains a formidable task. To address these challenges, we propose Spacetime Gaussian Feature Splatting as a novel dynamic scene representation, composed of three pivotal components. First, we formulate expressive Spacetime Gaussians by enhancing 3D Gaussians with temporal opacity and parametric motion/rotation. This enables Spacetime Gaussians to capture static, dynamic, as well as transient content within a scene. Second, we introduce splatted feature rendering, which replaces spherical harmonics with neural features. These features facilitate the modeling of view- and time-dependent appearance while maintaining small size. Third, we leverage the guidance of training error and coarse depth to sample new Gaussians in areas that are challenging to converge with existing pipelines. Experiments on several established real-world datasets demonstrate that our method achieves state-of-the-art rendering quality and speed, while retaining compact storage. At 8K resolution, our lite-version model can render at 60 FPS on an Nvidia RTX 4090 GPU. Our code is available at https://github.com/oppo-us-research/SpacetimeGaussians.\n\n10. Title: Pixel-GS: Density Control with Pixel-aware Gradient for 3D Gaussian Splatting\n   Abstract: Pixel-GS: Density Control with Pixel-aware Gradient for 3D Gaussian Splatting: 3D Gaussian Splatting (3DGS) has demonstrated impressive novel view synthesis results while advancing real-time rendering performance. However, it relies heavily on the quality of the initial point cloud, resulting in blurring and needle-like artifacts in areas with insufficient initializing points. This is mainly attributed to the point cloud growth condition in 3DGS that only considers the average gradient magnitude of points from observable views, thereby failing to grow for large Gaussians that are observable for many viewpoints while many of them are only covered in the boundaries. To this end, we propose a novel method, named Pixel-GS, to take into account the number of pixels covered by the Gaussian in each view during the computation of the growth condition. We regard the covered pixel numbers as the weights to dynamically average the gradients from different views, such that the growth of large Gaussians can be prompted. As a result, points within the areas with insufficient initializing points can be grown more effectively, leading to a more accurate and detailed reconstruction. In addition, we propose a simple yet effective strategy to scale the gradient field according to the distance to the camera, to suppress the growth of floaters near the camera. Extensive experiments both qualitatively and quantitatively demonstrate that our method achieves state-of-the-art rendering quality while maintaining real-time rendering speed, on the challenging Mip-NeRF 360 and Tanks & Temples datasets.\n\nThink step-by-step about which candidate best matches the context. Summarize your reasoning, then provide the exact title that should fill the citation. Respond in JSON with fields:\n{\n  \"reasoning\": \"<your step-by-step analysis>\",\n  \"selected_title\": \"<exact candidate title>\"\n}", "response": {"reasoning": "The citation context states that 3D Gaussian Splatting (3DGS) has been applied to domains including autonomous driving, content generation, and 4D dynamic scenes, emphasizing its real-time, high-quality rendering capabilities. I compared candidates for which paper best supports the claim about applications to 4D/dynamic scenes and real-time rendering. Candidate 9 (Spacetime Gaussian Feature Splatting for Real-Time Dynamic View Synthesis) explicitly extends 3D Gaussians to spatiotemporal (4D) settings, emphasizes real-time high-quality rendering and compact storage, and demonstrates dynamic scene (4D) applications in its abstractâ€”directly matching the â€œ4D dynamic scenesâ€ example in the context. Other candidates: (1) is the original 3D Gaussian Splatting paper (introduces real-time rendering) but does not specifically target 4D/dynamic scenes; (4) (UniSim) concerns autonomous-driving sensor simulation but does not use 3DGS; (10) (Pixel-GS) is an optimization/improvement to 3DGS focused on density control, not on applications across domains. Therefore the best match to support the claim about 3DGS being applied to 4D dynamic scenes (and its real-time rendering strengths) is candidate 9.", "selected_title": "Spacetime gaussian feature splatting for real-time dynamic view synthesis"}, "selected_title": "Spacetime gaussian feature splatting for real-time dynamic view synthesis", "positive_title": "Spacetime gaussian feature splatting for real-time dynamic view synthesis", "is_correct": true, "reasoning": "The citation context states that 3D Gaussian Splatting (3DGS) has been applied to domains including autonomous driving, content generation, and 4D dynamic scenes, emphasizing its real-time, high-quality rendering capabilities. I compared candidates for which paper best supports the claim about applications to 4D/dynamic scenes and real-time rendering. Candidate 9 (Spacetime Gaussian Feature Splatting for Real-Time Dynamic View Synthesis) explicitly extends 3D Gaussians to spatiotemporal (4D) settings, emphasizes real-time high-quality rendering and compact storage, and demonstrates dynamic scene (4D) applications in its abstractâ€”directly matching the â€œ4D dynamic scenesâ€ example in the context. Other candidates: (1) is the original 3D Gaussian Splatting paper (introduces real-time rendering) but does not specifically target 4D/dynamic scenes; (4) (UniSim) concerns autonomous-driving sensor simulation but does not use 3DGS; (10) (Pixel-GS) is an optimization/improvement to 3DGS focused on density control, not on applications across domains. Therefore the best match to support the claim about 3DGS being applied to 4D dynamic scenes (and its real-time rendering strengths) is candidate 9."}
{"example_id": "blind_295", "prompt": "You are an expert citation retrieval system as described in the paper \"Multi-Agent System for Reliable Citation Retrieval\".\nYour goal is to autonomously retrieve, verify, and recommend academic references given a query or document excerpt.\n\nTask:\nGiven a citation context from a scientific paper (where a citation is missing), identify the correct paper from a list of candidates.\nAnalyze the context to understand the specific claim, method, or result being cited.\nThen, evaluate each candidate paper to see if it matches the context.\nFinally, select the best matching paper.\n\nCitation Context:\nults well demonstrate that both proposed optimizations bring substantial improvements to unsup-SimCSE. \\end{itemize} Related Work Unsupervised sentence representation learning has been widely studied.[CITATION] propose to learn sentence representation according to the internal structure of each sentence. predict the surrounding sentences of a given sentence based on the distribution hypothesis.<|c\n\nCandidate Papers:\n1. Title: Simcse: Simple contrastive learning of sentence embeddings\n   Abstract: SimCSE: Simple Contrastive Learning of Sentence Embeddings: This paper presents SimCSE, a simple contrastive learning framework that greatly advances state-of-the-art sentence embeddings. We first describe an unsupervised approach, which takes an input sentence and predicts itself in a contrastive objective, with only standard dropout used as noise. This simple method works surprisingly well, performing on par with previous supervised counterparts. We find that dropout acts as minimal data augmentation, and removing it leads to a representation collapse. Then, we propose a supervised approach, which incorporates annotated pairs from natural language inference datasets into our contrastive learning framework by using \"entailment\" pairs as positives and \"contradiction\" pairs as hard negatives. We evaluate SimCSE on standard semantic textual similarity (STS) tasks, and our unsupervised and supervised models using BERT base achieve an average of 76.3% and 81.6% Spearman's correlation respectively, a 4.2% and 2.2% improvement compared to the previous best results. We also show -- both theoretically and empirically -- that the contrastive learning objective regularizes pre-trained embeddings' anisotropic space to be more uniform, and it better aligns positive pairs when supervised signals are available.\n\n2. Title: Consert: A contrastive framework for self-supervised sentence representation transfer\n   Abstract: ConSERT: A Contrastive Framework for Self-Supervised Sentence Representation Transfer: Learning high-quality sentence representations benefits a wide range of natural language processing tasks. Though BERT-based pre-trained language models achieve high performance on many downstream tasks, the native derived sentence representations are proved to be collapsed and thus produce a poor performance on the semantic textual similarity (STS) tasks. In this paper, we present ConSERT, a Contrastive Framework for Self-Supervised Sentence Representation Transfer, that adopts contrastive learning to fine-tune BERT in an unsupervised and effective way. By making use of unlabeled texts, ConSERT solves the collapse issue of BERT-derived sentence representations and make them more applicable for downstream tasks. Experiments on STS datasets demonstrate that ConSERT achieves an 8\\% relative improvement over the previous state-of-the-art, even comparable to the supervised SBERT-NLI. And when further incorporating NLI supervision, we achieve new state-of-the-art performance on STS tasks. Moreover, ConSERT obtains comparable results with only 1000 samples available, showing its robustness in data scarcity scenarios.\n\n3. Title: An unsupervised sentence embedding method by mutual information maximization\n   Abstract: An Unsupervised Sentence Embedding Method by Mutual Information Maximization: BERT is inefficient for sentence-pair tasks such as clustering or semantic search as it needs to evaluate combinatorially many sentence pairs which is very time-consuming. Sentence BERT (SBERT) attempted to solve this challenge by learning semantically meaningful representations of single sentences, such that similarity comparison can be easily accessed. However, SBERT is trained on corpus with high-quality labeled sentence pairs, which limits its application to tasks where labeled data is extremely scarce. In this paper, we propose a lightweight extension on top of BERT and a novel self-supervised learning objective based on mutual information maximization strategies to derive meaningful sentence embeddings in an unsupervised manner. Unlike SBERT, our method is not restricted by the availability of labeled data, such that it can be applied on different domain-specific corpus. Experimental results show that the proposed method significantly outperforms other unsupervised sentence embedding baselines on common semantic textual similarity (STS) tasks and downstream supervised tasks. It also outperforms SBERT in a setting where in-domain labeled data is not available, and achieves performance competitive with supervised methods on various tasks.\n\n4. Title: Learning distributed representations of sentences from unlabelled data\n   Abstract: Learning Distributed Representations of Sentences from Unlabelled Data: Unsupervised methods for learning distributed representations of words are ubiquitous in today's NLP research, but far less is known about the best ways to learn distributed phrase or sentence representations from unlabelled data. This paper is a systematic comparison of models that learn such representations. We find that the optimal approach depends critically on the intended application. Deeper, more complex models are preferable for representations to be used in supervised systems, but shallow log-linear models work best for building representation spaces that can be decoded with simple spatial distance metrics. We also propose two new unsupervised representation-learning objectives designed to optimise the trade-off between training time, domain portability and performance.\n\n5. Title: Clear: Contrastive learning for sentence representation\n   Abstract: CLEAR: Contrastive Learning for Sentence Representation: Pre-trained language models have proven their unique powers in capturing implicit language features. However, most pre-training approaches focus on the word-level training objective, while sentence-level objectives are rarely studied. In this paper, we propose Contrastive LEArning for sentence Representation (CLEAR), which employs multiple sentence-level augmentation strategies in order to learn a noise-invariant sentence representation. These augmentations include word and span deletion, reordering, and substitution. Furthermore, we investigate the key reasons that make contrastive learning effective through numerous experiments. We observe that different sentence augmentations during pre-training lead to different performance improvements on various downstream tasks. Our approach is shown to outperform multiple existing methods on both SentEval and GLUE benchmarks.\n\n6. Title: A simple framework for contrastive learning of visual representations\n   Abstract: A Simple Framework for Contrastive Learning of Visual Representations: This paper presents SimCLR: a simple framework for contrastive learning of visual representations. We simplify recently proposed contrastive self-supervised learning algorithms without requiring specialized architectures or a memory bank. In order to understand what enables the contrastive prediction tasks to learn useful representations, we systematically study the major components of our framework. We show that (1) composition of data augmentations plays a critical role in defining effective predictive tasks, (2) introducing a learnable nonlinear transformation between the representation and the contrastive loss substantially improves the quality of the learned representations, and (3) contrastive learning benefits from larger batch sizes and more training steps compared to supervised learning. By combining these findings, we are able to considerably outperform previous methods for self-supervised and semi-supervised learning on ImageNet. A linear classifier trained on self-supervised representations learned by SimCLR achieves 76.5% top-1 accuracy, which is a 7% relative improvement over previous state-of-the-art, matching the performance of a supervised ResNet-50. When fine-tuned on only 1% of the labels, we achieve 85.8% top-5 accuracy, outperforming AlexNet with 100X fewer labels.\n\n7. Title: Bert: Pre-training of deep bidirectional transformers for language understanding\n   Abstract: BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding: We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models, BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications.   BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5% (7.7% point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).\n\n8. Title: An efficient framework for learning sentence representations\n   Abstract: An efficient framework for learning sentence representations: In this work we propose a simple and efficient framework for learning sentence representations from unlabelled data. Drawing inspiration from the distributional hypothesis and recent work on learning sentence representations, we reformulate the problem of predicting the context in which a sentence appears as a classification problem. Given a sentence and its context, a classifier distinguishes context sentences from other contrastive sentences based on their vector representations. This allows us to efficiently learn different types of encoding functions, and we show that the model learns high-quality sentence representations. We demonstrate that our sentence representations outperform state-of-the-art unsupervised and supervised representation learning methods on several downstream NLP tasks that involve understanding sentence semantics while achieving an order of magnitude speedup in training time.\n\n9. Title: Momentum contrast for unsupervised visual representation learning\n   Abstract: Momentum Contrast for Unsupervised Visual Representation Learning: We present Momentum Contrast (MoCo) for unsupervised visual representation learning. From a perspective on contrastive learning as dictionary look-up, we build a dynamic dictionary with a queue and a moving-averaged encoder. This enables building a large and consistent dictionary on-the-fly that facilitates contrastive unsupervised learning. MoCo provides competitive results under the common linear protocol on ImageNet classification. More importantly, the representations learned by MoCo transfer well to downstream tasks. MoCo can outperform its supervised pre-training counterpart in 7 detection/segmentation tasks on PASCAL VOC, COCO, and other datasets, sometimes surpassing it by large margins. This suggests that the gap between unsupervised and supervised representation learning has been largely closed in many vision tasks.\n\nThink step-by-step about which candidate best matches the context. Summarize your reasoning, then provide the exact title that should fill the citation. Respond in JSON with fields:\n{\n  \"reasoning\": \"<your step-by-step analysis>\",\n  \"selected_title\": \"<exact candidate title>\"\n}", "response": {"reasoning": "The context mentions predicting surrounding sentences based on the distributional hypothesis and learning sentence representations by modeling a sentence's context. Candidate 8's abstract explicitly states it draws inspiration from the distributional hypothesis and reformulates predicting the context (i.e., surrounding sentences) as a classification problem to learn sentence embeddings. This directly matches the described method in the citation context. Other candidates (e.g., Skip-Thoughts / candidate 4) also predict surrounding sentences, but candidate 8's abstract specifically references the distributional hypothesis and the efficient context-prediction formulation, making it the best match.", "selected_title": "An efficient framework for learning sentence representations"}, "selected_title": "An efficient framework for learning sentence representations", "positive_title": "Learning distributed representations of sentences from unlabelled data", "is_correct": false, "reasoning": "The context mentions predicting surrounding sentences based on the distributional hypothesis and learning sentence representations by modeling a sentence's context. Candidate 8's abstract explicitly states it draws inspiration from the distributional hypothesis and reformulates predicting the context (i.e., surrounding sentences) as a classification problem to learn sentence embeddings. This directly matches the described method in the citation context. Other candidates (e.g., Skip-Thoughts / candidate 4) also predict surrounding sentences, but candidate 8's abstract specifically references the distributional hypothesis and the efficient context-prediction formulation, making it the best match."}
{"example_id": "blind_3495", "prompt": "You are an expert citation retrieval system as described in the paper \"Multi-Agent System for Reliable Citation Retrieval\".\nYour goal is to autonomously retrieve, verify, and recommend academic references given a query or document excerpt.\n\nTask:\nGiven a citation context from a scientific paper (where a citation is missing), identify the correct paper from a list of candidates.\nAnalyze the context to understand the specific claim, method, or result being cited.\nThen, evaluate each candidate paper to see if it matches the context.\nFinally, select the best matching paper.\n\nCitation Context:\nr both mel-spectrogram generation, and waveform generation. However, the iterative generation process can be slow for high-dimensional data. AudioLDM, Make-An-Audio[CITATION], and Tango, which trained in a continuous latent space. However, the control of LDM in audio generation tasks still falls short of achieving satisfactory results. In this paper, we attemp\n\nCandidate Papers:\n1. Title: Bilateral denoising diffusion models\n   Abstract: Bilateral Denoising Diffusion Models: Denoising diffusion probabilistic models (DDPMs) have emerged as competitive generative models yet brought challenges to efficient sampling. In this paper, we propose novel bilateral denoising diffusion models (BDDMs), which take significantly fewer steps to generate high-quality samples. From a bilateral modeling objective, BDDMs parameterize the forward and reverse processes with a score network and a scheduling network, respectively. We show that a new lower bound tighter than the standard evidence lower bound can be derived as a surrogate objective for training the two networks. In particular, BDDMs are efficient, simple-to-train, and capable of further improving any pre-trained DDPM by optimizing the inference noise schedules. Our experiments demonstrated that BDDMs can generate high-fidelity samples with as few as 3 sampling steps and produce comparable or even higher quality samples than DDPMs using 1000 steps with only 16 sampling steps (a 62x speedup).\n\n2. Title: Diff-Foley: Synchronized Video-to-Audio Synthesis with Latent Diffusion Models\n   Abstract: Diff-Foley: Synchronized Video-to-Audio Synthesis with Latent Diffusion Models: The Video-to-Audio (V2A) model has recently gained attention for its practical application in generating audio directly from silent videos, particularly in video/film production. However, previous methods in V2A have limited generation quality in terms of temporal synchronization and audio-visual relevance. We present Diff-Foley, a synchronized Video-to-Audio synthesis method with a latent diffusion model (LDM) that generates high-quality audio with improved synchronization and audio-visual relevance. We adopt contrastive audio-visual pretraining (CAVP) to learn more temporally and semantically aligned features, then train an LDM with CAVP-aligned visual features on spectrogram latent space. The CAVP-aligned features enable LDM to capture the subtler audio-visual correlation via a cross-attention module. We further significantly improve sample quality with `double guidance'. Diff-Foley achieves state-of-the-art V2A performance on current large scale V2A dataset. Furthermore, we demonstrate Diff-Foley practical applicability and generalization capabilities via downstream finetuning. Project Page: see https://diff-foley.github.io/\n\n3. Title: Conditional Generation of Audio from Video via Foley Analogies\n   Abstract: Conditional Generation of Audio from Video via Foley Analogies: The sound effects that designers add to videos are designed to convey a particular artistic effect and, thus, may be quite different from a scene's true sound. Inspired by the challenges of creating a soundtrack for a video that differs from its true sound, but that nonetheless matches the actions occurring on screen, we propose the problem of conditional Foley. We present the following contributions to address this problem. First, we propose a pretext task for training our model to predict sound for an input video clip using a conditional audio-visual clip sampled from another time within the same source video. Second, we propose a model for generating a soundtrack for a silent input video, given a user-supplied example that specifies what the video should \"sound like\". We show through human studies and automated evaluation metrics that our model successfully generates sound from video, while varying its output according to the content of a supplied example. Project site: https://xypb.github.io/CondFoleyGen/\n\n4. Title: I hear your true colors: Image guided audio generation\n   Abstract: I Hear Your True Colors: Image Guided Audio Generation: We propose Im2Wav, an image guided open-domain audio generation system. Given an input image or a sequence of images, Im2Wav generates a semantically relevant sound. Im2Wav is based on two Transformer language models, that operate over a hierarchical discrete audio representation obtained from a VQ-VAE based model. We first produce a low-level audio representation using a language model. Then, we upsample the audio tokens using an additional language model to generate a high-fidelity audio sample. We use the rich semantics of a pre-trained CLIP (Contrastive Language-Image Pre-training) embedding as a visual representation to condition the language model. In addition, to steer the generation process towards the conditioning image, we apply the classifier-free guidance method. Results suggest that Im2Wav significantly outperforms the evaluated baselines in both fidelity and relevance evaluation metrics. Additionally, we provide an ablation study to better assess the impact of each of the method components on overall performance. Lastly, to better evaluate image-to-audio models, we propose an out-of-domain image dataset, denoted as ImageHear. ImageHear can be used as a benchmark for evaluating future image-to-audio models. Samples and code can be found inside the manuscript.\n\n5. Title: Efficient Neural Music Generation\n   Abstract: Efficient Neural Music Generation: Recent progress in music generation has been remarkably advanced by the state-of-the-art MusicLM, which comprises a hierarchy of three LMs, respectively, for semantic, coarse acoustic, and fine acoustic modelings. Yet, sampling with the MusicLM requires processing through these LMs one by one to obtain the fine-grained acoustic tokens, making it computationally expensive and prohibitive for a real-time generation. Efficient music generation with a quality on par with MusicLM remains a significant challenge. In this paper, we present MeLoDy (M for music; L for LM; D for diffusion), an LM-guided diffusion model that generates music audios of state-of-the-art quality meanwhile reducing 95.7% or 99.6% forward passes in MusicLM, respectively, for sampling 10s or 30s music. MeLoDy inherits the highest-level LM from MusicLM for semantic modeling, and applies a novel dual-path diffusion (DPD) model and an audio VAE-GAN to efficiently decode the conditioning semantic tokens into waveform. DPD is proposed to simultaneously model the coarse and fine acoustics by incorporating the semantic information into segments of latents effectively via cross-attention at each denoising step. Our experimental results suggest the superiority of MeLoDy, not only in its practical advantages on sampling speed and infinitely continuable generation, but also in its state-of-the-art musicality, audio quality, and text correlation.   Our samples are available at https://Efficient-MeLoDy.github.io/.\n\n6. Title: Make-an-audio: Text-to-audio generation with prompt-enhanced diffusion models\n   Abstract: Make-An-Audio: Text-To-Audio Generation with Prompt-Enhanced Diffusion Models: Large-scale multimodal generative modeling has created milestones in text-to-image and text-to-video generation. Its application to audio still lags behind for two main reasons: the lack of large-scale datasets with high-quality text-audio pairs, and the complexity of modeling long continuous audio data. In this work, we propose Make-An-Audio with a prompt-enhanced diffusion model that addresses these gaps by 1) introducing pseudo prompt enhancement with a distill-then-reprogram approach, it alleviates data scarcity with orders of magnitude concept compositions by using language-free audios; 2) leveraging spectrogram autoencoder to predict the self-supervised audio representation instead of waveforms. Together with robust contrastive language-audio pretraining (CLAP) representations, Make-An-Audio achieves state-of-the-art results in both objective and subjective benchmark evaluation. Moreover, we present its controllability and generalization for X-to-Audio with \"No Modality Left Behind\", for the first time unlocking the ability to generate high-definition, high-fidelity audios given a user-defined modality input. Audio samples are available at https://Text-to-Audio.github.io\n\n7. Title: Taming visually guided sound generation\n   Abstract: Taming Visually Guided Sound Generation: Recent advances in visually-induced audio generation are based on sampling short, low-fidelity, and one-class sounds. Moreover, sampling 1 second of audio from the state-of-the-art model takes minutes on a high-end GPU. In this work, we propose a single model capable of generating visually relevant, high-fidelity sounds prompted with a set of frames from open-domain videos in less time than it takes to play it on a single GPU.   We train a transformer to sample a new spectrogram from the pre-trained spectrogram codebook given the set of video features. The codebook is obtained using a variant of VQGAN trained to produce a compact sampling space with a novel spectrogram-based perceptual loss. The generated spectrogram is transformed into a waveform using a window-based GAN that significantly speeds up generation. Considering the lack of metrics for automatic evaluation of generated spectrograms, we also build a family of metrics called FID and MKL. These metrics are based on a novel sound classifier, called Melception, and designed to evaluate the fidelity and relevance of open-domain samples.   Both qualitative and quantitative studies are conducted on small- and large-scale datasets to evaluate the fidelity and relevance of generated samples. We also compare our model to the state-of-the-art and observe a substantial improvement in quality, size, and computation time. Code, demo, and samples: v-iashin.github.io/SpecVQGAN\n\n8. Title: Llama 2: Open foundation and fine-tuned chat models\n   Abstract: Llama 2: Open Foundation and Fine-Tuned Chat Models: In this work, we develop and release Llama 2, a collection of pretrained and fine-tuned large language models (LLMs) ranging in scale from 7 billion to 70 billion parameters. Our fine-tuned LLMs, called Llama 2-Chat, are optimized for dialogue use cases. Our models outperform open-source chat models on most benchmarks we tested, and based on our human evaluations for helpfulness and safety, may be a suitable substitute for closed-source models. We provide a detailed description of our approach to fine-tuning and safety improvements of Llama 2-Chat in order to enable the community to build on our work and contribute to the responsible development of LLMs.\n\n9. Title: Audiogen: Textually guided audio generation\n   Abstract: AudioGen: Textually Guided Audio Generation: We tackle the problem of generating audio samples conditioned on descriptive text captions. In this work, we propose AaudioGen, an auto-regressive generative model that generates audio samples conditioned on text inputs. AudioGen operates on a learnt discrete audio representation. The task of text-to-audio generation poses multiple challenges. Due to the way audio travels through a medium, differentiating ``objects'' can be a difficult task (e.g., separating multiple people simultaneously speaking). This is further complicated by real-world recording conditions (e.g., background noise, reverberation, etc.). Scarce text annotations impose another constraint, limiting the ability to scale models. Finally, modeling high-fidelity audio requires encoding audio at high sampling rate, leading to extremely long sequences. To alleviate the aforementioned challenges we propose an augmentation technique that mixes different audio samples, driving the model to internally learn to separate multiple sources. We curated 10 datasets containing different types of audio and text annotations to handle the scarcity of text-audio data points. For faster inference, we explore the use of multi-stream modeling, allowing the use of shorter sequences while maintaining a similar bitrate and perceptual quality. We apply classifier-free guidance to improve adherence to text. Comparing to the evaluated baselines, AudioGen outperforms over both objective and subjective metrics. Finally, we explore the ability of the proposed method to generate audio continuation conditionally and unconditionally. Samples: https://felixkreuk.github.io/audiogen\n\nThink step-by-step about which candidate best matches the context. Summarize your reasoning, then provide the exact title that should fill the citation. Respond in JSON with fields:\n{\n  \"reasoning\": \"<your step-by-step analysis>\",\n  \"selected_title\": \"<exact candidate title>\"\n}", "response": {"reasoning": "The citation in context lists AudioLDM, Make-An-Audio, and Tango as models trained in a continuous latent space and discusses limitations of latent diffusion models (LDMs) for audio control. Candidate 6 is titled Make-an-audio: Text-to-audio generation with prompt-enhanced diffusion models. Its abstract explicitly describes using a spectrogram autoencoder to predict a self-supervised audio representation (i.e., operating in a continuous latent/spectrogram space) and a prompt-enhanced diffusion model for text-to-audioâ€”matching the context's reference to Make-An-Audio and the continuous latent-space training. Other candidates are about related audio/video generation tasks but do not match the Make-An-Audio title or the specific description of spectrogram autoencoder + diffusion in latent space. Therefore candidate 6 is the correct match.", "selected_title": "Make-an-audio: Text-to-audio generation with prompt-enhanced diffusion models"}, "selected_title": "Make-an-audio: Text-to-audio generation with prompt-enhanced diffusion models", "positive_title": "Make-an-audio: Text-to-audio generation with prompt-enhanced diffusion models", "is_correct": true, "reasoning": "The citation in context lists AudioLDM, Make-An-Audio, and Tango as models trained in a continuous latent space and discusses limitations of latent diffusion models (LDMs) for audio control. Candidate 6 is titled Make-an-audio: Text-to-audio generation with prompt-enhanced diffusion models. Its abstract explicitly describes using a spectrogram autoencoder to predict a self-supervised audio representation (i.e., operating in a continuous latent/spectrogram space) and a prompt-enhanced diffusion model for text-to-audioâ€”matching the context's reference to Make-An-Audio and the continuous latent-space training. Other candidates are about related audio/video generation tasks but do not match the Make-An-Audio title or the specific description of spectrogram autoencoder + diffusion in latent space. Therefore candidate 6 is the correct match."}
{"example_id": "blind_7291", "prompt": "You are an expert citation retrieval system as described in the paper \"Multi-Agent System for Reliable Citation Retrieval\".\nYour goal is to autonomously retrieve, verify, and recommend academic references given a query or document excerpt.\n\nTask:\nGiven a citation context from a scientific paper (where a citation is missing), identify the correct paper from a list of candidates.\nAnalyze the context to understand the specific claim, method, or result being cited.\nThen, evaluate each candidate paper to see if it matches the context.\nFinally, select the best matching paper.\n\nCitation Context:\nnted Generation} Equipping a parametric language model with a non-parametric datastore has proven effective for various NLP tasks, such as language modeling, open-domain question answering[CITATION], machine translation and others. Given the vast design space of this generation paradigm, numerous approaches with different focuses have been proposed. For instance, RETRO and\n\nCandidate Papers:\n1. Title: Nearest Neighbor Machine Translation\n   Abstract: Nearest Neighbor Machine Translation: We introduce $k$-nearest-neighbor machine translation ($k$NN-MT), which predicts tokens with a nearest neighbor classifier over a large datastore of cached examples, using representations from a neural translation model for similarity search. This approach requires no additional training and scales to give the decoder direct access to billions of examples at test time, resulting in a highly expressive model that consistently improves performance across many settings. Simply adding nearest neighbor search improves a state-of-the-art German-English translation model by 1.5 BLEU. $k$NN-MT allows a single model to be adapted to diverse domains by using a domain-specific datastore, improving results by an average of 9.2 BLEU over zero-shot transfer, and achieving new state-of-the-art results -- without training on these domains. A massively multilingual model can also be specialized for particular language pairs, with improvements of 3 BLEU for translating from English into German and Chinese. Qualitatively, $k$NN-MT is easily interpretable; it combines source and target context to retrieve highly relevant examples.\n\n2. Title: Leveraging Passage Retrieval with Generative Models for Open Domain Question Answering\n   Abstract: Leveraging Passage Retrieval with Generative Models for Open Domain Question Answering: Generative models for open domain question answering have proven to be competitive, without resorting to external knowledge. While promising, this approach requires to use models with billions of parameters, which are expensive to train and query. In this paper, we investigate how much these models can benefit from retrieving text passages, potentially containing evidence. We obtain state-of-the-art results on the Natural Questions and TriviaQA open benchmarks. Interestingly, we observe that the performance of this method significantly improves when increasing the number of retrieved passages. This is evidence that generative models are good at aggregating and combining evidence from multiple passages.\n\n3. Title: RA-DIT: Retrieval-Augmented Dual Instruction Tuning\n   Abstract: RA-DIT: Retrieval-Augmented Dual Instruction Tuning: Retrieval-augmented language models (RALMs) improve performance by accessing long-tail and up-to-date knowledge from external data stores, but are challenging to build. Existing approaches require either expensive retrieval-specific modifications to LM pre-training or use post-hoc integration of the data store that leads to suboptimal performance. We introduce Retrieval-Augmented Dual Instruction Tuning (RA-DIT), a lightweight fine-tuning methodology that provides a third option by retrofitting any LLM with retrieval capabilities. Our approach operates in two distinct fine-tuning steps: (1) one updates a pre-trained LM to better use retrieved information, while (2) the other updates the retriever to return more relevant results, as preferred by the LM. By fine-tuning over tasks that require both knowledge utilization and contextual awareness, we demonstrate that each stage yields significant performance improvements, and using both leads to additional gains. Our best model, RA-DIT 65B, achieves state-of-the-art performance across a range of knowledge-intensive zero- and few-shot learning benchmarks, significantly outperforming existing in-context RALM approaches by up to +8.9% in 0-shot setting and +1.4% in 5-shot setting on average.\n\n4. Title: Visual instruction tuning\n   Abstract: Visual Instruction Tuning: Instruction tuning large language models (LLMs) using machine-generated instruction-following data has improved zero-shot capabilities on new tasks, but the idea is less explored in the multimodal field. In this paper, we present the first attempt to use language-only GPT-4 to generate multimodal language-image instruction-following data. By instruction tuning on such generated data, we introduce LLaVA: Large Language and Vision Assistant, an end-to-end trained large multimodal model that connects a vision encoder and LLM for general-purpose visual and language understanding.Our early experiments show that LLaVA demonstrates impressive multimodel chat abilities, sometimes exhibiting the behaviors of multimodal GPT-4 on unseen images/instructions, and yields a 85.1% relative score compared with GPT-4 on a synthetic multimodal instruction-following dataset. When fine-tuned on Science QA, the synergy of LLaVA and GPT-4 achieves a new state-of-the-art accuracy of 92.53%. We make GPT-4 generated visual instruction tuning data, our model and code base publicly available.\n\n5. Title: Adapting Language Models to Compress Contexts\n   Abstract: Adapting Language Models to Compress Contexts: Transformer-based language models (LMs) are powerful and widely-applicable tools, but their usefulness is constrained by a finite context window and the expensive computational cost of processing long text documents. We propose to adapt pre-trained LMs into AutoCompressors. These language models are capable of compressing long contexts into compact summary vectors, which are then accessible to the model as soft prompts. Summary vectors are trained with an unsupervised objective, whereby long documents are processed in segments, and summary vectors from all previous segments are used in language modeling. We fine-tune OPT and Llama-2 models on sequences of up to 30,720 tokens and show that AutoCompressors can utilize long contexts to improve perplexity. We evaluate AutoCompressors on in-context learning by compressing task demonstrations and find that summary vectors are good substitutes for plain-text demonstrations, increasing accuracy while reducing inference costs. Finally, we explore the benefits of pre-computing summary vectors for large corpora by applying summary vectors to retrievalaugmented language modeling and a passage re-ranking task. Overall, AutoCompressors emerge as a simple and inexpensive solution to extend the context window of LMs while speeding up inference over long contexts.\n\n6. Title: REALM: Retrieval-Augmented Language Model Pre-Training\n   Abstract: REALM: Retrieval-Augmented Language Model Pre-Training: Language model pre-training has been shown to capture a surprising amount of world knowledge, crucial for NLP tasks such as question answering. However, this knowledge is stored implicitly in the parameters of a neural network, requiring ever-larger networks to cover more facts.   To capture knowledge in a more modular and interpretable way, we augment language model pre-training with a latent knowledge retriever, which allows the model to retrieve and attend over documents from a large corpus such as Wikipedia, used during pre-training, fine-tuning and inference. For the first time, we show how to pre-train such a knowledge retriever in an unsupervised manner, using masked language modeling as the learning signal and backpropagating through a retrieval step that considers millions of documents.   We demonstrate the effectiveness of Retrieval-Augmented Language Model pre-training (REALM) by fine-tuning on the challenging task of Open-domain Question Answering (Open-QA). We compare against state-of-the-art models for both explicit and implicit knowledge storage on three popular Open-QA benchmarks, and find that we outperform all previous methods by a significant margin (4-16% absolute accuracy), while also providing qualitative benefits such as interpretability and modularity.\n\n7. Title: Neural Machine Translation with Contrastive Translation Memories\n   Abstract: Neural Machine Translation with Contrastive Translation Memories: Retrieval-augmented Neural Machine Translation models have been successful in many translation scenarios. Different from previous works that make use of mutually similar but redundant translation memories~(TMs), we propose a new retrieval-augmented NMT to model contrastively retrieved translation memories that are holistically similar to the source sentence while individually contrastive to each other providing maximal information gains in three phases. First, in TM retrieval phase, we adopt a contrastive retrieval algorithm to avoid redundancy and uninformativeness of similar translation pieces. Second, in memory encoding stage, given a set of TMs we propose a novel Hierarchical Group Attention module to gather both local context of each TM and global context of the whole TM set. Finally, in training phase, a Multi-TM contrastive learning objective is introduced to learn salient feature of each TM with respect to target sentence. Experimental results show that our framework obtains improvements over strong baselines on the benchmark datasets.\n\n8. Title: LLMLingua: Compressing Prompts for Accelerated Inference of Large Language Models\n   Abstract: LLMLingua: Compressing Prompts for Accelerated Inference of Large Language Models: Large language models (LLMs) have been applied in various applications due to their astonishing capabilities. With advancements in technologies such as chain-of-thought (CoT) prompting and in-context learning (ICL), the prompts fed to LLMs are becoming increasingly lengthy, even exceeding tens of thousands of tokens. To accelerate model inference and reduce cost, this paper presents LLMLingua, a coarse-to-fine prompt compression method that involves a budget controller to maintain semantic integrity under high compression ratios, a token-level iterative compression algorithm to better model the interdependence between compressed contents, and an instruction tuning based method for distribution alignment between language models. We conduct experiments and analysis over four datasets from different scenarios, i.e., GSM8K, BBH, ShareGPT, and Arxiv-March23; showing that the proposed approach yields state-of-the-art performance and allows for up to 20x compression with little performance loss. Our code is available at https://aka.ms/LLMLingua.\n\n9. Title: RECOMP: Improving Retrieval-Augmented LMs with Compression and Selective Augmentation\n   Abstract: RECOMP: Improving Retrieval-Augmented LMs with Compression and Selective Augmentation: Retrieving documents and prepending them in-context at inference time improves performance of language model (LMs) on a wide range of tasks. However, these documents, often spanning hundreds of words, make inference substantially more expensive. We propose compressing the retrieved documents into textual summaries prior to in-context integration. This not only reduces the computational costs but also relieves the burden of LMs to identify relevant information in long retrieved documents. We present two compressors -- an extractive compressor which selects useful sentences from retrieved documents and an abstractive compressor which generates summaries by synthesizing information from multiple documents. Both compressors are trained to improve LMs' performance on end tasks when the generated summaries are prepended to the LMs' input, while keeping the summary concise.If the retrieved documents are irrelevant to the input or offer no additional information to LM, our compressor can return an empty string, implementing selective augmentation.We evaluate our approach on language modeling task and open domain question answering task. We achieve a compression rate of as low as 6% with minimal loss in performance for both tasks, significantly outperforming the off-the-shelf summarization models. We show that our compressors trained for one LM can transfer to other LMs on the language modeling task and provide summaries largely faithful to the retrieved documents.\n\n10. Title: Decouple knowledge from parameters for plug-and-play language modeling\n   Abstract: Decouple knowledge from parameters for plug-and-play language modeling: Pre-trained language models(PLM) have made impressive results in various NLP tasks. It has been revealed that one of the key factors to their success is the parameters of these models implicitly learn all kinds of knowledge during pre-training. However, encoding knowledge implicitly in the model parameters has two fundamental drawbacks. First, the knowledge is neither editable nor scalable once the model is trained, which is especially problematic in that knowledge is consistently evolving. Second, it lacks interpretability and prevents humans from understanding which knowledge PLM requires for a certain problem. In this paper, we introduce PlugLM, a pre-training model with differentiable plug-in memory(DPM). The key intuition is to decouple the knowledge storage from model parameters with an editable and scalable key-value memory and leverage knowledge in an explainable manner by knowledge retrieval in the DPM. To justify this design choice, we conduct evaluations in three settings including: (1) domain adaptation. PlugLM obtains 3.95 F1 improvements across four domains on average without any in-domain pre-training. (2) knowledge update. PlugLM could absorb new knowledge in a training-free way after pre-training is done. (3) in-task knowledge learning. PlugLM could be further improved by incorporating training samples into DPM with knowledge prompting.\n\nThink step-by-step about which candidate best matches the context. Summarize your reasoning, then provide the exact title that should fill the citation. Respond in JSON with fields:\n{\n  \"reasoning\": \"<your step-by-step analysis>\",\n  \"selected_title\": \"<exact candidate title>\"\n}", "response": {"reasoning": "The sentence asserts that augmenting a parametric LM with a non-parametric datastore works for tasks including open-domain question answering. I looked for a candidate that specifically demonstrates retrieval-augmented LMs applied to Open-QA. Candidate 6 (REALM) explicitly introduces retrieval-augmented language model pre-training and reports strong gains on Open-domain Question Answering benchmarks, directly matching the cited claim. Other candidates either focus on machine translation (1,7), passage retrieval for generative QA research (2) or more recent augmentation/fine-tuning methods (3,9,10), but REALM is the canonical work that demonstrates retrieval-augmented LMs for Open-QA. Therefore the correct citation is REALM.", "selected_title": "REALM: Retrieval-Augmented Language Model Pre-Training"}, "selected_title": "REALM: Retrieval-Augmented Language Model Pre-Training", "positive_title": "Leveraging Passage Retrieval with Generative Models for Open Domain Question Answering", "is_correct": false, "reasoning": "The sentence asserts that augmenting a parametric LM with a non-parametric datastore works for tasks including open-domain question answering. I looked for a candidate that specifically demonstrates retrieval-augmented LMs applied to Open-QA. Candidate 6 (REALM) explicitly introduces retrieval-augmented language model pre-training and reports strong gains on Open-domain Question Answering benchmarks, directly matching the cited claim. Other candidates either focus on machine translation (1,7), passage retrieval for generative QA research (2) or more recent augmentation/fine-tuning methods (3,9,10), but REALM is the canonical work that demonstrates retrieval-augmented LMs for Open-QA. Therefore the correct citation is REALM."}
{"example_id": "blind_4606", "prompt": "You are an expert citation retrieval system as described in the paper \"Multi-Agent System for Reliable Citation Retrieval\".\nYour goal is to autonomously retrieve, verify, and recommend academic references given a query or document excerpt.\n\nTask:\nGiven a citation context from a scientific paper (where a citation is missing), identify the correct paper from a list of candidates.\nAnalyze the context to understand the specific claim, method, or result being cited.\nThen, evaluate each candidate paper to see if it matches the context.\nFinally, select the best matching paper.\n\nCitation Context:\n. While one-stage detectors such as FCOS and YOLO are more advantageous for real-world environments because of its efficient structures and high inference speed, most DAOD studies[CITATION] have been conducted on two-stage detectors, such as Faster R-CNN. They use proposals generated by Region Proposal Network (RPN) to obtain instance-level features corresponding to the object\n\nCandidate Papers:\n1. Title: Cross-domain object detection through coarse-to-fine feature adaptation\n   Abstract: Cross-domain Object Detection through Coarse-to-Fine Feature Adaptation: Recent years have witnessed great progress in deep learning based object detection. However, due to the domain shift problem, applying off-the-shelf detectors to an unseen domain leads to significant performance drop. To address such an issue, this paper proposes a novel coarse-to-fine feature adaptation approach to cross-domain object detection. At the coarse-grained stage, different from the rough image-level or instance-level feature alignment used in the literature, foreground regions are extracted by adopting the attention mechanism, and aligned according to their marginal distributions via multi-layer adversarial learning in the common feature space. At the fine-grained stage, we conduct conditional distribution alignment of foregrounds by minimizing the distance of global prototypes with the same category but from different domains. Thanks to this coarse-to-fine feature adaptation, domain knowledge in foreground regions can be effectively transferred. Extensive experiments are carried out in various cross-domain detection scenarios. The results are state-of-the-art, which demonstrate the broad applicability and effectiveness of the proposed approach.\n\n2. Title: Adaptive Object Detection with Dual Multi-Label Prediction\n   Abstract: Adaptive Object Detection with Dual Multi-Label Prediction: In this paper, we propose a novel end-to-end unsupervised deep domain adaptation model for adaptive object detection by exploiting multi-label object recognition as a dual auxiliary task. The model exploits multi-label prediction to reveal the object category information in each image and then uses the prediction results to perform conditional adversarial global feature alignment, such that the multi-modal structure of image features can be tackled to bridge the domain divergence at the global feature level while preserving the discriminability of the features. Moreover, we introduce a prediction consistency regularization mechanism to assist object detection, which uses the multi-label prediction results as an auxiliary regularization information to ensure consistent object category discoveries between the object recognition task and the object detection task. Experiments are conducted on a few benchmark datasets and the results show the proposed model outperforms the state-of-the-art comparison methods.\n\n3. Title: A robust learning approach to domain adaptive object detection\n   Abstract: A Robust Learning Approach to Domain Adaptive Object Detection: Domain shift is unavoidable in real-world applications of object detection. For example, in self-driving cars, the target domain consists of unconstrained road environments which cannot all possibly be observed in training data. Similarly, in surveillance applications sufficiently representative training data may be lacking due to privacy regulations. In this paper, we address the domain adaptation problem from the perspective of robust learning and show that the problem may be formulated as training with noisy labels. We propose a robust object detection framework that is resilient to noise in bounding box class labels, locations and size annotations. To adapt to the domain shift, the model is trained on the target domain using a set of noisy object bounding boxes that are obtained by a detection model trained only in the source domain. We evaluate the accuracy of our approach in various source/target domain pairs and demonstrate that the model significantly improves the state-of-the-art on multiple domain adaptation scenarios on the SIM10K, Cityscapes and KITTI datasets.\n\n4. Title: Domain Adaptive Faster R-CNN for Object Detection in the Wild\n   Abstract: Domain Adaptive Faster R-CNN for Object Detection in the Wild: Object detection typically assumes that training and test data are drawn from an identical distribution, which, however, does not always hold in practice. Such a distribution mismatch will lead to a significant performance drop. In this work, we aim to improve the cross-domain robustness of object detection. We tackle the domain shift on two levels: 1) the image-level shift, such as image style, illumination, etc, and 2) the instance-level shift, such as object appearance, size, etc. We build our approach based on the recent state-of-the-art Faster R-CNN model, and design two domain adaptation components, on image level and instance level, to reduce the domain discrepancy. The two domain adaptation components are based on H-divergence theory, and are implemented by learning a domain classifier in adversarial training manner. The domain classifiers on different levels are further reinforced with a consistency regularization to learn a domain-invariant region proposal network (RPN) in the Faster R-CNN model. We evaluate our newly proposed approach using multiple datasets including Cityscapes, KITTI, SIM10K, etc. The results demonstrate the effectiveness of our proposed approach for robust object detection in various domain shift scenarios.\n\n5. Title: Cross-domain Detection via Graph-induced Prototype Alignment\n   Abstract: Cross-domain Detection via Graph-induced Prototype Alignment: Applying the knowledge of an object detector trained on a specific domain directly onto a new domain is risky, as the gap between two domains can severely degrade model's performance. Furthermore, since different instances commonly embody distinct modal information in object detection scenario, the feature alignment of source and target domain is hard to be realized. To mitigate these problems, we propose a Graph-induced Prototype Alignment (GPA) framework to seek for category-level domain alignment via elaborate prototype representations. In the nutshell, more precise instance-level features are obtained through graph-based information propagation among region proposals, and, on such basis, the prototype representation of each class is derived for category-level domain alignment. In addition, in order to alleviate the negative effect of class-imbalance on domain adaptation, we design a Class-reweighted Contrastive Loss to harmonize the adaptation training process. Combining with Faster R-CNN, the proposed framework conducts feature alignment in a two-stage manner. Comprehensive results on various cross-domain detection tasks demonstrate that our approach outperforms existing methods with a remarkable margin. Our code is available at https://github.com/ChrisAllenMing/GPA-detection.\n\n6. Title: You Only Look Once: Unified, Real-Time Object Detection\n   Abstract: You Only Look Once: Unified, Real-Time Object Detection: We present YOLO, a new approach to object detection. Prior work on object detection repurposes classifiers to perform detection. Instead, we frame object detection as a regression problem to spatially separated bounding boxes and associated class probabilities. A single neural network predicts bounding boxes and class probabilities directly from full images in one evaluation. Since the whole detection pipeline is a single network, it can be optimized end-to-end directly on detection performance.   Our unified architecture is extremely fast. Our base YOLO model processes images in real-time at 45 frames per second. A smaller version of the network, Fast YOLO, processes an astounding 155 frames per second while still achieving double the mAP of other real-time detectors. Compared to state-of-the-art detection systems, YOLO makes more localization errors but is far less likely to predict false detections where nothing exists. Finally, YOLO learns very general representations of objects. It outperforms all other detection methods, including DPM and R-CNN, by a wide margin when generalizing from natural images to artwork on both the Picasso Dataset and the People-Art Dataset.\n\n7. Title: The cityscapes dataset for semantic urban scene understanding\n   Abstract: The Cityscapes Dataset for Semantic Urban Scene Understanding: Visual understanding of complex urban street scenes is an enabling factor for a wide range of applications. Object detection has benefited enormously from large-scale datasets, especially in the context of deep learning. For semantic urban scene understanding, however, no current dataset adequately captures the complexity of real-world urban scenes.   To address this, we introduce Cityscapes, a benchmark suite and large-scale dataset to train and test approaches for pixel-level and instance-level semantic labeling. Cityscapes is comprised of a large, diverse set of stereo video sequences recorded in streets from 50 different cities. 5000 of these images have high quality pixel-level annotations; 20000 additional images have coarse annotations to enable methods that leverage large volumes of weakly-labeled data. Crucially, our effort exceeds previous attempts in terms of dataset size, annotation richness, scene variability, and complexity. Our accompanying empirical study provides an in-depth analysis of the dataset characteristics, as well as a performance evaluation of several state-of-the-art approaches based on our benchmark.\n\n8. Title: Exploiting Inter-pixel Correlations in Unsupervised Domain Adaptation for Semantic Segmentation\n   Abstract: Exploiting Inter-pixel Correlations in Unsupervised Domain Adaptation for Semantic Segmentation: \"Self-training\" has become a dominant method for semantic segmentation via unsupervised domain adaptation (UDA). It creates a set of pseudo labels for the target domain to give explicit supervision. However, the pseudo labels are noisy, sparse and do not provide any information about inter-pixel correlations. We regard inter-pixel correlation quite important because semantic segmentation is a task of predicting highly structured pixel-level outputs. Therefore, in this paper, we propose a method of transferring the inter-pixel correlations from the source domain to the target domain via a self-attention module. The module takes the prediction of the segmentation network as an input and creates a self-attended prediction that correlates similar pixels. The module is trained only on the source domain to learn the domain-invariant inter-pixel correlations, then later, it is used to train the segmentation network on the target domain. The network learns not only from the pseudo labels but also by following the output of the self-attention module which provides additional knowledge about the inter-pixel correlations. Through extensive experiments, we show that our method significantly improves the performance on two standard UDA benchmarks and also can be combined with recent state-of-the-art method to achieve better performance.\n\n9. Title: Fcos: Fully convolutional one-stage object detection\n   Abstract: FCOS: Fully Convolutional One-Stage Object Detection: We propose a fully convolutional one-stage object detector (FCOS) to solve object detection in a per-pixel prediction fashion, analogue to semantic segmentation. Almost all state-of-the-art object detectors such as RetinaNet, SSD, YOLOv3, and Faster R-CNN rely on pre-defined anchor boxes. In contrast, our proposed detector FCOS is anchor box free, as well as proposal free. By eliminating the predefined set of anchor boxes, FCOS completely avoids the complicated computation related to anchor boxes such as calculating overlapping during training. More importantly, we also avoid all hyper-parameters related to anchor boxes, which are often very sensitive to the final detection performance. With the only post-processing non-maximum suppression (NMS), FCOS with ResNeXt-64x4d-101 achieves 44.7% in AP with single-model and single-scale testing, surpassing previous one-stage detectors with the advantage of being much simpler. For the first time, we demonstrate a much simpler and flexible detection framework achieving improved detection accuracy. We hope that the proposed FCOS framework can serve as a simple and strong alternative for many other instance-level tasks. Code is available at:Code is available at: https://tinyurl.com/FCOSv1\n\n10. Title: Strong-weak distribution alignment for adaptive object detection\n   Abstract: Strong-Weak Distribution Alignment for Adaptive Object Detection: We propose an approach for unsupervised adaptation of object detectors from label-rich to label-poor domains which can significantly reduce annotation costs associated with detection. Recently, approaches that align distributions of source and target images using an adversarial loss have been proven effective for adapting object classifiers. However, for object detection, fully matching the entire distributions of source and target images to each other at the global image level may fail, as domains could have distinct scene layouts and different combinations of objects. On the other hand, strong matching of local features such as texture and color makes sense, as it does not change category level semantics. This motivates us to propose a novel method for detector adaptation based on strong local alignment and weak global alignment. Our key contribution is the weak alignment model, which focuses the adversarial alignment loss on images that are globally similar and puts less emphasis on aligning images that are globally dissimilar. Additionally, we design the strong domain alignment model to only look at local receptive fields of the feature map. We empirically verify the effectiveness of our method on four datasets comprising both large and small domain shifts. Our code is available at \\url{https://github.com/VisionLearningGroup/DA_Detection}\n\nThink step-by-step about which candidate best matches the context. Summarize your reasoning, then provide the exact title that should fill the citation. Respond in JSON with fields:\n{\n  \"reasoning\": \"<your step-by-step analysis>\",\n  \"selected_title\": \"<exact candidate title>\"\n}", "response": {"reasoning": "The sentence claims that most domain-adaptive object detection (DAOD) work has focused on two-stage detectors like Faster R-CNN, which use an RPN to obtain instance-level features. I checked the candidates: several are DAOD or cross-domain detection papers (1,2,3,4,5,10). Candidate 4 is explicitly titled \"Domain Adaptive Faster R-CNN for Object Detection in the Wild\" and its abstract states it builds on Faster R-CNN and designs image-level and instance-level domain adaptation components, using an RPN and instance-level alignment â€” exactly matching the claim. Candidate 5 also integrates with Faster R-CNN, but candidate 4 is the canonical, direct reference that states Faster R-CNN and RPN-based instance-level adaptation. Therefore candidate 4 best matches the citation context.", "selected_title": "Domain Adaptive Faster R-CNN for Object Detection in the Wild"}, "selected_title": "Domain Adaptive Faster R-CNN for Object Detection in the Wild", "positive_title": "Domain Adaptive Faster R-CNN for Object Detection in the Wild", "is_correct": true, "reasoning": "The sentence claims that most domain-adaptive object detection (DAOD) work has focused on two-stage detectors like Faster R-CNN, which use an RPN to obtain instance-level features. I checked the candidates: several are DAOD or cross-domain detection papers (1,2,3,4,5,10). Candidate 4 is explicitly titled \"Domain Adaptive Faster R-CNN for Object Detection in the Wild\" and its abstract states it builds on Faster R-CNN and designs image-level and instance-level domain adaptation components, using an RPN and instance-level alignment â€” exactly matching the claim. Candidate 5 also integrates with Faster R-CNN, but candidate 4 is the canonical, direct reference that states Faster R-CNN and RPN-based instance-level adaptation. Therefore candidate 4 best matches the citation context."}
{"example_id": "blind_3084", "prompt": "You are an expert citation retrieval system as described in the paper \"Multi-Agent System for Reliable Citation Retrieval\".\nYour goal is to autonomously retrieve, verify, and recommend academic references given a query or document excerpt.\n\nTask:\nGiven a citation context from a scientific paper (where a citation is missing), identify the correct paper from a list of candidates.\nAnalyze the context to understand the specific claim, method, or result being cited.\nThen, evaluate each candidate paper to see if it matches the context.\nFinally, select the best matching paper.\n\nCitation Context:\nconsidered the least expensive adversarial attack. \\begin{equation} x' = x+\\epsilon\\cdot\\text{sgn}(\\nabla_x \\ell(x, y; \\theta)) \\end{equation} \\textbf{Projected gradient descent (PGD)}[CITATION] uses multiple gradients to generate more powerful adversarial examples. With a step size $\\alpha$, PGD can be formalized as follows: \\begin{equation} x^{t+1} = \\Pi_{\\mathcal{B}(x, \\epsilon)} (x^t\n\nCandidate Papers:\n1. Title: Towards deep learning models resistant to adversarial attacks\n   Abstract: Towards Deep Learning Models Resistant to Adversarial Attacks: Recent work has demonstrated that deep neural networks are vulnerable to adversarial examples---inputs that are almost indistinguishable from natural data and yet classified incorrectly by the network. In fact, some of the latest findings suggest that the existence of adversarial attacks may be an inherent weakness of deep learning models. To address this problem, we study the adversarial robustness of neural networks through the lens of robust optimization. This approach provides us with a broad and unifying view on much of the prior work on this topic. Its principled nature also enables us to identify methods for both training and attacking neural networks that are reliable and, in a certain sense, universal. In particular, they specify a concrete security guarantee that would protect against any adversary. These methods let us train networks with significantly improved resistance to a wide range of adversarial attacks. They also suggest the notion of security against a first-order adversary as a natural and broad security guarantee. We believe that robustness against such well-defined classes of adversaries is an important stepping stone towards fully resistant deep learning models. Code and pre-trained models are available at https://github.com/MadryLab/mnist_challenge and https://github.com/MadryLab/cifar10_challenge.\n\n2. Title: Ensemble adversarial training: Attacks and defenses\n   Abstract: Ensemble Adversarial Training: Attacks and Defenses: Adversarial examples are perturbed inputs designed to fool machine learning models. Adversarial training injects such examples into training data to increase robustness. To scale this technique to large datasets, perturbations are crafted using fast single-step methods that maximize a linear approximation of the model's loss. We show that this form of adversarial training converges to a degenerate global minimum, wherein small curvature artifacts near the data points obfuscate a linear approximation of the loss. The model thus learns to generate weak perturbations, rather than defend against strong ones. As a result, we find that adversarial training remains vulnerable to black-box attacks, where we transfer perturbations computed on undefended models, as well as to a powerful novel single-step attack that escapes the non-smooth vicinity of the input data via a small random step. We further introduce Ensemble Adversarial Training, a technique that augments training data with perturbations transferred from other models. On ImageNet, Ensemble Adversarial Training yields models with strong robustness to black-box attacks. In particular, our most robust model won the first round of the NIPS 2017 competition on Defenses against Adversarial Attacks. However, subsequent work found that more elaborate black-box attacks could significantly enhance transferability and reduce the accuracy of our models.\n\n3. Title: Robustness to adversarial perturbations in learning from incomplete data\n   Abstract: Robustness to Adversarial Perturbations in Learning from Incomplete Data: What is the role of unlabeled data in an inference problem, when the presumed underlying distribution is adversarially perturbed? To provide a concrete answer to this question, this paper unifies two major learning frameworks: Semi-Supervised Learning (SSL) and Distributionally Robust Learning (DRL). We develop a generalization theory for our framework based on a number of novel complexity measures, such as an adversarial extension of Rademacher complexity and its semi-supervised analogue. Moreover, our analysis is able to quantify the role of unlabeled data in the generalization under a more general condition compared to the existing theoretical works in SSL. Based on our framework, we also present a hybrid of DRL and EM algorithms that has a guaranteed convergence rate. When implemented with deep neural networks, our method shows a comparable performance to those of the state-of-the-art on a number of real-world benchmark datasets.\n\n4. Title: Mixup inference: Better exploiting mixup to defend adversarial attacks\n   Abstract: Mixup Inference: Better Exploiting Mixup to Defend Adversarial Attacks: It has been widely recognized that adversarial examples can be easily crafted to fool deep networks, which mainly root from the locally non-linear behavior nearby input examples. Applying mixup in training provides an effective mechanism to improve generalization performance and model robustness against adversarial perturbations, which introduces the globally linear behavior in-between training examples. However, in previous work, the mixup-trained models only passively defend adversarial attacks in inference by directly classifying the inputs, where the induced global linearity is not well exploited. Namely, since the locality of the adversarial perturbations, it would be more efficient to actively break the locality via the globality of the model predictions. Inspired by simple geometric intuition, we develop an inference principle, named mixup inference (MI), for mixup-trained models. MI mixups the input with other random clean samples, which can shrink and transfer the equivalent perturbation if the input is adversarial. Our experiments on CIFAR-10 and CIFAR-100 demonstrate that MI can further improve the adversarial robustness for the models trained by mixup and its variants.\n\n5. Title: mixup: Beyond empirical risk minimization\n   Abstract: mixup: Beyond Empirical Risk Minimization: Large deep neural networks are powerful, but exhibit undesirable behaviors such as memorization and sensitivity to adversarial examples. In this work, we propose mixup, a simple learning principle to alleviate these issues. In essence, mixup trains a neural network on convex combinations of pairs of examples and their labels. By doing so, mixup regularizes the neural network to favor simple linear behavior in-between training examples. Our experiments on the ImageNet-2012, CIFAR-10, CIFAR-100, Google commands and UCI datasets show that mixup improves the generalization of state-of-the-art neural network architectures. We also find that mixup reduces the memorization of corrupt labels, increases the robustness to adversarial examples, and stabilizes the training of generative adversarial networks.\n\n6. Title: Are Labels Required for Improving Adversarial Robustness?\n   Abstract: Are Labels Required for Improving Adversarial Robustness?: Recent work has uncovered the interesting (and somewhat surprising) finding that training models to be invariant to adversarial perturbations requires substantially larger datasets than those required for standard classification. This result is a key hurdle in the deployment of robust machine learning models in many real world applications where labeled data is expensive. Our main insight is that unlabeled data can be a competitive alternative to labeled data for training adversarially robust models. Theoretically, we show that in a simple statistical setting, the sample complexity for learning an adversarially robust model from unlabeled data matches the fully supervised case up to constant factors. On standard datasets like CIFAR-10, a simple Unsupervised Adversarial Training (UAT) approach using unlabeled data improves robust accuracy by 21.7% over using 4K supervised examples alone, and captures over 95% of the improvement from the same number of labeled examples. Finally, we report an improvement of 4% over the previous state-of-the-art on CIFAR-10 against the strongest known attack by using additional unlabeled data from the uncurated 80 Million Tiny Images dataset. This demonstrates that our finding extends as well to the more realistic case where unlabeled data is also uncurated, therefore opening a new avenue for improving adversarial training.\n\n7. Title: Fast is better than free: Revisiting adversarial training\n   Abstract: Fast is better than free: Revisiting adversarial training: Adversarial training, a method for learning robust deep networks, is typically assumed to be more expensive than traditional training due to the necessity of constructing adversarial examples via a first-order method like projected gradient decent (PGD). In this paper, we make the surprising discovery that it is possible to train empirically robust models using a much weaker and cheaper adversary, an approach that was previously believed to be ineffective, rendering the method no more costly than standard training in practice. Specifically, we show that adversarial training with the fast gradient sign method (FGSM), when combined with random initialization, is as effective as PGD-based training but has significantly lower cost. Furthermore we show that FGSM adversarial training can be further accelerated by using standard techniques for efficient training of deep networks, allowing us to learn a robust CIFAR10 classifier with 45% robust accuracy to PGD attacks with $\\epsilon=8/255$ in 6 minutes, and a robust ImageNet classifier with 43% robust accuracy at $\\epsilon=2/255$ in 12 hours, in comparison to past work based on \"free\" adversarial training which took 10 and 50 hours to reach the same respective thresholds. Finally, we identify a failure mode referred to as \"catastrophic overfitting\" which may have caused previous attempts to use FGSM adversarial training to fail. All code for reproducing the experiments in this paper as well as pretrained model weights are at https://github.com/locuslab/fast_adversarial.\n\n8. Title: Explaining and harnessing adversarial examples\n   Abstract: Explaining and Harnessing Adversarial Examples: Several machine learning models, including neural networks, consistently misclassify adversarial examples---inputs formed by applying small but intentionally worst-case perturbations to examples from the dataset, such that the perturbed input results in the model outputting an incorrect answer with high confidence. Early attempts at explaining this phenomenon focused on nonlinearity and overfitting. We argue instead that the primary cause of neural networks' vulnerability to adversarial perturbation is their linear nature. This explanation is supported by new quantitative results while giving the first explanation of the most intriguing fact about them: their generalization across architectures and training sets. Moreover, this view yields a simple and fast method of generating adversarial examples. Using this approach to provide examples for adversarial training, we reduce the test set error of a maxout network on the MNIST dataset.\n\n9. Title: Intriguing properties of neural networks\n   Abstract: Intriguing properties of neural networks: Deep neural networks are highly expressive models that have recently achieved state of the art performance on speech and visual recognition tasks. While their expressiveness is the reason they succeed, it also causes them to learn uninterpretable solutions that could have counter-intuitive properties. In this paper we report two such properties.   First, we find that there is no distinction between individual high level units and random linear combinations of high level units, according to various methods of unit analysis. It suggests that it is the space, rather than the individual units, that contains of the semantic information in the high layers of neural networks.   Second, we find that deep neural networks learn input-output mappings that are fairly discontinuous to a significant extend. We can cause the network to misclassify an image by applying a certain imperceptible perturbation, which is found by maximizing the network's prediction error. In addition, the specific nature of these perturbations is not a random artifact of learning: the same perturbation can cause a different network, that was trained on a different subset of the dataset, to misclassify the same input.\n\nThink step-by-step about which candidate best matches the context. Summarize your reasoning, then provide the exact title that should fill the citation. Respond in JSON with fields:\n{\n  \"reasoning\": \"<your step-by-step analysis>\",\n  \"selected_title\": \"<exact candidate title>\"\n}", "response": {"reasoning": "The context first shows the FGSM update x' = x + Îµ Â· sgn(âˆ‡_x â„“(x,y;Î¸)), which is from Goodfellow et al. (Explaining and Harnessing Adversarial Examples). It then introduces Projected Gradient Descent (PGD) as an iterative, multi-step adversary and gives the iterative projection update x^{t+1} = Î _{B(x,Îµ)}(x^t ...). The canonical source that formalizes PGD as a strong iterative first-order adversary and frames robustness via PGD (including the projected gradient update and the security-against-first-order-adversary perspective) is Madry et al., titled â€œTowards Deep Learning Models Resistant to Adversarial Attacks.â€ Among the candidates, paper 1 is that Madry et al. work and precisely matches the PGD citation. Other candidates either introduce FGSM (paper 8) or discuss related defenses/training techniques, but do not serve as the primary source that formalizes PGD as the iterative projected-gradient adversary. Therefore the correct citation for PGD here is paper 1.", "selected_title": "Towards deep learning models resistant to adversarial attacks"}, "selected_title": "Towards deep learning models resistant to adversarial attacks", "positive_title": "Towards deep learning models resistant to adversarial attacks", "is_correct": true, "reasoning": "The context first shows the FGSM update x' = x + Îµ Â· sgn(âˆ‡_x â„“(x,y;Î¸)), which is from Goodfellow et al. (Explaining and Harnessing Adversarial Examples). It then introduces Projected Gradient Descent (PGD) as an iterative, multi-step adversary and gives the iterative projection update x^{t+1} = Î _{B(x,Îµ)}(x^t ...). The canonical source that formalizes PGD as a strong iterative first-order adversary and frames robustness via PGD (including the projected gradient update and the security-against-first-order-adversary perspective) is Madry et al., titled â€œTowards Deep Learning Models Resistant to Adversarial Attacks.â€ Among the candidates, paper 1 is that Madry et al. work and precisely matches the PGD citation. Other candidates either introduce FGSM (paper 8) or discuss related defenses/training techniques, but do not serve as the primary source that formalizes PGD as the iterative projected-gradient adversary. Therefore the correct citation for PGD here is paper 1."}
{"example_id": "blind_18578", "prompt": "You are an expert citation retrieval system as described in the paper \"Multi-Agent System for Reliable Citation Retrieval\".\nYour goal is to autonomously retrieve, verify, and recommend academic references given a query or document excerpt.\n\nTask:\nGiven a citation context from a scientific paper (where a citation is missing), identify the correct paper from a list of candidates.\nAnalyze the context to understand the specific claim, method, or result being cited.\nThen, evaluate each candidate paper to see if it matches the context.\nFinally, select the best matching paper.\n\nCitation Context:\nite_6|>, making it inefficient for real-world adversarial attacks. To improve the transferability of adversarial attacks, various techniques have been proposed, such as advanced gradient calculations[CITATION], ensemble-model attacks, input transformations and model-specific methods. The input transformation (\\eg randomly resizing and padding, translation, scale \\etc) is one\n\nCandidate Papers:\n1. Title: Imagenet large scale visual recognition challenge\n   Abstract: ImageNet Large Scale Visual Recognition Challenge: The ImageNet Large Scale Visual Recognition Challenge is a benchmark in object category classification and detection on hundreds of object categories and millions of images. The challenge has been run annually from 2010 to present, attracting participation from more than fifty institutions.   This paper describes the creation of this benchmark dataset and the advances in object recognition that have been possible as a result. We discuss the challenges of collecting large-scale ground truth annotation, highlight key breakthroughs in categorical object recognition, provide a detailed analysis of the current state of the field of large-scale image classification and object detection, and compare the state-of-the-art computer vision accuracy with human accuracy. We conclude with lessons learned in the five years of the challenge, and propose future directions and improvements.\n\n2. Title: Delving into transferable adversarial examples and black-box attacks\n   Abstract: Delving into Transferable Adversarial Examples and Black-box Attacks: An intriguing property of deep neural networks is the existence of adversarial examples, which can transfer among different architectures. These transferable adversarial examples may severely hinder deep neural network-based applications. Previous works mostly study the transferability using small scale datasets. In this work, we are the first to conduct an extensive study of the transferability over large models and a large scale dataset, and we are also the first to study the transferability of targeted adversarial examples with their target labels. We study both non-targeted and targeted adversarial examples, and show that while transferable non-targeted adversarial examples are easy to find, targeted adversarial examples generated using existing approaches almost never transfer with their target labels. Therefore, we propose novel ensemble-based approaches to generating transferable adversarial examples. Using such approaches, we observe a large proportion of targeted adversarial examples that are able to transfer with their target labels for the first time. We also present some geometric studies to help understanding the transferable adversarial examples. Finally, we show that the adversarial examples generated using ensemble-based approaches can successfully attack Clarifai.com, which is a black-box image classification system.\n\n3. Title: Manifold mixup: Better representations by interpolating hidden states\n   Abstract: Manifold Mixup: Better Representations by Interpolating Hidden States: Deep neural networks excel at learning the training data, but often provide incorrect and confident predictions when evaluated on slightly different test examples. This includes distribution shifts, outliers, and adversarial examples. To address these issues, we propose Manifold Mixup, a simple regularizer that encourages neural networks to predict less confidently on interpolations of hidden representations. Manifold Mixup leverages semantic interpolations as additional training signal, obtaining neural networks with smoother decision boundaries at multiple levels of representation. As a result, neural networks trained with Manifold Mixup learn class-representations with fewer directions of variance. We prove theory on why this flattening happens under ideal conditions, validate it on practical situations, and connect it to previous works on information theory and generalization. In spite of incurring no significant computation and being implemented in a few lines of code, Manifold Mixup improves strong baselines in supervised learning, robustness to single-step adversarial attacks, and test log-likelihood.\n\n4. Title: Explaining and harnessing adversarial examples\n   Abstract: Explaining and Harnessing Adversarial Examples: Several machine learning models, including neural networks, consistently misclassify adversarial examples---inputs formed by applying small but intentionally worst-case perturbations to examples from the dataset, such that the perturbed input results in the model outputting an incorrect answer with high confidence. Early attempts at explaining this phenomenon focused on nonlinearity and overfitting. We argue instead that the primary cause of neural networks' vulnerability to adversarial perturbation is their linear nature. This explanation is supported by new quantitative results while giving the first explanation of the most intriguing fact about them: their generalization across architectures and training sets. Moreover, this view yields a simple and fast method of generating adversarial examples. Using this approach to provide examples for adversarial training, we reduce the test set error of a maxout network on the MNIST dataset.\n\n5. Title: Interpolated adversarial training: Achieving robust neural networks without sacrificing too much accuracy\n   Abstract: Interpolated Adversarial Training: Achieving Robust Neural Networks without Sacrificing Too Much Accuracy: Adversarial robustness has become a central goal in deep learning, both in the theory and the practice. However, successful methods to improve the adversarial robustness (such as adversarial training) greatly hurt generalization performance on the unperturbed data. This could have a major impact on how the adversarial robustness affects real world systems (i.e. many may opt to forego robustness if it can improve accuracy on the unperturbed data). We propose Interpolated Adversarial Training, which employs recently proposed interpolation based training methods in the framework of adversarial training. On CIFAR-10, adversarial training increases the standard test error (when there is no adversary) from 4.43% to 12.32%, whereas with our Interpolated adversarial training we retain the adversarial robustness while achieving a standard test error of only 6.45%. With our technique, the relative increase in the standard error for the robust model is reduced from 178.1% to just 45.5%. Moreover, we provide mathematical analysis of Interpolated Adversarial Training to confirm its efficiencies and demonstrate its advantages in terms of robustness and generalization.\n\n6. Title: Evading defenses to transferable adversarial examples by translation-invariant attacks\n   Abstract: Evading Defenses to Transferable Adversarial Examples by Translation-Invariant Attacks: Deep neural networks are vulnerable to adversarial examples, which can mislead classifiers by adding imperceptible perturbations. An intriguing property of adversarial examples is their good transferability, making black-box attacks feasible in real-world applications. Due to the threat of adversarial attacks, many methods have been proposed to improve the robustness. Several state-of-the-art defenses are shown to be robust against transferable adversarial examples. In this paper, we propose a translation-invariant attack method to generate more transferable adversarial examples against the defense models. By optimizing a perturbation over an ensemble of translated images, the generated adversarial example is less sensitive to the white-box model being attacked and has better transferability. To improve the efficiency of attacks, we further show that our method can be implemented by convolving the gradient at the untranslated image with a pre-defined kernel. Our method is generally applicable to any gradient-based attack method. Extensive experiments on the ImageNet dataset validate the effectiveness of the proposed method. Our best attack fools eight state-of-the-art defenses at an 82% success rate on average based only on the transferability, demonstrating the insecurity of the current defense techniques.\n\n7. Title: Feature squeezing: Detecting adversarial examples in deep neural networks\n   Abstract: Feature Squeezing: Detecting Adversarial Examples in Deep Neural Networks: Although deep neural networks (DNNs) have achieved great success in many tasks, they can often be fooled by \\emph{adversarial examples} that are generated by adding small but purposeful distortions to natural examples. Previous studies to defend against adversarial examples mostly focused on refining the DNN models, but have either shown limited success or required expensive computation. We propose a new strategy, \\emph{feature squeezing}, that can be used to harden DNN models by detecting adversarial examples. Feature squeezing reduces the search space available to an adversary by coalescing samples that correspond to many different feature vectors in the original space into a single sample. By comparing a DNN model's prediction on the original input with that on squeezed inputs, feature squeezing detects adversarial examples with high accuracy and few false positives. This paper explores two feature squeezing methods: reducing the color bit depth of each pixel and spatial smoothing. These simple strategies are inexpensive and complementary to other defenses, and can be combined in a joint detection framework to achieve high detection rates against state-of-the-art attacks.\n\n8. Title: Towards deep learning models resistant to adversarial attacks\n   Abstract: Towards Deep Learning Models Resistant to Adversarial Attacks: Recent work has demonstrated that deep neural networks are vulnerable to adversarial examples---inputs that are almost indistinguishable from natural data and yet classified incorrectly by the network. In fact, some of the latest findings suggest that the existence of adversarial attacks may be an inherent weakness of deep learning models. To address this problem, we study the adversarial robustness of neural networks through the lens of robust optimization. This approach provides us with a broad and unifying view on much of the prior work on this topic. Its principled nature also enables us to identify methods for both training and attacking neural networks that are reliable and, in a certain sense, universal. In particular, they specify a concrete security guarantee that would protect against any adversary. These methods let us train networks with significantly improved resistance to a wide range of adversarial attacks. They also suggest the notion of security against a first-order adversary as a natural and broad security guarantee. We believe that robustness against such well-defined classes of adversaries is an important stepping stone towards fully resistant deep learning models. Code and pre-trained models are available at https://github.com/MadryLab/mnist_challenge and https://github.com/MadryLab/cifar10_challenge.\n\n9. Title: Boosting adversarial attacks with momentum\n   Abstract: Boosting Adversarial Attacks with Momentum: Deep neural networks are vulnerable to adversarial examples, which poses security concerns on these algorithms due to the potentially severe consequences. Adversarial attacks serve as an important surrogate to evaluate the robustness of deep learning models before they are deployed. However, most of existing adversarial attacks can only fool a black-box model with a low success rate. To address this issue, we propose a broad class of momentum-based iterative algorithms to boost adversarial attacks. By integrating the momentum term into the iterative process for attacks, our methods can stabilize update directions and escape from poor local maxima during the iterations, resulting in more transferable adversarial examples. To further improve the success rates for black-box attacks, we apply momentum iterative algorithms to an ensemble of models, and show that the adversarially trained models with a strong defense ability are also vulnerable to our black-box attacks. We hope that the proposed methods will serve as a benchmark for evaluating the robustness of various deep models and defense methods. With this method, we won the first places in NIPS 2017 Non-targeted Adversarial Attack and Targeted Adversarial Attack competitions.\n\nThink step-by-step about which candidate best matches the context. Summarize your reasoning, then provide the exact title that should fill the citation. Respond in JSON with fields:\n{\n  \"reasoning\": \"<your step-by-step analysis>\",\n  \"selected_title\": \"<exact candidate title>\"\n}", "response": {"reasoning": "1) The citation in context specifically follows the phrase \"advanced gradient calculations,\" listed alongside \"ensemble-model attacks, input transformations and model-specific methods.\" This indicates the cited work should propose a gradient-based improvement to generate more transferable adversarial examples. 2) I checked candidates: (2) focuses on ensemble-based attacks (matches the next item), (6) and (input-transformation text) concern translation/invariance and input transformations (matches a later item), (4) introduces FGSM (early gradient method) but is the original fast method rather than an \"advanced\" gradient calculation for transferability, and (9) proposes momentum-based iterative algorithms that modify gradient updates to stabilize direction and increase transferability. 3) The description of candidate 9 explicitly says it integrates a momentum term into iterative gradient updates to boost transferability and black-box success â€” a clear match for \"advanced gradient calculations.\" Therefore the best match for the cited work is candidate 9.", "selected_title": "Boosting adversarial attacks with momentum"}, "selected_title": "Boosting adversarial attacks with momentum", "positive_title": "Boosting adversarial attacks with momentum", "is_correct": true, "reasoning": "1) The citation in context specifically follows the phrase \"advanced gradient calculations,\" listed alongside \"ensemble-model attacks, input transformations and model-specific methods.\" This indicates the cited work should propose a gradient-based improvement to generate more transferable adversarial examples. 2) I checked candidates: (2) focuses on ensemble-based attacks (matches the next item), (6) and (input-transformation text) concern translation/invariance and input transformations (matches a later item), (4) introduces FGSM (early gradient method) but is the original fast method rather than an \"advanced\" gradient calculation for transferability, and (9) proposes momentum-based iterative algorithms that modify gradient updates to stabilize direction and increase transferability. 3) The description of candidate 9 explicitly says it integrates a momentum term into iterative gradient updates to boost transferability and black-box success â€” a clear match for \"advanced gradient calculations.\" Therefore the best match for the cited work is candidate 9."}
{"example_id": "blind_19933", "prompt": "You are an expert citation retrieval system as described in the paper \"Multi-Agent System for Reliable Citation Retrieval\".\nYour goal is to autonomously retrieve, verify, and recommend academic references given a query or document excerpt.\n\nTask:\nGiven a citation context from a scientific paper (where a citation is missing), identify the correct paper from a list of candidates.\nAnalyze the context to understand the specific claim, method, or result being cited.\nThen, evaluate each candidate paper to see if it matches the context.\nFinally, select the best matching paper.\n\nCitation Context:\n> propose a information-theoretic framework and prove that their method can learn ground-truth Bayesian posterior classifier for each modality and the Bayesian posterior aggregator for all modalities.[CITATION] proves that the multi-modal models can learn better representations than single-modal models in certain conditions. However, both of their analysis do not focus on the multi-modal \\emph{contrastive}\n\nCandidate Papers:\n1. Title: A theoretical analysis of contrastive unsupervised representation learning\n   Abstract: A Theoretical Analysis of Contrastive Unsupervised Representation Learning: Recent empirical works have successfully used unlabeled data to learn feature representations that are broadly useful in downstream classification tasks. Several of these methods are reminiscent of the well-known word2vec embedding algorithm: leveraging availability of pairs of semantically \"similar\" data points and \"negative samples,\" the learner forces the inner product of representations of similar pairs with each other to be higher on average than with negative samples. The current paper uses the term contrastive learning for such algorithms and presents a theoretical framework for analyzing them by introducing latent classes and hypothesizing that semantically similar points are sampled from the same latent class. This framework allows us to show provable guarantees on the performance of the learned representations on the average classification task that is comprised of a subset of the same set of latent classes. Our generalization bound also shows that learned representations can reduce (labeled) sample complexity on downstream tasks. We conduct controlled experiments in both the text and image domains to support the theory.\n\n2. Title: Residual relaxation for multi-view representation learning\n   Abstract: Residual Relaxation for Multi-view Representation Learning: Multi-view methods learn representations by aligning multiple views of the same image and their performance largely depends on the choice of data augmentation. In this paper, we notice that some other useful augmentations, such as image rotation, are harmful for multi-view methods because they cause a semantic shift that is too large to be aligned well. This observation motivates us to relax the exact alignment objective to better cultivate stronger augmentations. Taking image rotation as a case study, we develop a generic approach, Pretext-aware Residual Relaxation (Prelax), that relaxes the exact alignment by allowing an adaptive residual vector between different views and encoding the semantic shift through pretext-aware learning. Extensive experiments on different backbones show that our method can not only improve multi-view methods with existing augmentations, but also benefit from stronger image augmentations like rotation.\n\n3. Title: What makes multi-modal learning better than single (provably)\n   Abstract: What Makes Multi-modal Learning Better than Single (Provably): The world provides us with data of multiple modalities. Intuitively, models fusing data from different modalities outperform their uni-modal counterparts, since more information is aggregated. Recently, joining the success of deep learning, there is an influential line of work on deep multi-modal learning, which has remarkable empirical results on various applications. However, theoretical justifications in this field are notably lacking.   Can multi-modal learning provably perform better than uni-modal?   In this paper, we answer this question under a most popular multi-modal fusion framework, which firstly encodes features from different modalities into a common latent space and seamlessly maps the latent representations into the task space. We prove that learning with multiple modalities achieves a smaller population risk than only using its subset of modalities. The main intuition is that the former has a more accurate estimate of the latent space representation. To the best of our knowledge, this is the first theoretical treatment to capture important qualitative phenomena observed in real multi-modal applications from the generalization perspective. Combining with experiment results, we show that multi-modal learning does possess an appealing formal guarantee.\n\n4. Title: Understanding contrastive learning requires incorporating inductive biases\n   Abstract: Understanding Contrastive Learning Requires Incorporating Inductive Biases: Contrastive learning is a popular form of self-supervised learning that encourages augmentations (views) of the same input to have more similar representations compared to augmentations of different inputs. Recent attempts to theoretically explain the success of contrastive learning on downstream classification tasks prove guarantees depending on properties of {\\em augmentations} and the value of {\\em contrastive loss} of representations. We demonstrate that such analyses, that ignore {\\em inductive biases} of the function class and training algorithm, cannot adequately explain the success of contrastive learning, even {\\em provably} leading to vacuous guarantees in some settings. Extensive experiments on image and text domains highlight the ubiquity of this problem -- different function classes and algorithms behave very differently on downstream tasks, despite having the same augmentations and contrastive losses. Theoretical analysis is presented for the class of linear representations, where incorporating inductive biases of the function class allows contrastive learning to work with less stringent conditions compared to prior analyses.\n\n5. Title: Slip: Self-supervision meets language-image pre-training\n   Abstract: SLIP: Self-supervision meets Language-Image Pre-training: Recent work has shown that self-supervised pre-training leads to improvements over supervised learning on challenging visual recognition tasks. CLIP, an exciting new approach to learning with language supervision, demonstrates promising performance on a wide variety of benchmarks. In this work, we explore whether self-supervised learning can aid in the use of language supervision for visual representation learning. We introduce SLIP, a multi-task learning framework for combining self-supervised learning and CLIP pre-training. After pre-training with Vision Transformers, we thoroughly evaluate representation quality and compare performance to both CLIP and self-supervised learning under three distinct settings: zero-shot transfer, linear classification, and end-to-end finetuning. Across ImageNet and a battery of additional datasets, we find that SLIP improves accuracy by a large margin. We validate our results further with experiments on different model sizes, training schedules, and pre-training datasets. Our findings show that SLIP enjoys the best of both worlds: better performance than self-supervision (+8.1% linear accuracy) and language supervision (+5.2% zero-shot accuracy).\n\n6. Title: Filip: Fine-grained interactive language-image pre-training\n   Abstract: FILIP: Fine-grained Interactive Language-Image Pre-Training: Unsupervised large-scale vision-language pre-training has shown promising advances on various downstream tasks. Existing methods often model the cross-modal interaction either via the similarity of the global feature of each modality which misses sufficient information, or finer-grained interactions using cross/self-attention upon visual and textual tokens. However, cross/self-attention suffers from inferior efficiency in both training and inference. In this paper, we introduce a large-scale Fine-grained Interactive Language-Image Pre-training (FILIP) to achieve finer-level alignment through a cross-modal late interaction mechanism, which uses a token-wise maximum similarity between visual and textual tokens to guide the contrastive objective. FILIP successfully leverages the finer-grained expressiveness between image patches and textual words by modifying only contrastive loss, while simultaneously gaining the ability to pre-compute image and text representations offline at inference, keeping both large-scale training and inference efficient. Furthermore, we construct a new large-scale image-text pair dataset called FILIP300M for pre-training. Experiments show that FILIP achieves state-of-the-art performance on multiple downstream vision-language tasks including zero-shot image classification and image-text retrieval. The visualization on word-patch alignment further shows that FILIP can learn meaningful fine-grained features with promising localization ability.\n\n7. Title: Provable Guarantees for Self-Supervised Deep Learning with Spectral Contrastive Loss\n   Abstract: Provable Guarantees for Self-Supervised Deep Learning with Spectral Contrastive Loss: Recent works in self-supervised learning have advanced the state-of-the-art by relying on the contrastive learning paradigm, which learns representations by pushing positive pairs, or similar examples from the same class, closer together while keeping negative pairs far apart. Despite the empirical successes, theoretical foundations are limited -- prior analyses assume conditional independence of the positive pairs given the same class label, but recent empirical applications use heavily correlated positive pairs (i.e., data augmentations of the same image). Our work analyzes contrastive learning without assuming conditional independence of positive pairs using a novel concept of the augmentation graph on data. Edges in this graph connect augmentations of the same data, and ground-truth classes naturally form connected sub-graphs. We propose a loss that performs spectral decomposition on the population augmentation graph and can be succinctly written as a contrastive learning objective on neural net representations. Minimizing this objective leads to features with provable accuracy guarantees under linear probe evaluation. By standard generalization bounds, these accuracy guarantees also hold when minimizing the training contrastive loss. Empirically, the features learned by our objective can match or outperform several strong baselines on benchmark vision datasets. In all, this work provides the first provable analysis for contrastive learning where guarantees for linear probe evaluation can apply to realistic empirical settings.\n\n8. Title: Learning transferable visual models from natural language supervision\n   Abstract: Learning Transferable Visual Models From Natural Language Supervision: State-of-the-art computer vision systems are trained to predict a fixed set of predetermined object categories. This restricted form of supervision limits their generality and usability since additional labeled data is needed to specify any other visual concept. Learning directly from raw text about images is a promising alternative which leverages a much broader source of supervision. We demonstrate that the simple pre-training task of predicting which caption goes with which image is an efficient and scalable way to learn SOTA image representations from scratch on a dataset of 400 million (image, text) pairs collected from the internet. After pre-training, natural language is used to reference learned visual concepts (or describe new ones) enabling zero-shot transfer of the model to downstream tasks. We study the performance of this approach by benchmarking on over 30 different existing computer vision datasets, spanning tasks such as OCR, action recognition in videos, geo-localization, and many types of fine-grained object classification. The model transfers non-trivially to most tasks and is often competitive with a fully supervised baseline without the need for any dataset specific training. For instance, we match the accuracy of the original ResNet-50 on ImageNet zero-shot without needing to use any of the 1.28 million training examples it was trained on. We release our code and pre-trained model weights at https://github.com/OpenAI/CLIP.\n\n9. Title: Learning deep representations by mutual information estimation and maximization\n   Abstract: Learning deep representations by mutual information estimation and maximization: In this work, we perform unsupervised learning of representations by maximizing mutual information between an input and the output of a deep neural network encoder. Importantly, we show that structure matters: incorporating knowledge about locality of the input to the objective can greatly influence a representation's suitability for downstream tasks. We further control characteristics of the representation by matching to a prior distribution adversarially. Our method, which we call Deep InfoMax (DIM), outperforms a number of popular unsupervised learning methods and competes with fully-supervised learning on several classification tasks. DIM opens new avenues for unsupervised learning of representations and is an important step towards flexible formulations of representation-learning objectives for specific end-goals.\n\n10. Title: A Message Passing Perspective on Learning Dynamics of Contrastive Learning\n   Abstract: A Message Passing Perspective on Learning Dynamics of Contrastive Learning: In recent years, contrastive learning achieves impressive results on self-supervised visual representation learning, but there still lacks a rigorous understanding of its learning dynamics. In this paper, we show that if we cast a contrastive objective equivalently into the feature space, then its learning dynamics admits an interpretable form. Specifically, we show that its gradient descent corresponds to a specific message passing scheme on the corresponding augmentation graph. Based on this perspective, we theoretically characterize how contrastive learning gradually learns discriminative features with the alignment update and the uniformity update. Meanwhile, this perspective also establishes an intriguing connection between contrastive learning and Message Passing Graph Neural Networks (MP-GNNs). This connection not only provides a unified understanding of many techniques independently developed in each community, but also enables us to borrow techniques from MP-GNNs to design new contrastive learning variants, such as graph attention, graph rewiring, jumpy knowledge techniques, etc. We believe that our message passing perspective not only provides a new theoretical understanding of contrastive learning dynamics, but also bridges the two seemingly independent areas together, which could inspire more interleaving studies to benefit from each other. The code is available at https://github.com/PKU-ML/Message-Passing-Contrastive-Learning.\n\nThink step-by-step about which candidate best matches the context. Summarize your reasoning, then provide the exact title that should fill the citation. Respond in JSON with fields:\n{\n  \"reasoning\": \"<your step-by-step analysis>\",\n  \"selected_title\": \"<exact candidate title>\"\n}", "response": {"reasoning": "1) Parse the citation context: the missing citation is attached to the claim that â€œthe multi-modal models can learn better representations than single-modal models in certain conditions.â€ This is a theoretical/provable claim about multi-modal learning.\n2) Compare candidates: Candidate 3 (â€œWhat Makes Multi-modal Learning Better than Single (Provably)â€) explicitly provides a theoretical treatment proving multi-modal learning can outperform uni-modal learning under a formal framework. Its abstract states it proves smaller population risk for multi-modal learning and gives provable guaranteesâ€”directly matching the cited claim.\n3) Reject others: Candidate 1 and several others focus on contrastive or unsupervised representation learning (single-modal) or empirical vision-language pretraining (CLIP, SLIP, FILIP) and do not make the provable multi-modal vs. single-modal claim. Candidates about contrastive dynamics or inductive biases (4,7,10) are not about provable multi-modal advantages. Multi-view or augmentation papers (2,9) also do not match.\n4) Conclusion: Candidate 3 best matches the claim and should be cited.\n\nTherefore the correct paper to fill the citation is candidate 3.", "selected_title": "What Makes Multi-modal Learning Better than Single (Provably)"}, "selected_title": "What Makes Multi-modal Learning Better than Single (Provably)", "positive_title": "What makes multi-modal learning better than single (provably)", "is_correct": true, "reasoning": "1) Parse the citation context: the missing citation is attached to the claim that â€œthe multi-modal models can learn better representations than single-modal models in certain conditions.â€ This is a theoretical/provable claim about multi-modal learning.\n2) Compare candidates: Candidate 3 (â€œWhat Makes Multi-modal Learning Better than Single (Provably)â€) explicitly provides a theoretical treatment proving multi-modal learning can outperform uni-modal learning under a formal framework. Its abstract states it proves smaller population risk for multi-modal learning and gives provable guaranteesâ€”directly matching the cited claim.\n3) Reject others: Candidate 1 and several others focus on contrastive or unsupervised representation learning (single-modal) or empirical vision-language pretraining (CLIP, SLIP, FILIP) and do not make the provable multi-modal vs. single-modal claim. Candidates about contrastive dynamics or inductive biases (4,7,10) are not about provable multi-modal advantages. Multi-view or augmentation papers (2,9) also do not match.\n4) Conclusion: Candidate 3 best matches the claim and should be cited.\n\nTherefore the correct paper to fill the citation is candidate 3."}
{"example_id": "blind_16318", "prompt": "You are an expert citation retrieval system as described in the paper \"Multi-Agent System for Reliable Citation Retrieval\".\nYour goal is to autonomously retrieve, verify, and recommend academic references given a query or document excerpt.\n\nTask:\nGiven a citation context from a scientific paper (where a citation is missing), identify the correct paper from a list of candidates.\nAnalyze the context to understand the specific claim, method, or result being cited.\nThen, evaluate each candidate paper to see if it matches the context.\nFinally, select the best matching paper.\n\nCitation Context:\nnerate fixed-length tokens for all modalities. \\noindent \\textbf{Multimodal Large Language Models.} In addition to vision LLMs, recent works proposed to extend LLMs to other modalities, such as audio[CITATION], video and point cloud. These works make it possible to unify multiple modalities into one LLM. X-LLM adopts modality-specific Q-Former and adapters to con\n\nCandidate Papers:\n1. Title: Imagebind-llm: Multi-modality instruction tuning\n   Abstract: ImageBind-LLM: Multi-modality Instruction Tuning: We present ImageBind-LLM, a multi-modality instruction tuning method of large language models (LLMs) via ImageBind. Existing works mainly focus on language and image instruction tuning, different from which, our ImageBind-LLM can respond to multi-modality conditions, including audio, 3D point clouds, video, and their embedding-space arithmetic by only image-text alignment training. During training, we adopt a learnable bind network to align the embedding space between LLaMA and ImageBind's image encoder. Then, the image features transformed by the bind network are added to word tokens of all layers in LLaMA, which progressively injects visual instructions via an attention-free and zero-initialized gating mechanism. Aided by the joint embedding of ImageBind, the simple image-text training enables our model to exhibit superior multi-modality instruction-following capabilities. During inference, the multi-modality inputs are fed into the corresponding ImageBind encoders, and processed by a proposed visual cache model for further cross-modal embedding enhancement. The training-free cache model retrieves from three million image features extracted by ImageBind, which effectively mitigates the training-inference modality discrepancy. Notably, with our approach, ImageBind-LLM can respond to instructions of diverse modalities and demonstrate significant language generation quality. Code is released at https://github.com/OpenGVLab/LLaMA-Adapter.\n\n2. Title: Speechgpt: Empowering large language models with intrinsic cross-modal conversational abilities\n   Abstract: SpeechGPT: Empowering Large Language Models with Intrinsic Cross-Modal Conversational Abilities: Multi-modal large language models are regarded as a crucial step towards Artificial General Intelligence (AGI) and have garnered significant interest with the emergence of ChatGPT. However, current speech-language models typically adopt the cascade paradigm, preventing inter-modal knowledge transfer. In this paper, we propose SpeechGPT, a large language model with intrinsic cross-modal conversational abilities, capable of perceiving and generating multi-model content. With discrete speech representations, we first construct SpeechInstruct, a large-scale cross-modal speech instruction dataset. Additionally, we employ a three-stage training strategy that includes modality-adaptation pre-training, cross-modal instruction fine-tuning, and chain-of-modality instruction fine-tuning. The experimental results demonstrate that SpeechGPT has an impressive capacity to follow multi-modal human instructions and highlight the potential of handling multiple modalities with one model. Demos are shown in https://0nutation.github.io/SpeechGPT.github.io/.\n\n3. Title: Point-bind \\& point-llm: Aligning point cloud with multi-modality for 3d understanding, generation, and instruction following\n   Abstract: Point-Bind & Point-LLM: Aligning Point Cloud with Multi-modality for 3D Understanding, Generation, and Instruction Following: We introduce Point-Bind, a 3D multi-modality model aligning point clouds with 2D image, language, audio, and video. Guided by ImageBind, we construct a joint embedding space between 3D and multi-modalities, enabling many promising applications, e.g., any-to-3D generation, 3D embedding arithmetic, and 3D open-world understanding. On top of this, we further present Point-LLM, the first 3D large language model (LLM) following 3D multi-modal instructions. By parameter-efficient fine-tuning techniques, Point-LLM injects the semantics of Point-Bind into pre-trained LLMs, e.g., LLaMA, which requires no 3D instruction data, but exhibits superior 3D and multi-modal question-answering capacity. We hope our work may cast a light on the community for extending 3D point clouds to multi-modality applications. Code is available at https://github.com/ZiyuGuo99/Point-Bind_Point-LLM.\n\n4. Title: From sparse to soft mixtures of experts\n   Abstract: From Sparse to Soft Mixtures of Experts: Sparse mixture of expert architectures (MoEs) scale model capacity without significant increases in training or inference costs. Despite their success, MoEs suffer from a number of issues: training instability, token dropping, inability to scale the number of experts, or ineffective finetuning. In this work, we propose Soft MoE, a fully-differentiable sparse Transformer that addresses these challenges, while maintaining the benefits of MoEs. Soft MoE performs an implicit soft assignment by passing different weighted combinations of all input tokens to each expert. As in other MoEs, experts in Soft MoE only process a subset of the (combined) tokens, enabling larger model capacity (and performance) at lower inference cost. In the context of visual recognition, Soft MoE greatly outperforms dense Transformers (ViTs) and popular MoEs (Tokens Choice and Experts Choice). Furthermore, Soft MoE scales well: Soft MoE Huge/14 with 128 experts in 16 MoE layers has over 40x more parameters than ViT Huge/14, with only 2% increased inference time, and substantially better quality.\n\n5. Title: VAST: A Vision-Audio-Subtitle-Text Omni-Modality Foundation Model and Dataset\n   Abstract: VAST: A Vision-Audio-Subtitle-Text Omni-Modality Foundation Model and Dataset: Vision and text have been fully explored in contemporary video-text foundational models, while other modalities such as audio and subtitles in videos have not received sufficient attention. In this paper, we resort to establish connections between multi-modality video tracks, including Vision, Audio, and Subtitle, and Text by exploring an automatically generated large-scale omni-modality video caption dataset called VAST-27M. Specifically, we first collect 27 million open-domain video clips and separately train a vision and an audio captioner to generate vision and audio captions. Then, we employ an off-the-shelf Large Language Model (LLM) to integrate the generated captions, together with subtitles and instructional prompts into omni-modality captions. Based on the proposed VAST-27M dataset, we train an omni-modality video-text foundational model named VAST, which can perceive and process vision, audio, and subtitle modalities from video, and better support various tasks including vision-text, audio-text, and multi-modal video-text tasks (retrieval, captioning and QA). Extensive experiments have been conducted to demonstrate the effectiveness of our proposed VAST-27M corpus and VAST foundation model. VAST achieves 22 new state-of-the-art results on various cross-modality benchmarks. Code, model and dataset will be released at https://github.com/TXH-mercury/VAST.\n\n6. Title: Chatbridge: Bridging modalities with large language model as a language catalyst\n   Abstract: ChatBridge: Bridging Modalities with Large Language Model as a Language Catalyst: Building general-purpose models that can perceive diverse real-world modalities and solve various tasks is an appealing target in artificial intelligence. In this paper, we present ChatBridge, a novel multimodal language model that leverages the expressive capabilities of language as the catalyst to bridge the gap between various modalities. We show that only language-paired two-modality data is sufficient to connect all modalities. ChatBridge leverages recent large language models (LLM) and extends their zero-shot capabilities to incorporate diverse multimodal inputs. ChatBridge undergoes a two-stage training. The first stage aligns each modality with language, which brings emergent multimodal correlation and collaboration abilities. The second stage instruction-finetunes ChatBridge to align it with user intent with our newly proposed multimodal instruction tuning dataset, named MULTIS, which covers a wide range of 16 multimodal tasks of text, image, video, and audio modalities. We show strong quantitative and qualitative results on zero-shot multimodal tasks covering text, image, video, and audio modalities. All codes, data, and models of ChatBridge will be open-sourced.\n\n7. Title: Listen, Think, and Understand\n   Abstract: Listen, Think, and Understand: The ability of artificial intelligence (AI) systems to perceive and comprehend audio signals is crucial for many applications. Although significant progress has been made in this area since the development of AudioSet, most existing models are designed to map audio inputs to pre-defined, discrete sound label sets. In contrast, humans possess the ability to not only classify sounds into general categories, but also to listen to the finer details of the sounds, explain the reason for the predictions, think about what the sound infers, and understand the scene and what action needs to be taken, if any. Such capabilities beyond perception are not yet present in existing audio models. On the other hand, modern large language models (LLMs) exhibit emerging reasoning ability but they lack audio perception capabilities. Therefore, we ask the question: can we build a model that has both audio perception and a reasoning ability?   In this paper, we propose a new audio foundation model, called LTU (Listen, Think, and Understand). To train LTU, we created a new OpenAQA-5M dataset consisting of 1.9 million closed-ended and 3.7 million open-ended, diverse (audio, question, answer) tuples, and have used an autoregressive training framework with a perception-to-understanding curriculum. LTU demonstrates strong performance and generalization ability on conventional audio tasks such as classification and captioning. More importantly, it exhibits emerging audio reasoning and comprehension abilities that are absent in existing audio models. To the best of our knowledge, LTU is one of the first multimodal large language models that focus on general audio (rather than just speech) understanding.\n\n8. Title: Learning transferable visual models from natural language supervision\n   Abstract: Learning Transferable Visual Models From Natural Language Supervision: State-of-the-art computer vision systems are trained to predict a fixed set of predetermined object categories. This restricted form of supervision limits their generality and usability since additional labeled data is needed to specify any other visual concept. Learning directly from raw text about images is a promising alternative which leverages a much broader source of supervision. We demonstrate that the simple pre-training task of predicting which caption goes with which image is an efficient and scalable way to learn SOTA image representations from scratch on a dataset of 400 million (image, text) pairs collected from the internet. After pre-training, natural language is used to reference learned visual concepts (or describe new ones) enabling zero-shot transfer of the model to downstream tasks. We study the performance of this approach by benchmarking on over 30 different existing computer vision datasets, spanning tasks such as OCR, action recognition in videos, geo-localization, and many types of fine-grained object classification. The model transfers non-trivially to most tasks and is often competitive with a fully supervised baseline without the need for any dataset specific training. For instance, we match the accuracy of the original ResNet-50 on ImageNet zero-shot without needing to use any of the 1.28 million training examples it was trained on. We release our code and pre-trained model weights at https://github.com/OpenAI/CLIP.\n\n9. Title: Audioclip: Extending clip to image, text and audio\n   Abstract: AudioCLIP: Extending CLIP to Image, Text and Audio: In the past, the rapidly evolving field of sound classification greatly benefited from the application of methods from other domains. Today, we observe the trend to fuse domain-specific tasks and approaches together, which provides the community with new outstanding models.   In this work, we present an extension of the CLIP model that handles audio in addition to text and images. Our proposed model incorporates the ESResNeXt audio-model into the CLIP framework using the AudioSet dataset. Such a combination enables the proposed model to perform bimodal and unimodal classification and querying, while keeping CLIP's ability to generalize to unseen datasets in a zero-shot inference fashion.   AudioCLIP achieves new state-of-the-art results in the Environmental Sound Classification (ESC) task, out-performing other approaches by reaching accuracies of 90.07% on the UrbanSound8K and 97.15% on the ESC-50 datasets. Further it sets new baselines in the zero-shot ESC-task on the same datasets (68.78% and 69.40%, respectively).   Finally, we also assess the cross-modal querying performance of the proposed model as well as the influence of full and partial training on the results. For the sake of reproducibility, our code is published.\n\n10. Title: Pointclip: Point cloud understanding by clip\n   Abstract: PointCLIP: Point Cloud Understanding by CLIP: Recently, zero-shot and few-shot learning via Contrastive Vision-Language Pre-training (CLIP) have shown inspirational performance on 2D visual recognition, which learns to match images with their corresponding texts in open-vocabulary settings. However, it remains under explored that whether CLIP, pre-trained by large-scale image-text pairs in 2D, can be generalized to 3D recognition. In this paper, we identify such a setting is feasible by proposing PointCLIP, which conducts alignment between CLIP-encoded point cloud and 3D category texts. Specifically, we encode a point cloud by projecting it into multi-view depth maps without rendering, and aggregate the view-wise zero-shot prediction to achieve knowledge transfer from 2D to 3D. On top of that, we design an inter-view adapter to better extract the global feature and adaptively fuse the few-shot knowledge learned from 3D into CLIP pre-trained in 2D. By just fine-tuning the lightweight adapter in the few-shot settings, the performance of PointCLIP could be largely improved. In addition, we observe the complementary property between PointCLIP and classical 3D-supervised networks. By simple ensembling, PointCLIP boosts baseline's performance and even surpasses state-of-the-art models. Therefore, PointCLIP is a promising alternative for effective 3D point cloud understanding via CLIP under low resource cost and data regime. We conduct thorough experiments on widely-adopted ModelNet10, ModelNet40 and the challenging ScanObjectNN to demonstrate the effectiveness of PointCLIP. The code is released at https://github.com/ZrrSkywalker/PointCLIP.\n\nThink step-by-step about which candidate best matches the context. Summarize your reasoning, then provide the exact title that should fill the citation. Respond in JSON with fields:\n{\n  \"reasoning\": \"<your step-by-step analysis>\",\n  \"selected_title\": \"<exact candidate title>\"\n}", "response": {"reasoning": "The sentence lists extensions of LLMs beyond vision to other modalities (audio, video, point cloud) and claims these works enable unifying multiple modalities into one LLM. I reviewed the candidates for which explicitly extend LLMs to multiple modalities including audio, video, and 3D point clouds:  \n- ImageBind-LLM (candidate 1) explicitly presents a multi-modality instruction-tuning method that supports audio, 3D point clouds, and video by leveraging ImageBind to align modalities into a single LLM. This directly matches the claim about unifying multiple modalities into one LLM.  \n- Point-Bind & Point-LLM (candidate 3) focuses on aligning point clouds with other modalities and creating a 3D LLM; it emphasizes point clouds but is narrower (3D-centric).  \n- SpeechGPT (candidate 2) and LTU (candidate 7) focus on audio/speech capabilities but do not emphasize unifying audio, video, and point clouds into one LLM.  \n- VAST and ChatBridge (candidates 5 and 6) cover multiple modalities including audio and video but are more video-centric datasets/models or language-catalyst frameworks; they are plausible but ImageBind-LLM most directly claims multi-modality instruction tuning across audio, video, and point clouds for LLMs.  \nTherefore the best match for the citation is ImageBind-LLM, which directly supports the statement about extending LLMs to audio, video, and point cloud and unifying modalities into one LLM.", "selected_title": "Imagebind-llm: Multi-modality instruction tuning"}, "selected_title": "Imagebind-llm: Multi-modality instruction tuning", "positive_title": "Listen, Think, and Understand", "is_correct": false, "reasoning": "The sentence lists extensions of LLMs beyond vision to other modalities (audio, video, point cloud) and claims these works enable unifying multiple modalities into one LLM. I reviewed the candidates for which explicitly extend LLMs to multiple modalities including audio, video, and 3D point clouds:  \n- ImageBind-LLM (candidate 1) explicitly presents a multi-modality instruction-tuning method that supports audio, 3D point clouds, and video by leveraging ImageBind to align modalities into a single LLM. This directly matches the claim about unifying multiple modalities into one LLM.  \n- Point-Bind & Point-LLM (candidate 3) focuses on aligning point clouds with other modalities and creating a 3D LLM; it emphasizes point clouds but is narrower (3D-centric).  \n- SpeechGPT (candidate 2) and LTU (candidate 7) focus on audio/speech capabilities but do not emphasize unifying audio, video, and point clouds into one LLM.  \n- VAST and ChatBridge (candidates 5 and 6) cover multiple modalities including audio and video but are more video-centric datasets/models or language-catalyst frameworks; they are plausible but ImageBind-LLM most directly claims multi-modality instruction tuning across audio, video, and point clouds for LLMs.  \nTherefore the best match for the citation is ImageBind-LLM, which directly supports the statement about extending LLMs to audio, video, and point cloud and unifying modalities into one LLM."}
{"example_id": "blind_2856", "prompt": "You are an expert citation retrieval system as described in the paper \"Multi-Agent System for Reliable Citation Retrieval\".\nYour goal is to autonomously retrieve, verify, and recommend academic references given a query or document excerpt.\n\nTask:\nGiven a citation context from a scientific paper (where a citation is missing), identify the correct paper from a list of candidates.\nAnalyze the context to understand the specific claim, method, or result being cited.\nThen, evaluate each candidate paper to see if it matches the context.\nFinally, select the best matching paper.\n\nCitation Context:\nribution. Models were evaluated on increasingly difficult problems beyond the difficulty of the training set. In all cases they find that loss scales with respect to model size and amount of training.[CITATION] train a model to match images with their captions. They vary the amount of compute and show that zero-shot error averaged over multiple vision datasets scales as a function of compute. use\n\nCandidate Papers:\n1. Title: Bias and Generalization in Deep Generative Models: An Empirical Study\n   Abstract: Bias and Generalization in Deep Generative Models: An Empirical Study: In high dimensional settings, density estimation algorithms rely crucially on their inductive bias. Despite recent empirical success, the inductive bias of deep generative models is not well understood. In this paper we propose a framework to systematically investigate bias and generalization in deep generative models of images. Inspired by experimental methods from cognitive psychology, we probe each learning algorithm with carefully designed training datasets to characterize when and how existing models generate novel attributes and their combinations. We identify similarities to human psychology and verify that these patterns are consistent across commonly used models and architectures.\n\n2. Title: Attention Is All You Need\n   Abstract: Attention Is All You Need: The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.\n\n3. Title: Auto-Encoding Variational Bayes\n   Abstract: Auto-Encoding Variational Bayes: How can we perform efficient inference and learning in directed probabilistic models, in the presence of continuous latent variables with intractable posterior distributions, and large datasets? We introduce a stochastic variational inference and learning algorithm that scales to large datasets and, under some mild differentiability conditions, even works in the intractable case. Our contributions are two-fold. First, we show that a reparameterization of the variational lower bound yields a lower bound estimator that can be straightforwardly optimized using standard stochastic gradient methods. Second, we show that for i.i.d. datasets with continuous latent variables per datapoint, posterior inference can be made especially efficient by fitting an approximate inference model (also called a recognition model) to the intractable posterior using the proposed lower bound estimator. Theoretical advantages are reflected in experimental results.\n\n4. Title: Zero-Shot Text-to-Image Generation\n   Abstract: Zero-Shot Text-to-Image Generation: Text-to-image generation has traditionally focused on finding better modeling assumptions for training on a fixed dataset. These assumptions might involve complex architectures, auxiliary losses, or side information such as object part labels or segmentation masks supplied during training. We describe a simple approach for this task based on a transformer that autoregressively models the text and image tokens as a single stream of data. With sufficient data and scale, our approach is competitive with previous domain-specific models when evaluated in a zero-shot fashion.\n\n5. Title: Language Models are Few-Shot Learners\n   Abstract: Language Models are Few-Shot Learners: Recent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training on a large corpus of text followed by fine-tuning on a specific task. While typically task-agnostic in architecture, this method still requires task-specific fine-tuning datasets of thousands or tens of thousands of examples. By contrast, humans can generally perform a new language task from only a few examples or from simple instructions - something which current NLP systems still largely struggle to do. Here we show that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art fine-tuning approaches. Specifically, we train GPT-3, an autoregressive language model with 175 billion parameters, 10x more than any previous non-sparse language model, and test its performance in the few-shot setting. For all tasks, GPT-3 is applied without any gradient updates or fine-tuning, with tasks and few-shot demonstrations specified purely via text interaction with the model. GPT-3 achieves strong performance on many NLP datasets, including translation, question-answering, and cloze tasks, as well as several tasks that require on-the-fly reasoning or domain adaptation, such as unscrambling words, using a novel word in a sentence, or performing 3-digit arithmetic. At the same time, we also identify some datasets where GPT-3's few-shot learning still struggles, as well as some datasets where GPT-3 faces methodological issues related to training on large web corpora. Finally, we find that GPT-3 can generate samples of news articles which human evaluators have difficulty distinguishing from articles written by humans. We discuss broader societal impacts of this finding and of GPT-3 in general.\n\n6. Title: Learning Transferable Visual Models From Natural Language Supervision\n   Abstract: Learning Transferable Visual Models From Natural Language Supervision: State-of-the-art computer vision systems are trained to predict a fixed set of predetermined object categories. This restricted form of supervision limits their generality and usability since additional labeled data is needed to specify any other visual concept. Learning directly from raw text about images is a promising alternative which leverages a much broader source of supervision. We demonstrate that the simple pre-training task of predicting which caption goes with which image is an efficient and scalable way to learn SOTA image representations from scratch on a dataset of 400 million (image, text) pairs collected from the internet. After pre-training, natural language is used to reference learned visual concepts (or describe new ones) enabling zero-shot transfer of the model to downstream tasks. We study the performance of this approach by benchmarking on over 30 different existing computer vision datasets, spanning tasks such as OCR, action recognition in videos, geo-localization, and many types of fine-grained object classification. The model transfers non-trivially to most tasks and is often competitive with a fully supervised baseline without the need for any dataset specific training. For instance, we match the accuracy of the original ResNet-50 on ImageNet zero-shot without needing to use any of the 1.28 million training examples it was trained on. We release our code and pre-trained model weights at https://github.com/OpenAI/CLIP.\n\n7. Title: Survey on deep neural networks in speech and vision systems\n   Abstract: Survey on Deep Neural Networks in Speech and Vision Systems: This survey presents a review of state-of-the-art deep neural network architectures, algorithms, and systems in vision and speech applications. Recent advances in deep artificial neural network algorithms and architectures have spurred rapid innovation and development of intelligent vision and speech systems. With availability of vast amounts of sensor data and cloud computing for processing and training of deep neural networks, and with increased sophistication in mobile and embedded technology, the next-generation intelligent systems are poised to revolutionize personal and commercial computing. This survey begins by providing background and evolution of some of the most successful deep learning models for intelligent vision and speech systems to date. An overview of large-scale industrial research and development efforts is provided to emphasize future trends and prospects of intelligent vision and speech systems. Robust and efficient intelligent systems demand low-latency and high fidelity in resource-constrained hardware platforms such as mobile devices, robots, and automobiles. Therefore, this survey also provides a summary of key challenges and recent successes in running deep neural networks on hardware-restricted platforms, i.e. within limited memory, battery life, and processing capabilities. Finally, emerging applications of vision and speech across disciplines such as affective computing, intelligent transportation, and precision medicine are discussed. To our knowledge, this paper provides one of the most comprehensive surveys on the latest developments in intelligent vision and speech applications from the perspectives of both software and hardware systems. Many of these emerging technologies using deep neural networks show tremendous promise to revolutionize research and development for future vision and speech systems.\n\n8. Title: On Robustness and Transferability of Convolutional Neural Networks\n   Abstract: On Robustness and Transferability of Convolutional Neural Networks: Modern deep convolutional networks (CNNs) are often criticized for not generalizing under distributional shifts. However, several recent breakthroughs in transfer learning suggest that these networks can cope with severe distribution shifts and successfully adapt to new tasks from a few training examples. In this work we study the interplay between out-of-distribution and transfer performance of modern image classification CNNs for the first time and investigate the impact of the pre-training data size, the model scale, and the data preprocessing pipeline. We find that increasing both the training set and model sizes significantly improve the distributional shift robustness. Furthermore, we show that, perhaps surprisingly, simple changes in the preprocessing such as modifying the image resolution can significantly mitigate robustness issues in some cases. Finally, we outline the shortcomings of existing robustness evaluation datasets and introduce a synthetic dataset SI-Score we use for a systematic analysis across factors of variation common in visual data such as object size and position.\n\n9. Title: Deep Learning Scaling is Predictable, Empirically\n   Abstract: Deep Learning Scaling is Predictable, Empirically: Deep learning (DL) creates impactful advances following a virtuous recipe: model architecture search, creating large training data sets, and scaling computation. It is widely believed that growing training sets and models should improve accuracy and result in better products. As DL application domains grow, we would like a deeper understanding of the relationships between training set size, computational scale, and model accuracy improvements to advance the state-of-the-art.   This paper presents a large scale empirical characterization of generalization error and model size growth as training sets grow. We introduce a methodology for this measurement and test four machine learning domains: machine translation, language modeling, image processing, and speech recognition. Our empirical results show power-law generalization error scaling across a breadth of factors, resulting in power-law exponents---the \"steepness\" of the learning curve---yet to be explained by theoretical work. Further, model improvements only shift the error but do not appear to affect the power-law exponent. We also show that model size scales sublinearly with data size. These scaling relationships have significant implications on deep learning research, practice, and systems. They can assist model debugging, setting accuracy targets, and decisions about data set growth. They can also guide computing system design and underscore the importance of continued computational scaling.\n\n10. Title: Deep Learning\n   Abstract: Deep Learning: Deep learning (DL) is a high dimensional data reduction technique for constructing high-dimensional predictors in input-output models. DL is a form of machine learning that uses hierarchical layers of latent features. In this article, we review the state-of-the-art of deep learning from a modeling and algorithmic perspective. We provide a list of successful areas of applications in Artificial Intelligence (AI), Image Processing, Robotics and Automation. Deep learning is predictive in its nature rather then inferential and can be viewed as a black-box methodology for high-dimensional function estimation.\n\nThink step-by-step about which candidate best matches the context. Summarize your reasoning, then provide the exact title that should fill the citation. Respond in JSON with fields:\n{\n  \"reasoning\": \"<your step-by-step analysis>\",\n  \"selected_title\": \"<exact candidate title>\"\n}", "response": {"reasoning": "The sentence cited states: \"In all cases they find that loss scales with respect to model size and amount of training.\" This directly matches the topic of empirical scaling laws for deep learning (power-law relationships between error, model size, and training data/compute). Candidate 9's abstract explicitly studies generalization error scaling across domains and reports power-law scaling with training set and model size, which is a direct match. While the next sentence in the context (about training a model to match images with captions and zero-shot error scaling) corresponds to Candidate 6 (the CLIP paper), the bracketed citation appears immediately after the sentence about loss/model/data scaling, so the correct reference for that citation is Candidate 9. Other candidates (e.g., GPT-3 in Candidate 5) discuss scaling effects in language models but do not present the broad empirical scaling-law characterization across domains described in the context; Candidate 9 most precisely fits the cited claim.", "selected_title": "Deep Learning Scaling is Predictable, Empirically"}, "selected_title": "Deep Learning Scaling is Predictable, Empirically", "positive_title": "Learning Transferable Visual Models From Natural Language Supervision", "is_correct": false, "reasoning": "The sentence cited states: \"In all cases they find that loss scales with respect to model size and amount of training.\" This directly matches the topic of empirical scaling laws for deep learning (power-law relationships between error, model size, and training data/compute). Candidate 9's abstract explicitly studies generalization error scaling across domains and reports power-law scaling with training set and model size, which is a direct match. While the next sentence in the context (about training a model to match images with captions and zero-shot error scaling) corresponds to Candidate 6 (the CLIP paper), the bracketed citation appears immediately after the sentence about loss/model/data scaling, so the correct reference for that citation is Candidate 9. Other candidates (e.g., GPT-3 in Candidate 5) discuss scaling effects in language models but do not present the broad empirical scaling-law characterization across domains described in the context; Candidate 9 most precisely fits the cited claim."}
{"example_id": "blind_65", "prompt": "You are an expert citation retrieval system as described in the paper \"Multi-Agent System for Reliable Citation Retrieval\".\nYour goal is to autonomously retrieve, verify, and recommend academic references given a query or document excerpt.\n\nTask:\nGiven a citation context from a scientific paper (where a citation is missing), identify the correct paper from a list of candidates.\nAnalyze the context to understand the specific claim, method, or result being cited.\nThen, evaluate each candidate paper to see if it matches the context.\nFinally, select the best matching paper.\n\nCitation Context:\nlobal average pooling to aggregate features from all pixels, thus generating image-level global information representation. BiseNet adds the global pooling at the end of context path and DFN[CITATION] adds it on the top of base network. Moreover, inspired by the squeeze-and-excitation attention mechanism, further extract channel-wise relationship utilizing features captured b\n\nCandidate Papers:\n1. Title: Dual attention network for scene segmentation\n   Abstract: Dual Attention Network for Scene Segmentation: In this paper, we address the scene segmentation task by capturing rich contextual dependencies based on the selfattention mechanism. Unlike previous works that capture contexts by multi-scale features fusion, we propose a Dual Attention Networks (DANet) to adaptively integrate local features with their global dependencies. Specifically, we append two types of attention modules on top of traditional dilated FCN, which model the semantic interdependencies in spatial and channel dimensions respectively. The position attention module selectively aggregates the features at each position by a weighted sum of the features at all positions. Similar features would be related to each other regardless of their distances. Meanwhile, the channel attention module selectively emphasizes interdependent channel maps by integrating associated features among all channel maps. We sum the outputs of the two attention modules to further improve feature representation which contributes to more precise segmentation results. We achieve new state-of-the-art segmentation performance on three challenging scene segmentation datasets, i.e., Cityscapes, PASCAL Context and COCO Stuff dataset. In particular, a Mean IoU score of 81.5% on Cityscapes test set is achieved without using coarse data. We make the code and trained model publicly available at https://github.com/junfu1115/DANet\n\n2. Title: Pyramid scene parsing network\n   Abstract: Pyramid Scene Parsing Network: Scene parsing is challenging for unrestricted open vocabulary and diverse scenes. In this paper, we exploit the capability of global context information by different-region-based context aggregation through our pyramid pooling module together with the proposed pyramid scene parsing network (PSPNet). Our global prior representation is effective to produce good quality results on the scene parsing task, while PSPNet provides a superior framework for pixel-level prediction tasks. The proposed approach achieves state-of-the-art performance on various datasets. It came first in ImageNet scene parsing challenge 2016, PASCAL VOC 2012 benchmark and Cityscapes benchmark. A single PSPNet yields new record of mIoU accuracy 85.4% on PASCAL VOC 2012 and accuracy 80.2% on Cityscapes.\n\n3. Title: Bisenet: Bilateral segmentation network for real-time semantic segmentation\n   Abstract: BiSeNet: Bilateral Segmentation Network for Real-time Semantic Segmentation: Semantic segmentation requires both rich spatial information and sizeable receptive field. However, modern approaches usually compromise spatial resolution to achieve real-time inference speed, which leads to poor performance. In this paper, we address this dilemma with a novel Bilateral Segmentation Network (BiSeNet). We first design a Spatial Path with a small stride to preserve the spatial information and generate high-resolution features. Meanwhile, a Context Path with a fast downsampling strategy is employed to obtain sufficient receptive field. On top of the two paths, we introduce a new Feature Fusion Module to combine features efficiently. The proposed architecture makes a right balance between the speed and segmentation performance on Cityscapes, CamVid, and COCO-Stuff datasets. Specifically, for a 2048x1024 input, we achieve 68.4% Mean IOU on the Cityscapes test dataset with speed of 105 FPS on one NVIDIA Titan XP card, which is significantly faster than the existing methods with comparable performance.\n\n4. Title: Fully convolutional networks for semantic segmentation\n   Abstract: Fully Convolutional Networks for Semantic Segmentation: Convolutional networks are powerful visual models that yield hierarchies of features. We show that convolutional networks by themselves, trained end-to-end, pixels-to-pixels, exceed the state-of-the-art in semantic segmentation. Our key insight is to build \"fully convolutional\" networks that take input of arbitrary size and produce correspondingly-sized output with efficient inference and learning. We define and detail the space of fully convolutional networks, explain their application to spatially dense prediction tasks, and draw connections to prior models. We adapt contemporary classification networks (AlexNet, the VGG net, and GoogLeNet) into fully convolutional networks and transfer their learned representations by fine-tuning to the segmentation task. We then define a novel architecture that combines semantic information from a deep, coarse layer with appearance information from a shallow, fine layer to produce accurate and detailed segmentations. Our fully convolutional network achieves state-of-the-art segmentation of PASCAL VOC (20% relative improvement to 62.2% mean IU on 2012), NYUDv2, and SIFT Flow, while inference takes one third of a second for a typical image.\n\n5. Title: Rethinking atrous convolution for semantic image segmentation\n   Abstract: Rethinking Atrous Convolution for Semantic Image Segmentation: In this work, we revisit atrous convolution, a powerful tool to explicitly adjust filter's field-of-view as well as control the resolution of feature responses computed by Deep Convolutional Neural Networks, in the application of semantic image segmentation. To handle the problem of segmenting objects at multiple scales, we design modules which employ atrous convolution in cascade or in parallel to capture multi-scale context by adopting multiple atrous rates. Furthermore, we propose to augment our previously proposed Atrous Spatial Pyramid Pooling module, which probes convolutional features at multiple scales, with image-level features encoding global context and further boost performance. We also elaborate on implementation details and share our experience on training our system. The proposed `DeepLabv3' system significantly improves over our previous DeepLab versions without DenseCRF post-processing and attains comparable performance with other state-of-art models on the PASCAL VOC 2012 semantic image segmentation benchmark.\n\n6. Title: Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs\n   Abstract: DeepLab: Semantic Image Segmentation with Deep Convolutional Nets, Atrous Convolution, and Fully Connected CRFs: In this work we address the task of semantic image segmentation with Deep Learning and make three main contributions that are experimentally shown to have substantial practical merit. First, we highlight convolution with upsampled filters, or 'atrous convolution', as a powerful tool in dense prediction tasks. Atrous convolution allows us to explicitly control the resolution at which feature responses are computed within Deep Convolutional Neural Networks. It also allows us to effectively enlarge the field of view of filters to incorporate larger context without increasing the number of parameters or the amount of computation. Second, we propose atrous spatial pyramid pooling (ASPP) to robustly segment objects at multiple scales. ASPP probes an incoming convolutional feature layer with filters at multiple sampling rates and effective fields-of-views, thus capturing objects as well as image context at multiple scales. Third, we improve the localization of object boundaries by combining methods from DCNNs and probabilistic graphical models. The commonly deployed combination of max-pooling and downsampling in DCNNs achieves invariance but has a toll on localization accuracy. We overcome this by combining the responses at the final DCNN layer with a fully connected Conditional Random Field (CRF), which is shown both qualitatively and quantitatively to improve localization performance. Our proposed \"DeepLab\" system sets the new state-of-art at the PASCAL VOC-2012 semantic image segmentation task, reaching 79.7% mIOU in the test set, and advances the results on three other datasets: PASCAL-Context, PASCAL-Person-Part, and Cityscapes. All of our code is made publicly available online.\n\n7. Title: Learning a discriminative feature network for semantic segmentation\n   Abstract: Learning a Discriminative Feature Network for Semantic Segmentation: Most existing methods of semantic segmentation still suffer from two aspects of challenges: intra-class inconsistency and inter-class indistinction. To tackle these two problems, we propose a Discriminative Feature Network (DFN), which contains two sub-networks: Smooth Network and Border Network. Specifically, to handle the intra-class inconsistency problem, we specially design a Smooth Network with Channel Attention Block and global average pooling to select the more discriminative features. Furthermore, we propose a Border Network to make the bilateral features of boundary distinguishable with deep semantic boundary supervision. Based on our proposed DFN, we achieve state-of-the-art performance 86.2% mean IOU on PASCAL VOC 2012 and 80.3% mean IOU on Cityscapes dataset.\n\n8. Title: U-net: Convolutional networks for biomedical image segmentation\n   Abstract: U-Net: Convolutional Networks for Biomedical Image Segmentation: There is large consent that successful training of deep networks requires many thousand annotated training samples. In this paper, we present a network and training strategy that relies on the strong use of data augmentation to use the available annotated samples more efficiently. The architecture consists of a contracting path to capture context and a symmetric expanding path that enables precise localization. We show that such a network can be trained end-to-end from very few images and outperforms the prior best method (a sliding-window convolutional network) on the ISBI challenge for segmentation of neuronal structures in electron microscopic stacks. Using the same network trained on transmitted light microscopy images (phase contrast and DIC) we won the ISBI cell tracking challenge 2015 in these categories by a large margin. Moreover, the network is fast. Segmentation of a 512x512 image takes less than a second on a recent GPU. The full implementation (based on Caffe) and the trained networks are available at http://lmb.informatik.uni-freiburg.de/people/ronneber/u-net .\n\n9. Title: Refinenet: Multi-path refinement networks for high-resolution semantic segmentation\n   Abstract: RefineNet: Multi-Path Refinement Networks for High-Resolution Semantic Segmentation: Recently, very deep convolutional neural networks (CNNs) have shown outstanding performance in object recognition and have also been the first choice for dense classification problems such as semantic segmentation. However, repeated subsampling operations like pooling or convolution striding in deep CNNs lead to a significant decrease in the initial image resolution. Here, we present RefineNet, a generic multi-path refinement network that explicitly exploits all the information available along the down-sampling process to enable high-resolution prediction using long-range residual connections. In this way, the deeper layers that capture high-level semantic features can be directly refined using fine-grained features from earlier convolutions. The individual components of RefineNet employ residual connections following the identity mapping mindset, which allows for effective end-to-end training. Further, we introduce chained residual pooling, which captures rich background context in an efficient manner. We carry out comprehensive experiments and set new state-of-the-art results on seven public datasets. In particular, we achieve an intersection-over-union score of 83.4 on the challenging PASCAL VOC 2012 dataset, which is the best reported result to date.\n\nThink step-by-step about which candidate best matches the context. Summarize your reasoning, then provide the exact title that should fill the citation. Respond in JSON with fields:\n{\n  \"reasoning\": \"<your step-by-step analysis>\",\n  \"selected_title\": \"<exact candidate title>\"\n}", "response": {"reasoning": "1) The citation context discusses (a) use of global average pooling to obtain image-level global information, (b) BiSeNet placing global pooling at the end of its context path, and (c) a DFN that adds global pooling on top of the base network and uses a squeeze-and-excitationâ€“style channel attention to model channel-wise relationships. 2) Among candidates, several mention global/context pooling (PSPNet, DeepLab/DeepLabv3) and attention (DANet), but candidate 7â€™s abstract explicitly states both â€œglobal average poolingâ€ and a â€œChannel Attention Blockâ€ (and a two-subnetwork DFN with Smooth Network and Border Network), and even frames the Channel Attention Block to select discriminative featuresâ€”matching the mention of squeeze-and-excitation inspired channel-wise relationship extraction. 3) BiSeNet is candidate 3 and matches the BiSeNet reference in the context, but the DFN description in the context corresponds directly and verbatim to candidate 7. Therefore the best match is candidate 7.", "selected_title": "Learning a discriminative feature network for semantic segmentation"}, "selected_title": "Learning a discriminative feature network for semantic segmentation", "positive_title": "Learning a discriminative feature network for semantic segmentation", "is_correct": true, "reasoning": "1) The citation context discusses (a) use of global average pooling to obtain image-level global information, (b) BiSeNet placing global pooling at the end of its context path, and (c) a DFN that adds global pooling on top of the base network and uses a squeeze-and-excitationâ€“style channel attention to model channel-wise relationships. 2) Among candidates, several mention global/context pooling (PSPNet, DeepLab/DeepLabv3) and attention (DANet), but candidate 7â€™s abstract explicitly states both â€œglobal average poolingâ€ and a â€œChannel Attention Blockâ€ (and a two-subnetwork DFN with Smooth Network and Border Network), and even frames the Channel Attention Block to select discriminative featuresâ€”matching the mention of squeeze-and-excitation inspired channel-wise relationship extraction. 3) BiSeNet is candidate 3 and matches the BiSeNet reference in the context, but the DFN description in the context corresponds directly and verbatim to candidate 7. Therefore the best match is candidate 7."}
{"example_id": "blind_2044", "prompt": "You are an expert citation retrieval system as described in the paper \"Multi-Agent System for Reliable Citation Retrieval\".\nYour goal is to autonomously retrieve, verify, and recommend academic references given a query or document excerpt.\n\nTask:\nGiven a citation context from a scientific paper (where a citation is missing), identify the correct paper from a list of candidates.\nAnalyze the context to understand the specific claim, method, or result being cited.\nThen, evaluate each candidate paper to see if it matches the context.\nFinally, select the best matching paper.\n\nCitation Context:\ns rate. UGBA introduces an unnoticeable loss function aimed at maximizing the cosine similarity between backdoor triggers and target nodes to improve the stealthiness of their attack. DPGBA[CITATION] introduces an outlier detector and uses adversarial learning to generate in-distribution triggers, addressing low ASR or outlier issues in existing graph backdoor attacks. More about graph backdoor a\n\nCandidate Papers:\n1. Title: Rethinking Graph Backdoor Attacks: A Distribution-Preserving Perspective\n   Abstract: Rethinking Graph Backdoor Attacks: A Distribution-Preserving Perspective: Graph Neural Networks (GNNs) have shown remarkable performance in various tasks. However, recent works reveal that GNNs are vulnerable to backdoor attacks. Generally, backdoor attack poisons the graph by attaching backdoor triggers and the target class label to a set of nodes in the training graph. A GNN trained on the poisoned graph will then be misled to predict test nodes attached with trigger to the target class. Despite their effectiveness, our empirical analysis shows that triggers generated by existing methods tend to be out-of-distribution (OOD), which significantly differ from the clean data. Hence, these injected triggers can be easily detected and pruned with widely used outlier detection methods in real-world applications. Therefore, in this paper, we study a novel problem of unnoticeable graph backdoor attacks with in-distribution (ID) triggers. To generate ID triggers, we introduce an OOD detector in conjunction with an adversarial learning strategy to generate the attributes of the triggers within distribution. To ensure a high attack success rate with ID triggers, we introduce novel modules designed to enhance trigger memorization by the victim model trained on poisoned graph. Extensive experiments on real-world datasets demonstrate the effectiveness of the proposed method in generating in distribution triggers that can by-pass various defense strategies while maintaining a high attack success rate.\n\n2. Title: Link prediction based on graph neural networks\n   Abstract: Link Prediction Based on Graph Neural Networks: Link prediction is a key problem for network-structured data. Link prediction heuristics use some score functions, such as common neighbors and Katz index, to measure the likelihood of links. They have obtained wide practical uses due to their simplicity, interpretability, and for some of them, scalability. However, every heuristic has a strong assumption on when two nodes are likely to link, which limits their effectiveness on networks where these assumptions fail. In this regard, a more reasonable way should be learning a suitable heuristic from a given network instead of using predefined ones. By extracting a local subgraph around each target link, we aim to learn a function mapping the subgraph patterns to link existence, thus automatically learning a `heuristic' that suits the current network. In this paper, we study this heuristic learning paradigm for link prediction. First, we develop a novel $\\gamma$-decaying heuristic theory. The theory unifies a wide range of heuristics in a single framework, and proves that all these heuristics can be well approximated from local subgraphs. Our results show that local subgraphs reserve rich information related to link existence. Second, based on the $\\gamma$-decaying theory, we propose a new algorithm to learn heuristics from local subgraphs using a graph neural network (GNN). Its experimental results show unprecedented performance, working consistently well on a wide range of problems.\n\n3. Title: Graph backdoor\n   Abstract: Graph Backdoor: One intriguing property of deep neural networks (DNNs) is their inherent vulnerability to backdoor attacks -- a trojan model responds to trigger-embedded inputs in a highly predictable manner while functioning normally otherwise. Despite the plethora of prior work on DNNs for continuous data (e.g., images), the vulnerability of graph neural networks (GNNs) for discrete-structured data (e.g., graphs) is largely unexplored, which is highly concerning given their increasing use in security-sensitive domains. To bridge this gap, we present GTA, the first backdoor attack on GNNs. Compared with prior work, GTA departs in significant ways: graph-oriented -- it defines triggers as specific subgraphs, including both topological structures and descriptive features, entailing a large design spectrum for the adversary; input-tailored -- it dynamically adapts triggers to individual graphs, thereby optimizing both attack effectiveness and evasiveness; downstream model-agnostic -- it can be readily launched without knowledge regarding downstream models or fine-tuning strategies; and attack-extensible -- it can be instantiated for both transductive (e.g., node classification) and inductive (e.g., graph classification) tasks, constituting severe threats for a range of security-critical applications. Through extensive evaluation using benchmark datasets and state-of-the-art models, we demonstrate the effectiveness of GTA. We further provide analytical justification for its effectiveness and discuss potential countermeasures, pointing to several promising research directions.\n\n4. Title: Semi-supervised classification with graph convolutional networks\n   Abstract: Semi-Supervised Classification with Graph Convolutional Networks: We present a scalable approach for semi-supervised learning on graph-structured data that is based on an efficient variant of convolutional neural networks which operate directly on graphs. We motivate the choice of our convolutional architecture via a localized first-order approximation of spectral graph convolutions. Our model scales linearly in the number of graph edges and learns hidden layer representations that encode both local graph structure and features of nodes. In a number of experiments on citation networks and on a knowledge graph dataset we demonstrate that our approach outperforms related methods by a significant margin.\n\n5. Title: Graph attention networks\n   Abstract: Graph Attention Networks: We present graph attention networks (GATs), novel neural network architectures that operate on graph-structured data, leveraging masked self-attentional layers to address the shortcomings of prior methods based on graph convolutions or their approximations. By stacking layers in which nodes are able to attend over their neighborhoods' features, we enable (implicitly) specifying different weights to different nodes in a neighborhood, without requiring any kind of costly matrix operation (such as inversion) or depending on knowing the graph structure upfront. In this way, we address several key challenges of spectral-based graph neural networks simultaneously, and make our model readily applicable to inductive as well as transductive problems. Our GAT models have achieved or matched state-of-the-art results across four established transductive and inductive graph benchmarks: the Cora, Citeseer and Pubmed citation network datasets, as well as a protein-protein interaction dataset (wherein test graphs remain unseen during training).\n\n6. Title: Graph neural networks for social recommendation\n   Abstract: Graph Neural Networks for Social Recommendation: In recent years, Graph Neural Networks (GNNs), which can naturally integrate node information and topological structure, have been demonstrated to be powerful in learning on graph data. These advantages of GNNs provide great potential to advance social recommendation since data in social recommender systems can be represented as user-user social graph and user-item graph; and learning latent factors of users and items is the key. However, building social recommender systems based on GNNs faces challenges. For example, the user-item graph encodes both interactions and their associated opinions; social relations have heterogeneous strengths; users involve in two graphs (e.g., the user-user social graph and the user-item graph). To address the three aforementioned challenges simultaneously, in this paper, we present a novel graph neural network framework (GraphRec) for social recommendations. In particular, we provide a principled approach to jointly capture interactions and opinions in the user-item graph and propose the framework GraphRec, which coherently models two graphs and heterogeneous strengths. Extensive experiments on two real-world datasets demonstrate the effectiveness of the proposed framework GraphRec. Our code is available at \\url{https://github.com/wenqifan03/GraphRec-WWW19}\n\n7. Title: Backdoor attacks to graph neural networks\n   Abstract: Backdoor Attacks to Graph Neural Networks: In this work, we propose the first backdoor attack to graph neural networks (GNN). Specifically, we propose a \\emph{subgraph based backdoor attack} to GNN for graph classification. In our backdoor attack, a GNN classifier predicts an attacker-chosen target label for a testing graph once a predefined subgraph is injected to the testing graph. Our empirical results on three real-world graph datasets show that our backdoor attacks are effective with a small impact on a GNN's prediction accuracy for clean testing graphs. Moreover, we generalize a randomized smoothing based certified defense to defend against our backdoor attacks. Our empirical results show that the defense is effective in some cases but ineffective in other cases, highlighting the needs of new defenses for our backdoor attacks.\n\n8. Title: Unnoticeable Backdoor Attacks on Graph Neural Networks\n   Abstract: Unnoticeable Backdoor Attacks on Graph Neural Networks: Graph Neural Networks (GNNs) have achieved promising results in various tasks such as node classification and graph classification. Recent studies find that GNNs are vulnerable to adversarial attacks. However, effective backdoor attacks on graphs are still an open problem. In particular, backdoor attack poisons the graph by attaching triggers and the target class label to a set of nodes in the training graph. The backdoored GNNs trained on the poisoned graph will then be misled to predict test nodes to target class once attached with triggers. Though there are some initial efforts in graph backdoor attacks, our empirical analysis shows that they may require a large attack budget for effective backdoor attacks and the injected triggers can be easily detected and pruned. Therefore, in this paper, we study a novel problem of unnoticeable graph backdoor attacks with limited attack budget. To fully utilize the attack budget, we propose to deliberately select the nodes to inject triggers and target class labels in the poisoning phase. An adaptive trigger generator is deployed to obtain effective triggers that are difficult to be noticed. Extensive experiments on real-world datasets against various defense strategies demonstrate the effectiveness of our proposed method in conducting effective unnoticeable backdoor attacks.\n\nThink step-by-step about which candidate best matches the context. Summarize your reasoning, then provide the exact title that should fill the citation. Respond in JSON with fields:\n{\n  \"reasoning\": \"<your step-by-step analysis>\",\n  \"selected_title\": \"<exact candidate title>\"\n}", "response": {"reasoning": "The citation context describes a method that (1) introduces an outlier detector and (2) uses adversarial learning to generate in-distribution (ID) triggers to avoid OOD/outlier detection and improve attack stealthiness. Candidate 1's abstract explicitly states an OOD detector combined with an adversarial learning strategy to generate trigger attributes within distribution and emphasizes distribution-preserving (in-distribution) trigger generation. Other candidates: 8 (Unnoticeable Backdoor Attacks...) discusses adaptive trigger generation and stealthiness but does not mention an outlier/OOD detector or adversarial learning for distribution-preserving triggers; 3,7 and others describe general graph backdoor attacks but lack the specific combination of an OOD detector and adversarial learning to produce in-distribution triggers. Therefore Candidate 1 precisely matches the described DPGBA work.", "selected_title": "Rethinking Graph Backdoor Attacks: A Distribution-Preserving Perspective"}, "selected_title": "Rethinking Graph Backdoor Attacks: A Distribution-Preserving Perspective", "positive_title": "Rethinking Graph Backdoor Attacks: A Distribution-Preserving Perspective", "is_correct": true, "reasoning": "The citation context describes a method that (1) introduces an outlier detector and (2) uses adversarial learning to generate in-distribution (ID) triggers to avoid OOD/outlier detection and improve attack stealthiness. Candidate 1's abstract explicitly states an OOD detector combined with an adversarial learning strategy to generate trigger attributes within distribution and emphasizes distribution-preserving (in-distribution) trigger generation. Other candidates: 8 (Unnoticeable Backdoor Attacks...) discusses adaptive trigger generation and stealthiness but does not mention an outlier/OOD detector or adversarial learning for distribution-preserving triggers; 3,7 and others describe general graph backdoor attacks but lack the specific combination of an OOD detector and adversarial learning to produce in-distribution triggers. Therefore Candidate 1 precisely matches the described DPGBA work."}
{"example_id": "blind_7652", "prompt": "You are an expert citation retrieval system as described in the paper \"Multi-Agent System for Reliable Citation Retrieval\".\nYour goal is to autonomously retrieve, verify, and recommend academic references given a query or document excerpt.\n\nTask:\nGiven a citation context from a scientific paper (where a citation is missing), identify the correct paper from a list of candidates.\nAnalyze the context to understand the specific claim, method, or result being cited.\nThen, evaluate each candidate paper to see if it matches the context.\nFinally, select the best matching paper.\n\nCitation Context:\nent 3D segmentation. Recent work has also introduced 3D Gaussian representations into the domain of 3D language localization and segmentation, achieving more precise localization. LangSplat[CITATION] proposes using SAM to learn hierarchical semantics, eliminating the need for DINO feature regularization. Additionally, they trained a scene-specific language autoencoder to red\n\nCandidate Papers:\n1. Title: Instruct-nerf2nerf: Editing 3d scenes with instructions\n   Abstract: Instruct-NeRF2NeRF: Editing 3D Scenes with Instructions: We propose a method for editing NeRF scenes with text-instructions. Given a NeRF of a scene and the collection of images used to reconstruct it, our method uses an image-conditioned diffusion model (InstructPix2Pix) to iteratively edit the input images while optimizing the underlying scene, resulting in an optimized 3D scene that respects the edit instruction. We demonstrate that our proposed method is able to edit large-scale, real-world scenes, and is able to accomplish more realistic, targeted edits than prior work.\n\n2. Title: Fmgs: Foundation model embedded 3d gaussian splatting for holistic 3d scene understanding\n   Abstract: FMGS: Foundation Model Embedded 3D Gaussian Splatting for Holistic 3D Scene Understanding: Precisely perceiving the geometric and semantic properties of real-world 3D objects is crucial for the continued evolution of augmented reality and robotic applications. To this end, we present Foundation Model Embedded Gaussian Splatting (FMGS), which incorporates vision-language embeddings of foundation models into 3D Gaussian Splatting (GS). The key contribution of this work is an efficient method to reconstruct and represent 3D vision-language models. This is achieved by distilling feature maps generated from image-based foundation models into those rendered from our 3D model. To ensure high-quality rendering and fast training, we introduce a novel scene representation by integrating strengths from both GS and multi-resolution hash encodings (MHE). Our effective training procedure also introduces a pixel alignment loss that makes the rendered feature distance of the same semantic entities close, following the pixel-level semantic boundaries. Our results demonstrate remarkable multi-view semantic consistency, facilitating diverse downstream tasks, beating state-of-the-art methods by 10.2 percent on open-vocabulary language-based object detection, despite that we are 851X faster for inference. This research explores the intersection of vision, language, and 3D scene representation, paving the way for enhanced scene understanding in uncontrolled real-world environments. We plan to release the code on the project page.\n\n3. Title: Language grounding with 3d objects\n   Abstract: Language Grounding with 3D Objects: Seemingly simple natural language requests to a robot are generally underspecified, for example \"Can you bring me the wireless mouse?\" Flat images of candidate mice may not provide the discriminative information needed for \"wireless.\" The world, and objects in it, are not flat images but complex 3D shapes. If a human requests an object based on any of its basic properties, such as color, shape, or texture, robots should perform the necessary exploration to accomplish the task. In particular, while substantial effort and progress has been made on understanding explicitly visual attributes like color and category, comparatively little progress has been made on understanding language about shapes and contours. In this work, we introduce a novel reasoning task that targets both visual and non-visual language about 3D objects. Our new benchmark, ShapeNet Annotated with Referring Expressions (SNARE) requires a model to choose which of two objects is being referenced by a natural language description. We introduce several CLIP-based models for distinguishing objects and demonstrate that while recent advances in jointly modeling vision and language are useful for robotic language understanding, it is still the case that these image-based models are weaker at understanding the 3D nature of objects -- properties which play a key role in manipulation. We find that adding view estimation to language grounding models improves accuracy on both SNARE and when identifying objects referred to in language on a robot platform, but note that a large gap remains between these models and human performance.\n\n4. Title: LangSplat: 3D Language Gaussian Splatting\n   Abstract: LangSplat: 3D Language Gaussian Splatting: Humans live in a 3D world and commonly use natural language to interact with a 3D scene. Modeling a 3D language field to support open-ended language queries in 3D has gained increasing attention recently. This paper introduces LangSplat, which constructs a 3D language field that enables precise and efficient open-vocabulary querying within 3D spaces. Unlike existing methods that ground CLIP language embeddings in a NeRF model, LangSplat advances the field by utilizing a collection of 3D Gaussians, each encoding language features distilled from CLIP, to represent the language field. By employing a tile-based splatting technique for rendering language features, we circumvent the costly rendering process inherent in NeRF. Instead of directly learning CLIP embeddings, LangSplat first trains a scene-wise language autoencoder and then learns language features on the scene-specific latent space, thereby alleviating substantial memory demands imposed by explicit modeling. Existing methods struggle with imprecise and vague 3D language fields, which fail to discern clear boundaries between objects. We delve into this issue and propose to learn hierarchical semantics using SAM, thereby eliminating the need for extensively querying the language field across various scales and the regularization of DINO features. Extensive experimental results show that LangSplat significantly outperforms the previous state-of-the-art method LERF by a large margin. Notably, LangSplat is extremely efficient, achieving a 199 $\\times$ speedup compared to LERF at the resolution of 1440 $\\times$ 1080. We strongly recommend readers to check out our video results at https://langsplat.github.io/\n\n5. Title: Learning transferable visual models from natural language supervision\n   Abstract: Learning Transferable Visual Models From Natural Language Supervision: State-of-the-art computer vision systems are trained to predict a fixed set of predetermined object categories. This restricted form of supervision limits their generality and usability since additional labeled data is needed to specify any other visual concept. Learning directly from raw text about images is a promising alternative which leverages a much broader source of supervision. We demonstrate that the simple pre-training task of predicting which caption goes with which image is an efficient and scalable way to learn SOTA image representations from scratch on a dataset of 400 million (image, text) pairs collected from the internet. After pre-training, natural language is used to reference learned visual concepts (or describe new ones) enabling zero-shot transfer of the model to downstream tasks. We study the performance of this approach by benchmarking on over 30 different existing computer vision datasets, spanning tasks such as OCR, action recognition in videos, geo-localization, and many types of fine-grained object classification. The model transfers non-trivially to most tasks and is often competitive with a fully supervised baseline without the need for any dataset specific training. For instance, we match the accuracy of the original ResNet-50 on ImageNet zero-shot without needing to use any of the 1.28 million training examples it was trained on. We release our code and pre-trained model weights at https://github.com/OpenAI/CLIP.\n\n6. Title: Instructpix2pix: Learning to follow image editing instructions\n   Abstract: InstructPix2Pix: Learning to Follow Image Editing Instructions: We propose a method for editing images from human instructions: given an input image and a written instruction that tells the model what to do, our model follows these instructions to edit the image. To obtain training data for this problem, we combine the knowledge of two large pretrained models -- a language model (GPT-3) and a text-to-image model (Stable Diffusion) -- to generate a large dataset of image editing examples. Our conditional diffusion model, InstructPix2Pix, is trained on our generated data, and generalizes to real images and user-written instructions at inference time. Since it performs edits in the forward pass and does not require per example fine-tuning or inversion, our model edits images quickly, in a matter of seconds. We show compelling editing results for a diverse collection of input images and written instructions.\n\n7. Title: Snerf: stylized neural implicit representations for 3d scenes\n   Abstract: SNeRF: Stylized Neural Implicit Representations for 3D Scenes: This paper presents a stylized novel view synthesis method. Applying state-of-the-art stylization methods to novel views frame by frame often causes jittering artifacts due to the lack of cross-view consistency. Therefore, this paper investigates 3D scene stylization that provides a strong inductive bias for consistent novel view synthesis. Specifically, we adopt the emerging neural radiance fields (NeRF) as our choice of 3D scene representation for their capability to render high-quality novel views for a variety of scenes. However, as rendering a novel view from a NeRF requires a large number of samples, training a stylized NeRF requires a large amount of GPU memory that goes beyond an off-the-shelf GPU capacity. We introduce a new training method to address this problem by alternating the NeRF and stylization optimization steps. Such a method enables us to make full use of our hardware memory capacity to both generate images at higher resolution and adopt more expressive image style transfer methods. Our experiments show that our method produces stylized NeRFs for a wide range of content, including indoor, outdoor and dynamic scenes, and synthesizes high-quality novel views with cross-view consistency.\n\n8. Title: Dist: Rendering deep implicit signed distance function with differentiable sphere tracing\n   Abstract: DIST: Rendering Deep Implicit Signed Distance Function with Differentiable Sphere Tracing: We propose a differentiable sphere tracing algorithm to bridge the gap between inverse graphics methods and the recently proposed deep learning based implicit signed distance function. Due to the nature of the implicit function, the rendering process requires tremendous function queries, which is particularly problematic when the function is represented as a neural network. We optimize both the forward and backward passes of our rendering layer to make it run efficiently with affordable memory consumption on a commodity graphics card. Our rendering method is fully differentiable such that losses can be directly computed on the rendered 2D observations, and the gradients can be propagated backwards to optimize the 3D geometry. We show that our rendering method can effectively reconstruct accurate 3D shapes from various inputs, such as sparse depth and multi-view images, through inverse optimization. With the geometry based reasoning, our 3D shape prediction methods show excellent generalization capability and robustness against various noises.\n\n9. Title: Panoptic lifting for 3d scene understanding with neural fields\n   Abstract: Panoptic Lifting for 3D Scene Understanding with Neural Fields: We propose Panoptic Lifting, a novel approach for learning panoptic 3D volumetric representations from images of in-the-wild scenes. Once trained, our model can render color images together with 3D-consistent panoptic segmentation from novel viewpoints.   Unlike existing approaches which use 3D input directly or indirectly, our method requires only machine-generated 2D panoptic segmentation masks inferred from a pre-trained network. Our core contribution is a panoptic lifting scheme based on a neural field representation that generates a unified and multi-view consistent, 3D panoptic representation of the scene. To account for inconsistencies of 2D instance identifiers across views, we solve a linear assignment with a cost based on the model's current predictions and the machine-generated segmentation masks, thus enabling us to lift 2D instances to 3D in a consistent way. We further propose and ablate contributions that make our method more robust to noisy, machine-generated labels, including test-time augmentations for confidence estimates, segment consistency loss, bounded segmentation fields, and gradient stopping.   Experimental results validate our approach on the challenging Hypersim, Replica, and ScanNet datasets, improving by 8.4, 13.8, and 10.6% in scene-level PQ over state of the art.\n\n10. Title: Plenoctrees for real-time rendering of neural radiance fields\n   Abstract: PlenOctrees for Real-time Rendering of Neural Radiance Fields: We introduce a method to render Neural Radiance Fields (NeRFs) in real time using PlenOctrees, an octree-based 3D representation which supports view-dependent effects. Our method can render 800x800 images at more than 150 FPS, which is over 3000 times faster than conventional NeRFs. We do so without sacrificing quality while preserving the ability of NeRFs to perform free-viewpoint rendering of scenes with arbitrary geometry and view-dependent effects. Real-time performance is achieved by pre-tabulating the NeRF into a PlenOctree. In order to preserve view-dependent effects such as specularities, we factorize the appearance via closed-form spherical basis functions. Specifically, we show that it is possible to train NeRFs to predict a spherical harmonic representation of radiance, removing the viewing direction as an input to the neural network. Furthermore, we show that PlenOctrees can be directly optimized to further minimize the reconstruction loss, which leads to equal or better quality compared to competing methods. Moreover, this octree optimization step can be used to reduce the training time, as we no longer need to wait for the NeRF training to converge fully. Our real-time neural rendering approach may potentially enable new applications such as 6-DOF industrial and product visualizations, as well as next generation AR/VR systems. PlenOctrees are amenable to in-browser rendering as well; please visit the project page for the interactive online demo, as well as video and code: https://alexyu.net/plenoctrees\n\nThink step-by-step about which candidate best matches the context. Summarize your reasoning, then provide the exact title that should fill the citation. Respond in JSON with fields:\n{\n  \"reasoning\": \"<your step-by-step analysis>\",\n  \"selected_title\": \"<exact candidate title>\"\n}", "response": {"reasoning": "The citation context refers to introducing 3D Gaussian representations for 3D language localization/segmentation, specifically mentioning that \"LangSplat\" uses SAM to learn hierarchical semantics (removing the need for DINO feature regularization) and that they trained a scene-specific language autoencoder. Candidate 4's abstract (LangSplat: 3D Language Gaussian Splatting) explicitly states: it uses a collection of 3D Gaussians to represent a language field, trains a scene-wise language autoencoder/latent space, and proposes learning hierarchical semantics using SAM thereby eliminating DINO regularization. None of the other candidates mention 3D Gaussian splatting combined with SAM or a scene-specific language autoencoder. Therefore candidate 4 matches the described claims exactly.", "selected_title": "LangSplat: 3D Language Gaussian Splatting"}, "selected_title": "LangSplat: 3D Language Gaussian Splatting", "positive_title": "LangSplat: 3D Language Gaussian Splatting", "is_correct": true, "reasoning": "The citation context refers to introducing 3D Gaussian representations for 3D language localization/segmentation, specifically mentioning that \"LangSplat\" uses SAM to learn hierarchical semantics (removing the need for DINO feature regularization) and that they trained a scene-specific language autoencoder. Candidate 4's abstract (LangSplat: 3D Language Gaussian Splatting) explicitly states: it uses a collection of 3D Gaussians to represent a language field, trains a scene-wise language autoencoder/latent space, and proposes learning hierarchical semantics using SAM thereby eliminating DINO regularization. None of the other candidates mention 3D Gaussian splatting combined with SAM or a scene-specific language autoencoder. Therefore candidate 4 matches the described claims exactly."}
{"example_id": "blind_1822", "prompt": "You are an expert citation retrieval system as described in the paper \"Multi-Agent System for Reliable Citation Retrieval\".\nYour goal is to autonomously retrieve, verify, and recommend academic references given a query or document excerpt.\n\nTask:\nGiven a citation context from a scientific paper (where a citation is missing), identify the correct paper from a list of candidates.\nAnalyze the context to understand the specific claim, method, or result being cited.\nThen, evaluate each candidate paper to see if it matches the context.\nFinally, select the best matching paper.\n\nCitation Context:\nated target video data, achieving promising results. Another popular method involves injecting information about the input or generated video from keyframes through cross-attention[CITATION]. Some methods denoise original videos by extracting essential features, like depth maps or optical flow which should be retained in the edited video. Additionally, some works<|cit\n\nCandidate Papers:\n1. Title: VMC: Video Motion Customization using Temporal Attention Adaption for Text-to-Video Diffusion Models\n   Abstract: VMC: Video Motion Customization using Temporal Attention Adaption for Text-to-Video Diffusion Models: Text-to-video diffusion models have advanced video generation significantly. However, customizing these models to generate videos with tailored motions presents a substantial challenge. In specific, they encounter hurdles in (a) accurately reproducing motion from a target video, and (b) creating diverse visual variations. For example, straightforward extensions of static image customization methods to video often lead to intricate entanglements of appearance and motion data. To tackle this, here we present the Video Motion Customization (VMC) framework, a novel one-shot tuning approach crafted to adapt temporal attention layers within video diffusion models. Our approach introduces a novel motion distillation objective using residual vectors between consecutive frames as a motion reference. The diffusion process then preserves low-frequency motion trajectories while mitigating high-frequency motion-unrelated noise in image space. We validate our method against state-of-the-art video generative models across diverse real-world motions and contexts. Our codes, data and the project demo can be found at https://video-motion-customization.github.io\n\n2. Title: Lumiere: A Space-Time Diffusion Model for Video Generation\n   Abstract: Lumiere: A Space-Time Diffusion Model for Video Generation: We introduce Lumiere -- a text-to-video diffusion model designed for synthesizing videos that portray realistic, diverse and coherent motion -- a pivotal challenge in video synthesis. To this end, we introduce a Space-Time U-Net architecture that generates the entire temporal duration of the video at once, through a single pass in the model. This is in contrast to existing video models which synthesize distant keyframes followed by temporal super-resolution -- an approach that inherently makes global temporal consistency difficult to achieve. By deploying both spatial and (importantly) temporal down- and up-sampling and leveraging a pre-trained text-to-image diffusion model, our model learns to directly generate a full-frame-rate, low-resolution video by processing it in multiple space-time scales. We demonstrate state-of-the-art text-to-video generation results, and show that our design easily facilitates a wide range of content creation tasks and video editing applications, including image-to-video, video inpainting, and stylized generation.\n\n3. Title: Stylegan-v: A continuous video generator with the price, image quality and perks of stylegan2\n   Abstract: StyleGAN-V: A Continuous Video Generator with the Price, Image Quality and Perks of StyleGAN2: Videos show continuous events, yet most $-$ if not all $-$ video synthesis frameworks treat them discretely in time. In this work, we think of videos of what they should be $-$ time-continuous signals, and extend the paradigm of neural representations to build a continuous-time video generator. For this, we first design continuous motion representations through the lens of positional embeddings. Then, we explore the question of training on very sparse videos and demonstrate that a good generator can be learned by using as few as 2 frames per clip. After that, we rethink the traditional image + video discriminators pair and design a holistic discriminator that aggregates temporal information by simply concatenating frames' features. This decreases the training cost and provides richer learning signal to the generator, making it possible to train directly on 1024$^2$ videos for the first time. We build our model on top of StyleGAN2 and it is just ${\\approx}5\\%$ more expensive to train at the same resolution while achieving almost the same image quality. Moreover, our latent space features similar properties, enabling spatial manipulations that our method can propagate in time. We can generate arbitrarily long videos at arbitrary high frame rate, while prior work struggles to generate even 64 frames at a fixed rate. Our model is tested on four modern 256$^2$ and one 1024$^2$-resolution video synthesis benchmarks. In terms of sheer metrics, it performs on average ${\\approx}30\\%$ better than the closest runner-up. Project website: https://universome.github.io.\n\n4. Title: Unsupervised learning of video representations using lstms\n   Abstract: Unsupervised Learning of Video Representations using LSTMs: We use multilayer Long Short Term Memory (LSTM) networks to learn representations of video sequences. Our model uses an encoder LSTM to map an input sequence into a fixed length representation. This representation is decoded using single or multiple decoder LSTMs to perform different tasks, such as reconstructing the input sequence, or predicting the future sequence. We experiment with two kinds of input sequences - patches of image pixels and high-level representations (\"percepts\") of video frames extracted using a pretrained convolutional net. We explore different design choices such as whether the decoder LSTMs should condition on the generated output. We analyze the outputs of the model qualitatively to see how well the model can extrapolate the learned video representation into the future and into the past. We try to visualize and interpret the learned features. We stress test the model by running it on longer time scales and on out-of-domain data. We further evaluate the representations by finetuning them for a supervised learning problem - human action recognition on the UCF-101 and HMDB-51 datasets. We show that the representations help improve classification accuracy, especially when there are only a few training examples. Even models pretrained on unrelated datasets (300 hours of YouTube videos) can help action recognition performance.\n\n5. Title: Zero-shot video editing using off-the-shelf image diffusion models\n   Abstract: Zero-Shot Video Editing Using Off-The-Shelf Image Diffusion Models: Large-scale text-to-image diffusion models achieve unprecedented success in image generation and editing. However, how to extend such success to video editing is unclear. Recent initial attempts at video editing require significant text-to-video data and computation resources for training, which is often not accessible. In this work, we propose vid2vid-zero, a simple yet effective method for zero-shot video editing. Our vid2vid-zero leverages off-the-shelf image diffusion models, and doesn't require training on any video. At the core of our method is a null-text inversion module for text-to-video alignment, a cross-frame modeling module for temporal consistency, and a spatial regularization module for fidelity to the original video. Without any training, we leverage the dynamic nature of the attention mechanism to enable bi-directional temporal modeling at test time. Experiments and analyses show promising results in editing attributes, subjects, places, etc., in real-world videos. Code is made available at \\url{https://github.com/baaivision/vid2vid-zero}.\n\n6. Title: Vatex: A large-scale, high-quality multilingual dataset for video-and-language research\n   Abstract: VATEX: A Large-Scale, High-Quality Multilingual Dataset for Video-and-Language Research: We present a new large-scale multilingual video description dataset, VATEX, which contains over 41,250 videos and 825,000 captions in both English and Chinese. Among the captions, there are over 206,000 English-Chinese parallel translation pairs. Compared to the widely-used MSR-VTT dataset, VATEX is multilingual, larger, linguistically complex, and more diverse in terms of both video and natural language descriptions. We also introduce two tasks for video-and-language research based on VATEX: (1) Multilingual Video Captioning, aimed at describing a video in various languages with a compact unified captioning model, and (2) Video-guided Machine Translation, to translate a source language description into the target language using the video information as additional spatiotemporal context. Extensive experiments on the VATEX dataset show that, first, the unified multilingual model can not only produce both English and Chinese descriptions for a video more efficiently, but also offer improved performance over the monolingual models. Furthermore, we demonstrate that the spatiotemporal video context can be effectively utilized to align source and target languages and thus assist machine translation. In the end, we discuss the potentials of using VATEX for other video-and-language research.\n\n7. Title: Hierarchical text-conditional image generation with clip latents\n   Abstract: Hierarchical Text-Conditional Image Generation with CLIP Latents: Contrastive models like CLIP have been shown to learn robust representations of images that capture both semantics and style. To leverage these representations for image generation, we propose a two-stage model: a prior that generates a CLIP image embedding given a text caption, and a decoder that generates an image conditioned on the image embedding. We show that explicitly generating image representations improves image diversity with minimal loss in photorealism and caption similarity. Our decoders conditioned on image representations can also produce variations of an image that preserve both its semantics and style, while varying the non-essential details absent from the image representation. Moreover, the joint embedding space of CLIP enables language-guided image manipulations in a zero-shot fashion. We use diffusion models for the decoder and experiment with both autoregressive and diffusion models for the prior, finding that the latter are computationally more efficient and produce higher-quality samples.\n\n8. Title: SmoothVideo: Smooth Video Synthesis with Noise Constraints on Diffusion Models for One-shot Video Tuning\n   Abstract: SmoothVideo: Smooth Video Synthesis with Noise Constraints on Diffusion Models for One-shot Video Tuning: Recent one-shot video tuning methods, which fine-tune the network on a specific video based on pre-trained text-to-image models (e.g., Stable Diffusion), are popular in the community because of the flexibility. However, these methods often produce videos marred by incoherence and inconsistency. To address these limitations, this paper introduces a simple yet effective noise constraint across video frames. This constraint aims to regulate noise predictions across their temporal neighbors, resulting in smooth latents. It can be simply included as a loss term during the training phase. By applying the loss to existing one-shot video tuning methods, we significantly improve the overall consistency and smoothness of the generated videos. Furthermore, we argue that current video evaluation metrics inadequately capture smoothness. To address this, we introduce a novel metric that considers detailed features and their temporal dynamics. Experimental results validate the effectiveness of our approach in producing smoother videos on various one-shot video tuning baselines. The source codes and video demos are available at \\href{https://github.com/SPengLiang/SmoothVideo}{https://github.com/SPengLiang/SmoothVideo}.\n\nThink step-by-step about which candidate best matches the context. Summarize your reasoning, then provide the exact title that should fill the citation. Respond in JSON with fields:\n{\n  \"reasoning\": \"<your step-by-step analysis>\",\n  \"selected_title\": \"<exact candidate title>\"\n}", "response": {"reasoning": "The citation context refers to methods that inject information from keyframes into the generation process via cross-attention. Candidate 5 (vid2vid-zero) explicitly describes a cross-frame modeling module and leverages the dynamic nature of the attention mechanism to enable bi-directional temporal modeling at test time, which corresponds directly to \"injecting information about the input or generated video from keyframes through cross-attention.\" Other candidates: (1) VMC focuses on adapting temporal attention for motion customization (one-shot tuning) rather than keyframe cross-attention injection; (2) Lumiere proposes a space-time U-Net that generates full videos rather than cross-attention from keyframes; (3) StyleGAN-V, (4) Unsupervised LSTMs, (6) VATEX, (7) CLIP-latent image generation, and (8) SmoothVideo address different aspects (continuous generation, representation learning, datasets, image generation, noise constraints) and do not specifically describe cross-frame/keyframe injection via cross-attention. Therefore the best match for the cited statement is candidate 5.", "selected_title": "Zero-shot video editing using off-the-shelf image diffusion models"}, "selected_title": "Zero-shot video editing using off-the-shelf image diffusion models", "positive_title": "Zero-shot video editing using off-the-shelf image diffusion models", "is_correct": true, "reasoning": "The citation context refers to methods that inject information from keyframes into the generation process via cross-attention. Candidate 5 (vid2vid-zero) explicitly describes a cross-frame modeling module and leverages the dynamic nature of the attention mechanism to enable bi-directional temporal modeling at test time, which corresponds directly to \"injecting information about the input or generated video from keyframes through cross-attention.\" Other candidates: (1) VMC focuses on adapting temporal attention for motion customization (one-shot tuning) rather than keyframe cross-attention injection; (2) Lumiere proposes a space-time U-Net that generates full videos rather than cross-attention from keyframes; (3) StyleGAN-V, (4) Unsupervised LSTMs, (6) VATEX, (7) CLIP-latent image generation, and (8) SmoothVideo address different aspects (continuous generation, representation learning, datasets, image generation, noise constraints) and do not specifically describe cross-frame/keyframe injection via cross-attention. Therefore the best match for the cited statement is candidate 5."}
{"example_id": "blind_7735", "prompt": "You are an expert citation retrieval system as described in the paper \"Multi-Agent System for Reliable Citation Retrieval\".\nYour goal is to autonomously retrieve, verify, and recommend academic references given a query or document excerpt.\n\nTask:\nGiven a citation context from a scientific paper (where a citation is missing), identify the correct paper from a list of candidates.\nAnalyze the context to understand the specific claim, method, or result being cited.\nThen, evaluate each candidate paper to see if it matches the context.\nFinally, select the best matching paper.\n\nCitation Context:\nution.} \\label{intro_fig} \\end{figure} While the above studies have demonstrated efficacy in improving SR models, they neglect the rationality of the augmented positive samples. Most existing methods[CITATION] employ random augmentation either at the data or model level to generate augmented views, and regard two augmented views of the same user as a pair of positive samples. However, these methods introdu\n\nCandidate Papers:\n1. Title: Diffusion-lm improves controllable text generation\n   Abstract: Diffusion-LM Improves Controllable Text Generation: Controlling the behavior of language models (LMs) without re-training is a major open problem in natural language generation. While recent works have demonstrated successes on controlling simple sentence attributes (e.g., sentiment), there has been little progress on complex, fine-grained controls (e.g., syntactic structure). To address this challenge, we develop a new non-autoregressive language model based on continuous diffusions that we call Diffusion-LM. Building upon the recent successes of diffusion models in continuous domains, Diffusion-LM iteratively denoises a sequence of Gaussian vectors into word vectors, yielding a sequence of intermediate latent variables. The continuous, hierarchical nature of these intermediate variables enables a simple gradient-based algorithm to perform complex, controllable generation tasks. We demonstrate successful control of Diffusion-LM for six challenging fine-grained control tasks, significantly outperforming prior work.\n\n2. Title: Improving contrastive learning with model augmentation\n   Abstract: Improving Contrastive Learning with Model Augmentation: The sequential recommendation aims at predicting the next items in user behaviors, which can be solved by characterizing item relationships in sequences. Due to the data sparsity and noise issues in sequences, a new self-supervised learning (SSL) paradigm is proposed to improve the performance, which employs contrastive learning between positive and negative views of sequences.   However, existing methods all construct views by adopting augmentation from data perspectives, while we argue that 1) optimal data augmentation methods are hard to devise, 2) data augmentation methods destroy sequential correlations, and 3) data augmentation fails to incorporate comprehensive self-supervised signals.   Therefore, we investigate the possibility of model augmentation to construct view pairs. We propose three levels of model augmentation methods: neuron masking, layer dropping, and encoder complementing.   This work opens up a novel direction in constructing views for contrastive SSL. Experiments verify the efficacy of model augmentation for the SSL in the sequential recommendation. Code is available\\footnote{\\url{https://github.com/salesforce/SRMA}}.\n\n3. Title: Contrastive self-supervised sequential recommendation with robust augmentation\n   Abstract: Contrastive Self-supervised Sequential Recommendation with Robust Augmentation: Sequential Recommendationdescribes a set of techniques to model dynamic user behavior in order to predict future interactions in sequential user data. At their core, such approaches model transition probabilities between items in a sequence, whether through Markov chains, recurrent networks, or more recently, Transformers. However both old and new issues remain, including data-sparsity and noisy data; such issues can impair the performance, especially in complex, parameter-hungry models. In this paper, we investigate the application of contrastive Self-Supervised Learning (SSL) to the sequential recommendation, as a way to alleviate some of these issues. Contrastive SSL constructs augmentations from unlabelled instances, where agreements among positive pairs are maximized. It is challenging to devise a contrastive SSL framework for a sequential recommendation, due to its discrete nature, correlations among items, and skewness of length distributions. To this end, we propose a novel framework, Contrastive Self-supervised Learning for sequential Recommendation (CoSeRec). We introduce two informative augmentation operators leveraging item correlations to create high-quality views for contrastive learning. Experimental results on three real-world datasets demonstrate the effectiveness of the proposed method on improving model performance and the robustness against sparse and noisy data. Our implementation is available online at \\url{https://github.com/YChen1993/CoSeRec}\n\n4. Title: Contrastive learning for sequential recommendation\n   Abstract: Contrastive Learning for Sequential Recommendation: Sequential recommendation methods play a crucial role in modern recommender systems because of their ability to capture a user's dynamic interest from her/his historical interactions. Despite their success, we argue that these approaches usually rely on the sequential prediction task to optimize the huge amounts of parameters. They usually suffer from the data sparsity problem, which makes it difficult for them to learn high-quality user representations. To tackle that, inspired by recent advances of contrastive learning techniques in the computer version, we propose a novel multi-task model called \\textbf{C}ontrastive \\textbf{L}earning for \\textbf{S}equential \\textbf{Rec}ommendation~(\\textbf{CL4SRec}). CL4SRec not only takes advantage of the traditional next item prediction task but also utilizes the contrastive learning framework to derive self-supervision signals from the original user behavior sequences. Therefore, it can extract more meaningful user patterns and further encode the user representation effectively. In addition, we propose three data augmentation approaches to construct self-supervision signals. Extensive experiments on four public datasets demonstrate that CL4SRec achieves state-of-the-art performance over existing baselines by inferring better user representations.\n\n5. Title: Intent contrastive learning for sequential recommendation\n   Abstract: Intent Contrastive Learning for Sequential Recommendation: Users' interactions with items are driven by various intents (e.g., preparing for holiday gifts, shopping for fishing equipment, etc.).However, users' underlying intents are often unobserved/latent, making it challenging to leverage such latent intents forSequentialrecommendation(SR). To investigate the benefits of latent intents and leverage them effectively for recommendation, we proposeIntentContrastiveLearning(ICL), a general learning paradigm that leverages a latent intent variable into SR. The core idea is to learn users' intent distribution functions from unlabeled user behavior sequences and optimize SR models with contrastive self-supervised learning (SSL) by considering the learned intents to improve recommendation. Specifically, we introduce a latent variable to represent users' intents and learn the distribution function of the latent variable via clustering. We propose to leverage the learned intents into SR models via contrastive SSL, which maximizes the agreement between a view of sequence and its corresponding intent. The training is alternated between intent representation learning and the SR model optimization steps within the generalized expectation-maximization (EM) framework. Fusing user intent information into SR also improves model robustness. Experiments conducted on four real-world datasets demonstrate the superiority of the proposed learning paradigm, which improves performance, and robustness against data sparsity and noisy interaction issues.\n\n6. Title: Meta-optimized Contrastive Learning for Sequential Recommendation\n   Abstract: Meta-optimized Contrastive Learning for Sequential Recommendation: Contrastive Learning (CL) performances as a rising approach to address the challenge of sparse and noisy recommendation data. Although having achieved promising results, most existing CL methods only perform either hand-crafted data or model augmentation for generating contrastive pairs to find a proper augmentation operation for different datasets, which makes the model hard to generalize. Additionally, since insufficient input data may lead the encoder to learn collapsed embeddings, these CL methods expect a relatively large number of training data (e.g., large batch size or memory bank) to contrast. However, not all contrastive pairs are always informative and discriminative enough for the training processing. Therefore, a more general CL-based recommendation model called Meta-optimized Contrastive Learning for sequential Recommendation (MCLRec) is proposed in this work. By applying both data augmentation and learnable model augmentation operations, this work innovates the standard CL framework by contrasting data and model augmented views for adaptively capturing the informative features hidden in stochastic data augmentation. Moreover, MCLRec utilizes a meta-learning manner to guide the updating of the model augmenters, which helps to improve the quality of contrastive pairs without enlarging the amount of input data. Finally, a contrastive regularization term is considered to encourage the augmentation model to generate more informative augmented views and avoid too similar contrastive pairs within the meta updating. The experimental results on commonly used datasets validate the effectiveness of MCLRec.\n\n7. Title: Contrastive learning for representation degeneration problem in sequential recommendation\n   Abstract: Contrastive Learning for Representation Degeneration Problem in Sequential Recommendation: Recent advancements of sequential deep learning models such as Transformer and BERT have significantly facilitated the sequential recommendation. However, according to our study, the distribution of item embeddings generated by these models tends to degenerate into an anisotropic shape, which may result in high semantic similarities among embeddings. In this paper, both empirical and theoretical investigations of this representation degeneration problem are first provided, based on which a novel recommender model DuoRec is proposed to improve the item embeddings distribution. Specifically, in light of the uniformity property of contrastive learning, a contrastive regularization is designed for DuoRec to reshape the distribution of sequence representations. Given the convention that the recommendation task is performed by measuring the similarity between sequence representations and item embeddings in the same space via dot product, the regularization can be implicitly applied to the item embedding distribution. Existing contrastive learning methods mainly rely on data level augmentation for user-item interaction sequences through item cropping, masking, or reordering and can hardly provide semantically consistent augmentation samples. In DuoRec, a model-level augmentation is proposed based on Dropout to enable better semantic preserving. Furthermore, a novel sampling strategy is developed, where sequences having the same target item are chosen hard positive samples. Extensive experiments conducted on five datasets demonstrate the superior performance of the proposed DuoRec model compared with baseline methods. Visualization results of the learned representations validate that DuoRec can largely alleviate the representation degeneration problem.\n\n8. Title: Equivariant contrastive learning for sequential recommendation\n   Abstract: Equivariant Contrastive Learning for Sequential Recommendation: Contrastive learning (CL) benefits the training of sequential recommendation models with informative self-supervision signals. Existing solutions apply general sequential data augmentation strategies to generate positive pairs and encourage their representations to be invariant. However, due to the inherent properties of user behavior sequences, some augmentation strategies, such as item substitution, can lead to changes in user intent. Learning indiscriminately invariant representations for all augmentation strategies might be suboptimal. Therefore, we propose Equivariant Contrastive Learning for Sequential Recommendation (ECL-SR), which endows SR models with great discriminative power, making the learned user behavior representations sensitive to invasive augmentations (e.g., item substitution) and insensitive to mild augmentations (e.g., featurelevel dropout masking). In detail, we use the conditional discriminator to capture differences in behavior due to item substitution, which encourages the user behavior encoder to be equivariant to invasive augmentations. Comprehensive experiments on four benchmark datasets show that the proposed ECL-SR framework achieves competitive performance compared to state-of-the-art SR models. The source code is available at https://github.com/Tokkiu/ECL.\n\nThink step-by-step about which candidate best matches the context. Summarize your reasoning, then provide the exact title that should fill the citation. Respond in JSON with fields:\n{\n  \"reasoning\": \"<your step-by-step analysis>\",\n  \"selected_title\": \"<exact candidate title>\"\n}", "response": {"reasoning": "The citation context criticizes prior sequential recommendation (SR) contrastive learning methods for using random/hand-crafted augmentations at either the data or model level to form positive pairs. I looked for a candidate that explicitly (1) addresses contrastive learning for SR, and (2) critiques existing methods for relying on ad-hoc/random data or model augmentations. Candidates 2, 6, and 7 are the closest: 2 advocates model augmentation because data augmentation is problematic; 7 criticizes data-level augmentations as producing semantically inconsistent samples and proposes model-level augmentation; 6 explicitly states that â€œmost existing CL methods only perform either hand-crafted data or model augmentation for generating contrastive pairsâ€ and criticizes such random/hand-crafted augmentations, which matches the phrasing and point in the context. Candidate 6 therefore best matches the intended citation.", "selected_title": "Meta-optimized Contrastive Learning for Sequential Recommendation"}, "selected_title": "Meta-optimized Contrastive Learning for Sequential Recommendation", "positive_title": "Contrastive learning for sequential recommendation", "is_correct": false, "reasoning": "The citation context criticizes prior sequential recommendation (SR) contrastive learning methods for using random/hand-crafted augmentations at either the data or model level to form positive pairs. I looked for a candidate that explicitly (1) addresses contrastive learning for SR, and (2) critiques existing methods for relying on ad-hoc/random data or model augmentations. Candidates 2, 6, and 7 are the closest: 2 advocates model augmentation because data augmentation is problematic; 7 criticizes data-level augmentations as producing semantically inconsistent samples and proposes model-level augmentation; 6 explicitly states that â€œmost existing CL methods only perform either hand-crafted data or model augmentation for generating contrastive pairsâ€ and criticizes such random/hand-crafted augmentations, which matches the phrasing and point in the context. Candidate 6 therefore best matches the intended citation."}
{"example_id": "blind_17714", "prompt": "You are an expert citation retrieval system as described in the paper \"Multi-Agent System for Reliable Citation Retrieval\".\nYour goal is to autonomously retrieve, verify, and recommend academic references given a query or document excerpt.\n\nTask:\nGiven a citation context from a scientific paper (where a citation is missing), identify the correct paper from a list of candidates.\nAnalyze the context to understand the specific claim, method, or result being cited.\nThen, evaluate each candidate paper to see if it matches the context.\nFinally, select the best matching paper.\n\nCitation Context:\ner what learning paradigms or model architectures are adopted. Commonly used learning paradigms include supervised learning, weakly supervised learning, self-supervised learning[CITATION], and semi-supervised learning. Currently, some technologies are borrowed from NLP, such as masked image modeling, auto-regressive modeling. Besides, multi-modal foundation model\n\nCandidate Papers:\n1. Title: Optimal transport for domain adaptation\n   Abstract: Optimal Transport for Domain Adaptation: Domain adaptation from one data space (or domain) to another is one of the most challenging tasks of modern data analytics. If the adaptation is done correctly, models built on a specific data space become more robust when confronted to data depicting the same semantic concepts (the classes), but observed by another observation system with its own specificities. Among the many strategies proposed to adapt a domain to another, finding a common representation has shown excellent properties: by finding a common representation for both domains, a single classifier can be effective in both and use labelled samples from the source domain to predict the unlabelled samples of the target domain. In this paper, we propose a regularized unsupervised optimal transportation model to perform the alignment of the representations in the source and target domains. We learn a transportation plan matching both PDFs, which constrains labelled samples in the source domain to remain close during transport. This way, we exploit at the same time the few labeled information in the source and the unlabelled distributions observed in both domains. Experiments in toy and challenging real visual adaptation examples show the interest of the method, that consistently outperforms state of the art approaches.\n\n2. Title: AutoLR: Layer-wise Pruning and Auto-tuning of Learning Rates in Fine-tuning of Deep Networks\n   Abstract: AutoLR: Layer-wise Pruning and Auto-tuning of Learning Rates in Fine-tuning of Deep Networks: Existing fine-tuning methods use a single learning rate over all layers. In this paper, first, we discuss that trends of layer-wise weight variations by fine-tuning using a single learning rate do not match the well-known notion that lower-level layers extract general features and higher-level layers extract specific features. Based on our discussion, we propose an algorithm that improves fine-tuning performance and reduces network complexity through layer-wise pruning and auto-tuning of layer-wise learning rates. The proposed algorithm has verified the effectiveness by achieving state-of-the-art performance on the image retrieval benchmark datasets (CUB-200, Cars-196, Stanford online product, and Inshop). Code is available at https://github.com/youngminPIL/AutoLR.\n\n3. Title: Parameter-efficient transfer learning for NLP\n   Abstract: Parameter-Efficient Transfer Learning for NLP: Fine-tuning large pre-trained models is an effective transfer mechanism in NLP. However, in the presence of many downstream tasks, fine-tuning is parameter inefficient: an entire new model is required for every task. As an alternative, we propose transfer with adapter modules. Adapter modules yield a compact and extensible model; they add only a few trainable parameters per task, and new tasks can be added without revisiting previous ones. The parameters of the original network remain fixed, yielding a high degree of parameter sharing. To demonstrate adapter's effectiveness, we transfer the recently proposed BERT Transformer model to 26 diverse text classification tasks, including the GLUE benchmark. Adapters attain near state-of-the-art performance, whilst adding only a few parameters per task. On GLUE, we attain within 0.4% of the performance of full fine-tuning, adding only 3.6% parameters per task. By contrast, fine-tuning trains 100% of the parameters per task.\n\n4. Title: Momentum contrast for unsupervised visual representation learning\n   Abstract: Momentum Contrast for Unsupervised Visual Representation Learning: We present Momentum Contrast (MoCo) for unsupervised visual representation learning. From a perspective on contrastive learning as dictionary look-up, we build a dynamic dictionary with a queue and a moving-averaged encoder. This enables building a large and consistent dictionary on-the-fly that facilitates contrastive unsupervised learning. MoCo provides competitive results under the common linear protocol on ImageNet classification. More importantly, the representations learned by MoCo transfer well to downstream tasks. MoCo can outperform its supervised pre-training counterpart in 7 detection/segmentation tasks on PASCAL VOC, COCO, and other datasets, sometimes surpassing it by large margins. This suggests that the gap between unsupervised and supervised representation learning has been largely closed in many vision tasks.\n\n5. Title: Dmcp: Differentiable markov channel pruning for neural networks\n   Abstract: DMCP: Differentiable Markov Channel Pruning for Neural Networks: Recent works imply that the channel pruning can be regarded as searching optimal sub-structure from unpruned networks. However, existing works based on this observation require training and evaluating a large number of structures, which limits their application. In this paper, we propose a novel differentiable method for channel pruning, named Differentiable Markov Channel Pruning (DMCP), to efficiently search the optimal sub-structure. Our method is differentiable and can be directly optimized by gradient descent with respect to standard task loss and budget regularization (e.g. FLOPs constraint). In DMCP, we model the channel pruning as a Markov process, in which each state represents for retaining the corresponding channel during pruning, and transitions between states denote the pruning process. In the end, our method is able to implicitly select the proper number of channels in each layer by the Markov process with optimized transitions. To validate the effectiveness of our method, we perform extensive experiments on Imagenet with ResNet and MobilenetV2. Results show our method can achieve consistent improvement than state-of-the-art pruning methods in various FLOPs settings. The code is available at https://github.com/zx55/dmcp\n\n6. Title: Spottune: transfer learning through adaptive fine-tuning\n   Abstract: SpotTune: Transfer Learning through Adaptive Fine-tuning: Transfer learning, which allows a source task to affect the inductive bias of the target task, is widely used in computer vision. The typical way of conducting transfer learning with deep neural networks is to fine-tune a model pre-trained on the source task using data from the target task. In this paper, we propose an adaptive fine-tuning approach, called SpotTune, which finds the optimal fine-tuning strategy per instance for the target data. In SpotTune, given an image from the target task, a policy network is used to make routing decisions on whether to pass the image through the fine-tuned layers or the pre-trained layers. We conduct extensive experiments to demonstrate the effectiveness of the proposed approach. Our method outperforms the traditional fine-tuning approach on 12 out of 14 standard datasets.We also compare SpotTune with other state-of-the-art fine-tuning strategies, showing superior performance. On the Visual Decathlon datasets, our method achieves the highest score across the board without bells and whistles.\n\n7. Title: Learning transferable features with deep adaptation networks\n   Abstract: Learning Transferable Features with Deep Adaptation Networks: Recent studies reveal that a deep neural network can learn transferable features which generalize well to novel tasks for domain adaptation. However, as deep features eventually transition from general to specific along the network, the feature transferability drops significantly in higher layers with increasing domain discrepancy. Hence, it is important to formally reduce the dataset bias and enhance the transferability in task-specific layers. In this paper, we propose a new Deep Adaptation Network (DAN) architecture, which generalizes deep convolutional neural network to the domain adaptation scenario. In DAN, hidden representations of all task-specific layers are embedded in a reproducing kernel Hilbert space where the mean embeddings of different domain distributions can be explicitly matched. The domain discrepancy is further reduced using an optimal multi-kernel selection method for mean embedding matching. DAN can learn transferable features with statistical guarantees, and can scale linearly by unbiased estimate of kernel embedding. Extensive empirical evidence shows that the proposed architecture yields state-of-the-art image classification error rates on standard domain adaptation benchmarks.\n\n8. Title: Dreaming to Distill: Data-Free Knowledge Transfer via DeepInversion\n   Abstract: Dreaming to Distill: Data-free Knowledge Transfer via DeepInversion: We introduce DeepInversion, a new method for synthesizing images from the image distribution used to train a deep neural network. We 'invert' a trained network (teacher) to synthesize class-conditional input images starting from random noise, without using any additional information about the training dataset. Keeping the teacher fixed, our method optimizes the input while regularizing the distribution of intermediate feature maps using information stored in the batch normalization layers of the teacher. Further, we improve the diversity of synthesized images using Adaptive DeepInversion, which maximizes the Jensen-Shannon divergence between the teacher and student network logits. The resulting synthesized images from networks trained on the CIFAR-10 and ImageNet datasets demonstrate high fidelity and degree of realism, and help enable a new breed of data-free applications - ones that do not require any real images or labeled data. We demonstrate the applicability of our proposed method to three tasks of immense practical importance -- (i) data-free network pruning, (ii) data-free knowledge transfer, and (iii) data-free continual learning. Code is available at https://github.com/NVlabs/DeepInversion\n\n9. Title: Universal source-free domain adaptation\n   Abstract: Universal Source-Free Domain Adaptation: There is a strong incentive to develop versatile learning techniques that can transfer the knowledge of class-separability from a labeled source domain to an unlabeled target domain in the presence of a domain-shift. Existing domain adaptation (DA) approaches are not equipped for practical DA scenarios as a result of their reliance on the knowledge of source-target label-set relationship (e.g. Closed-set, Open-set or Partial DA). Furthermore, almost all prior unsupervised DA works require coexistence of source and target samples even during deployment, making them unsuitable for real-time adaptation. Devoid of such impractical assumptions, we propose a novel two-stage learning process. 1) In the Procurement stage, we aim to equip the model for future source-free deployment, assuming no prior knowledge of the upcoming category-gap and domain-shift. To achieve this, we enhance the model's ability to reject out-of-source distribution samples by leveraging the available source data, in a novel generative classifier framework. 2) In the Deployment stage, the goal is to design a unified adaptation algorithm capable of operating across a wide range of category-gaps, with no access to the previously seen source samples. To this end, in contrast to the usage of complex adversarial training regimes, we define a simple yet effective source-free adaptation objective by utilizing a novel instance-level weighting mechanism, named as Source Similarity Metric (SSM). A thorough evaluation shows the practical usability of the proposed learning framework with superior DA performance even over state-of-the-art source-dependent approaches.\n\n10. Title: Self-supervised Knowledge Distillation for Few-shot Learning\n   Abstract: Self-supervised Knowledge Distillation for Few-shot Learning: Real-world contains an overwhelmingly large number of object classes, learning all of which at once is infeasible. Few shot learning is a promising learning paradigm due to its ability to learn out of order distributions quickly with only a few samples. Recent works [7, 41] show that simply learning a good feature embedding can outperform more sophisticated meta-learning and metric learning algorithms for few-shot learning. In this paper, we propose a simple approach to improve the representation capacity of deep neural networks for few-shot learning tasks. We follow a two-stage learning process: First, we train a neural network to maximize the entropy of the feature embedding, thus creating an optimal output manifold using a self-supervised auxiliary loss. In the second stage, we minimize the entropy on feature embedding by bringing self-supervised twins together, while constraining the manifold with student-teacher distillation. Our experiments show that, even in the first stage, self-supervision can outperform current state-of-the-art methods, with further gains achieved by our second stage distillation process. Our codes are available at: https://github.com/brjathu/SKD.\n\nThink step-by-step about which candidate best matches the context. Summarize your reasoning, then provide the exact title that should fill the citation. Respond in JSON with fields:\n{\n  \"reasoning\": \"<your step-by-step analysis>\",\n  \"selected_title\": \"<exact candidate title>\"\n}", "response": {"reasoning": "The citation position lists common learning paradigms and needs a representative reference for self-supervised (unsupervised) visual representation learning. Among candidates, paper 4 (Momentum Contrast, MoCo) is a seminal, widely-cited self-/unsupervised visual representation learning method and is commonly cited when mentioning self-supervised learning in vision. Candidate 10 is also self-supervised but is a specialized work on few-shot learning and not the canonical general self-supervised method one would cite in this broad enumeration. Other candidates do not address self-supervised representation learning. Therefore the best match is the MoCo paper.", "selected_title": "Momentum contrast for unsupervised visual representation learning"}, "selected_title": "Momentum contrast for unsupervised visual representation learning", "positive_title": "Momentum contrast for unsupervised visual representation learning", "is_correct": true, "reasoning": "The citation position lists common learning paradigms and needs a representative reference for self-supervised (unsupervised) visual representation learning. Among candidates, paper 4 (Momentum Contrast, MoCo) is a seminal, widely-cited self-/unsupervised visual representation learning method and is commonly cited when mentioning self-supervised learning in vision. Candidate 10 is also self-supervised but is a specialized work on few-shot learning and not the canonical general self-supervised method one would cite in this broad enumeration. Other candidates do not address self-supervised representation learning. Therefore the best match is the MoCo paper."}
{"example_id": "blind_11684", "prompt": "You are an expert citation retrieval system as described in the paper \"Multi-Agent System for Reliable Citation Retrieval\".\nYour goal is to autonomously retrieve, verify, and recommend academic references given a query or document excerpt.\n\nTask:\nGiven a citation context from a scientific paper (where a citation is missing), identify the correct paper from a list of candidates.\nAnalyze the context to understand the specific claim, method, or result being cited.\nThen, evaluate each candidate paper to see if it matches the context.\nFinally, select the best matching paper.\n\nCitation Context:\nses by comparing the true colour of a given pixel to the one rendered from the NeRF. Despite it's strengths, NeRF's focus is on visual rendering which can lead to inaccurate geometric representations[CITATION]. Surface neural network models $(S)$, seek to solve this by describing a continuous function $f$ mapping $\\textbf{x}$ to the distance to the nearest surface $f(\\textbf{x}) \\in \\mathbb{R}$\n\nCandidate Papers:\n1. Title: Nerf: Representing scenes as neural radiance fields for view synthesis\n   Abstract: NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis: We present a method that achieves state-of-the-art results for synthesizing novel views of complex scenes by optimizing an underlying continuous volumetric scene function using a sparse set of input views. Our algorithm represents a scene using a fully-connected (non-convolutional) deep network, whose input is a single continuous 5D coordinate (spatial location $(x,y,z)$ and viewing direction $(\\theta, \\phi)$) and whose output is the volume density and view-dependent emitted radiance at that spatial location. We synthesize views by querying 5D coordinates along camera rays and use classic volume rendering techniques to project the output colors and densities into an image. Because volume rendering is naturally differentiable, the only input required to optimize our representation is a set of images with known camera poses. We describe how to effectively optimize neural radiance fields to render photorealistic novel views of scenes with complicated geometry and appearance, and demonstrate results that outperform prior work on neural rendering and view synthesis. View synthesis results are best viewed as videos, so we urge readers to view our supplementary video for convincing comparisons.\n\n2. Title: Mip-nerf: A multiscale representation for anti-aliasing neural radiance fields\n   Abstract: Mip-NeRF: A Multiscale Representation for Anti-Aliasing Neural Radiance Fields: The rendering procedure used by neural radiance fields (NeRF) samples a scene with a single ray per pixel and may therefore produce renderings that are excessively blurred or aliased when training or testing images observe scene content at different resolutions. The straightforward solution of supersampling by rendering with multiple rays per pixel is impractical for NeRF, because rendering each ray requires querying a multilayer perceptron hundreds of times. Our solution, which we call \"mip-NeRF\" (a la \"mipmap\"), extends NeRF to represent the scene at a continuously-valued scale. By efficiently rendering anti-aliased conical frustums instead of rays, mip-NeRF reduces objectionable aliasing artifacts and significantly improves NeRF's ability to represent fine details, while also being 7% faster than NeRF and half the size. Compared to NeRF, mip-NeRF reduces average error rates by 17% on the dataset presented with NeRF and by 60% on a challenging multiscale variant of that dataset that we present. Mip-NeRF is also able to match the accuracy of a brute-force supersampled NeRF on our multiscale dataset while being 22x faster.\n\n3. Title: Zero NeRF: Registration with Zero Overlap\n   Abstract: Zero NeRF: Registration with Zero Overlap: We present Zero-NeRF, a projective surface registration method that, to the best of our knowledge, offers the first general solution capable of alignment between scene representations with minimal or zero visual correspondence. To do this, we enforce consistency between visible surfaces of partial and complete reconstructions, which allows us to constrain occluded geometry. We use a NeRF as our surface representation and the NeRF rendering pipeline to perform this alignment. To demonstrate the efficacy of our method, we register real-world scenes from opposite sides with infinitesimal overlaps that cannot be accurately registered using prior methods, and we compare these results against widely used registration methods.\n\n4. Title: Neural grasp distance fields for robot manipulation\n   Abstract: Neural Grasp Distance Fields for Robot Manipulation: We formulate grasp learning as a neural field and present Neural Grasp Distance Fields (NGDF). Here, the input is a 6D pose of a robot end effector and output is a distance to a continuous manifold of valid grasps for an object. In contrast to current approaches that predict a set of discrete candidate grasps, the distance-based NGDF representation is easily interpreted as a cost, and minimizing this cost produces a successful grasp pose. This grasp distance cost can be incorporated directly into a trajectory optimizer for joint optimization with other costs such as trajectory smoothness and collision avoidance. During optimization, as the various costs are balanced and minimized, the grasp target is allowed to smoothly vary, as the learned grasp field is continuous. We evaluate NGDF on joint grasp and motion planning in simulation and the real world, outperforming baselines by 63% execution success while generalizing to unseen query poses and unseen object shapes. Project page: https://sites.google.com/view/neural-grasp-distance-fields.\n\n5. Title: Vision-only robot navigation in a neural radiance world\n   Abstract: Vision-Only Robot Navigation in a Neural Radiance World: Neural Radiance Fields (NeRFs) have recently emerged as a powerful paradigm for the representation of natural, complex 3D scenes. NeRFs represent continuous volumetric density and RGB values in a neural network, and generate photo-realistic images from unseen camera viewpoints through ray tracing. We propose an algorithm for navigating a robot through a 3D environment represented as a NeRF using only an on-board RGB camera for localization. We assume the NeRF for the scene has been pre-trained offline, and the robot's objective is to navigate through unoccupied space in the NeRF to reach a goal pose. We introduce a trajectory optimization algorithm that avoids collisions with high-density regions in the NeRF based on a discrete time version of differential flatness that is amenable to constraining the robot's full pose and control inputs. We also introduce an optimization based filtering method to estimate 6DoF pose and velocities for the robot in the NeRF given only an onboard RGB camera. We combine the trajectory planner with the pose filter in an online replanning loop to give a vision-based robot navigation pipeline. We present simulation results with a quadrotor robot navigating through a jungle gym environment, the inside of a church, and Stonehenge using only an RGB camera. We also demonstrate an omnidirectional ground robot navigating through the church, requiring it to reorient to fit through the narrow gap. Videos of this work can be found at https://mikh3x4.github.io/nerf-navigation/ .\n\n6. Title: Volume rendering of neural implicit surfaces\n   Abstract: Volume Rendering of Neural Implicit Surfaces: Neural volume rendering became increasingly popular recently due to its success in synthesizing novel views of a scene from a sparse set of input images. So far, the geometry learned by neural volume rendering techniques was modeled using a generic density function. Furthermore, the geometry itself was extracted using an arbitrary level set of the density function leading to a noisy, often low fidelity reconstruction. The goal of this paper is to improve geometry representation and reconstruction in neural volume rendering. We achieve that by modeling the volume density as a function of the geometry. This is in contrast to previous work modeling the geometry as a function of the volume density. In more detail, we define the volume density function as Laplace's cumulative distribution function (CDF) applied to a signed distance function (SDF) representation. This simple density representation has three benefits: (i) it provides a useful inductive bias to the geometry learned in the neural volume rendering process; (ii) it facilitates a bound on the opacity approximation error, leading to an accurate sampling of the viewing ray. Accurate sampling is important to provide a precise coupling of geometry and radiance; and (iii) it allows efficient unsupervised disentanglement of shape and appearance in volume rendering. Applying this new density representation to challenging scene multiview datasets produced high quality geometry reconstructions, outperforming relevant baselines. Furthermore, switching shape and appearance between scenes is possible due to the disentanglement of the two.\n\n7. Title: Nice-slam: Neural implicit scalable encoding for slam\n   Abstract: NICE-SLAM: Neural Implicit Scalable Encoding for SLAM: Neural implicit representations have recently shown encouraging results in various domains, including promising progress in simultaneous localization and mapping (SLAM). Nevertheless, existing methods produce over-smoothed scene reconstructions and have difficulty scaling up to large scenes. These limitations are mainly due to their simple fully-connected network architecture that does not incorporate local information in the observations. In this paper, we present NICE-SLAM, a dense SLAM system that incorporates multi-level local information by introducing a hierarchical scene representation. Optimizing this representation with pre-trained geometric priors enables detailed reconstruction on large indoor scenes. Compared to recent neural implicit SLAM systems, our approach is more scalable, efficient, and robust. Experiments on five challenging datasets demonstrate competitive results of NICE-SLAM in both mapping and tracking quality. Project page: https://pengsongyou.github.io/nice-slam\n\n8. Title: Neural Unsigned Distance Fields for Implicit Function Learning\n   Abstract: Neural Unsigned Distance Fields for Implicit Function Learning: In this work we target a learnable output representation that allows continuous, high resolution outputs of arbitrary shape. Recent works represent 3D surfaces implicitly with a Neural Network, thereby breaking previous barriers in resolution, and ability to represent diverse topologies. However, neural implicit representations are limited to closed surfaces, which divide the space into inside and outside. Many real world objects such as walls of a scene scanned by a sensor, clothing, or a car with inner structures are not closed. This constitutes a significant barrier, in terms of data pre-processing (objects need to be artificially closed creating artifacts), and the ability to output open surfaces. In this work, we propose Neural Distance Fields (NDF), a neural network based model which predicts the unsigned distance field for arbitrary 3D shapes given sparse point clouds. NDF represent surfaces at high resolutions as prior implicit models, but do not require closed surface data, and significantly broaden the class of representable shapes in the output. NDF allow to extract the surface as very dense point clouds and as meshes. We also show that NDF allow for surface normal calculation and can be rendered using a slight modification of sphere tracing. We find NDF can be used for multi-target regression (multiple outputs for one input) with techniques that have been exclusively used for rendering in graphics. Experiments on ShapeNet show that NDF, while simple, is the state-of-the art, and allows to reconstruct shapes with inner structures, such as the chairs inside a bus. Notably, we show that NDF are not restricted to 3D shapes, and can approximate more general open surfaces such as curves, manifolds, and functions. Code is available for research at https://virtualhumans.mpi-inf.mpg.de/ndf/.\n\nThink step-by-step about which candidate best matches the context. Summarize your reasoning, then provide the exact title that should fill the citation. Respond in JSON with fields:\n{\n  \"reasoning\": \"<your step-by-step analysis>\",\n  \"selected_title\": \"<exact candidate title>\"\n}", "response": {"reasoning": "1) Understand the cited claim: the sentence criticizes NeRF for focusing on photorealistic rendering and producing inaccurate/noisy geometric reconstructions; it then introduces surface neural network models that represent a continuous distance function f(x) (i.e., SDF-like representations) to improve geometry. 2) Evaluate candidates: (1) NeRF introduces the rendering approach but is the method being criticized, not a paper documenting that limitation. (2) Mip-NeRF addresses multiscale/anti-aliasing, not geometry fidelity. (3) Zero NeRF is about registration using NeRFs, not about NeRF geometry limitations or SDF remedies. (4,5) Robot/grasp/navigation papers are application-focused and not about NeRF geometry shortcomings. (7) NICE-SLAM discusses over-smoothed reconstructions in SLAM, but its focus is SLAM scalability/representation, not the specific volume-rendering â†’ geometry critique and SDF solution. (8) Neural Unsigned Distance Fields presents unsigned distance fields for open surfaces, but does not frame the critique of NeRF's geometry in volume rendering or propose coupling SDF with volume rendering. (6) Volume Rendering of Neural Implicit Surfaces explicitly states that neural volume rendering techniques model geometry via a generic density and that extracting geometry yields noisy, low-fidelity reconstructions; it proposes modeling density as a function of geometry (using an SDF) to improve geometryâ€”exactly matching the criticized limitation and the suggested SDF-based remedy. 3) Conclusion: candidate 6 best matches the context and should be cited for the claim about NeRF's geometric inaccuracies and the SDF-based surface neural network solution.", "selected_title": "Volume rendering of neural implicit surfaces"}, "selected_title": "Volume rendering of neural implicit surfaces", "positive_title": "Volume rendering of neural implicit surfaces", "is_correct": true, "reasoning": "1) Understand the cited claim: the sentence criticizes NeRF for focusing on photorealistic rendering and producing inaccurate/noisy geometric reconstructions; it then introduces surface neural network models that represent a continuous distance function f(x) (i.e., SDF-like representations) to improve geometry. 2) Evaluate candidates: (1) NeRF introduces the rendering approach but is the method being criticized, not a paper documenting that limitation. (2) Mip-NeRF addresses multiscale/anti-aliasing, not geometry fidelity. (3) Zero NeRF is about registration using NeRFs, not about NeRF geometry limitations or SDF remedies. (4,5) Robot/grasp/navigation papers are application-focused and not about NeRF geometry shortcomings. (7) NICE-SLAM discusses over-smoothed reconstructions in SLAM, but its focus is SLAM scalability/representation, not the specific volume-rendering â†’ geometry critique and SDF solution. (8) Neural Unsigned Distance Fields presents unsigned distance fields for open surfaces, but does not frame the critique of NeRF's geometry in volume rendering or propose coupling SDF with volume rendering. (6) Volume Rendering of Neural Implicit Surfaces explicitly states that neural volume rendering techniques model geometry via a generic density and that extracting geometry yields noisy, low-fidelity reconstructions; it proposes modeling density as a function of geometry (using an SDF) to improve geometryâ€”exactly matching the criticized limitation and the suggested SDF-based remedy. 3) Conclusion: candidate 6 best matches the context and should be cited for the claim about NeRF's geometric inaccuracies and the SDF-based surface neural network solution."}
{"example_id": "blind_17850", "prompt": "You are an expert citation retrieval system as described in the paper \"Multi-Agent System for Reliable Citation Retrieval\".\nYour goal is to autonomously retrieve, verify, and recommend academic references given a query or document excerpt.\n\nTask:\nGiven a citation context from a scientific paper (where a citation is missing), identify the correct paper from a list of candidates.\nAnalyze the context to understand the specific claim, method, or result being cited.\nThen, evaluate each candidate paper to see if it matches the context.\nFinally, select the best matching paper.\n\nCitation Context:\ntextit{MixTraining} can appreciably improve the performance of leading object detectors such as Faster R-CNN with a ResNet-50 backbone (from 41.7 mAP to 44.0 mAP) and Cascade R-CNN[CITATION] with the Swin-Transformer backbone (from 50.9 mAP to 52.8 mAP).\n\nCandidate Papers:\n1. Title: Autoassign: Differentiable label assignment for dense object detection\n   Abstract: AutoAssign: Differentiable Label Assignment for Dense Object Detection: Determining positive/negative samples for object detection is known as label assignment. Here we present an anchor-free detector named AutoAssign. It requires little human knowledge and achieves appearance-aware through a fully differentiable weighting mechanism. During training, to both satisfy the prior distribution of data and adapt to category characteristics, we present Center Weighting to adjust the category-specific prior distributions. To adapt to object appearances, Confidence Weighting is proposed to adjust the specific assign strategy of each instance. The two weighting modules are then combined to generate positive and negative weights to adjust each location's confidence. Extensive experiments on the MS COCO show that our method steadily surpasses other best sampling strategies by large margins with various backbones. Moreover, our best model achieves 52.1% AP, outperforming all existing one-stage detectors. Besides, experiments on other datasets, e.g., PASCAL VOC, Objects365, and WiderFace, demonstrate the broad applicability of AutoAssign.\n\n2. Title: Cornernet: Detecting objects as paired keypoints\n   Abstract: CornerNet: Detecting Objects as Paired Keypoints: We propose CornerNet, a new approach to object detection where we detect an object bounding box as a pair of keypoints, the top-left corner and the bottom-right corner, using a single convolution neural network. By detecting objects as paired keypoints, we eliminate the need for designing a set of anchor boxes commonly used in prior single-stage detectors. In addition to our novel formulation, we introduce corner pooling, a new type of pooling layer that helps the network better localize corners. Experiments show that CornerNet achieves a 42.2% AP on MS COCO, outperforming all existing one-stage detectors.\n\n3. Title: End-to-end object detection with transformers\n   Abstract: End-to-End Object Detection with Transformers: We present a new method that views object detection as a direct set prediction problem. Our approach streamlines the detection pipeline, effectively removing the need for many hand-designed components like a non-maximum suppression procedure or anchor generation that explicitly encode our prior knowledge about the task. The main ingredients of the new framework, called DEtection TRansformer or DETR, are a set-based global loss that forces unique predictions via bipartite matching, and a transformer encoder-decoder architecture. Given a fixed small set of learned object queries, DETR reasons about the relations of the objects and the global image context to directly output the final set of predictions in parallel. The new model is conceptually simple and does not require a specialized library, unlike many other modern detectors. DETR demonstrates accuracy and run-time performance on par with the well-established and highly-optimized Faster RCNN baseline on the challenging COCO object detection dataset. Moreover, DETR can be easily generalized to produce panoptic segmentation in a unified manner. We show that it significantly outperforms competitive baselines. Training code and pretrained models are available at https://github.com/facebookresearch/detr.\n\n4. Title: Deep residual learning for image recognition\n   Abstract: Deep Residual Learning for Image Recognition: Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers---8x deeper than VGG nets but still having lower complexity. An ensemble of these residual nets achieves 3.57% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers.   The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28% relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC & COCO 2015 competitions, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation.\n\n5. Title: Cascade r-cnn: Delving into high quality object detection\n   Abstract: Cascade R-CNN: Delving into High Quality Object Detection: In object detection, an intersection over union (IoU) threshold is required to define positives and negatives. An object detector, trained with low IoU threshold, e.g. 0.5, usually produces noisy detections. However, detection performance tends to degrade with increasing the IoU thresholds. Two main factors are responsible for this: 1) overfitting during training, due to exponentially vanishing positive samples, and 2) inference-time mismatch between the IoUs for which the detector is optimal and those of the input hypotheses. A multi-stage object detection architecture, the Cascade R-CNN, is proposed to address these problems. It consists of a sequence of detectors trained with increasing IoU thresholds, to be sequentially more selective against close false positives. The detectors are trained stage by stage, leveraging the observation that the output of a detector is a good distribution for training the next higher quality detector. The resampling of progressively improved hypotheses guarantees that all detectors have a positive set of examples of equivalent size, reducing the overfitting problem. The same cascade procedure is applied at inference, enabling a closer match between the hypotheses and the detector quality of each stage. A simple implementation of the Cascade R-CNN is shown to surpass all single-model object detectors on the challenging COCO dataset. Experiments also show that the Cascade R-CNN is widely applicable across detector architectures, achieving consistent gains independently of the baseline detector strength. The code will be made available at https://github.com/zhaoweicai/cascade-rcnn.\n\n6. Title: Reppoints v2: Verification meets regression for object detection\n   Abstract: RepPoints V2: Verification Meets Regression for Object Detection: Verification and regression are two general methodologies for prediction in neural networks. Each has its own strengths: verification can be easier to infer accurately, and regression is more efficient and applicable to continuous target variables. Hence, it is often beneficial to carefully combine them to take advantage of their benefits. In this paper, we take this philosophy to improve state-of-the-art object detection, specifically by RepPoints. Though RepPoints provides high performance, we find that its heavy reliance on regression for object localization leaves room for improvement. We introduce verification tasks into the localization prediction of RepPoints, producing RepPoints v2, which provides consistent improvements of about 2.0 mAP over the original RepPoints on the COCO object detection benchmark using different backbones and training methods. RepPoints v2 also achieves 52.1 mAP on COCO \\texttt{test-dev} by a single model. Moreover, we show that the proposed approach can more generally elevate other object detection frameworks as well as applications such as instance segmentation. The code is available at https://github.com/Scalsol/RepPointsV2.\n\n7. Title: Dynamic R-CNN: Towards high quality object detection via dynamic training\n   Abstract: Dynamic R-CNN: Towards High Quality Object Detection via Dynamic Training: Although two-stage object detectors have continuously advanced the state-of-the-art performance in recent years, the training process itself is far from crystal. In this work, we first point out the inconsistency problem between the fixed network settings and the dynamic training procedure, which greatly affects the performance. For example, the fixed label assignment strategy and regression loss function cannot fit the distribution change of proposals and thus are harmful to training high quality detectors. Consequently, we propose Dynamic R-CNN to adjust the label assignment criteria (IoU threshold) and the shape of regression loss function (parameters of SmoothL1 Loss) automatically based on the statistics of proposals during training. This dynamic design makes better use of the training samples and pushes the detector to fit more high quality samples. Specifically, our method improves upon ResNet-50-FPN baseline with 1.9% AP and 5.5% AP$_{90}$ on the MS COCO dataset with no extra overhead. Codes and models are available at https://github.com/hkzhang95/DynamicRCNN.\n\n8. Title: Ssd: Single shot multibox detector\n   Abstract: SSD: Single Shot MultiBox Detector: We present a method for detecting objects in images using a single deep neural network. Our approach, named SSD, discretizes the output space of bounding boxes into a set of default boxes over different aspect ratios and scales per feature map location. At prediction time, the network generates scores for the presence of each object category in each default box and produces adjustments to the box to better match the object shape. Additionally, the network combines predictions from multiple feature maps with different resolutions to naturally handle objects of various sizes. Our SSD model is simple relative to methods that require object proposals because it completely eliminates proposal generation and subsequent pixel or feature resampling stage and encapsulates all computation in a single network. This makes SSD easy to train and straightforward to integrate into systems that require a detection component. Experimental results on the PASCAL VOC, MS COCO, and ILSVRC datasets confirm that SSD has comparable accuracy to methods that utilize an additional object proposal step and is much faster, while providing a unified framework for both training and inference. Compared to other single stage methods, SSD has much better accuracy, even with a smaller input image size. For $300\\times 300$ input, SSD achieves 72.1% mAP on VOC2007 test at 58 FPS on a Nvidia Titan X and for $500\\times 500$ input, SSD achieves 75.1% mAP, outperforming a comparable state of the art Faster R-CNN model. Code is available at https://github.com/weiliu89/caffe/tree/ssd .\n\n9. Title: Nas-fpn: Learning scalable feature pyramid architecture for object detection\n   Abstract: NAS-FPN: Learning Scalable Feature Pyramid Architecture for Object Detection: Current state-of-the-art convolutional architectures for object detection are manually designed. Here we aim to learn a better architecture of feature pyramid network for object detection. We adopt Neural Architecture Search and discover a new feature pyramid architecture in a novel scalable search space covering all cross-scale connections. The discovered architecture, named NAS-FPN, consists of a combination of top-down and bottom-up connections to fuse features across scales. NAS-FPN, combined with various backbone models in the RetinaNet framework, achieves better accuracy and latency tradeoff compared to state-of-the-art object detection models. NAS-FPN improves mobile detection accuracy by 2 AP compared to state-of-the-art SSDLite with MobileNetV2 model in [32] and achieves 48.3 AP which surpasses Mask R-CNN [10] detection accuracy with less computation time.\n\n10. Title: Relation networks for object detection\n   Abstract: Relation Networks for Object Detection: Although it is well believed for years that modeling relations between objects would help object recognition, there has not been evidence that the idea is working in the deep learning era. All state-of-the-art object detection systems still rely on recognizing object instances individually, without exploiting their relations during learning.   This work proposes an object relation module. It processes a set of objects simultaneously through interaction between their appearance feature and geometry, thus allowing modeling of their relations. It is lightweight and in-place. It does not require additional supervision and is easy to embed in existing networks. It is shown effective on improving object recognition and duplicate removal steps in the modern object detection pipeline. It verifies the efficacy of modeling object relations in CNN based detection. It gives rise to the first fully end-to-end object detector.\n\nThink step-by-step about which candidate best matches the context. Summarize your reasoning, then provide the exact title that should fill the citation. Respond in JSON with fields:\n{\n  \"reasoning\": \"<your step-by-step analysis>\",\n  \"selected_title\": \"<exact candidate title>\"\n}", "response": {"reasoning": "1) The missing citation immediately follows the mention of \"Cascade R-CNN\", so the cited work should be the original Cascade R-CNN paper. 2) Among the candidates, paper 5 is titled \"Cascade r-cnn: Delving into high quality object detection\", which is the canonical Cascade R-CNN reference. 3) Although the sentence also mentions a Swin-Transformer backbone (a later backbone choice), authors commonly cite the original Cascade R-CNN paper when referring to that model variant. 4) No other candidate describes Cascade R-CNN. Therefore candidate 5 is the correct citation to fill the reference.", "selected_title": "Cascade r-cnn: Delving into high quality object detection"}, "selected_title": "Cascade r-cnn: Delving into high quality object detection", "positive_title": "Cascade r-cnn: Delving into high quality object detection", "is_correct": true, "reasoning": "1) The missing citation immediately follows the mention of \"Cascade R-CNN\", so the cited work should be the original Cascade R-CNN paper. 2) Among the candidates, paper 5 is titled \"Cascade r-cnn: Delving into high quality object detection\", which is the canonical Cascade R-CNN reference. 3) Although the sentence also mentions a Swin-Transformer backbone (a later backbone choice), authors commonly cite the original Cascade R-CNN paper when referring to that model variant. 4) No other candidate describes Cascade R-CNN. Therefore candidate 5 is the correct citation to fill the reference."}
{"example_id": "blind_927", "prompt": "You are an expert citation retrieval system as described in the paper \"Multi-Agent System for Reliable Citation Retrieval\".\nYour goal is to autonomously retrieve, verify, and recommend academic references given a query or document excerpt.\n\nTask:\nGiven a citation context from a scientific paper (where a citation is missing), identify the correct paper from a list of candidates.\nAnalyze the context to understand the specific claim, method, or result being cited.\nThen, evaluate each candidate paper to see if it matches the context.\nFinally, select the best matching paper.\n\nCitation Context:\nin Generative Adversarial Networks (GANs) not only for high-quality generated images but also an intermediate latent space with disentangled attributes, making latent-based image manipulation possible[CITATION]. However, only randomly generated images can enjoy this editability. The lack of making inferences on a target image in generic GANs gives birth to GAN inversion, which can map any given image into l\n\nCandidate Papers:\n1. Title: Encoding in style: a stylegan encoder for image-to-image translation\n   Abstract: Encoding in Style: a StyleGAN Encoder for Image-to-Image Translation: We present a generic image-to-image translation framework, pixel2style2pixel (pSp). Our pSp framework is based on a novel encoder network that directly generates a series of style vectors which are fed into a pretrained StyleGAN generator, forming the extended W+ latent space. We first show that our encoder can directly embed real images into W+, with no additional optimization. Next, we propose utilizing our encoder to directly solve image-to-image translation tasks, defining them as encoding problems from some input domain into the latent domain. By deviating from the standard invert first, edit later methodology used with previous StyleGAN encoders, our approach can handle a variety of tasks even when the input image is not represented in the StyleGAN domain. We show that solving translation tasks through StyleGAN significantly simplifies the training process, as no adversary is required, has better support for solving tasks without pixel-to-pixel correspondence, and inherently supports multi-modal synthesis via the resampling of styles. Finally, we demonstrate the potential of our framework on a variety of facial image-to-image translation tasks, even when compared to state-of-the-art solutions designed specifically for a single task, and further show that it can be extended beyond the human facial domain.\n\n2. Title: High-fidelity gan inversion for image attribute editing\n   Abstract: High-Fidelity GAN Inversion for Image Attribute Editing: We present a novel high-fidelity generative adversarial network (GAN) inversion framework that enables attribute editing with image-specific details well-preserved (e.g., background, appearance, and illumination). We first analyze the challenges of high-fidelity GAN inversion from the perspective of lossy data compression. With a low bit-rate latent code, previous works have difficulties in preserving high-fidelity details in reconstructed and edited images. Increasing the size of a latent code can improve the accuracy of GAN inversion but at the cost of inferior editability. To improve image fidelity without compromising editability, we propose a distortion consultation approach that employs a distortion map as a reference for high-fidelity reconstruction. In the distortion consultation inversion (DCI), the distortion map is first projected to a high-rate latent map, which then complements the basic low-rate latent code with more details via consultation fusion. To achieve high-fidelity editing, we propose an adaptive distortion alignment (ADA) module with a self-supervised training scheme, which bridges the gap between the edited and inversion images. Extensive experiments in the face and car domains show a clear improvement in both inversion and editing quality.\n\n3. Title: Interpreting the latent space of gans for semantic face editing\n   Abstract: Interpreting the Latent Space of GANs for Semantic Face Editing: Despite the recent advance of Generative Adversarial Networks (GANs) in high-fidelity image synthesis, there lacks enough understanding of how GANs are able to map a latent code sampled from a random distribution to a photo-realistic image. Previous work assumes the latent space learned by GANs follows a distributed representation but observes the vector arithmetic phenomenon. In this work, we propose a novel framework, called InterFaceGAN, for semantic face editing by interpreting the latent semantics learned by GANs. In this framework, we conduct a detailed study on how different semantics are encoded in the latent space of GANs for face synthesis. We find that the latent code of well-trained generative models actually learns a disentangled representation after linear transformations. We explore the disentanglement between various semantics and manage to decouple some entangled semantics with subspace projection, leading to more precise control of facial attributes. Besides manipulating gender, age, expression, and the presence of eyeglasses, we can even vary the face pose as well as fix the artifacts accidentally generated by GAN models. The proposed method is further applied to achieve real image manipulation when combined with GAN inversion methods or some encoder-involved models. Extensive results suggest that learning to synthesize faces spontaneously brings a disentangled and controllable facial attribute representation.\n\n4. Title: Designing an encoder for stylegan image manipulation\n   Abstract: Designing an Encoder for StyleGAN Image Manipulation: Recently, there has been a surge of diverse methods for performing image editing by employing pre-trained unconditional generators. Applying these methods on real images, however, remains a challenge, as it necessarily requires the inversion of the images into their latent space. To successfully invert a real image, one needs to find a latent code that reconstructs the input image accurately, and more importantly, allows for its meaningful manipulation. In this paper, we carefully study the latent space of StyleGAN, the state-of-the-art unconditional generator. We identify and analyze the existence of a distortion-editability tradeoff and a distortion-perception tradeoff within the StyleGAN latent space. We then suggest two principles for designing encoders in a manner that allows one to control the proximity of the inversions to regions that StyleGAN was originally trained on. We present an encoder based on our two principles that is specifically designed for facilitating editing on real images by balancing these tradeoffs. By evaluating its performance qualitatively and quantitatively on numerous challenging domains, including cars and horses, we show that our inversion method, followed by common editing techniques, achieves superior real-image editing quality, with only a small reconstruction accuracy drop.\n\n5. Title: Swin transformer: Hierarchical vision transformer using shifted windows\n   Abstract: Swin Transformer: Hierarchical Vision Transformer using Shifted Windows: This paper presents a new vision Transformer, called Swin Transformer, that capably serves as a general-purpose backbone for computer vision. Challenges in adapting Transformer from language to vision arise from differences between the two domains, such as large variations in the scale of visual entities and the high resolution of pixels in images compared to words in text. To address these differences, we propose a hierarchical Transformer whose representation is computed with \\textbf{S}hifted \\textbf{win}dows. The shifted windowing scheme brings greater efficiency by limiting self-attention computation to non-overlapping local windows while also allowing for cross-window connection. This hierarchical architecture has the flexibility to model at various scales and has linear computational complexity with respect to image size. These qualities of Swin Transformer make it compatible with a broad range of vision tasks, including image classification (87.3 top-1 accuracy on ImageNet-1K) and dense prediction tasks such as object detection (58.7 box AP and 51.1 mask AP on COCO test-dev) and semantic segmentation (53.5 mIoU on ADE20K val). Its performance surpasses the previous state-of-the-art by a large margin of +2.7 box AP and +2.6 mask AP on COCO, and +3.2 mIoU on ADE20K, demonstrating the potential of Transformer-based models as vision backbones. The hierarchical design and the shifted window approach also prove beneficial for all-MLP architectures. The code and models are publicly available at~\\url{https://github.com/microsoft/Swin-Transformer}.\n\n6. Title: Pivotal tuning for latent-based editing of real images\n   Abstract: Pivotal Tuning for Latent-based Editing of Real Images: Recently, a surge of advanced facial editing techniques have been proposed that leverage the generative power of a pre-trained StyleGAN. To successfully edit an image this way, one must first project (or invert) the image into the pre-trained generator's domain. As it turns out, however, StyleGAN's latent space induces an inherent tradeoff between distortion and editability, i.e. between maintaining the original appearance and convincingly altering some of its attributes. Practically, this means it is still challenging to apply ID-preserving facial latent-space editing to faces which are out of the generator's domain. In this paper, we present an approach to bridge this gap. Our technique slightly alters the generator, so that an out-of-domain image is faithfully mapped into an in-domain latent code. The key idea is pivotal tuning - a brief training process that preserves the editing quality of an in-domain latent region, while changing its portrayed identity and appearance. In Pivotal Tuning Inversion (PTI), an initial inverted latent code serves as a pivot, around which the generator is fined-tuned. At the same time, a regularization term keeps nearby identities intact, to locally contain the effect. This surgical training process ends up altering appearance features that represent mostly identity, without affecting editing capabilities. We validate our technique through inversion and editing metrics, and show preferable scores to state-of-the-art methods. We further qualitatively demonstrate our technique by applying advanced edits (such as pose, age, or expression) to numerous images of well-known and recognizable identities. Finally, we demonstrate resilience to harder cases, including heavy make-up, elaborate hairstyles and/or headwear, which otherwise could not have been successfully inverted and edited by state-of-the-art methods.\n\n7. Title: Style transformer for image inversion and editing\n   Abstract: Style Transformer for Image Inversion and Editing: Existing GAN inversion methods fail to provide latent codes for reliable reconstruction and flexible editing simultaneously. This paper presents a transformer-based image inversion and editing model for pretrained StyleGAN which is not only with less distortions, but also of high quality and flexibility for editing. The proposed model employs a CNN encoder to provide multi-scale image features as keys and values. Meanwhile it regards the style code to be determined for different layers of the generator as queries. It first initializes query tokens as learnable parameters and maps them into W+ space. Then the multi-stage alternate self- and cross-attention are utilized, updating queries with the purpose of inverting the input by the generator. Moreover, based on the inverted code, we investigate the reference- and label-based attribute editing through a pretrained latent classifier, and achieve flexible image-to-image translation with high quality results. Extensive experiments are carried out, showing better performances on both inversion and editing tasks within StyleGAN.\n\n8. Title: Ganalyze: Toward visual definitions of cognitive image properties\n   Abstract: GANalyze: Toward Visual Definitions of Cognitive Image Properties: We introduce a framework that uses Generative Adversarial Networks (GANs) to study cognitive properties like memorability, aesthetics, and emotional valence. These attributes are of interest because we do not have a concrete visual definition of what they entail. What does it look like for a dog to be more or less memorable? GANs allow us to generate a manifold of natural-looking images with fine-grained differences in their visual attributes. By navigating this manifold in directions that increase memorability, we can visualize what it looks like for a particular generated image to become more or less memorable. The resulting ``visual definitions\" surface image properties (like ``object size\") that may underlie memorability. Through behavioral experiments, we verify that our method indeed discovers image manipulations that causally affect human memory performance. We further demonstrate that the same framework can be used to analyze image aesthetics and emotional valence. Visit the GANalyze website at http://ganalyze.csail.mit.edu/.\n\n9. Title: Ganspace: Discovering interpretable gan controls\n   Abstract: GANSpace: Discovering Interpretable GAN Controls: This paper describes a simple technique to analyze Generative Adversarial Networks (GANs) and create interpretable controls for image synthesis, such as change of viewpoint, aging, lighting, and time of day. We identify important latent directions based on Principal Components Analysis (PCA) applied either in latent space or feature space. Then, we show that a large number of interpretable controls can be defined by layer-wise perturbation along the principal directions. Moreover, we show that BigGAN can be controlled with layer-wise inputs in a StyleGAN-like manner. We show results on different GANs trained on various datasets, and demonstrate good qualitative matches to edit directions found through earlier supervised approaches.\n\nThink step-by-step about which candidate best matches the context. Summarize your reasoning, then provide the exact title that should fill the citation. Respond in JSON with fields:\n{\n  \"reasoning\": \"<your step-by-step analysis>\",\n  \"selected_title\": \"<exact candidate title>\"\n}", "response": {"reasoning": "The citation context states that GANs provide an intermediate latent space with disentangled attributes enabling latent-based image manipulation, and that this observation motivates GAN inversion for mapping real images into that latent space. Candidate evaluation:\n\n- (3) \"Interpreting the latent space of gans for semantic face editing\" (InterFaceGAN) explicitly studies how GAN latent codes encode disentangled semantics, demonstrates semantic face editing via linear subspaces, and discusses applying these latent-space edits to real images when combined with GAN inversion methods. This directly matches the claim about disentangled latent space enabling latent-based manipulation and the connection to inversion.\n- (9) \"Ganspace: Discovering interpretable gan controls\" is also about finding interpretable latent directions, but its emphasis is on PCA-based discovery of controls rather than the disentanglement analysis and semantic interpretation highlighted in the context. It is a close second but less directly tied to the wording about disentangled representation.\n- (1,4,6,7,2) focus on encoders/inversion methods or improving fidelity/editability; they address the inversion side rather than the original claim about GANs learning a disentangled latent space that enables manipulation.\n- (8,5) are unrelated (cognitive properties; vision transformer).\n\nTherefore the best match for the cited claim is InterFaceGAN (candidate 3).", "selected_title": "Interpreting the latent space of gans for semantic face editing"}, "selected_title": "Interpreting the latent space of gans for semantic face editing", "positive_title": "Encoding in style: a stylegan encoder for image-to-image translation", "is_correct": false, "reasoning": "The citation context states that GANs provide an intermediate latent space with disentangled attributes enabling latent-based image manipulation, and that this observation motivates GAN inversion for mapping real images into that latent space. Candidate evaluation:\n\n- (3) \"Interpreting the latent space of gans for semantic face editing\" (InterFaceGAN) explicitly studies how GAN latent codes encode disentangled semantics, demonstrates semantic face editing via linear subspaces, and discusses applying these latent-space edits to real images when combined with GAN inversion methods. This directly matches the claim about disentangled latent space enabling latent-based manipulation and the connection to inversion.\n- (9) \"Ganspace: Discovering interpretable gan controls\" is also about finding interpretable latent directions, but its emphasis is on PCA-based discovery of controls rather than the disentanglement analysis and semantic interpretation highlighted in the context. It is a close second but less directly tied to the wording about disentangled representation.\n- (1,4,6,7,2) focus on encoders/inversion methods or improving fidelity/editability; they address the inversion side rather than the original claim about GANs learning a disentangled latent space that enables manipulation.\n- (8,5) are unrelated (cognitive properties; vision transformer).\n\nTherefore the best match for the cited claim is InterFaceGAN (candidate 3)."}
{"example_id": "blind_14963", "prompt": "You are an expert citation retrieval system as described in the paper \"Multi-Agent System for Reliable Citation Retrieval\".\nYour goal is to autonomously retrieve, verify, and recommend academic references given a query or document excerpt.\n\nTask:\nGiven a citation context from a scientific paper (where a citation is missing), identify the correct paper from a list of candidates.\nAnalyze the context to understand the specific claim, method, or result being cited.\nThen, evaluate each candidate paper to see if it matches the context.\nFinally, select the best matching paper.\n\nCitation Context:\nimproves accuracy. Unlike our work, do not quantify when their proposed algorithm improves upon the standard approach of using just hard labels. An important difference between our work and[CITATION] as well as is that the results of these two papers are with the squared loss, whereas we provide results with the cross-entropy loss in addition to squared loss. The cross-entropy loss is\n\nCandidate Papers:\n1. Title: Distillation $\\approx$ Early Stopping? Harvesting Dark Knowledge Utilizing Anisotropic Information Retrieval For Overparameterized Neural Network\n   Abstract: Distillation $\\approx$ Early Stopping? Harvesting Dark Knowledge Utilizing Anisotropic Information Retrieval For Overparameterized Neural Network: Distillation is a method to transfer knowledge from one model to another and often achieves higher accuracy with the same capacity. In this paper, we aim to provide a theoretical understanding on what mainly helps with the distillation. Our answer is \"early stopping\". Assuming that the teacher network is overparameterized, we argue that the teacher network is essentially harvesting dark knowledge from the data via early stopping. This can be justified by a new concept, {Anisotropic Information Retrieval (AIR)}, which means that the neural network tends to fit the informative information first and the non-informative information (including noise) later. Motivated by the recent development on theoretically analyzing overparameterized neural networks, we can characterize AIR by the eigenspace of the Neural Tangent Kernel(NTK). AIR facilities a new understanding of distillation. With that, we further utilize distillation to refine noisy labels. We propose a self-distillation algorithm to sequentially distill knowledge from the network in the previous training epoch to avoid memorizing the wrong labels. We also demonstrate, both theoretically and empirically, that self-distillation can benefit from more than just early stopping. Theoretically, we prove convergence of the proposed algorithm to the ground truth labels for randomly initialized overparameterized neural networks in terms of $\\ell_2$ distance, while the previous result was on convergence in $0$-$1$ loss. The theoretical result ensures the learned neural network enjoy a margin on the training data which leads to better generalization. Empirically, we achieve better testing accuracy and entirely avoid early stopping which makes the algorithm more user-friendly.\n\n2. Title: Robust Active Distillation\n   Abstract: Robust Active Distillation: Distilling knowledge from a large teacher model to a lightweight one is a widely successful approach for generating compact, powerful models in the semi-supervised learning setting where a limited amount of labeled data is available. In large-scale applications, however, the teacher tends to provide a large number of incorrect soft-labels that impairs student performance. The sheer size of the teacher additionally constrains the number of soft-labels that can be queried due to prohibitive computational and/or financial costs. The difficulty in achieving simultaneous \\emph{efficiency} (i.e., minimizing soft-label queries) and \\emph{robustness} (i.e., avoiding student inaccuracies due to incorrect labels) hurts the widespread application of knowledge distillation to many modern tasks. In this paper, we present a parameter-free approach with provable guarantees to query the soft-labels of points that are simultaneously informative and correctly labeled by the teacher. At the core of our work lies a game-theoretic formulation that explicitly considers the inherent trade-off between the informativeness and correctness of input instances. We establish bounds on the expected performance of our approach that hold even in worst-case distillation instances. We present empirical evaluations on popular benchmarks that demonstrate the improved distillation performance enabled by our work relative to that of state-of-the-art active learning and active distillation methods.\n\n3. Title: Knowledge distillation in wide neural networks: Risk bound, data efficiency and imperfect teacher\n   Abstract: Knowledge Distillation in Wide Neural Networks: Risk Bound, Data Efficiency and Imperfect Teacher: Knowledge distillation is a strategy of training a student network with guide of the soft output from a teacher network. It has been a successful method of model compression and knowledge transfer. However, currently knowledge distillation lacks a convincing theoretical understanding. On the other hand, recent finding on neural tangent kernel enables us to approximate a wide neural network with a linear model of the network's random features. In this paper, we theoretically analyze the knowledge distillation of a wide neural network. First we provide a transfer risk bound for the linearized model of the network. Then we propose a metric of the task's training difficulty, called data inefficiency. Based on this metric, we show that for a perfect teacher, a high ratio of teacher's soft labels can be beneficial. Finally, for the case of imperfect teacher, we find that hard labels can correct teacher's wrong prediction, which explains the practice of mixing hard and soft labels.\n\n4. Title: Align before fuse: Vision and language representation learning with momentum distillation\n   Abstract: Align before Fuse: Vision and Language Representation Learning with Momentum Distillation: Large-scale vision and language representation learning has shown promising improvements on various vision-language tasks. Most existing methods employ a transformer-based multimodal encoder to jointly model visual tokens (region-based image features) and word tokens. Because the visual tokens and word tokens are unaligned, it is challenging for the multimodal encoder to learn image-text interactions. In this paper, we introduce a contrastive loss to ALign the image and text representations BEfore Fusing (ALBEF) them through cross-modal attention, which enables more grounded vision and language representation learning. Unlike most existing methods, our method does not require bounding box annotations nor high-resolution images. In order to improve learning from noisy web data, we propose momentum distillation, a self-training method which learns from pseudo-targets produced by a momentum model. We provide a theoretical analysis of ALBEF from a mutual information maximization perspective, showing that different training tasks can be interpreted as different ways to generate views for an image-text pair. ALBEF achieves state-of-the-art performance on multiple downstream vision-language tasks. On image-text retrieval, ALBEF outperforms methods that are pre-trained on orders of magnitude larger datasets. On VQA and NLVR$^2$, ALBEF achieves absolute improvements of 2.37% and 3.84% compared to the state-of-the-art, while enjoying faster inference speed. Code and pre-trained models are available at https://github.com/salesforce/ALBEF/.\n\n5. Title: Knowledge distillation: A survey\n   Abstract: Knowledge Distillation: A Survey: In recent years, deep neural networks have been successful in both industry and academia, especially for computer vision tasks. The great success of deep learning is mainly due to its scalability to encode large-scale data and to maneuver billions of model parameters. However, it is a challenge to deploy these cumbersome deep models on devices with limited resources, e.g., mobile phones and embedded devices, not only because of the high computational complexity but also the large storage requirements. To this end, a variety of model compression and acceleration techniques have been developed. As a representative type of model compression and acceleration, knowledge distillation effectively learns a small student model from a large teacher model. It has received rapid increasing attention from the community. This paper provides a comprehensive survey of knowledge distillation from the perspectives of knowledge categories, training schemes, teacher-student architecture, distillation algorithms, performance comparison and applications. Furthermore, challenges in knowledge distillation are briefly reviewed and comments on future research are discussed and forwarded.\n\n6. Title: Knowledge distillation beyond model compression\n   Abstract: Knowledge Distillation Beyond Model Compression: Knowledge distillation (KD) is commonly deemed as an effective model compression technique in which a compact model (student) is trained under the supervision of a larger pretrained model or an ensemble of models (teacher). Various techniques have been proposed since the original formulation, which mimic different aspects of the teacher such as the representation space, decision boundary, or intra-data relationship. Some methods replace the one-way knowledge distillation from a static teacher with collaborative learning between a cohort of students. Despite the recent advances, a clear understanding of where knowledge resides in a deep neural network and an optimal method for capturing knowledge from teacher and transferring it to student remains an open question. In this study, we provide an extensive study on nine different KD methods which covers a broad spectrum of approaches to capture and transfer knowledge. We demonstrate the versatility of the KD framework on different datasets and network architectures under varying capacity gaps between the teacher and student. The study provides intuition for the effects of mimicking different aspects of the teacher and derives insights from the performance of the different distillation approaches to guide the design of more effective KD methods. Furthermore, our study shows the effectiveness of the KD framework in learning efficiently under varying severity levels of label noise and class imbalance, consistently providing generalization gains over standard training. We emphasize that the efficacy of KD goes much beyond a model compression technique and it should be considered as a general-purpose training paradigm which offers more robustness to common challenges in the real-world datasets compared to the standard training procedure.\n\n7. Title: Born again neural networks\n   Abstract: Born Again Neural Networks: Knowledge Distillation (KD) consists of transferring Ã¢Â€ÂœknowledgeÃ¢Â€Â from one machine learning model (the teacher) to another (the student). Commonly, the teacher is a high-capacity model with formidable performance, while the student is more compact. By transferring knowledge, one hopes to benefit from the studentÃ¢Â€Â™s compactness, without sacrificing too much performance. We study KD from a new perspective: rather than compressing models, we train students parameterized identically to their teachers. Surprisingly, these Born-Again Networks (BANs), outperform their teachers significantly, both on computer vision and language modeling tasks. Our experiments with BANs based on DenseNets demonstrate state-of-the-art performance on the CIFAR-10 (3.5%) and CIFAR-100 (15.5%) datasets, by validation error. Additional experiments explore two distillation objectives: (i) Confidence-Weighted by Teacher Max (CWTM) and (ii) Dark Knowledge with Permuted Predictions (DKPP). Both methods elucidate the essential components of KD, demonstrating the effect of the teacher outputs on both predicted and non-predicted classes.\n\n8. Title: Knowledge Distillation: Bad Models Can Be Good Role Models\n   Abstract: Knowledge Distillation: Bad Models Can Be Good Role Models: Large neural networks trained in the overparameterized regime are able to fit noise to zero train error. Recent work \\citep{nakkiran2020distributional} has empirically observed that such networks behave as \"conditional samplers\" from the noisy distribution. That is, they replicate the noise in the train data to unseen examples. We give a theoretical framework for studying this conditional sampling behavior in the context of learning theory. We relate the notion of such samplers to knowledge distillation, where a student network imitates the outputs of a teacher on unlabeled data. We show that samplers, while being bad classifiers, can be good teachers. Concretely, we prove that distillation from samplers is guaranteed to produce a student which approximates the Bayes optimal classifier. Finally, we show that some common learning algorithms (e.g., Nearest-Neighbours and Kernel Machines) can generate samplers when applied in the overparameterized regime.\n\n9. Title: Knowledge distillation: A good teacher is patient and consistent\n   Abstract: Knowledge distillation: A good teacher is patient and consistent: There is a growing discrepancy in computer vision between large-scale models that achieve state-of-the-art performance and models that are affordable in practical applications. In this paper we address this issue and significantly bridge the gap between these two types of models. Throughout our empirical investigation we do not aim to necessarily propose a new method, but strive to identify a robust and effective recipe for making state-of-the-art large scale models affordable in practice. We demonstrate that, when performed correctly, knowledge distillation can be a powerful tool for reducing the size of large models without compromising their performance. In particular, we uncover that there are certain implicit design choices, which may drastically affect the effectiveness of distillation. Our key contribution is the explicit identification of these design choices, which were not previously articulated in the literature. We back up our findings by a comprehensive empirical study, demonstrate compelling results on a wide range of vision datasets and, in particular, obtain a state-of-the-art ResNet-50 model for ImageNet, which achieves 82.8% top-1 accuracy.\n\n10. Title: Explaining knowledge distillation by quantifying the knowledge\n   Abstract: Explaining Knowledge Distillation by Quantifying the Knowledge: This paper presents a method to interpret the success of knowledge distillation by quantifying and analyzing task-relevant and task-irrelevant visual concepts that are encoded in intermediate layers of a deep neural network (DNN). More specifically, three hypotheses are proposed as follows. 1. Knowledge distillation makes the DNN learn more visual concepts than learning from raw data. 2. Knowledge distillation ensures that the DNN is prone to learning various visual concepts simultaneously. Whereas, in the scenario of learning from raw data, the DNN learns visual concepts sequentially. 3. Knowledge distillation yields more stable optimization directions than learning from raw data. Accordingly, we design three types of mathematical metrics to evaluate feature representations of the DNN. In experiments, we diagnosed various DNNs, and above hypotheses were verified.\n\nThink step-by-step about which candidate best matches the context. Summarize your reasoning, then provide the exact title that should fill the citation. Respond in JSON with fields:\n{\n  \"reasoning\": \"<your step-by-step analysis>\",\n  \"selected_title\": \"<exact candidate title>\"\n}", "response": {"reasoning": "The context criticizes a prior work for proposing an algorithm that \"improves accuracy\" but \"do not quantify when their proposed algorithm improves upon the standard approach of using just hard labels,\" then contrasts that prior work (and another) by noting both previous results are shown with the squared loss while the current paper also provides cross-entropy results. From the candidate list, the papers that present theoretical results in an â„“2 / squared-loss style are the NTK/overparameterized analyses. Candidate 1 explicitly proves convergence in â„“_2 distance and frames distillation as related to early stopping (i.e., theoretical results in an â„“2/squared-loss setting). Candidate 3 also has NTK-style analysis, but its abstract indicates it explicitly studies when soft labels help and when mixing hard labels is beneficial (i.e., it does quantify when distillation helps). Therefore the remark that the prior work \"does not quantify when... improves upon... hard labels\" best matches Candidate 1, which focuses on an â„“2/squared-loss theoretical treatment of self-distillation without the kind of quantification the context describes.", "selected_title": "Distillation $\\approx$ Early Stopping? Harvesting Dark Knowledge Utilizing Anisotropic Information Retrieval For Overparameterized Neural Network"}, "selected_title": "Distillation $\\approx$ Early Stopping? Harvesting Dark Knowledge Utilizing Anisotropic Information Retrieval For Overparameterized Neural Network", "positive_title": "Distillation $\\approx$ Early Stopping? Harvesting Dark Knowledge Utilizing Anisotropic Information Retrieval For Overparameterized Neural Network", "is_correct": true, "reasoning": "The context criticizes a prior work for proposing an algorithm that \"improves accuracy\" but \"do not quantify when their proposed algorithm improves upon the standard approach of using just hard labels,\" then contrasts that prior work (and another) by noting both previous results are shown with the squared loss while the current paper also provides cross-entropy results. From the candidate list, the papers that present theoretical results in an â„“2 / squared-loss style are the NTK/overparameterized analyses. Candidate 1 explicitly proves convergence in â„“_2 distance and frames distillation as related to early stopping (i.e., theoretical results in an â„“2/squared-loss setting). Candidate 3 also has NTK-style analysis, but its abstract indicates it explicitly studies when soft labels help and when mixing hard labels is beneficial (i.e., it does quantify when distillation helps). Therefore the remark that the prior work \"does not quantify when... improves upon... hard labels\" best matches Candidate 1, which focuses on an â„“2/squared-loss theoretical treatment of self-distillation without the kind of quantification the context describes."}
{"example_id": "blind_2063", "prompt": "You are an expert citation retrieval system as described in the paper \"Multi-Agent System for Reliable Citation Retrieval\".\nYour goal is to autonomously retrieve, verify, and recommend academic references given a query or document excerpt.\n\nTask:\nGiven a citation context from a scientific paper (where a citation is missing), identify the correct paper from a list of candidates.\nAnalyze the context to understand the specific claim, method, or result being cited.\nThen, evaluate each candidate paper to see if it matches the context.\nFinally, select the best matching paper.\n\nCitation Context:\nnisms initially emerged in computer vision, with pivotal work in demonstrating how models could focus on relevant parts of the input data. The adoption of attention in NLP by Bahdanau et al.[CITATION] marked a turning point, offering a new paradigm for sequence-to-sequence tasks. The Transformer model, introduced in the seminal paper by Vaswani et al., has become a cornerstone in modern\n\nCandidate Papers:\n1. Title: Pix2Face: Direct 3D Face Model Estimation\n   Abstract: Pix2face: Direct 3D Face Model Estimation: An efficient, fully automatic method for 3D face shape and pose estimation in unconstrained 2D imagery is presented. The proposed method jointly estimates a dense set of 3D landmarks and facial geometry using a single pass of a modified version of the popular \"U-Net\" neural network architecture. Additionally, we propose a method for directly estimating a set of 3D Morphable Model (3DMM) parameters, using the estimated 3D landmarks and geometry as constraints in a simple linear system. Qualitative modeling results are presented, as well as quantitative evaluation of predicted 3D face landmarks in unconstrained video sequences.\n\n2. Title: 3D Human Pose Estimation with Spatial and Temporal Transformers\n   Abstract: 3D Human Pose Estimation with Spatial and Temporal Transformers: Transformer architectures have become the model of choice in natural language processing and are now being introduced into computer vision tasks such as image classification, object detection, and semantic segmentation. However, in the field of human pose estimation, convolutional architectures still remain dominant. In this work, we present PoseFormer, a purely transformer-based approach for 3D human pose estimation in videos without convolutional architectures involved. Inspired by recent developments in vision transformers, we design a spatial-temporal transformer structure to comprehensively model the human joint relations within each frame as well as the temporal correlations across frames, then output an accurate 3D human pose of the center frame. We quantitatively and qualitatively evaluate our method on two popular and standard benchmark datasets: Human3.6M and MPI-INF-3DHP. Extensive experiments show that PoseFormer achieves state-of-the-art performance on both datasets. Code is available at \\url{https://github.com/zczcwh/PoseFormer}\n\n3. Title: Graph R-CNN for Scene Graph Generation\n   Abstract: Graph R-CNN for Scene Graph Generation: We propose a novel scene graph generation model called Graph R-CNN, that is both effective and efficient at detecting objects and their relations in images. Our model contains a Relation Proposal Network (RePN) that efficiently deals with the quadratic number of potential relations between objects in an image. We also propose an attentional Graph Convolutional Network (aGCN) that effectively captures contextual information between objects and relations. Finally, we introduce a new evaluation metric that is more holistic and realistic than existing metrics. We report state-of-the-art performance on scene graph generation as evaluated using both existing and our proposed metrics.\n\n4. Title: Jointformer: Single-Frame Lifting Transformer with Error Prediction and Refinement for 3D Human Pose Estimation\n   Abstract: Jointformer: Single-Frame Lifting Transformer with Error Prediction and Refinement for 3D Human Pose Estimation: Monocular 3D human pose estimation technologies have the potential to greatly increase the availability of human movement data. The best-performing models for single-image 2D-3D lifting use graph convolutional networks (GCNs) that typically require some manual input to define the relationships between different body joints. We propose a novel transformer-based approach that uses the more generalised self-attention mechanism to learn these relationships within a sequence of tokens representing joints. We find that the use of intermediate supervision, as well as residual connections between the stacked encoders benefits performance. We also suggest that using error prediction as part of a multi-task learning framework improves performance by allowing the network to compensate for its confidence level. We perform extensive ablation studies to show that each of our contributions increases performance. Furthermore, we show that our approach outperforms the recent state of the art for single-frame 3D human pose estimation by a large margin. Our code and trained models are made publicly available on Github.\n\n5. Title: Graph Stacked Hourglass Networks for 3D Human Pose Estimation\n   Abstract: Graph Stacked Hourglass Networks for 3D Human Pose Estimation: In this paper, we propose a novel graph convolutional network architecture, Graph Stacked Hourglass Networks, for 2D-to-3D human pose estimation tasks. The proposed architecture consists of repeated encoder-decoder, in which graph-structured features are processed across three different scales of human skeletal representations. This multi-scale architecture enables the model to learn both local and global feature representations, which are critical for 3D human pose estimation. We also introduce a multi-level feature learning approach using different-depth intermediate features and show the performance improvements that result from exploiting multi-scale, multi-level feature representations. Extensive experiments are conducted to validate our approach, and the results show that our model outperforms the state-of-the-art.\n\n6. Title: Spatial Temporal Graph Convolutional Networks for Skeleton-Based Action Recognition\n   Abstract: Spatial Temporal Graph Convolutional Networks for Skeleton-Based Action Recognition: Dynamics of human body skeletons convey significant information for human action recognition. Conventional approaches for modeling skeletons usually rely on hand-crafted parts or traversal rules, thus resulting in limited expressive power and difficulties of generalization. In this work, we propose a novel model of dynamic skeletons called Spatial-Temporal Graph Convolutional Networks (ST-GCN), which moves beyond the limitations of previous methods by automatically learning both the spatial and temporal patterns from data. This formulation not only leads to greater expressive power but also stronger generalization capability. On two large datasets, Kinetics and NTU-RGBD, it achieves substantial improvements over mainstream methods.\n\n7. Title: Neural machine translation by jointly learning to align and translate\n   Abstract: Neural Machine Translation by Jointly Learning to Align and Translate: Neural machine translation is a recently proposed approach to machine translation. Unlike the traditional statistical machine translation, the neural machine translation aims at building a single neural network that can be jointly tuned to maximize the translation performance. The models proposed recently for neural machine translation often belong to a family of encoder-decoders and consists of an encoder that encodes a source sentence into a fixed-length vector from which a decoder generates a translation. In this paper, we conjecture that the use of a fixed-length vector is a bottleneck in improving the performance of this basic encoder-decoder architecture, and propose to extend this by allowing a model to automatically (soft-)search for parts of a source sentence that are relevant to predicting a target word, without having to form these parts as a hard segment explicitly. With this new approach, we achieve a translation performance comparable to the existing state-of-the-art phrase-based system on the task of English-to-French translation. Furthermore, qualitative analysis reveals that the (soft-)alignments found by the model agree well with our intuition.\n\n8. Title: Attention is All you Need\n   Abstract: Attention Is All You Need: The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.\n\n9. Title: Microsoft COCO Captions: Data Collection and Evaluation Server\n   Abstract: Microsoft COCO Captions: Data Collection and Evaluation Server: In this paper we describe the Microsoft COCO Caption dataset and evaluation server. When completed, the dataset will contain over one and a half million captions describing over 330,000 images. For the training and validation images, five independent human generated captions will be provided. To ensure consistency in evaluation of automatic caption generation algorithms, an evaluation server is used. The evaluation server receives candidate captions and scores them using several popular metrics, including BLEU, METEOR, ROUGE and CIDEr. Instructions for using the evaluation server are provided.\n\n10. Title: Single-Shot Multi-person 3D Pose Estimation from Monocular RGB\n   Abstract: Single-Shot Multi-Person 3D Pose Estimation From Monocular RGB: We propose a new single-shot method for multi-person 3D pose estimation in general scenes from a monocular RGB camera. Our approach uses novel occlusion-robust pose-maps (ORPM) which enable full body pose inference even under strong partial occlusions by other people and objects in the scene. ORPM outputs a fixed number of maps which encode the 3D joint locations of all people in the scene. Body part associations allow us to infer 3D pose for an arbitrary number of people without explicit bounding box prediction. To train our approach we introduce MuCo-3DHP, the first large scale training data set showing real images of sophisticated multi-person interactions and occlusions. We synthesize a large corpus of multi-person images by compositing images of individual people (with ground truth from mutli-view performance capture). We evaluate our method on our new challenging 3D annotated multi-person test set MuPoTs-3D where we achieve state-of-the-art performance. To further stimulate research in multi-person 3D pose estimation, we will make our new datasets, and associated code publicly available for research purposes.\n\nThink step-by-step about which candidate best matches the context. Summarize your reasoning, then provide the exact title that should fill the citation. Respond in JSON with fields:\n{\n  \"reasoning\": \"<your step-by-step analysis>\",\n  \"selected_title\": \"<exact candidate title>\"\n}", "response": {"reasoning": "Context: The sentence refers to the adoption of attention in NLP by Bahdanau et al., which introduced an attention mechanism for sequence-to-sequence (neural machine translation) models and showed soft alignments that improved NMT performance. Matching candidates: I reviewed each candidate's title and abstract. Candidate 7, \"Neural machine translation by jointly learning to align and translate,\" is the Bahdanau et al. paper that explicitly proposes adding a (soft) attention mechanism to encoder-decoder NMT to replace a fixed-length vectorâ€”exactly the work described in the citation context. Candidate 8 is the Vaswani et al. Transformer paper, which is mentioned elsewhere in the sentence and is a different citation. All other candidates are unrelated to the original Bahdanau attention-for-NMT contribution. Conclusion: Candidate 7 is the correct match for the Bahdanau et al. citation.", "selected_title": "Neural machine translation by jointly learning to align and translate"}, "selected_title": "Neural machine translation by jointly learning to align and translate", "positive_title": "Neural machine translation by jointly learning to align and translate", "is_correct": true, "reasoning": "Context: The sentence refers to the adoption of attention in NLP by Bahdanau et al., which introduced an attention mechanism for sequence-to-sequence (neural machine translation) models and showed soft alignments that improved NMT performance. Matching candidates: I reviewed each candidate's title and abstract. Candidate 7, \"Neural machine translation by jointly learning to align and translate,\" is the Bahdanau et al. paper that explicitly proposes adding a (soft) attention mechanism to encoder-decoder NMT to replace a fixed-length vectorâ€”exactly the work described in the citation context. Candidate 8 is the Vaswani et al. Transformer paper, which is mentioned elsewhere in the sentence and is a different citation. All other candidates are unrelated to the original Bahdanau attention-for-NMT contribution. Conclusion: Candidate 7 is the correct match for the Bahdanau et al. citation."}
{"example_id": "blind_16939", "prompt": "You are an expert citation retrieval system as described in the paper \"Multi-Agent System for Reliable Citation Retrieval\".\nYour goal is to autonomously retrieve, verify, and recommend academic references given a query or document excerpt.\n\nTask:\nGiven a citation context from a scientific paper (where a citation is missing), identify the correct paper from a list of candidates.\nAnalyze the context to understand the specific claim, method, or result being cited.\nThen, evaluate each candidate paper to see if it matches the context.\nFinally, select the best matching paper.\n\nCitation Context:\nsynthesis, image inpainting, and view synthesis does not fully address these requirements due to various limitations. Recent text-to-video generation methods leverage advancements in diffusion models[CITATION] to extend their application to the temporal domain. These approaches show promise, but often require extensive training on large datasets, a costly and resource-intensive endeavor. T2V-0 pi\n\nCandidate Papers:\n1. Title: Zero-shot text-guided object generation with dream fields\n   Abstract: Zero-Shot Text-Guided Object Generation with Dream Fields: We combine neural rendering with multi-modal image and text representations to synthesize diverse 3D objects solely from natural language descriptions. Our method, Dream Fields, can generate the geometry and color of a wide range of objects without 3D supervision. Due to the scarcity of diverse, captioned 3D data, prior methods only generate objects from a handful of categories, such as ShapeNet. Instead, we guide generation with image-text models pre-trained on large datasets of captioned images from the web. Our method optimizes a Neural Radiance Field from many camera views so that rendered images score highly with a target caption according to a pre-trained CLIP model. To improve fidelity and visual quality, we introduce simple geometric priors, including sparsity-inducing transmittance regularization, scene bounds, and new MLP architectures. In experiments, Dream Fields produce realistic, multi-view consistent object geometry and color from a variety of natural language captions.\n\n2. Title: Text2room: Extracting textured 3d meshes from 2d text-to-image models\n   Abstract: Text2Room: Extracting Textured 3D Meshes from 2D Text-to-Image Models: We present Text2Room, a method for generating room-scale textured 3D meshes from a given text prompt as input. To this end, we leverage pre-trained 2D text-to-image models to synthesize a sequence of images from different poses. In order to lift these outputs into a consistent 3D scene representation, we combine monocular depth estimation with a text-conditioned inpainting model. The core idea of our approach is a tailored viewpoint selection such that the content of each image can be fused into a seamless, textured 3D mesh. More specifically, we propose a continuous alignment strategy that iteratively fuses scene frames with the existing geometry to create a seamless mesh. Unlike existing works that focus on generating single objects or zoom-out trajectories from text, our method generates complete 3D scenes with multiple objects and explicit 3D geometry. We evaluate our approach using qualitative and quantitative metrics, demonstrating it as the first method to generate room-scale 3D geometry with compelling textures from only text as input.\n\n3. Title: Score jacobian chaining: Lifting pretrained 2d diffusion models for 3d generation\n   Abstract: Score Jacobian Chaining: Lifting Pretrained 2D Diffusion Models for 3D Generation: A diffusion model learns to predict a vector field of gradients. We propose to apply chain rule on the learned gradients, and back-propagate the score of a diffusion model through the Jacobian of a differentiable renderer, which we instantiate to be a voxel radiance field. This setup aggregates 2D scores at multiple camera viewpoints into a 3D score, and repurposes a pretrained 2D model for 3D data generation. We identify a technical challenge of distribution mismatch that arises in this application, and propose a novel estimation mechanism to resolve it. We run our algorithm on several off-the-shelf diffusion image generative models, including the recently released Stable Diffusion trained on the large-scale LAION dataset.\n\n4. Title: Align your latents: High-resolution video synthesis with latent diffusion models\n   Abstract: Align your Latents: High-Resolution Video Synthesis with Latent Diffusion Models: Latent Diffusion Models (LDMs) enable high-quality image synthesis while avoiding excessive compute demands by training a diffusion model in a compressed lower-dimensional latent space. Here, we apply the LDM paradigm to high-resolution video generation, a particularly resource-intensive task. We first pre-train an LDM on images only; then, we turn the image generator into a video generator by introducing a temporal dimension to the latent space diffusion model and fine-tuning on encoded image sequences, i.e., videos. Similarly, we temporally align diffusion model upsamplers, turning them into temporally consistent video super resolution models. We focus on two relevant real-world applications: Simulation of in-the-wild driving data and creative content creation with text-to-video modeling. In particular, we validate our Video LDM on real driving videos of resolution 512 x 1024, achieving state-of-the-art performance. Furthermore, our approach can easily leverage off-the-shelf pre-trained image LDMs, as we only need to train a temporal alignment model in that case. Doing so, we turn the publicly available, state-of-the-art text-to-image LDM Stable Diffusion into an efficient and expressive text-to-video model with resolution up to 1280 x 2048. We show that the temporal layers trained in this way generalize to different fine-tuned text-to-image LDMs. Utilizing this property, we show the first results for personalized text-to-video generation, opening exciting directions for future content creation. Project page: https://research.nvidia.com/labs/toronto-ai/VideoLDM/\n\n5. Title: Clip-mesh: Generating textured meshes from text using pretrained image-text models\n   Abstract: CLIP-Mesh: Generating textured meshes from text using pretrained image-text models: We present a technique for zero-shot generation of a 3D model using only a target text prompt. Without any 3D supervision our method deforms the control shape of a limit subdivided surface along with its texture map and normal map to obtain a 3D asset that corresponds to the input text prompt and can be easily deployed into games or modeling applications. We rely only on a pre-trained CLIP model that compares the input text prompt with differentiably rendered images of our 3D model. While previous works have focused on stylization or required training of generative models we perform optimization on mesh parameters directly to generate shape, texture or both. To constrain the optimization to produce plausible meshes and textures we introduce a number of techniques using image augmentations and the use of a pretrained prior that generates CLIP image embeddings given a text embedding.\n\n6. Title: ProlificDreamer: High-Fidelity and Diverse Text-to-3D Generation with Variational Score Distillation\n   Abstract: ProlificDreamer: High-Fidelity and Diverse Text-to-3D Generation with Variational Score Distillation: Score distillation sampling (SDS) has shown great promise in text-to-3D generation by distilling pretrained large-scale text-to-image diffusion models, but suffers from over-saturation, over-smoothing, and low-diversity problems. In this work, we propose to model the 3D parameter as a random variable instead of a constant as in SDS and present variational score distillation (VSD), a principled particle-based variational framework to explain and address the aforementioned issues in text-to-3D generation. We show that SDS is a special case of VSD and leads to poor samples with both small and large CFG weights. In comparison, VSD works well with various CFG weights as ancestral sampling from diffusion models and simultaneously improves the diversity and sample quality with a common CFG weight (i.e., $7.5$). We further present various improvements in the design space for text-to-3D such as distillation time schedule and density initialization, which are orthogonal to the distillation algorithm yet not well explored. Our overall approach, dubbed ProlificDreamer, can generate high rendering resolution (i.e., $512\\times512$) and high-fidelity NeRF with rich structure and complex effects (e.g., smoke and drops). Further, initialized from NeRF, meshes fine-tuned by VSD are meticulously detailed and photo-realistic. Project page and codes: https://ml.cs.tsinghua.edu.cn/prolificdreamer/\n\n7. Title: Dreamfusion: Text-to-3d using 2d diffusion\n   Abstract: DreamFusion: Text-to-3D using 2D Diffusion: Recent breakthroughs in text-to-image synthesis have been driven by diffusion models trained on billions of image-text pairs. Adapting this approach to 3D synthesis would require large-scale datasets of labeled 3D data and efficient architectures for denoising 3D data, neither of which currently exist. In this work, we circumvent these limitations by using a pretrained 2D text-to-image diffusion model to perform text-to-3D synthesis. We introduce a loss based on probability density distillation that enables the use of a 2D diffusion model as a prior for optimization of a parametric image generator. Using this loss in a DeepDream-like procedure, we optimize a randomly-initialized 3D model (a Neural Radiance Field, or NeRF) via gradient descent such that its 2D renderings from random angles achieve a low loss. The resulting 3D model of the given text can be viewed from any angle, relit by arbitrary illumination, or composited into any 3D environment. Our approach requires no 3D training data and no modifications to the image diffusion model, demonstrating the effectiveness of pretrained image diffusion models as priors.\n\n8. Title: Zero-1-to-3: Zero-shot one image to 3d object\n   Abstract: Zero-1-to-3: Zero-shot One Image to 3D Object: We introduce Zero-1-to-3, a framework for changing the camera viewpoint of an object given just a single RGB image. To perform novel view synthesis in this under-constrained setting, we capitalize on the geometric priors that large-scale diffusion models learn about natural images. Our conditional diffusion model uses a synthetic dataset to learn controls of the relative camera viewpoint, which allow new images to be generated of the same object under a specified camera transformation. Even though it is trained on a synthetic dataset, our model retains a strong zero-shot generalization ability to out-of-distribution datasets as well as in-the-wild images, including impressionist paintings. Our viewpoint-conditioned diffusion approach can further be used for the task of 3D reconstruction from a single image. Qualitative and quantitative experiments show that our method significantly outperforms state-of-the-art single-view 3D reconstruction and novel view synthesis models by leveraging Internet-scale pre-training.\n\n9. Title: Cogview2: Faster and better text-to-image generation via hierarchical transformers\n   Abstract: CogView2: Faster and Better Text-to-Image Generation via Hierarchical Transformers: The development of the transformer-based text-to-image models are impeded by its slow generation and complexity for high-resolution images. In this work, we put forward a solution based on hierarchical transformers and local parallel auto-regressive generation. We pretrain a 6B-parameter transformer with a simple and flexible self-supervised task, Cross-modal general language model (CogLM), and finetune it for fast super-resolution. The new text-to-image system, CogView2, shows very competitive generation compared to concurrent state-of-the-art DALL-E-2, and naturally supports interactive text-guided editing on images.\n\n10. Title: Realfusion: 360deg reconstruction of any object from a single image\n   Abstract: RealFusion: 360\\deg Reconstruction of Any Object from a Single Image: We consider the problem of reconstructing a full 360{\\deg} photographic model of an object from a single image of it. We do so by fitting a neural radiance field to the image, but find this problem to be severely ill-posed. We thus take an off-the-self conditional image generator based on diffusion and engineer a prompt that encourages it to \"dream up\" novel views of the object. Using an approach inspired by DreamFields and DreamFusion, we fuse the given input view, the conditional prior, and other regularizers in a final, consistent reconstruction. We demonstrate state-of-the-art reconstruction results on benchmark images when compared to prior methods for monocular 3D reconstruction of objects. Qualitatively, our reconstructions provide a faithful match of the input view and a plausible extrapolation of its appearance and 3D shape, including to the side of the object not visible in the image.\n\nThink step-by-step about which candidate best matches the context. Summarize your reasoning, then provide the exact title that should fill the citation. Respond in JSON with fields:\n{\n  \"reasoning\": \"<your step-by-step analysis>\",\n  \"selected_title\": \"<exact candidate title>\"\n}", "response": {"reasoning": "1) Identify the cited claim: the context states that recent text-to-video methods leverage diffusion models to extend to the temporal domain but often require extensive training on large datasets (i.e., video generation using diffusion models). 2) Check candidates for relevance to text-to-video / temporal extension of diffusion models: most candidates focus on text-to-3D, text-to-image, or single-image view synthesis (e.g., DreamFusion, ProlificDreamer, Zero-1-to-3, RealFusion). These do not primarily address text-to-video. 3) Candidate 4 (Align your Latents: High-Resolution Video Synthesis with Latent Diffusion Models) explicitly addresses turning pretrained image latent diffusion models into video generators by adding a temporal dimension and fine-tuning; it discusses text-to-video modeling, leveraging pretrained image LDMs (e.g., Stable Diffusion), and concerns about training/compute for videoâ€”matching both the technical content and the critique about costly/resource-intensive training. 4) Therefore candidate 4 best matches the context and should be cited.", "selected_title": "Align your latents: High-resolution video synthesis with latent diffusion models"}, "selected_title": "Align your latents: High-resolution video synthesis with latent diffusion models", "positive_title": "Align your latents: High-resolution video synthesis with latent diffusion models", "is_correct": true, "reasoning": "1) Identify the cited claim: the context states that recent text-to-video methods leverage diffusion models to extend to the temporal domain but often require extensive training on large datasets (i.e., video generation using diffusion models). 2) Check candidates for relevance to text-to-video / temporal extension of diffusion models: most candidates focus on text-to-3D, text-to-image, or single-image view synthesis (e.g., DreamFusion, ProlificDreamer, Zero-1-to-3, RealFusion). These do not primarily address text-to-video. 3) Candidate 4 (Align your Latents: High-Resolution Video Synthesis with Latent Diffusion Models) explicitly addresses turning pretrained image latent diffusion models into video generators by adding a temporal dimension and fine-tuning; it discusses text-to-video modeling, leveraging pretrained image LDMs (e.g., Stable Diffusion), and concerns about training/compute for videoâ€”matching both the technical content and the critique about costly/resource-intensive training. 4) Therefore candidate 4 best matches the context and should be cited."}
{"example_id": "blind_17341", "prompt": "You are an expert citation retrieval system as described in the paper \"Multi-Agent System for Reliable Citation Retrieval\".\nYour goal is to autonomously retrieve, verify, and recommend academic references given a query or document excerpt.\n\nTask:\nGiven a citation context from a scientific paper (where a citation is missing), identify the correct paper from a list of candidates.\nAnalyze the context to understand the specific claim, method, or result being cited.\nThen, evaluate each candidate paper to see if it matches the context.\nFinally, select the best matching paper.\n\nCitation Context:\nnce is less satisfactory. Feature-disentangled control was incorporated in using CLIP to demonstrate interpolatable latents. With the prior of pretrained text-image diffusion models such as[CITATION], DreamFusion and Magic-3D can generate 3D scenes from text prompts while using CLIP to finetune NeRF or Mesh. This approach however makes speed a severe issue, since multiple ba\n\nCandidate Papers:\n1. Title: CLIP-Forge: Towards Zero-Shot Text-to-Shape Generation\n   Abstract: CLIP-Forge: Towards Zero-Shot Text-to-Shape Generation: Generating shapes using natural language can enable new ways of imagining and creating the things around us. While significant recent progress has been made in text-to-image generation, text-to-shape generation remains a challenging problem due to the unavailability of paired text and shape data at a large scale. We present a simple yet effective method for zero-shot text-to-shape generation that circumvents such data scarcity. Our proposed method, named CLIP-Forge, is based on a two-stage training process, which only depends on an unlabelled shape dataset and a pre-trained image-text network such as CLIP. Our method has the benefits of avoiding expensive inference time optimization, as well as the ability to generate multiple shapes for a given text. We not only demonstrate promising zero-shot generalization of the CLIP-Forge model qualitatively and quantitatively, but also provide extensive comparative evaluations to better understand its behavior.\n\n2. Title: ProlificDreamer: High-Fidelity and Diverse Text-to-3D Generation with Variational Score Distillation\n   Abstract: ProlificDreamer: High-Fidelity and Diverse Text-to-3D Generation with Variational Score Distillation: Score distillation sampling (SDS) has shown great promise in text-to-3D generation by distilling pretrained large-scale text-to-image diffusion models, but suffers from over-saturation, over-smoothing, and low-diversity problems. In this work, we propose to model the 3D parameter as a random variable instead of a constant as in SDS and present variational score distillation (VSD), a principled particle-based variational framework to explain and address the aforementioned issues in text-to-3D generation. We show that SDS is a special case of VSD and leads to poor samples with both small and large CFG weights. In comparison, VSD works well with various CFG weights as ancestral sampling from diffusion models and simultaneously improves the diversity and sample quality with a common CFG weight (i.e., $7.5$). We further present various improvements in the design space for text-to-3D such as distillation time schedule and density initialization, which are orthogonal to the distillation algorithm yet not well explored. Our overall approach, dubbed ProlificDreamer, can generate high rendering resolution (i.e., $512\\times512$) and high-fidelity NeRF with rich structure and complex effects (e.g., smoke and drops). Further, initialized from NeRF, meshes fine-tuned by VSD are meticulously detailed and photo-realistic. Project page and codes: https://ml.cs.tsinghua.edu.cn/prolificdreamer/\n\n3. Title: Text2mesh: Text-driven neural stylization for meshes\n   Abstract: Text2Mesh: Text-Driven Neural Stylization for Meshes: In this work, we develop intuitive controls for editing the style of 3D objects. Our framework, Text2Mesh, stylizes a 3D mesh by predicting color and local geometric details which conform to a target text prompt. We consider a disentangled representation of a 3D object using a fixed mesh input (content) coupled with a learned neural network, which we term neural style field network. In order to modify style, we obtain a similarity score between a text prompt (describing style) and a stylized mesh by harnessing the representational power of CLIP. Text2Mesh requires neither a pre-trained generative model nor a specialized 3D mesh dataset. It can handle low-quality meshes (non-manifold, boundaries, etc.) with arbitrary genus, and does not require UV parameterization. We demonstrate the ability of our technique to synthesize a myriad of styles over a wide variety of 3D meshes.\n\n4. Title: Instant neural graphics primitives with a multiresolution hash encoding\n   Abstract: Instant Neural Graphics Primitives with a Multiresolution Hash Encoding: Neural graphics primitives, parameterized by fully connected neural networks, can be costly to train and evaluate. We reduce this cost with a versatile new input encoding that permits the use of a smaller network without sacrificing quality, thus significantly reducing the number of floating point and memory access operations: a small neural network is augmented by a multiresolution hash table of trainable feature vectors whose values are optimized through stochastic gradient descent. The multiresolution structure allows the network to disambiguate hash collisions, making for a simple architecture that is trivial to parallelize on modern GPUs. We leverage this parallelism by implementing the whole system using fully-fused CUDA kernels with a focus on minimizing wasted bandwidth and compute operations. We achieve a combined speedup of several orders of magnitude, enabling training of high-quality neural graphics primitives in a matter of seconds, and rendering in tens of milliseconds at a resolution of ${1920\\!\\times\\!1080}$.\n\n5. Title: Bungeenerf: Progressive neural radiance field for extreme multi-scale scene rendering\n   Abstract: BungeeNeRF: Progressive Neural Radiance Field for Extreme Multi-scale Scene Rendering: Neural radiance fields (NeRF) has achieved outstanding performance in modeling 3D objects and controlled scenes, usually under a single scale. In this work, we focus on multi-scale cases where large changes in imagery are observed at drastically different scales. This scenario vastly exists in real-world 3D environments, such as city scenes, with views ranging from satellite level that captures the overview of a city, to ground level imagery showing complex details of an architecture; and can also be commonly identified in landscape and delicate minecraft 3D models. The wide span of viewing positions within these scenes yields multi-scale renderings with very different levels of detail, which poses great challenges to neural radiance field and biases it towards compromised results. To address these issues, we introduce BungeeNeRF, a progressive neural radiance field that achieves level-of-detail rendering across drastically varied scales. Starting from fitting distant views with a shallow base block, as training progresses, new blocks are appended to accommodate the emerging details in the increasingly closer views. The strategy progressively activates high-frequency channels in NeRF's positional encoding inputs and successively unfolds more complex details as the training proceeds. We demonstrate the superiority of BungeeNeRF in modeling diverse multi-scale scenes with drastically varying views on multiple data sources (city models, synthetic, and drone captured data) and its support for high-quality rendering in different levels of detail.\n\n6. Title: Mvdream: Multi-view diffusion for 3d generation\n   Abstract: MVDream: Multi-view Diffusion for 3D Generation: We introduce MVDream, a diffusion model that is able to generate consistent multi-view images from a given text prompt. Learning from both 2D and 3D data, a multi-view diffusion model can achieve the generalizability of 2D diffusion models and the consistency of 3D renderings. We demonstrate that such a multi-view diffusion model is implicitly a generalizable 3D prior agnostic to 3D representations. It can be applied to 3D generation via Score Distillation Sampling, significantly enhancing the consistency and stability of existing 2D-lifting methods. It can also learn new concepts from a few 2D examples, akin to DreamBooth, but for 3D generation.\n\n7. Title: Dreamfusion: Text-to-3d using 2d diffusion\n   Abstract: DreamFusion: Text-to-3D using 2D Diffusion: Recent breakthroughs in text-to-image synthesis have been driven by diffusion models trained on billions of image-text pairs. Adapting this approach to 3D synthesis would require large-scale datasets of labeled 3D data and efficient architectures for denoising 3D data, neither of which currently exist. In this work, we circumvent these limitations by using a pretrained 2D text-to-image diffusion model to perform text-to-3D synthesis. We introduce a loss based on probability density distillation that enables the use of a 2D diffusion model as a prior for optimization of a parametric image generator. Using this loss in a DeepDream-like procedure, we optimize a randomly-initialized 3D model (a Neural Radiance Field, or NeRF) via gradient descent such that its 2D renderings from random angles achieve a low loss. The resulting 3D model of the given text can be viewed from any angle, relit by arbitrary illumination, or composited into any 3D environment. Our approach requires no 3D training data and no modifications to the image diffusion model, demonstrating the effectiveness of pretrained image diffusion models as priors.\n\n8. Title: High-Resolution Image Synthesis with Latent Diffusion Models\n   Abstract: High-Resolution Image Synthesis with Latent Diffusion Models: By decomposing the image formation process into a sequential application of denoising autoencoders, diffusion models (DMs) achieve state-of-the-art synthesis results on image data and beyond. Additionally, their formulation allows for a guiding mechanism to control the image generation process without retraining. However, since these models typically operate directly in pixel space, optimization of powerful DMs often consumes hundreds of GPU days and inference is expensive due to sequential evaluations. To enable DM training on limited computational resources while retaining their quality and flexibility, we apply them in the latent space of powerful pretrained autoencoders. In contrast to previous work, training diffusion models on such a representation allows for the first time to reach a near-optimal point between complexity reduction and detail preservation, greatly boosting visual fidelity. By introducing cross-attention layers into the model architecture, we turn diffusion models into powerful and flexible generators for general conditioning inputs such as text or bounding boxes and high-resolution synthesis becomes possible in a convolutional manner. Our latent diffusion models (LDMs) achieve a new state of the art for image inpainting and highly competitive performance on various tasks, including unconditional image generation, semantic scene synthesis, and super-resolution, while significantly reducing computational requirements compared to pixel-based DMs. Code is available at https://github.com/CompVis/latent-diffusion .\n\n9. Title: CLIP4Clip: An Empirical Study of CLIP for End to End Video Clip Retrieval\n   Abstract: CLIP4Clip: An Empirical Study of CLIP for End to End Video Clip Retrieval: Video-text retrieval plays an essential role in multi-modal research and has been widely used in many real-world web applications. The CLIP (Contrastive Language-Image Pre-training), an image-language pre-training model, has demonstrated the power of visual concepts learning from web collected image-text datasets. In this paper, we propose a CLIP4Clip model to transfer the knowledge of the CLIP model to video-language retrieval in an end-to-end manner. Several questions are investigated via empirical studies: 1) Whether image feature is enough for video-text retrieval? 2) How a post-pretraining on a large-scale video-text dataset based on the CLIP affect the performance? 3) What is the practical mechanism to model temporal dependency between video frames? And 4) The Hyper-parameters sensitivity of the model on video-text retrieval task. Extensive experimental results present that the CLIP4Clip model transferred from the CLIP can achieve SOTA results on various video-text retrieval datasets, including MSR-VTT, MSVC, LSMDC, ActivityNet, and DiDeMo. We release our code at https://github.com/ArrowLuo/CLIP4Clip.\n\nThink step-by-step about which candidate best matches the context. Summarize your reasoning, then provide the exact title that should fill the citation. Respond in JSON with fields:\n{\n  \"reasoning\": \"<your step-by-step analysis>\",\n  \"selected_title\": \"<exact candidate title>\"\n}", "response": {"reasoning": "The citation slot refers to a pretrained text-image diffusion model used as a prior for text-to-3D systems (the sentence: â€œWith the prior of pretrained text-image diffusion models such as [CITATION], DreamFusion and Magic-3D can generate 3D scenesâ€¦â€). I examine the candidates for which is a foundational pretrained text-to-image diffusion model paper:\n\n- Candidate 1 (CLIP-Forge): zero-shot text-to-shape using CLIP, not a text-image diffusion model.\n- Candidate 2 (ProlificDreamer): a text-to-3D method (VSD) building on image diffusion priors, but not the general pretrained text-image diffusion model the sentence cites.\n- Candidate 3 (Text2Mesh): CLIP-based mesh stylization, not a text-image diffusion model.\n- Candidate 4 (Instant NGP): efficient neural graphics encoding, unrelated to text-image diffusion.\n- Candidate 5 (BungeeNeRF): multi-scale NeRF, not a text-image diffusion model.\n- Candidate 6 (MVDream): multi-view diffusion for 3D generation â€” a diffusion-based 3D prior, but the sentence asks for a pretrained text-image diffusion model â€œsuch as â€¦â€ (i.e., a general image diffusion backbone), not a 3D-specific diffusion model.\n- Candidate 7 (DreamFusion): already named in the sentence as using such priors, so it cannot be the antecedent for â€œsuch as [CITATION].â€\n- Candidate 8 (High-Resolution Image Synthesis with Latent Diffusion Models): introduces Latent Diffusion Models (LDMs), a widely used pretrained text-conditioned image diffusion framework (the basis for systems like Stable Diffusion). This matches the description â€œpretrained text-image diffusion modelsâ€ that DreamFusion and Magic-3D leverage as priors.\n- Candidate 9 (CLIP4Clip): video retrieval using CLIP, not a diffusion model.\n\nTherefore the best match is Candidate 8, which is the Latent Diffusion Models paper describing pretrained text-image diffusion models used as priors for downstream 3D synthesis methods.", "selected_title": "High-Resolution Image Synthesis with Latent Diffusion Models"}, "selected_title": "High-Resolution Image Synthesis with Latent Diffusion Models", "positive_title": "High-Resolution Image Synthesis with Latent Diffusion Models", "is_correct": true, "reasoning": "The citation slot refers to a pretrained text-image diffusion model used as a prior for text-to-3D systems (the sentence: â€œWith the prior of pretrained text-image diffusion models such as [CITATION], DreamFusion and Magic-3D can generate 3D scenesâ€¦â€). I examine the candidates for which is a foundational pretrained text-to-image diffusion model paper:\n\n- Candidate 1 (CLIP-Forge): zero-shot text-to-shape using CLIP, not a text-image diffusion model.\n- Candidate 2 (ProlificDreamer): a text-to-3D method (VSD) building on image diffusion priors, but not the general pretrained text-image diffusion model the sentence cites.\n- Candidate 3 (Text2Mesh): CLIP-based mesh stylization, not a text-image diffusion model.\n- Candidate 4 (Instant NGP): efficient neural graphics encoding, unrelated to text-image diffusion.\n- Candidate 5 (BungeeNeRF): multi-scale NeRF, not a text-image diffusion model.\n- Candidate 6 (MVDream): multi-view diffusion for 3D generation â€” a diffusion-based 3D prior, but the sentence asks for a pretrained text-image diffusion model â€œsuch as â€¦â€ (i.e., a general image diffusion backbone), not a 3D-specific diffusion model.\n- Candidate 7 (DreamFusion): already named in the sentence as using such priors, so it cannot be the antecedent for â€œsuch as [CITATION].â€\n- Candidate 8 (High-Resolution Image Synthesis with Latent Diffusion Models): introduces Latent Diffusion Models (LDMs), a widely used pretrained text-conditioned image diffusion framework (the basis for systems like Stable Diffusion). This matches the description â€œpretrained text-image diffusion modelsâ€ that DreamFusion and Magic-3D leverage as priors.\n- Candidate 9 (CLIP4Clip): video retrieval using CLIP, not a diffusion model.\n\nTherefore the best match is Candidate 8, which is the Latent Diffusion Models paper describing pretrained text-image diffusion models used as priors for downstream 3D synthesis methods."}
{"example_id": "blind_4117", "prompt": "You are an expert citation retrieval system as described in the paper \"Multi-Agent System for Reliable Citation Retrieval\".\nYour goal is to autonomously retrieve, verify, and recommend academic references given a query or document excerpt.\n\nTask:\nGiven a citation context from a scientific paper (where a citation is missing), identify the correct paper from a list of candidates.\nAnalyze the context to understand the specific claim, method, or result being cited.\nThen, evaluate each candidate paper to see if it matches the context.\nFinally, select the best matching paper.\n\nCitation Context:\no 12K problems in the MATH dataset, with each step labeled as correct, incorrect or neutral. curated FELM, a factuality benchmark, including annotations of solutions to 208 GSM8K[CITATION] and 194 MATH problems. Compared to CHAMP, where annotations are made exclusively by the paper authors, both PRM800K and FELM are labeled via crowdsourcing. Moreover, solutions in PRM800K are selected\n\nCandidate Papers:\n1. Title: Cumulative Reasoning With Large Language Models\n   Abstract: Cumulative Reasoning with Large Language Models: Despite the recent advancements in language models (LMs), their ability to solve complex problems remains limited. This paper introduces Cumulative Reasoning (CR), a novel approach that utilizes LMs cumulatively and iteratively, mirroring human thought processes for problem-solving. CR decomposes tasks into smaller, manageable components and leverages previous propositions for effective composition, significantly enhancing problem-solving capabilities. We demonstrate CR's superiority through several complex reasoning tasks: it outperforms existing methods in logical inference tasks with up to a 9.3% improvement, achieving 98.04% accuracy on the curated FOLIO wiki dataset. In the Game of 24, it achieves 98% accuracy, marking a 24% improvement over the prior state-of-the-art. Additionally, CR sets new state-of-the-art on the MATH dataset, achieving a 4.2% increase from previous methods and a 43% relative improvement in the most challenging problems. By extending CR to incorporate a code environment without external aids like retrieval or web browsing, we further harness the computational and logical reasoning capabilities of LMs, achieving a remarkable 72.2% accuracy on the MATH dataset and outperforming the PAL/PoT method by 38.8%. Our work not only sets new state-of-the-art but also paves the way toward more sophisticated AI reasoning methods. The code is available at https://github.com/iiis-ai/cumulative-reasoning.\n\n2. Title: Solving challenging math word problems using gpt-4 code interpreter with code-based self-verification\n   Abstract: Solving Challenging Math Word Problems Using GPT-4 Code Interpreter with Code-based Self-Verification: Recent progress in large language models (LLMs) like GPT-4 and PaLM-2 has brought significant advancements in addressing math reasoning problems. In particular, OpenAI's latest version of GPT-4, known as GPT-4 Code Interpreter, shows remarkable performance on challenging math datasets. In this paper, we explore the effect of code on enhancing LLMs' reasoning capability by introducing different constraints on the \\textit{Code Usage Frequency} of GPT-4 Code Interpreter. We found that its success can be largely attributed to its powerful skills in generating and executing code, evaluating the output of code execution, and rectifying its solution when receiving unreasonable outputs. Based on this insight, we propose a novel and effective prompting method, explicit \\uline{c}ode-based \\uline{s}elf-\\uline{v}erification~(CSV), to further boost the mathematical reasoning potential of GPT-4 Code Interpreter. This method employs a zero-shot prompt on GPT-4 Code Interpreter to encourage it to use code to self-verify its answers. In instances where the verification state registers as ``False'', the model shall automatically amend its solution, analogous to our approach of rectifying errors during a mathematics examination. Furthermore, we recognize that the states of the verification result indicate the confidence of a solution, which can improve the effectiveness of majority voting. With GPT-4 Code Interpreter and CSV, we achieve an impressive zero-shot accuracy on MATH dataset \\textbf{(53.9\\% $\\to$ 84.3\\%)}.\n\n3. Title: GPT-4 Technical Report\n   Abstract: GPT-4 Technical Report: We report the development of GPT-4, a large-scale, multimodal model which can accept image and text inputs and produce text outputs. While less capable than humans in many real-world scenarios, GPT-4 exhibits human-level performance on various professional and academic benchmarks, including passing a simulated bar exam with a score around the top 10% of test takers. GPT-4 is a Transformer-based model pre-trained to predict the next token in a document. The post-training alignment process results in improved performance on measures of factuality and adherence to desired behavior. A core component of this project was developing infrastructure and optimization methods that behave predictably across a wide range of scales. This allowed us to accurately predict some aspects of GPT-4's performance based on models trained with no more than 1/1,000th the compute of GPT-4.\n\n4. Title: Let's Verify Step by Step\n   Abstract: Let's Verify Step by Step: In recent years, large language models have greatly improved in their ability to perform complex multi-step reasoning. However, even state-of-the-art models still regularly produce logical mistakes. To train more reliable models, we can turn either to outcome supervision, which provides feedback for a final result, or process supervision, which provides feedback for each intermediate reasoning step. Given the importance of training reliable models, and given the high cost of human feedback, it is important to carefully compare the both methods. Recent work has already begun this comparison, but many questions still remain. We conduct our own investigation, finding that process supervision significantly outperforms outcome supervision for training models to solve problems from the challenging MATH dataset. Our process-supervised model solves 78% of problems from a representative subset of the MATH test set. Additionally, we show that active learning significantly improves the efficacy of process supervision. To support related research, we also release PRM800K, the complete dataset of 800,000 step-level human feedback labels used to train our best reward model.\n\n5. Title: Felm: Benchmarking factuality evaluation of large language models\n   Abstract: FELM: Benchmarking Factuality Evaluation of Large Language Models: Assessing factuality of text generated by large language models (LLMs) is an emerging yet crucial research area, aimed at alerting users to potential errors and guiding the development of more reliable LLMs. Nonetheless, the evaluators assessing factuality necessitate suitable evaluation themselves to gauge progress and foster advancements. This direction remains under-explored, resulting in substantial impediments to the progress of factuality evaluators. To mitigate this issue, we introduce a benchmark for Factuality Evaluation of large Language Models, referred to as felm. In this benchmark, we collect responses generated from LLMs and annotate factuality labels in a fine-grained manner. Contrary to previous studies that primarily concentrate on the factuality of world knowledge (e.g.~information from Wikipedia), felm focuses on factuality across diverse domains, spanning from world knowledge to math and reasoning. Our annotation is based on text segments, which can help pinpoint specific factual errors. The factuality annotations are further supplemented by predefined error types and reference links that either support or contradict the statement. In our experiments, we investigate the performance of several LLM-based factuality evaluators on felm, including both vanilla LLMs and those augmented with retrieval mechanisms and chain-of-thought processes. Our findings reveal that while retrieval aids factuality evaluation, current LLMs are far from satisfactory to faithfully detect factual errors.\n\n6. Title: Pal: Program-aided language models\n   Abstract: PAL: Program-aided Language Models: Large language models (LLMs) have recently demonstrated an impressive ability to perform arithmetic and symbolic reasoning tasks, when provided with a few examples at test time (\"few-shot prompting\"). Much of this success can be attributed to prompting methods such as \"chain-of-thought'', which employ LLMs for both understanding the problem description by decomposing it into steps, as well as solving each step of the problem. While LLMs seem to be adept at this sort of step-by-step decomposition, LLMs often make logical and arithmetic mistakes in the solution part, even when the problem is decomposed correctly. In this paper, we present Program-Aided Language models (PAL): a novel approach that uses the LLM to read natural language problems and generate programs as the intermediate reasoning steps, but offloads the solution step to a runtime such as a Python interpreter. With PAL, decomposing the natural language problem into runnable steps remains the only learning task for the LLM, while solving is delegated to the interpreter. We demonstrate this synergy between a neural LLM and a symbolic interpreter across 13 mathematical, symbolic, and algorithmic reasoning tasks from BIG-Bench Hard and other benchmarks. In all these natural language reasoning tasks, generating code using an LLM and reasoning using a Python interpreter leads to more accurate results than much larger models. For example, PAL using Codex achieves state-of-the-art few-shot accuracy on the GSM8K benchmark of math word problems, surpassing PaLM-540B which uses chain-of-thought by absolute 15% top-1. Our code and data are publicly available at http://reasonwithpal.com/ .\n\n7. Title: Training verifiers to solve math word problems\n   Abstract: Training Verifiers to Solve Math Word Problems: State-of-the-art language models can match human performance on many tasks, but they still struggle to robustly perform multi-step mathematical reasoning. To diagnose the failures of current models and support research, we introduce GSM8K, a dataset of 8.5K high quality linguistically diverse grade school math word problems. We find that even the largest transformer models fail to achieve high test performance, despite the conceptual simplicity of this problem distribution. To increase performance, we propose training verifiers to judge the correctness of model completions. At test time, we generate many candidate solutions and select the one ranked highest by the verifier. We demonstrate that verification significantly improves performance on GSM8K, and we provide strong empirical evidence that verification scales more effectively with increased data than a finetuning baseline.\n\n8. Title: Rethinking the role of demonstrations: What makes in-context learning work?\n   Abstract: Rethinking the Role of Demonstrations: What Makes In-Context Learning Work?: Large language models (LMs) are able to in-context learn -- perform a new task via inference alone by conditioning on a few input-label pairs (demonstrations) and making predictions for new inputs. However, there has been little understanding of how the model learns and which aspects of the demonstrations contribute to end task performance. In this paper, we show that ground truth demonstrations are in fact not required -- randomly replacing labels in the demonstrations barely hurts performance on a range of classification and multi-choce tasks, consistently over 12 different models including GPT-3. Instead, we find that other aspects of the demonstrations are the key drivers of end task performance, including the fact that they provide a few examples of (1) the label space, (2) the distribution of the input text, and (3) the overall format of the sequence. Together, our analysis provides a new way of understanding how and why in-context learning works, while opening up new questions about how much can be learned from large language models through inference alone.\n\n9. Title: Scaling laws for neural language models\n   Abstract: Scaling Laws for Neural Language Models: We study empirical scaling laws for language model performance on the cross-entropy loss. The loss scales as a power-law with model size, dataset size, and the amount of compute used for training, with some trends spanning more than seven orders of magnitude. Other architectural details such as network width or depth have minimal effects within a wide range. Simple equations govern the dependence of overfitting on model/dataset size and the dependence of training speed on model size. These relationships allow us to determine the optimal allocation of a fixed compute budget. Larger models are significantly more sample-efficient, such that optimally compute-efficient training involves training very large models on a relatively modest amount of data and stopping significantly before convergence.\n\n10. Title: Reasoning or Reciting? Exploring the Capabilities and Limitations of Language Models Through Counterfactual Tasks\n   Abstract: Reasoning or Reciting? Exploring the Capabilities and Limitations of Language Models Through Counterfactual Tasks: The impressive performance of recent language models across a wide range of tasks suggests that they possess a degree of abstract reasoning skills. Are these skills general and transferable, or specialized to specific tasks seen during pretraining? To disentangle these effects, we propose an evaluation framework based on \"counterfactual\" task variants that deviate from the default assumptions underlying standard tasks. Across a suite of 11 tasks, we observe nontrivial performance on the counterfactual variants, but nevertheless find that performance substantially and consistently degrades compared to the default conditions. This suggests that while current LMs may possess abstract task-solving skills to an extent, they often also rely on narrow, non-transferable procedures for task-solving. These results motivate a more careful interpretation of language model performance that teases apart these aspects of behavior.\n\nThink step-by-step about which candidate best matches the context. Summarize your reasoning, then provide the exact title that should fill the citation. Respond in JSON with fields:\n{\n  \"reasoning\": \"<your step-by-step analysis>\",\n  \"selected_title\": \"<exact candidate title>\"\n}", "response": {"reasoning": "The context mentions FELM as a factuality benchmark that includes annotations of solutions to GSM8K and MATH problems, and contrasts crowdsourced labeling in PRM800K and FELM with author-only annotations in CHAMP. Candidate 5's abstract (FELM) explicitly describes a benchmark for factuality evaluation that annotates LLM responses across domains including math and reasoning, which matches the claim about annotations for GSM8K and MATH problems. Candidate 4 (Let's Verify Step by Step) is the source for PRM800K, but the specific statement being cited â€” annotations of 208 GSM8K and 194 MATH problems â€” corresponds to the FELM paper. Therefore the correct citation is the FELM paper.", "selected_title": "Felm: Benchmarking factuality evaluation of large language models"}, "selected_title": "Felm: Benchmarking factuality evaluation of large language models", "positive_title": "Training verifiers to solve math word problems", "is_correct": false, "reasoning": "The context mentions FELM as a factuality benchmark that includes annotations of solutions to GSM8K and MATH problems, and contrasts crowdsourced labeling in PRM800K and FELM with author-only annotations in CHAMP. Candidate 5's abstract (FELM) explicitly describes a benchmark for factuality evaluation that annotates LLM responses across domains including math and reasoning, which matches the claim about annotations for GSM8K and MATH problems. Candidate 4 (Let's Verify Step by Step) is the source for PRM800K, but the specific statement being cited â€” annotations of 208 GSM8K and 194 MATH problems â€” corresponds to the FELM paper. Therefore the correct citation is the FELM paper."}
{"example_id": "blind_13804", "prompt": "You are an expert citation retrieval system as described in the paper \"Multi-Agent System for Reliable Citation Retrieval\".\nYour goal is to autonomously retrieve, verify, and recommend academic references given a query or document excerpt.\n\nTask:\nGiven a citation context from a scientific paper (where a citation is missing), identify the correct paper from a list of candidates.\nAnalyze the context to understand the specific claim, method, or result being cited.\nThen, evaluate each candidate paper to see if it matches the context.\nFinally, select the best matching paper.\n\nCitation Context:\nence, it is difficult to find a perfect latent representation, which provides the capacity for accurate reconstruction of the input image as well as realistic editing of the image. In fact, some works[CITATION] have shown that StyleGAN-based methods suffer from the trade-off between low-distortion and editability when inverting a real image into the latent space of StyleGAN. It has been reported that editin\n\nCandidate Papers:\n1. Title: Stylespace analysis: Disentangled controls for stylegan image generation\n   Abstract: StyleSpace Analysis: Disentangled Controls for StyleGAN Image Generation: We explore and analyze the latent style space of StyleGAN2, a state-of-the-art architecture for image generation, using models pretrained on several different datasets. We first show that StyleSpace, the space of channel-wise style parameters, is significantly more disentangled than the other intermediate latent spaces explored by previous works. Next, we describe a method for discovering a large collection of style channels, each of which is shown to control a distinct visual attribute in a highly localized and disentangled manner. Third, we propose a simple method for identifying style channels that control a specific attribute, using a pretrained classifier or a small number of example images. Manipulation of visual attributes via these StyleSpace controls is shown to be better disentangled than via those proposed in previous works. To show this, we make use of a newly proposed Attribute Dependency metric. Finally, we demonstrate the applicability of StyleSpace controls to the manipulation of real images. Our findings pave the way to semantically meaningful and well-disentangled image manipulations via simple and intuitive interfaces.\n\n2. Title: Inverting the generator of a generative adversarial network\n   Abstract: Inverting The Generator Of A Generative Adversarial Network: Generative adversarial networks (GANs) learn to synthesise new samples from a high-dimensional distribution by passing samples drawn from a latent space through a generative network. When the high-dimensional distribution describes images of a particular data set, the network should learn to generate visually similar image samples for latent variables that are close to each other in the latent space. For tasks such as image retrieval and image classification, it may be useful to exploit the arrangement of the latent space by projecting images into it, and using this as a representation for discriminative tasks. GANs often consist of multiple layers of non-linear computations, making them very difficult to invert. This paper introduces techniques for projecting image samples into the latent space using any pre-trained GAN, provided that the computational graph is available. We evaluate these techniques on both MNIST digits and Omniglot handwritten characters. In the case of MNIST digits, we show that projections into the latent space maintain information about the style and the identity of the digit. In the case of Omniglot characters, we show that even characters from alphabets that have not been seen during training may be projected well into the latent space; this suggests that this approach may have applications in one-shot learning.\n\n3. Title: Unsupervised discovery of interpretable directions in the gan latent space\n   Abstract: Unsupervised Discovery of Interpretable Directions in the GAN Latent Space: The latent spaces of GAN models often have semantically meaningful directions. Moving in these directions corresponds to human-interpretable image transformations, such as zooming or recoloring, enabling a more controllable generation process. However, the discovery of such directions is currently performed in a supervised manner, requiring human labels, pretrained models, or some form of self-supervision. These requirements severely restrict a range of directions existing approaches can discover. In this paper, we introduce an unsupervised method to identify interpretable directions in the latent space of a pretrained GAN model. By a simple model-agnostic procedure, we find directions corresponding to sensible semantic manipulations without any form of (self-)supervision. Furthermore, we reveal several non-trivial findings, which would be difficult to obtain by existing methods, e.g., a direction corresponding to background removal. As an immediate practical benefit of our work, we show how to exploit this finding to achieve competitive performance for weakly-supervised saliency detection.\n\n4. Title: Encoding in style: a stylegan encoder for image-to-image translation\n   Abstract: Encoding in Style: a StyleGAN Encoder for Image-to-Image Translation: We present a generic image-to-image translation framework, pixel2style2pixel (pSp). Our pSp framework is based on a novel encoder network that directly generates a series of style vectors which are fed into a pretrained StyleGAN generator, forming the extended W+ latent space. We first show that our encoder can directly embed real images into W+, with no additional optimization. Next, we propose utilizing our encoder to directly solve image-to-image translation tasks, defining them as encoding problems from some input domain into the latent domain. By deviating from the standard invert first, edit later methodology used with previous StyleGAN encoders, our approach can handle a variety of tasks even when the input image is not represented in the StyleGAN domain. We show that solving translation tasks through StyleGAN significantly simplifies the training process, as no adversary is required, has better support for solving tasks without pixel-to-pixel correspondence, and inherently supports multi-modal synthesis via the resampling of styles. Finally, we demonstrate the potential of our framework on a variety of facial image-to-image translation tasks, even when compared to state-of-the-art solutions designed specifically for a single task, and further show that it can be extended beyond the human facial domain.\n\n5. Title: Styleflow: Attribute-conditioned exploration of stylegan-generated images using conditional continuous normalizing flows\n   Abstract: StyleFlow: Attribute-conditioned Exploration of StyleGAN-Generated Images using Conditional Continuous Normalizing Flows: High-quality, diverse, and photorealistic images can now be generated by unconditional GANs (e.g., StyleGAN). However, limited options exist to control the generation process using (semantic) attributes, while still preserving the quality of the output. Further, due to the entangled nature of the GAN latent space, performing edits along one attribute can easily result in unwanted changes along other attributes. In this paper, in the context of conditional exploration of entangled latent spaces, we investigate the two sub-problems of attribute-conditioned sampling and attribute-controlled editing. We present StyleFlow as a simple, effective, and robust solution to both the sub-problems by formulating conditional exploration as an instance of conditional continuous normalizing flows in the GAN latent space conditioned by attribute features. We evaluate our method using the face and the car latent space of StyleGAN, and demonstrate fine-grained disentangled edits along various attributes on both real photographs and StyleGAN generated images. For example, for faces, we vary camera pose, illumination variation, expression, facial hair, gender, and age. Finally, via extensive qualitative and quantitative comparisons, we demonstrate the superiority of StyleFlow to other concurrent works.\n\n6. Title: Image2stylegan: How to embed images into the stylegan latent space?\n   Abstract: Image2StyleGAN: How to Embed Images Into the StyleGAN Latent Space?: We propose an efficient algorithm to embed a given image into the latent space of StyleGAN. This embedding enables semantic image editing operations that can be applied to existing photographs. Taking the StyleGAN trained on the FFHQ dataset as an example, we show results for image morphing, style transfer, and expression transfer. Studying the results of the embedding algorithm provides valuable insights into the structure of the StyleGAN latent space. We propose a set of experiments to test what class of images can be embedded, how they are embedded, what latent space is suitable for embedding, and if the embedding is semantically meaningful.\n\n7. Title: In-domain gan inversion for real image editing\n   Abstract: In-Domain GAN Inversion for Real Image Editing: Recent work has shown that a variety of semantics emerge in the latent space of Generative Adversarial Networks (GANs) when being trained to synthesize images. However, it is difficult to use these learned semantics for real image editing. A common practice of feeding a real image to a trained GAN generator is to invert it back to a latent code. However, existing inversion methods typically focus on reconstructing the target image by pixel values yet fail to land the inverted code in the semantic domain of the original latent space. As a result, the reconstructed image cannot well support semantic editing through varying the inverted code. To solve this problem, we propose an in-domain GAN inversion approach, which not only faithfully reconstructs the input image but also ensures the inverted code to be semantically meaningful for editing. We first learn a novel domain-guided encoder to project a given image to the native latent space of GANs. We then propose domain-regularized optimization by involving the encoder as a regularizer to fine-tune the code produced by the encoder and better recover the target image. Extensive experiments suggest that our inversion method achieves satisfying real image reconstruction and more importantly facilitates various image editing tasks, significantly outperforming start-of-the-arts.\n\n8. Title: Disentangled Face Attribute Editing via Instance-Aware Latent Space Search\n   Abstract: Disentangled Face Attribute Editing via Instance-Aware Latent Space Search: Recent works have shown that a rich set of semantic directions exist in the latent space of Generative Adversarial Networks (GANs), which enables various facial attribute editing applications. However, existing methods may suffer poor attribute variation disentanglement, leading to unwanted change of other attributes when altering the desired one. The semantic directions used by existing methods are at attribute level, which are difficult to model complex attribute correlations, especially in the presence of attribute distribution bias in GAN's training set. In this paper, we propose a novel framework (IALS) that performs Instance-Aware Latent-Space Search to find semantic directions for disentangled attribute editing. The instance information is injected by leveraging the supervision from a set of attribute classifiers evaluated on the input images. We further propose a Disentanglement-Transformation (DT) metric to quantify the attribute transformation and disentanglement efficacy and find the optimal control factor between attribute-level and instance-specific directions based on it. Experimental results on both GAN-generated and real-world images collectively show that our method outperforms state-of-the-art methods proposed recently by a wide margin. Code is available at https://github.com/yxuhan/IALS.\n\n9. Title: Image2stylegan++: How to edit the embedded images?\n   Abstract: Image2StyleGAN++: How to Edit the Embedded Images?: We propose Image2StyleGAN++, a flexible image editing framework with many applications. Our framework extends the recent Image2StyleGAN in three ways. First, we introduce noise optimization as a complement to the $W^+$ latent space embedding. Our noise optimization can restore high-frequency features in images and thus significantly improves the quality of reconstructed images, e.g. a big increase of PSNR from 20 dB to 45 dB. Second, we extend the global $W^+$ latent space embedding to enable local embeddings. Third, we combine embedding with activation tensor manipulation to perform high-quality local edits along with global semantic edits on images. Such edits motivate various high-quality image editing applications, e.g. image reconstruction, image inpainting, image crossover, local style transfer, image editing using scribbles, and attribute level feature transfer. Examples of the edited images are shown across the paper for visual inspection.\n\n10. Title: Designing an encoder for stylegan image manipulation\n   Abstract: Designing an Encoder for StyleGAN Image Manipulation: Recently, there has been a surge of diverse methods for performing image editing by employing pre-trained unconditional generators. Applying these methods on real images, however, remains a challenge, as it necessarily requires the inversion of the images into their latent space. To successfully invert a real image, one needs to find a latent code that reconstructs the input image accurately, and more importantly, allows for its meaningful manipulation. In this paper, we carefully study the latent space of StyleGAN, the state-of-the-art unconditional generator. We identify and analyze the existence of a distortion-editability tradeoff and a distortion-perception tradeoff within the StyleGAN latent space. We then suggest two principles for designing encoders in a manner that allows one to control the proximity of the inversions to regions that StyleGAN was originally trained on. We present an encoder based on our two principles that is specifically designed for facilitating editing on real images by balancing these tradeoffs. By evaluating its performance qualitatively and quantitatively on numerous challenging domains, including cars and horses, we show that our inversion method, followed by common editing techniques, achieves superior real-image editing quality, with only a small reconstruction accuracy drop.\n\nThink step-by-step about which candidate best matches the context. Summarize your reasoning, then provide the exact title that should fill the citation. Respond in JSON with fields:\n{\n  \"reasoning\": \"<your step-by-step analysis>\",\n  \"selected_title\": \"<exact candidate title>\"\n}", "response": {"reasoning": "The citation context states that prior works have shown a trade-off between low-distortion (accurate reconstruction) and editability when inverting real images into the StyleGAN latent space. I reviewed the candidate abstracts for mention of this specific trade-off and discussion of encoders/inversion for editability. Candidate 10 explicitly states that the authors \"identify and analyze the existence of a distortion-editability tradeoff and a distortion-perception tradeoff within the StyleGAN latent space,\" and presents encoder design principles to balance these tradeoffs for real-image editing. This is a direct match to the claim in the context. Other candidates (e.g., Image2StyleGAN, Image2StyleGAN++, pSp, In-domain GAN inversion) deal with inversion and editing broadly but do not explicitly identify or analyze the distortionâ€“editability trade-off in their abstracts. Therefore the best matching paper is candidate 10.", "selected_title": "Designing an Encoder for StyleGAN Image Manipulation"}, "selected_title": "Designing an Encoder for StyleGAN Image Manipulation", "positive_title": "Designing an encoder for stylegan image manipulation", "is_correct": true, "reasoning": "The citation context states that prior works have shown a trade-off between low-distortion (accurate reconstruction) and editability when inverting real images into the StyleGAN latent space. I reviewed the candidate abstracts for mention of this specific trade-off and discussion of encoders/inversion for editability. Candidate 10 explicitly states that the authors \"identify and analyze the existence of a distortion-editability tradeoff and a distortion-perception tradeoff within the StyleGAN latent space,\" and presents encoder design principles to balance these tradeoffs for real-image editing. This is a direct match to the claim in the context. Other candidates (e.g., Image2StyleGAN, Image2StyleGAN++, pSp, In-domain GAN inversion) deal with inversion and editing broadly but do not explicitly identify or analyze the distortionâ€“editability trade-off in their abstracts. Therefore the best matching paper is candidate 10."}
{"example_id": "blind_4021", "prompt": "You are an expert citation retrieval system as described in the paper \"Multi-Agent System for Reliable Citation Retrieval\".\nYour goal is to autonomously retrieve, verify, and recommend academic references given a query or document excerpt.\n\nTask:\nGiven a citation context from a scientific paper (where a citation is missing), identify the correct paper from a list of candidates.\nAnalyze the context to understand the specific claim, method, or result being cited.\nThen, evaluate each candidate paper to see if it matches the context.\nFinally, select the best matching paper.\n\nCitation Context:\nrained models as CLIP.} is becoming a {\\it de facto} choice for text-guided image morphing. This can be achieved by fine-tuning pre-trained generative models like StyleGAN and DDPM[CITATION], or by explicitly morphing the given images. Previous work on CLIP-guided image morphing commonly focuses on minimizing spherical distances or directional CLIP loss betw\n\nCandidate Papers:\n1. Title: Styleclip: Text-driven manipulation of stylegan imagery\n   Abstract: StyleCLIP: Text-Driven Manipulation of StyleGAN Imagery: Inspired by the ability of StyleGAN to generate highly realistic images in a variety of domains, much recent work has focused on understanding how to use the latent spaces of StyleGAN to manipulate generated and real images. However, discovering semantically meaningful latent manipulations typically involves painstaking human examination of the many degrees of freedom, or an annotated collection of images for each desired manipulation. In this work, we explore leveraging the power of recently introduced Contrastive Language-Image Pre-training (CLIP) models in order to develop a text-based interface for StyleGAN image manipulation that does not require such manual effort. We first introduce an optimization scheme that utilizes a CLIP-based loss to modify an input latent vector in response to a user-provided text prompt. Next, we describe a latent mapper that infers a text-guided latent manipulation step for a given input image, allowing faster and more stable text-based manipulation. Finally, we present a method for mapping a text prompts to input-agnostic directions in StyleGAN's style space, enabling interactive text-driven image manipulation. Extensive results and comparisons demonstrate the effectiveness of our approaches.\n\n2. Title: Learning without forgetting\n   Abstract: Learning without Forgetting: When building a unified vision system or gradually adding new capabilities to a system, the usual assumption is that training data for all tasks is always available. However, as the number of tasks grows, storing and retraining on such data becomes infeasible. A new problem arises where we add new capabilities to a Convolutional Neural Network (CNN), but the training data for its existing capabilities are unavailable. We propose our Learning without Forgetting method, which uses only new task data to train the network while preserving the original capabilities. Our method performs favorably compared to commonly used feature extraction and fine-tuning adaption techniques and performs similarly to multitask learning that uses original task data we assume unavailable. A more surprising observation is that Learning without Forgetting may be able to replace fine-tuning with similar old and new task datasets for improved new task performance.\n\n3. Title: Motionclip: Exposing human motion generation to clip space\n   Abstract: MotionCLIP: Exposing Human Motion Generation to CLIP Space: We introduce MotionCLIP, a 3D human motion auto-encoder featuring a latent embedding that is disentangled, well behaved, and supports highly semantic textual descriptions. MotionCLIP gains its unique power by aligning its latent space with that of the Contrastive Language-Image Pre-training (CLIP) model. Aligning the human motion manifold to CLIP space implicitly infuses the extremely rich semantic knowledge of CLIP into the manifold. In particular, it helps continuity by placing semantically similar motions close to one another, and disentanglement, which is inherited from the CLIP-space structure. MotionCLIP comprises a transformer-based motion auto-encoder, trained to reconstruct motion while being aligned to its text label's position in CLIP-space. We further leverage CLIP's unique visual understanding and inject an even stronger signal through aligning motion to rendered frames in a self-supervised manner. We show that although CLIP has never seen the motion domain, MotionCLIP offers unprecedented text-to-motion abilities, allowing out-of-domain actions, disentangled editing, and abstract language specification. For example, the text prompt \"couch\" is decoded into a sitting down motion, due to lingual similarity, and the prompt \"Spiderman\" results in a web-swinging-like solution that is far from seen during training. In addition, we show how the introduced latent space can be leveraged for motion interpolation, editing and recognition.\n\n4. Title: Diffusionclip: Text-guided diffusion models for robust image manipulation\n   Abstract: DiffusionCLIP: Text-Guided Diffusion Models for Robust Image Manipulation: Recently, GAN inversion methods combined with Contrastive Language-Image Pretraining (CLIP) enables zero-shot image manipulation guided by text prompts. However, their applications to diverse real images are still difficult due to the limited GAN inversion capability. Specifically, these approaches often have difficulties in reconstructing images with novel poses, views, and highly variable contents compared to the training data, altering object identity, or producing unwanted image artifacts. To mitigate these problems and enable faithful manipulation of real images, we propose a novel method, dubbed DiffusionCLIP, that performs text-driven image manipulation using diffusion models. Based on full inversion capability and high-quality image generation power of recent diffusion models, our method performs zero-shot image manipulation successfully even between unseen domains and takes another step towards general application by manipulating images from a widely varying ImageNet dataset. Furthermore, we propose a novel noise combination method that allows straightforward multi-attribute manipulation. Extensive experiments and human evaluation confirmed robust and superior manipulation performance of our methods compared to the existing baselines. Code is available at https://github.com/gwang-kim/DiffusionCLIP.git.\n\n5. Title: Text2live: Text-driven layered image and video editing\n   Abstract: Text2LIVE: Text-Driven Layered Image and Video Editing: We present a method for zero-shot, text-driven appearance manipulation in natural images and videos. Given an input image or video and a target text prompt, our goal is to edit the appearance of existing objects (e.g., object's texture) or augment the scene with visual effects (e.g., smoke, fire) in a semantically meaningful manner. We train a generator using an internal dataset of training examples, extracted from a single input (image or video and target text prompt), while leveraging an external pre-trained CLIP model to establish our losses. Rather than directly generating the edited output, our key idea is to generate an edit layer (color+opacity) that is composited over the original input. This allows us to constrain the generation process and maintain high fidelity to the original input via novel text-driven losses that are applied directly to the edit layer. Our method neither relies on a pre-trained generator nor requires user-provided edit masks. We demonstrate localized, semantic edits on high-resolution natural images and videos across a variety of objects and scenes.\n\n6. Title: StyleGAN-NADA: CLIP-guided domain adaptation of image generators\n   Abstract: StyleGAN-NADA: CLIP-Guided Domain Adaptation of Image Generators: Can a generative model be trained to produce images from a specific domain, guided by a text prompt only, without seeing any image? In other words: can an image generator be trained \"blindly\"? Leveraging the semantic power of large scale Contrastive-Language-Image-Pre-training (CLIP) models, we present a text-driven method that allows shifting a generative model to new domains, without having to collect even a single image. We show that through natural language prompts and a few minutes of training, our method can adapt a generator across a multitude of domains characterized by diverse styles and shapes. Notably, many of these modifications would be difficult or outright impossible to reach with existing methods. We conduct an extensive set of experiments and comparisons across a wide range of domains. These demonstrate the effectiveness of our approach and show that our shifted models maintain the latent-space properties that make generative models appealing for downstream tasks.\n\n7. Title: Image-based clip-guided essence transfer\n   Abstract: Image-Based CLIP-Guided Essence Transfer: We make the distinction between (i) style transfer, in which a source image is manipulated to match the textures and colors of a target image, and (ii) essence transfer, in which one edits the source image to include high-level semantic attributes from the target. Crucially, the semantic attributes that constitute the essence of an image may differ from image to image. Our blending operator combines the powerful StyleGAN generator and the semantic encoder of CLIP in a novel way that is simultaneously additive in both latent spaces, resulting in a mechanism that guarantees both identity preservation and high-level feature transfer without relying on a facial recognition network. We present two variants of our method. The first is based on optimization, while the second fine-tunes an existing inversion encoder to perform essence extraction. Through extensive experiments, we demonstrate the superiority of our methods for essence transfer over existing methods for style transfer, domain adaptation, and text-based semantic editing. Our code is available at https://github.com/hila-chefer/TargetCLIP.\n\n8. Title: Clipstyler: Image style transfer with a single text condition\n   Abstract: CLIPstyler: Image Style Transfer with a Single Text Condition: Existing neural style transfer methods require reference style images to transfer texture information of style images to content images. However, in many practical situations, users may not have reference style images but still be interested in transferring styles by just imagining them. In order to deal with such applications, we propose a new framework that enables a style transfer `without' a style image, but only with a text description of the desired style. Using the pre-trained text-image embedding model of CLIP, we demonstrate the modulation of the style of content images only with a single text condition. Specifically, we propose a patch-wise text-image matching loss with multiview augmentations for realistic texture transfer. Extensive experimental results confirmed the successful image style transfer with realistic textures that reflect semantic query texts.\n\n9. Title: On learning the geodesic path for incremental learning\n   Abstract: On Learning the Geodesic Path for Incremental Learning: Neural networks notoriously suffer from the problem of catastrophic forgetting, the phenomenon of forgetting the past knowledge when acquiring new knowledge. Overcoming catastrophic forgetting is of significant importance to emulate the process of \"incremental learning\", where the model is capable of learning from sequential experience in an efficient and robust way. State-of-the-art techniques for incremental learning make use of knowledge distillation towards preventing catastrophic forgetting. Therein, one updates the network while ensuring that the network's responses to previously seen concepts remain stable throughout updates. This in practice is done by minimizing the dissimilarity between current and previous responses of the network one way or another. Our work contributes a novel method to the arsenal of distillation techniques. In contrast to the previous state of the art, we propose to firstly construct low-dimensional manifolds for previous and current responses and minimize the dissimilarity between the responses along the geodesic connecting the manifolds. This induces a more formidable knowledge distillation with smooth properties which preserves the past knowledge more efficiently as observed by our comprehensive empirical study.\n\nThink step-by-step about which candidate best matches the context. Summarize your reasoning, then provide the exact title that should fill the citation. Respond in JSON with fields:\n{\n  \"reasoning\": \"<your step-by-step analysis>\",\n  \"selected_title\": \"<exact candidate title>\"\n}", "response": {"reasoning": "The context discusses CLIP-guided text-driven image morphing and explicitly mentions fine-tuning pre-trained generative models (StyleGAN, DDPM) and prior work focusing on minimizing spherical distances or the directional CLIP loss. The concept of minimizing directional CLIP loss and text-driven manipulation of StyleGAN latents is central to StyleCLIP. Among candidates: (1) StyleCLIP directly introduces CLIP-based losses (including directional CLIP loss) for text-driven manipulation of StyleGAN; (4) DiffusionCLIP covers CLIP guidance for diffusion models (DDPM) but does not originate the directional CLIP loss idea referenced next; (6) StyleGAN-NADA adapts StyleGAN with CLIP but is less directly associated with the directional CLIP loss formulation. Because the next sentence explicitly references minimizing spherical/directional CLIP distances â€” a key contribution of StyleCLIP â€” the best matching paper to cite here is StyleCLIP.", "selected_title": "Styleclip: Text-driven manipulation of stylegan imagery"}, "selected_title": "Styleclip: Text-driven manipulation of stylegan imagery", "positive_title": "Diffusionclip: Text-guided diffusion models for robust image manipulation", "is_correct": false, "reasoning": "The context discusses CLIP-guided text-driven image morphing and explicitly mentions fine-tuning pre-trained generative models (StyleGAN, DDPM) and prior work focusing on minimizing spherical distances or the directional CLIP loss. The concept of minimizing directional CLIP loss and text-driven manipulation of StyleGAN latents is central to StyleCLIP. Among candidates: (1) StyleCLIP directly introduces CLIP-based losses (including directional CLIP loss) for text-driven manipulation of StyleGAN; (4) DiffusionCLIP covers CLIP guidance for diffusion models (DDPM) but does not originate the directional CLIP loss idea referenced next; (6) StyleGAN-NADA adapts StyleGAN with CLIP but is less directly associated with the directional CLIP loss formulation. Because the next sentence explicitly references minimizing spherical/directional CLIP distances â€” a key contribution of StyleCLIP â€” the best matching paper to cite here is StyleCLIP."}
{"example_id": "blind_7518", "prompt": "You are an expert citation retrieval system as described in the paper \"Multi-Agent System for Reliable Citation Retrieval\".\nYour goal is to autonomously retrieve, verify, and recommend academic references given a query or document excerpt.\n\nTask:\nGiven a citation context from a scientific paper (where a citation is missing), identify the correct paper from a list of candidates.\nAnalyze the context to understand the specific claim, method, or result being cited.\nThen, evaluate each candidate paper to see if it matches the context.\nFinally, select the best matching paper.\n\nCitation Context:\neved remarkable improvement in many point cloud tasks. This approach first applies a pretext task to learn the latent 3D representation and then transfers it to various downstream tasks. PointContrast[CITATION] and CrossPoint initially explored utilizing contrastive learning for learning 3D representations, which achieved some success; however, there were still some shortcomings in cap\n\nCandidate Papers:\n1. Title: Masked autoencoders for point cloud self-supervised learning\n   Abstract: Masked Autoencoders for Point Cloud Self-supervised Learning: As a promising scheme of self-supervised learning, masked autoencoding has significantly advanced natural language processing and computer vision. Inspired by this, we propose a neat scheme of masked autoencoders for point cloud self-supervised learning, addressing the challenges posed by point cloud's properties, including leakage of location information and uneven information density. Concretely, we divide the input point cloud into irregular point patches and randomly mask them at a high ratio. Then, a standard Transformer based autoencoder, with an asymmetric design and a shifting mask tokens operation, learns high-level latent features from unmasked point patches, aiming to reconstruct the masked point patches. Extensive experiments show that our approach is efficient during pre-training and generalizes well on various downstream tasks. Specifically, our pre-trained models achieve 85.18% accuracy on ScanObjectNN and 94.04% accuracy on ModelNet40, outperforming all the other self-supervised learning methods. We show with our scheme, a simple architecture entirely based on standard Transformers can surpass dedicated Transformer models from supervised learning. Our approach also advances state-of-the-art accuracies by 1.5%-2.3% in the few-shot object classification. Furthermore, our work inspires the feasibility of applying unified architectures from languages and images to the point cloud.\n\n2. Title: Attention is all you need\n   Abstract: Attention Is All You Need: The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.\n\n3. Title: Simmim: A simple framework for masked image modeling\n   Abstract: SimMIM: A Simple Framework for Masked Image Modeling: This paper presents SimMIM, a simple framework for masked image modeling. We simplify recently proposed related approaches without special designs such as block-wise masking and tokenization via discrete VAE or clustering. To study what let the masked image modeling task learn good representations, we systematically study the major components in our framework, and find that simple designs of each component have revealed very strong representation learning performance: 1) random masking of the input image with a moderately large masked patch size (e.g., 32) makes a strong pre-text task; 2) predicting raw pixels of RGB values by direct regression performs no worse than the patch classification approaches with complex designs; 3) the prediction head can be as light as a linear layer, with no worse performance than heavier ones. Using ViT-B, our approach achieves 83.8% top-1 fine-tuning accuracy on ImageNet-1K by pre-training also on this dataset, surpassing previous best approach by +0.6%. When applied on a larger model of about 650 million parameters, SwinV2-H, it achieves 87.1% top-1 accuracy on ImageNet-1K using only ImageNet-1K data. We also leverage this approach to facilitate the training of a 3B model (SwinV2-G), that by $40\\times$ less data than that in previous practice, we achieve the state-of-the-art on four representative vision benchmarks. The code and models will be publicly available at https://github.com/microsoft/SimMIM.\n\n4. Title: Joint-mae: 2d-3d joint masked autoencoders for 3d point cloud pre-training\n   Abstract: Joint-MAE: 2D-3D Joint Masked Autoencoders for 3D Point Cloud Pre-training: Masked Autoencoders (MAE) have shown promising performance in self-supervised learning for both 2D and 3D computer vision. However, existing MAE-style methods can only learn from the data of a single modality, i.e., either images or point clouds, which neglect the implicit semantic and geometric correlation between 2D and 3D. In this paper, we explore how the 2D modality can benefit 3D masked autoencoding, and propose Joint-MAE, a 2D-3D joint MAE framework for self-supervised 3D point cloud pre-training. Joint-MAE randomly masks an input 3D point cloud and its projected 2D images, and then reconstructs the masked information of the two modalities. For better cross-modal interaction, we construct our JointMAE by two hierarchical 2D-3D embedding modules, a joint encoder, and a joint decoder with modal-shared and model-specific decoders. On top of this, we further introduce two cross-modal strategies to boost the 3D representation learning, which are local-aligned attention mechanisms for 2D-3D semantic cues, and a cross-reconstruction loss for 2D-3D geometric constraints. By our pre-training paradigm, Joint-MAE achieves superior performance on multiple downstream tasks, e.g., 92.4% accuracy for linear SVM on ModelNet40 and 86.07% accuracy on the hardest split of ScanObjectNN.\n\n5. Title: Pointgpt: Auto-regressively generative pre-training from point clouds\n   Abstract: PointGPT: Auto-regressively Generative Pre-training from Point Clouds: Large language models (LLMs) based on the generative pre-training transformer (GPT) have demonstrated remarkable effectiveness across a diverse range of downstream tasks. Inspired by the advancements of the GPT, we present PointGPT, a novel approach that extends the concept of GPT to point clouds, addressing the challenges associated with disorder properties, low information density, and task gaps. Specifically, a point cloud auto-regressive generation task is proposed to pre-train transformer models. Our method partitions the input point cloud into multiple point patches and arranges them in an ordered sequence based on their spatial proximity. Then, an extractor-generator based transformer decoder, with a dual masking strategy, learns latent representations conditioned on the preceding point patches, aiming to predict the next one in an auto-regressive manner. Our scalable approach allows for learning high-capacity models that generalize well, achieving state-of-the-art performance on various downstream tasks. In particular, our approach achieves classification accuracies of 94.9% on the ModelNet40 dataset and 93.4% on the ScanObjectNN dataset, outperforming all other transformer models. Furthermore, our method also attains new state-of-the-art accuracies on all four few-shot learning benchmarks.\n\n6. Title: Point-bert: Pre-training 3d point cloud transformers with masked point modeling\n   Abstract: Point-BERT: Pre-training 3D Point Cloud Transformers with Masked Point Modeling: We present Point-BERT, a new paradigm for learning Transformers to generalize the concept of BERT to 3D point cloud. Inspired by BERT, we devise a Masked Point Modeling (MPM) task to pre-train point cloud Transformers. Specifically, we first divide a point cloud into several local point patches, and a point cloud Tokenizer with a discrete Variational AutoEncoder (dVAE) is designed to generate discrete point tokens containing meaningful local information. Then, we randomly mask out some patches of input point clouds and feed them into the backbone Transformers. The pre-training objective is to recover the original point tokens at the masked locations under the supervision of point tokens obtained by the Tokenizer. Extensive experiments demonstrate that the proposed BERT-style pre-training strategy significantly improves the performance of standard point cloud Transformers. Equipped with our pre-training strategy, we show that a pure Transformer architecture attains 93.8% accuracy on ModelNet40 and 83.1% accuracy on the hardest setting of ScanObjectNN, surpassing carefully designed point cloud models with much fewer hand-made designs. We also demonstrate that the representations learned by Point-BERT transfer well to new tasks and domains, where our models largely advance the state-of-the-art of few-shot point cloud classification task. The code and pre-trained models are available at https://github.com/lulutang0608/Point-BERT\n\n7. Title: Pointcontrast: Unsupervised pre-training for 3d point cloud understanding\n   Abstract: PointContrast: Unsupervised Pre-training for 3D Point Cloud Understanding: Arguably one of the top success stories of deep learning is transfer learning. The finding that pre-training a network on a rich source set (eg., ImageNet) can help boost performance once fine-tuned on a usually much smaller target set, has been instrumental to many applications in language and vision. Yet, very little is known about its usefulness in 3D point cloud understanding. We see this as an opportunity considering the effort required for annotating data in 3D. In this work, we aim at facilitating research on 3D representation learning. Different from previous works, we focus on high-level scene understanding tasks. To this end, we select a suite of diverse datasets and tasks to measure the effect of unsupervised pre-training on a large source set of 3D scenes. Our findings are extremely encouraging: using a unified triplet of architecture, source dataset, and contrastive loss for pre-training, we achieve improvement over recent best results in segmentation and detection across 6 different benchmarks for indoor and outdoor, real and synthetic datasets -- demonstrating that the learned representation can generalize across domains. Furthermore, the improvement was similar to supervised pre-training, suggesting that future efforts should favor scaling data collection over more detailed annotation. We hope these findings will encourage more research on unsupervised pretext task design for 3D deep learning.\n\n8. Title: Masked discrimination for self-supervised learning on point clouds\n   Abstract: Masked Discrimination for Self-Supervised Learning on Point Clouds: Masked autoencoding has achieved great success for self-supervised learning in the image and language domains. However, mask based pretraining has yet to show benefits for point cloud understanding, likely due to standard backbones like PointNet being unable to properly handle the training versus testing distribution mismatch introduced by masking during training. In this paper, we bridge this gap by proposing a discriminative mask pretraining Transformer framework, MaskPoint}, for point clouds. Our key idea is to represent the point cloud as discrete occupancy values (1 if part of the point cloud; 0 if not), and perform simple binary classification between masked object points and sampled noise points as the proxy task. In this way, our approach is robust to the point sampling variance in point clouds, and facilitates learning rich representations. We evaluate our pretrained models across several downstream tasks, including 3D shape classification, segmentation, and real-word object detection, and demonstrate state-of-the-art results while achieving a significant pretraining speedup (e.g., 4.1x on ScanNet) compared to the prior state-of-the-art Transformer baseline. Code is available at https://github.com/haotian-liu/MaskPoint.\n\n9. Title: Learning 3d representations from 2d pre-trained models via image-to-point masked autoencoders\n   Abstract: Learning 3D Representations from 2D Pre-trained Models via Image-to-Point Masked Autoencoders: Pre-training by numerous image data has become de-facto for robust 2D representations. In contrast, due to the expensive data acquisition and annotation, a paucity of large-scale 3D datasets severely hinders the learning for high-quality 3D features. In this paper, we propose an alternative to obtain superior 3D representations from 2D pre-trained models via Image-to-Point Masked Autoencoders, named as I2P-MAE. By self-supervised pre-training, we leverage the well learned 2D knowledge to guide 3D masked autoencoding, which reconstructs the masked point tokens with an encoder-decoder architecture. Specifically, we first utilize off-the-shelf 2D models to extract the multi-view visual features of the input point cloud, and then conduct two types of image-to-point learning schemes on top. For one, we introduce a 2D-guided masking strategy that maintains semantically important point tokens to be visible for the encoder. Compared to random masking, the network can better concentrate on significant 3D structures and recover the masked tokens from key spatial cues. For another, we enforce these visible tokens to reconstruct the corresponding multi-view 2D features after the decoder. This enables the network to effectively inherit high-level 2D semantics learned from rich image data for discriminative 3D modeling. Aided by our image-to-point pre-training, the frozen I2P-MAE, without any fine-tuning, achieves 93.4% accuracy for linear SVM on ModelNet40, competitive to the fully trained results of existing methods. By further fine-tuning on on ScanObjectNN's hardest split, I2P-MAE attains the state-of-the-art 90.11% accuracy, +3.68% to the second-best, demonstrating superior transferable capacity. Code will be available at https://github.com/ZrrSkywalker/I2P-MAE.\n\nThink step-by-step about which candidate best matches the context. Summarize your reasoning, then provide the exact title that should fill the citation. Respond in JSON with fields:\n{\n  \"reasoning\": \"<your step-by-step analysis>\",\n  \"selected_title\": \"<exact candidate title>\"\n}", "response": {"reasoning": "The citation context refers to methods that \"initially explored utilizing contrastive learning for learning 3D representations\" and cites PointContrast alongside CrossPoint. Candidate 7, titled \"PointContrast: Unsupervised Pre-training for 3D Point Cloud Understanding\", explicitly describes unsupervised pre-training using a contrastive loss for 3D point clouds, demonstrating transfer improvements across multiple downstream tasks (segmentation, detection) and cross-domain generalization. Other candidates focus on masked autoencoding, masked point modeling, or autoregressive/generative pre-training (e.g., MAE, Point-BERT, PointGPT, Joint-MAE, I2P-MAE), which do not match the described contrastive-learning pretext. Therefore the best match is Candidate 7.", "selected_title": "PointContrast: Unsupervised Pre-training for 3D Point Cloud Understanding"}, "selected_title": "PointContrast: Unsupervised Pre-training for 3D Point Cloud Understanding", "positive_title": "Pointcontrast: Unsupervised pre-training for 3d point cloud understanding", "is_correct": true, "reasoning": "The citation context refers to methods that \"initially explored utilizing contrastive learning for learning 3D representations\" and cites PointContrast alongside CrossPoint. Candidate 7, titled \"PointContrast: Unsupervised Pre-training for 3D Point Cloud Understanding\", explicitly describes unsupervised pre-training using a contrastive loss for 3D point clouds, demonstrating transfer improvements across multiple downstream tasks (segmentation, detection) and cross-domain generalization. Other candidates focus on masked autoencoding, masked point modeling, or autoregressive/generative pre-training (e.g., MAE, Point-BERT, PointGPT, Joint-MAE, I2P-MAE), which do not match the described contrastive-learning pretext. Therefore the best match is Candidate 7."}
{"example_id": "blind_8971", "prompt": "You are an expert citation retrieval system as described in the paper \"Multi-Agent System for Reliable Citation Retrieval\".\nYour goal is to autonomously retrieve, verify, and recommend academic references given a query or document excerpt.\n\nTask:\nGiven a citation context from a scientific paper (where a citation is missing), identify the correct paper from a list of candidates.\nAnalyze the context to understand the specific claim, method, or result being cited.\nThen, evaluate each candidate paper to see if it matches the context.\nFinally, select the best matching paper.\n\nCitation Context:\norted in. \\textbf{Main problematic and contributions.} Although empirically observed, it is currently unknown how heavy-tailed behavior arises in offline SGD since the aforementioned theory[CITATION] requires infinite data. Our main goal in this paper is hence to develop a theoretical framework to catch \\emph{how} and \\emph{in what form} heavy tails may arise in offline SGD, where the dataset is\n\nCandidate Papers:\n1. Title: Algorithmic Stability of Heavy-Tailed SGD with General Loss Functions\n   Abstract: Algorithmic Stability of Heavy-Tailed SGD with General Loss Functions: Heavy-tail phenomena in stochastic gradient descent (SGD) have been reported in several empirical studies. Experimental evidence in previous works suggests a strong interplay between the heaviness of the tails and generalization behavior of SGD. To address this empirical phenomena theoretically, several works have made strong topological and statistical assumptions to link the generalization error to heavy tails. Very recently, new generalization bounds have been proven, indicating a non-monotonic relationship between the generalization error and heavy tails, which is more pertinent to the reported empirical observations. While these bounds do not require additional topological assumptions given that SGD can be modeled using a heavy-tailed stochastic differential equation (SDE), they can only apply to simple quadratic problems. In this paper, we build on this line of research and develop generalization bounds for a more general class of objective functions, which includes non-convex functions as well. Our approach is based on developing Wasserstein stability bounds for heavy-tailed SDEs and their discretizations, which we then convert to generalization bounds. Our results do not require any nontrivial assumptions; yet, they shed more light to the empirical observations, thanks to the generality of the loss functions.\n\n2. Title: Hausdorff dimension, heavy tails, and generalization in neural networks\n   Abstract: Hausdorff Dimension, Heavy Tails, and Generalization in Neural Networks: Despite its success in a wide range of applications, characterizing the generalization properties of stochastic gradient descent (SGD) in non-convex deep learning problems is still an important challenge. While modeling the trajectories of SGD via stochastic differential equations (SDE) under heavy-tailed gradient noise has recently shed light over several peculiar characteristics of SGD, a rigorous treatment of the generalization properties of such SDEs in a learning theoretical framework is still missing. Aiming to bridge this gap, in this paper, we prove generalization bounds for SGD under the assumption that its trajectories can be well-approximated by a \\emph{Feller process}, which defines a rich class of Markov processes that include several recent SDE representations (both Brownian or heavy-tailed) as its special case. We show that the generalization error can be controlled by the \\emph{Hausdorff dimension} of the trajectories, which is intimately linked to the tail behavior of the driving process. Our results imply that heavier-tailed processes should achieve better generalization; hence, the tail-index of the process can be used as a notion of \"capacity metric\". We support our theory with experiments on deep neural networks illustrating that the proposed capacity metric accurately estimates the generalization error, and it does not necessarily grow with the number of parameters unlike the existing capacity metrics in the literature.\n\n3. Title: The heavy-tail phenomenon in SGD\n   Abstract: The Heavy-Tail Phenomenon in SGD: In recent years, various notions of capacity and complexity have been proposed for characterizing the generalization properties of stochastic gradient descent (SGD) in deep learning. Some of the popular notions that correlate well with the performance on unseen data are (i) the `flatness' of the local minimum found by SGD, which is related to the eigenvalues of the Hessian, (ii) the ratio of the stepsize $\\eta$ to the batch-size $b$, which essentially controls the magnitude of the stochastic gradient noise, and (iii) the `tail-index', which measures the heaviness of the tails of the network weights at convergence. In this paper, we argue that these three seemingly unrelated perspectives for generalization are deeply linked to each other. We claim that depending on the structure of the Hessian of the loss at the minimum, and the choices of the algorithm parameters $\\eta$ and $b$, the SGD iterates will converge to a \\emph{heavy-tailed} stationary distribution. We rigorously prove this claim in the setting of quadratic optimization: we show that even in a simple linear regression problem with independent and identically distributed data whose distribution has finite moments of all order, the iterates can be heavy-tailed with infinite variance. We further characterize the behavior of the tails with respect to algorithm parameters, the dimension, and the curvature. We then translate our results into insights about the behavior of SGD in deep learning. We support our theory with experiments conducted on synthetic data, fully connected, and convolutional neural networks.\n\n4. Title: Towards theoretically understanding why sgd generalizes better than adam in deep learning\n   Abstract: Towards Theoretically Understanding Why SGD Generalizes Better Than ADAM in Deep Learning: It is not clear yet why ADAM-alike adaptive gradient algorithms suffer from worse generalization performance than SGD despite their faster training speed. This work aims to provide understandings on this generalization gap by analyzing their local convergence behaviors. Specifically, we observe the heavy tails of gradient noise in these algorithms. This motivates us to analyze these algorithms through their Levy-driven stochastic differential equations (SDEs) because of the similar convergence behaviors of an algorithm and its SDE. Then we establish the escaping time of these SDEs from a local basin. The result shows that (1) the escaping time of both SGD and ADAM~depends on the Radon measure of the basin positively and the heaviness of gradient noise negatively; (2) for the same basin, SGD enjoys smaller escaping time than ADAM, mainly because (a) the geometry adaptation in ADAM~via adaptively scaling each gradient coordinate well diminishes the anisotropic structure in gradient noise and results in larger Radon measure of a basin; (b) the exponential gradient average in ADAM~smooths its gradient and leads to lighter gradient noise tails than SGD. So SGD is more locally unstable than ADAM~at sharp minima defined as the minima whose local basins have small Radon measure, and can better escape from them to flatter ones with larger Radon measure. As flat minima here which often refer to the minima at flat or asymmetric basins/valleys often generalize better than sharp ones , our result explains the better generalization performance of SGD over ADAM. Finally, experimental results confirm our heavy-tailed gradient noise assumption and theoretical affirmation.\n\n5. Title: Heavy tails in SGD and compressibility of overparametrized neural networks\n   Abstract: Heavy Tails in SGD and Compressibility of Overparametrized Neural Networks: Neural network compression techniques have become increasingly popular as they can drastically reduce the storage and computation requirements for very large networks. Recent empirical studies have illustrated that even simple pruning strategies can be surprisingly effective, and several theoretical studies have shown that compressible networks (in specific senses) should achieve a low generalization error. Yet, a theoretical characterization of the underlying cause that makes the networks amenable to such simple compression schemes is still missing. In this study, we address this fundamental question and reveal that the dynamics of the training algorithm has a key role in obtaining such compressible networks. Focusing our attention on stochastic gradient descent (SGD), our main contribution is to link compressibility to two recently established properties of SGD: (i) as the network size goes to infinity, the system can converge to a mean-field limit, where the network weights behave independently, (ii) for a large step-size/batch-size ratio, the SGD iterates can converge to a heavy-tailed stationary distribution. In the case where these two phenomena occur simultaneously, we prove that the networks are guaranteed to be '$\\ell_p$-compressible', and the compression errors of different pruning techniques (magnitude, singular value, or node pruning) become arbitrarily small as the network size increases. We further prove generalization bounds adapted to our theoretical framework, which indeed confirm that the generalization error will be lower for more compressible networks. Our theory and numerical study on various neural networks show that large step-size/batch-size ratios introduce heavy-tails, which, in combination with overparametrization, result in compressibility.\n\n6. Title: Generalization Bounds using Lower Tail Exponents in Stochastic Optimizers\n   Abstract: Generalization Bounds using Lower Tail Exponents in Stochastic Optimizers: Despite the ubiquitous use of stochastic optimization algorithms in machine learning, the precise impact of these algorithms and their dynamics on generalization performance in realistic non-convex settings is still poorly understood. While recent work has revealed connections between generalization and heavy-tailed behavior in stochastic optimization, this work mainly relied on continuous-time approximations; and a rigorous treatment for the original discrete-time iterations is yet to be performed. To bridge this gap, we present novel bounds linking generalization to the lower tail exponent of the transition kernel associated with the optimizer around a local minimum, in both discrete- and continuous-time settings. To achieve this, we first prove a data- and algorithm-dependent generalization bound in terms of the celebrated Fernique-Talagrand functional applied to the trajectory of the optimizer. Then, we specialize this result by exploiting the Markovian structure of stochastic optimizers, and derive bounds in terms of their (data-dependent) transition kernels. We support our theory with empirical results from a variety of neural networks, showing correlations between generalization error and lower tail exponents.\n\n7. Title: Multiplicative noise and heavy tails in stochastic optimization\n   Abstract: Multiplicative noise and heavy tails in stochastic optimization: Although stochastic optimization is central to modern machine learning, the precise mechanisms underlying its success, and in particular, the precise role of the stochasticity, still remain unclear. Modelling stochastic optimization algorithms as discrete random recurrence relations, we show that multiplicative noise, as it commonly arises due to variance in local rates of convergence, results in heavy-tailed stationary behaviour in the parameters. A detailed analysis is conducted for SGD applied to a simple linear regression problem, followed by theoretical results for a much larger class of models (including non-linear and non-convex) and optimizers (including momentum, Adam, and stochastic Newton), demonstrating that our qualitative results hold much more generally. In each case, we describe dependence on key factors, including step size, batch size, and data variability, all of which exhibit similar qualitative behavior to recent empirical results on state-of-the-art neural network models from computer vision and natural language processing. Furthermore, we empirically demonstrate how multiplicative noise and heavy-tailed structure improve capacity for basin hopping and exploration of non-convex loss surfaces, over commonly-considered stochastic dynamics with only additive noise and light-tailed structure.\n\nThink step-by-step about which candidate best matches the context. Summarize your reasoning, then provide the exact title that should fill the citation. Respond in JSON with fields:\n{\n  \"reasoning\": \"<your step-by-step analysis>\",\n  \"selected_title\": \"<exact candidate title>\"\n}", "response": {"reasoning": "Context: the authors contrast their goal (explain how heavy tails can arise in offline SGD with a finite dataset) with an \"aforementioned theory\" that is being criticized for requiring an infinite-data/continuous-time type assumption. The cited work should therefore be a prominent paper on the heavy-tail phenomenon in SGD that previous literature frequently references and that provides a theoretical explanation of heavy tails in SGD (often via limiting/continuous approximations). Evaluation of candidates: (1) focuses on stability of heavy-tailed SDEs and discretizations (SDE viewpoint) but is about generalization bounds; (2) uses Feller processes and Hausdorff dimension (continuous-time style); (3) directly addresses the heavy-tail phenomenon in SGD, provides rigorous results (quadratic case) and is the widely-cited seminal paper on heavy tails in SGD; (4) analyzes escape times via LÃ©vy-driven SDEs comparing optimizers; (5) links heavy tails to mean-field/infinite-width limits (infinite-size assumption, but focuses on compressibility); (6) develops bounds using tail exponents and stresses discrete vs continuous differences; (7) studies multiplicative noise via discrete random recurrence relations and explains heavy tails from multiplicative noise. Which fits best: the sentence is likely referring to the well-known prior theory on the heavy-tail phenomenon in SGD (commonly cited) that later works contrast with when studying offline/finite-dataset settings. Candidate 3, \"The heavy-tail phenomenon in SGD,\" is the canonical reference for that prior theory and is the most natural fit for the citation in this context.", "selected_title": "The heavy-tail phenomenon in SGD"}, "selected_title": "The heavy-tail phenomenon in SGD", "positive_title": "The heavy-tail phenomenon in SGD", "is_correct": true, "reasoning": "Context: the authors contrast their goal (explain how heavy tails can arise in offline SGD with a finite dataset) with an \"aforementioned theory\" that is being criticized for requiring an infinite-data/continuous-time type assumption. The cited work should therefore be a prominent paper on the heavy-tail phenomenon in SGD that previous literature frequently references and that provides a theoretical explanation of heavy tails in SGD (often via limiting/continuous approximations). Evaluation of candidates: (1) focuses on stability of heavy-tailed SDEs and discretizations (SDE viewpoint) but is about generalization bounds; (2) uses Feller processes and Hausdorff dimension (continuous-time style); (3) directly addresses the heavy-tail phenomenon in SGD, provides rigorous results (quadratic case) and is the widely-cited seminal paper on heavy tails in SGD; (4) analyzes escape times via LÃ©vy-driven SDEs comparing optimizers; (5) links heavy tails to mean-field/infinite-width limits (infinite-size assumption, but focuses on compressibility); (6) develops bounds using tail exponents and stresses discrete vs continuous differences; (7) studies multiplicative noise via discrete random recurrence relations and explains heavy tails from multiplicative noise. Which fits best: the sentence is likely referring to the well-known prior theory on the heavy-tail phenomenon in SGD (commonly cited) that later works contrast with when studying offline/finite-dataset settings. Candidate 3, \"The heavy-tail phenomenon in SGD,\" is the canonical reference for that prior theory and is the most natural fit for the citation in this context."}
{"example_id": "blind_5562", "prompt": "You are an expert citation retrieval system as described in the paper \"Multi-Agent System for Reliable Citation Retrieval\".\nYour goal is to autonomously retrieve, verify, and recommend academic references given a query or document excerpt.\n\nTask:\nGiven a citation context from a scientific paper (where a citation is missing), identify the correct paper from a list of candidates.\nAnalyze the context to understand the specific claim, method, or result being cited.\nThen, evaluate each candidate paper to see if it matches the context.\nFinally, select the best matching paper.\n\nCitation Context:\npixel space} by padding learnable pixels around the input images. Such pixel-level prompt is difficult to learn and the tuned performance is unsatisfied compared to full fine-tuning. Recently, VPT[CITATION] was proposed to insert visual prompts in \\textit{embedding space} into a ViT in a shallow or deep manner where the prompts were inserted into each layer of ViT and learned independently.\n\nCandidate Papers:\n1. Title: Florence: A new foundation model for computer vision\n   Abstract: Florence: A New Foundation Model for Computer Vision: Automated visual understanding of our diverse and open world demands computer vision models to generalize well with minimal customization for specific tasks, similar to human vision. Computer vision foundation models, which are trained on diverse, large-scale dataset and can be adapted to a wide range of downstream tasks, are critical for this mission to solve real-world computer vision applications. While existing vision foundation models such as CLIP, ALIGN, and Wu Dao 2.0 focus mainly on mapping images and textual representations to a cross-modal shared representation, we introduce a new computer vision foundation model, Florence, to expand the representations from coarse (scene) to fine (object), from static (images) to dynamic (videos), and from RGB to multiple modalities (caption, depth). By incorporating universal visual-language representations from Web-scale image-text data, our Florence model can be easily adapted for various computer vision tasks, such as classification, retrieval, object detection, VQA, image caption, video retrieval and action recognition. Moreover, Florence demonstrates outstanding performance in many types of transfer learning: fully sampled fine-tuning, linear probing, few-shot transfer and zero-shot transfer for novel images and objects. All of these properties are critical for our vision foundation model to serve general purpose vision tasks. Florence achieves new state-of-the-art results in majority of 44 representative benchmarks, e.g., ImageNet-1K zero-shot classification with top-1 accuracy of 83.74 and the top-5 accuracy of 97.18, 62.4 mAP on COCO fine tuning, 80.36 on VQA, and 87.8 on Kinetics-600.\n\n2. Title: Test-time prompt tuning for zero-shot generalization in vision-language models\n   Abstract: Test-Time Prompt Tuning for Zero-Shot Generalization in Vision-Language Models: Pre-trained vision-language models (e.g., CLIP) have shown promising zero-shot generalization in many downstream tasks with properly designed text prompts. Instead of relying on hand-engineered prompts, recent works learn prompts using the training data from downstream tasks. While effective, training on domain-specific data reduces a model's generalization capability to unseen new domains. In this work, we propose test-time prompt tuning (TPT), a method that can learn adaptive prompts on the fly with a single test sample. For image classification, TPT optimizes the prompt by minimizing the entropy with confidence selection so that the model has consistent predictions across different augmented views of each test sample. In evaluating generalization to natural distribution shifts, TPT improves the zero-shot top-1 accuracy of CLIP by 3.6% on average, surpassing previous prompt tuning approaches that require additional task-specific training data. In evaluating cross-dataset generalization with unseen categories, TPT performs on par with the state-of-the-art approaches that use additional training data. Project page: https://azshue.github.io/TPT.\n\n3. Title: Continual Few-shot Relation Learning via Embedding Space Regularization and Data Augmentation\n   Abstract: Continual Few-shot Relation Learning via Embedding Space Regularization and Data Augmentation: Existing continual relation learning (CRL) methods rely on plenty of labeled training data for learning a new task, which can be hard to acquire in real scenario as getting large and representative labeled data is often expensive and time-consuming. It is therefore necessary for the model to learn novel relational patterns with very few labeled data while avoiding catastrophic forgetting of previous task knowledge. In this paper, we formulate this challenging yet practical problem as continual few-shot relation learning (CFRL). Based on the finding that learning for new emerging few-shot tasks often results in feature distributions that are incompatible with previous tasks' learned distributions, we propose a novel method based on embedding space regularization and data augmentation. Our method generalizes to new few-shot tasks and avoids catastrophic forgetting of previous tasks by enforcing extra constraints on the relational embeddings and by adding extra {relevant} data in a self-supervised manner. With extensive experiments we demonstrate that our method can significantly outperform previous state-of-the-art methods in CFRL task settings.\n\n4. Title: Learning to prompt for vision-language models\n   Abstract: Learning to Prompt for Vision-Language Models: Large pre-trained vision-language models like CLIP have shown great potential in learning representations that are transferable across a wide range of downstream tasks. Different from the traditional representation learning that is based mostly on discretized labels, vision-language pre-training aligns images and texts in a common feature space, which allows zero-shot transfer to a downstream task via prompting, i.e., classification weights are synthesized from natural language describing classes of interest. In this work, we show that a major challenge for deploying such models in practice is prompt engineering, which requires domain expertise and is extremely time-consuming -- one needs to spend a significant amount of time on words tuning since a slight change in wording could have a huge impact on performance. Inspired by recent advances in prompt learning research in natural language processing (NLP), we propose Context Optimization (CoOp), a simple approach specifically for adapting CLIP-like vision-language models for downstream image recognition. Concretely, CoOp models a prompt's context words with learnable vectors while the entire pre-trained parameters are kept fixed. To handle different image recognition tasks, we provide two implementations of CoOp: unified context and class-specific context. Through extensive experiments on 11 datasets, we demonstrate that CoOp requires as few as one or two shots to beat hand-crafted prompts with a decent margin and is able to gain significant improvements over prompt engineering with more shots, e.g., with 16 shots the average gain is around 15% (with the highest reaching over 45%). Despite being a learning-based approach, CoOp achieves superb domain generalization performance compared with the zero-shot model using hand-crafted prompts.\n\n5. Title: Prefix-Tuning: Optimizing Continuous Prompts for Generation\n   Abstract: Prefix-Tuning: Optimizing Continuous Prompts for Generation: Fine-tuning is the de facto way to leverage large pretrained language models to perform downstream tasks. However, it modifies all the language model parameters and therefore necessitates storing a full copy for each task. In this paper, we propose prefix-tuning, a lightweight alternative to fine-tuning for natural language generation tasks, which keeps language model parameters frozen, but optimizes a small continuous task-specific vector (called the prefix). Prefix-tuning draws inspiration from prompting, allowing subsequent tokens to attend to this prefix as if it were \"virtual tokens\". We apply prefix-tuning to GPT-2 for table-to-text generation and to BART for summarization. We find that by learning only 0.1\\% of the parameters, prefix-tuning obtains comparable performance in the full data setting, outperforms fine-tuning in low-data settings, and extrapolates better to examples with topics unseen during training.\n\n6. Title: Attention is all you need\n   Abstract: Attention Is All You Need: The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.\n\n7. Title: Visual Prompt Tuning\n   Abstract: Visual Prompt Tuning: The current modus operandi in adapting pre-trained models involves updating all the backbone parameters, ie, full fine-tuning. This paper introduces Visual Prompt Tuning (VPT) as an efficient and effective alternative to full fine-tuning for large-scale Transformer models in vision. Taking inspiration from recent advances in efficiently tuning large language models, VPT introduces only a small amount (less than 1% of model parameters) of trainable parameters in the input space while keeping the model backbone frozen. Via extensive experiments on a wide variety of downstream recognition tasks, we show that VPT achieves significant performance gains compared to other parameter efficient tuning protocols. Most importantly, VPT even outperforms full fine-tuning in many cases across model capacities and training data scales, while reducing per-task storage cost.\n\n8. Title: Cris: Clip-driven referring image segmentation\n   Abstract: CRIS: CLIP-Driven Referring Image Segmentation: Referring image segmentation aims to segment a referent via a natural linguistic expression.Due to the distinct data properties between text and image, it is challenging for a network to well align text and pixel-level features. Existing approaches use pretrained models to facilitate learning, yet separately transfer the language/vision knowledge from pretrained models, ignoring the multi-modal corresponding information. Inspired by the recent advance in Contrastive Language-Image Pretraining (CLIP), in this paper, we propose an end-to-end CLIP-Driven Referring Image Segmentation framework (CRIS). To transfer the multi-modal knowledge effectively, CRIS resorts to vision-language decoding and contrastive learning for achieving the text-to-pixel alignment. More specifically, we design a vision-language decoder to propagate fine-grained semantic information from textual representations to each pixel-level activation, which promotes consistency between the two modalities. In addition, we present text-to-pixel contrastive learning to explicitly enforce the text feature similar to the related pixel-level features and dissimilar to the irrelevances. The experimental results on three benchmark datasets demonstrate that our proposed framework significantly outperforms the state-of-the-art performance without any post-processing. The code will be released.\n\n9. Title: Prompt-aligned Gradient for Prompt Tuning\n   Abstract: Prompt-aligned Gradient for Prompt Tuning: Thanks to the large pre-trained vision-language models (VLMs) like CLIP, we can craft a zero-shot classifier by \"prompt\", e.g., the confidence score of an image being \"[CLASS]\" can be obtained by using the VLM provided similarity measure between the image and the prompt sentence \"a photo of a [CLASS]\". Therefore, prompt shows a great potential for fast adaptation of VLMs to downstream tasks if we fine-tune the prompt-based similarity measure. However, we find a common failure that improper fine-tuning may not only undermine the prompt's inherent prediction for the task-related classes, but also for other classes in the VLM vocabulary. Existing methods still address this problem by using traditional anti-overfitting techniques such as early stopping and data augmentation, which lack a principled solution specific to prompt. We present Prompt-aligned Gradient, dubbed ProGrad, to prevent prompt tuning from forgetting the the general knowledge learned from VLMs. In particular, ProGrad only updates the prompt whose gradient is aligned (or non-conflicting) to the \"general direction\", which is represented as the gradient of the KL loss of the pre-defined prompt prediction. Extensive experiments demonstrate the stronger few-shot generalization ability of ProGrad over state-of-the-art prompt tuning methods. Codes are available at https://github.com/BeierZhu/Prompt-align.\n\n10. Title: Conditional prompt learning for vision-language models\n   Abstract: Conditional Prompt Learning for Vision-Language Models: With the rise of powerful pre-trained vision-language models like CLIP, it becomes essential to investigate ways to adapt these models to downstream datasets. A recently proposed method named Context Optimization (CoOp) introduces the concept of prompt learning -- a recent trend in NLP -- to the vision domain for adapting pre-trained vision-language models. Specifically, CoOp turns context words in a prompt into a set of learnable vectors and, with only a few labeled images for learning, can achieve huge improvements over intensively-tuned manual prompts. In our study we identify a critical problem of CoOp: the learned context is not generalizable to wider unseen classes within the same dataset, suggesting that CoOp overfits base classes observed during training. To address the problem, we propose Conditional Context Optimization (CoCoOp), which extends CoOp by further learning a lightweight neural network to generate for each image an input-conditional token (vector). Compared to CoOp's static prompts, our dynamic prompts adapt to each instance and are thus less sensitive to class shift. Extensive experiments show that CoCoOp generalizes much better than CoOp to unseen classes, even showing promising transferability beyond a single dataset; and yields stronger domain generalization performance as well. Code is available at https://github.com/KaiyangZhou/CoOp.\n\nThink step-by-step about which candidate best matches the context. Summarize your reasoning, then provide the exact title that should fill the citation. Respond in JSON with fields:\n{\n  \"reasoning\": \"<your step-by-step analysis>\",\n  \"selected_title\": \"<exact candidate title>\"\n}", "response": {"reasoning": "The citation context contrasts pixel-space prompts (padding learnable pixels) with a recent method called VPT that inserts visual prompts in the embedding space of a ViT, mentioning shallow vs deep variants where prompts are inserted into each Transformer layer and learned independently. I inspected the candidates: most concern vision-language prompt tuning (CoOp, CoCoOp, ProGrad, TPT) or general foundation models (Florence) or unrelated topics (continual relation learning, Prefix-Tuning, Transformer). Candidate 7 is titled \"Visual Prompt Tuning\" and its abstract explicitly introduces VPT as an efficient alternative for adapting pretrained vision Transformers, describing inserting a small set of trainable parameters in the input space and variants across layers â€” matching the described shallow/deep prompt insertion into ViT layer embeddings. No other candidate matches this exact method or acronym. Therefore candidate 7 is the correct citation.", "selected_title": "Visual Prompt Tuning"}, "selected_title": "Visual Prompt Tuning", "positive_title": "Visual Prompt Tuning", "is_correct": true, "reasoning": "The citation context contrasts pixel-space prompts (padding learnable pixels) with a recent method called VPT that inserts visual prompts in the embedding space of a ViT, mentioning shallow vs deep variants where prompts are inserted into each Transformer layer and learned independently. I inspected the candidates: most concern vision-language prompt tuning (CoOp, CoCoOp, ProGrad, TPT) or general foundation models (Florence) or unrelated topics (continual relation learning, Prefix-Tuning, Transformer). Candidate 7 is titled \"Visual Prompt Tuning\" and its abstract explicitly introduces VPT as an efficient alternative for adapting pretrained vision Transformers, describing inserting a small set of trainable parameters in the input space and variants across layers â€” matching the described shallow/deep prompt insertion into ViT layer embeddings. No other candidate matches this exact method or acronym. Therefore candidate 7 is the correct citation."}
{"example_id": "blind_5240", "prompt": "You are an expert citation retrieval system as described in the paper \"Multi-Agent System for Reliable Citation Retrieval\".\nYour goal is to autonomously retrieve, verify, and recommend academic references given a query or document excerpt.\n\nTask:\nGiven a citation context from a scientific paper (where a citation is missing), identify the correct paper from a list of candidates.\nAnalyze the context to understand the specific claim, method, or result being cited.\nThen, evaluate each candidate paper to see if it matches the context.\nFinally, select the best matching paper.\n\nCitation Context:\nict binary mask for each instance inside the bounding box. Specifically, in the latter step, a pooling operation, \\eg, RoIAlign, is used to extract instance features from the feature pyramid[CITATION], then pixel-wise classification is performed based on the output features of the mask head. Despite the strong abilities provided by the powerful object detector to locate and distinguish\n\nCandidate Papers:\n1. Title: Faster R-CNN: Towards real-time object detection with region proposal networks\n   Abstract: Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks: State-of-the-art object detection networks depend on region proposal algorithms to hypothesize object locations. Advances like SPPnet and Fast R-CNN have reduced the running time of these detection networks, exposing region proposal computation as a bottleneck. In this work, we introduce a Region Proposal Network (RPN) that shares full-image convolutional features with the detection network, thus enabling nearly cost-free region proposals. An RPN is a fully convolutional network that simultaneously predicts object bounds and objectness scores at each position. The RPN is trained end-to-end to generate high-quality region proposals, which are used by Fast R-CNN for detection. We further merge RPN and Fast R-CNN into a single network by sharing their convolutional features---using the recently popular terminology of neural networks with 'attention' mechanisms, the RPN component tells the unified network where to look. For the very deep VGG-16 model, our detection system has a frame rate of 5fps (including all steps) on a GPU, while achieving state-of-the-art object detection accuracy on PASCAL VOC 2007, 2012, and MS COCO datasets with only 300 proposals per image. In ILSVRC and COCO 2015 competitions, Faster R-CNN and RPN are the foundations of the 1st-place winning entries in several tracks. Code has been made publicly available.\n\n2. Title: Feature pyramid networks for object detection\n   Abstract: Feature Pyramid Networks for Object Detection: Feature pyramids are a basic component in recognition systems for detecting objects at different scales. But recent deep learning object detectors have avoided pyramid representations, in part because they are compute and memory intensive. In this paper, we exploit the inherent multi-scale, pyramidal hierarchy of deep convolutional networks to construct feature pyramids with marginal extra cost. A top-down architecture with lateral connections is developed for building high-level semantic feature maps at all scales. This architecture, called a Feature Pyramid Network (FPN), shows significant improvement as a generic feature extractor in several applications. Using FPN in a basic Faster R-CNN system, our method achieves state-of-the-art single-model results on the COCO detection benchmark without bells and whistles, surpassing all existing single-model entries including those from the COCO 2016 challenge winners. In addition, our method can run at 5 FPS on a GPU and thus is a practical and accurate solution to multi-scale object detection. Code will be made publicly available.\n\n3. Title: SegFix: Model-agnostic boundary refinement for segmentation\n   Abstract: SegFix: Model-Agnostic Boundary Refinement for Segmentation: We present a model-agnostic post-processing scheme to improve the boundary quality for the segmentation result that is generated by any existing segmentation model. Motivated by the empirical observation that the label predictions of interior pixels are more reliable, we propose to replace the originally unreliable predictions of boundary pixels by the predictions of interior pixels. Our approach processes only the input image through two steps: (i) localize the boundary pixels and (ii) identify the corresponding interior pixel for each boundary pixel. We build the correspondence by learning a direction away from the boundary pixel to an interior pixel. Our method requires no prior information of the segmentation models and achieves nearly real-time speed. We empirically verify that our SegFix consistently reduces the boundary errors for segmentation results generated from various state-of-the-art models on Cityscapes, ADE20K and GTA5. Code is available at: https://github.com/openseg-group/openseg.pytorch.\n\n4. Title: Mask R-CNN\n   Abstract: Mask R-CNN: We present a conceptually simple, flexible, and general framework for object instance segmentation. Our approach efficiently detects objects in an image while simultaneously generating a high-quality segmentation mask for each instance. The method, called Mask R-CNN, extends Faster R-CNN by adding a branch for predicting an object mask in parallel with the existing branch for bounding box recognition. Mask R-CNN is simple to train and adds only a small overhead to Faster R-CNN, running at 5 fps. Moreover, Mask R-CNN is easy to generalize to other tasks, e.g., allowing us to estimate human poses in the same framework. We show top results in all three tracks of the COCO suite of challenges, including instance segmentation, bounding-box object detection, and person keypoint detection. Without bells and whistles, Mask R-CNN outperforms all existing, single-model entries on every task, including the COCO 2016 challenge winners. We hope our simple and effective approach will serve as a solid baseline and help ease future research in instance-level recognition. Code has been made available at: https://github.com/facebookresearch/Detectron\n\n5. Title: SOLO: Segmenting objects by locations\n   Abstract: SOLO: Segmenting Objects by Locations: We present a new, embarrassingly simple approach to instance segmentation in images. Compared to many other dense prediction tasks, e.g., semantic segmentation, it is the arbitrary number of instances that have made instance segmentation much more challenging. In order to predict a mask for each instance, mainstream approaches either follow the 'detect-thensegment' strategy as used by Mask R-CNN, or predict category masks first then use clustering techniques to group pixels into individual instances. We view the task of instance segmentation from a completely new perspective by introducing the notion of \"instance categories\", which assigns categories to each pixel within an instance according to the instance's location and size, thus nicely converting instance mask segmentation into a classification-solvable problem. Now instance segmentation is decomposed into two classification tasks. We demonstrate a much simpler and flexible instance segmentation framework with strong performance, achieving on par accuracy with Mask R-CNN and outperforming recent singleshot instance segmenters in accuracy. We hope that this very simple and strong framework can serve as a baseline for many instance-level recognition tasks besides instance segmentation.\n\n6. Title: Hybrid task cascade for instance segmentation\n   Abstract: Hybrid Task Cascade for Instance Segmentation: Cascade is a classic yet powerful architecture that has boosted performance on various tasks. However, how to introduce cascade to instance segmentation remains an open question. A simple combination of Cascade R-CNN and Mask R-CNN only brings limited gain. In exploring a more effective approach, we find that the key to a successful instance segmentation cascade is to fully leverage the reciprocal relationship between detection and segmentation. In this work, we propose a new framework, Hybrid Task Cascade (HTC), which differs in two important aspects: (1) instead of performing cascaded refinement on these two tasks separately, it interweaves them for a joint multi-stage processing; (2) it adopts a fully convolutional branch to provide spatial context, which can help distinguishing hard foreground from cluttered background. Overall, this framework can learn more discriminative features progressively while integrating complementary features together in each stage. Without bells and whistles, a single HTC obtains 38.4 and 1.5 improvement over a strong Cascade Mask R-CNN baseline on MSCOCO dataset. Moreover, our overall system achieves 48.6 mask AP on the test-challenge split, ranking 1st in the COCO 2018 Challenge Object Detection Task. Code is available at: https://github.com/open-mmlab/mmdetection.\n\n7. Title: Boundary-preserving {Mask R-CNN\n   Abstract: Boundary-preserving Mask R-CNN: Tremendous efforts have been made to improve mask localization accuracy in instance segmentation. Modern instance segmentation methods relying on fully convolutional networks perform pixel-wise classification, which ignores object boundaries and shapes, leading coarse and indistinct mask prediction results and imprecise localization. To remedy these problems, we propose a conceptually simple yet effective Boundary-preserving Mask R-CNN (BMask R-CNN) to leverage object boundary information to improve mask localization accuracy. BMask R-CNN contains a boundary-preserving mask head in which object boundary and mask are mutually learned via feature fusion blocks. As a result, the predicted masks are better aligned with object boundaries. Without bells and whistles, BMask R-CNN outperforms Mask R-CNN by a considerable margin on the COCO dataset; in the Cityscapes dataset, there are more accurate boundary groundtruths available, so that BMask R-CNN obtains remarkable improvements over Mask R-CNN. Besides, it is not surprising to observe that BMask R-CNN obtains more obvious improvement when the evaluation criterion requires better localization (e.g., AP$_{75}$) as shown in Fig.1. Code and models are available at \\url{https://github.com/hustvl/BMaskR-CNN}.\n\n8. Title: PointRend: Image segmentation as rendering\n   Abstract: PointRend: Image Segmentation as Rendering: We present a new method for efficient high-quality image segmentation of objects and scenes. By analogizing classical computer graphics methods for efficient rendering with over- and undersampling challenges faced in pixel labeling tasks, we develop a unique perspective of image segmentation as a rendering problem. From this vantage, we present the PointRend (Point-based Rendering) neural network module: a module that performs point-based segmentation predictions at adaptively selected locations based on an iterative subdivision algorithm. PointRend can be flexibly applied to both instance and semantic segmentation tasks by building on top of existing state-of-the-art models. While many concrete implementations of the general idea are possible, we show that a simple design already achieves excellent results. Qualitatively, PointRend outputs crisp object boundaries in regions that are over-smoothed by previous methods. Quantitatively, PointRend yields significant gains on COCO and Cityscapes, for both instance and semantic segmentation. PointRend's efficiency enables output resolutions that are otherwise impractical in terms of memory or computation compared to existing approaches. Code has been made available at https://github.com/facebookresearch/detectron2/tree/master/projects/PointRend.\n\nThink step-by-step about which candidate best matches the context. Summarize your reasoning, then provide the exact title that should fill the citation. Respond in JSON with fields:\n{\n  \"reasoning\": \"<your step-by-step analysis>\",\n  \"selected_title\": \"<exact candidate title>\"\n}", "response": {"reasoning": "The sentence refers specifically to extracting instance features â€œfrom the feature pyramidâ€ using a pooling operation such as RoIAlign and then performing pixel-wise classification with a mask head. The key phrase to match is â€œfeature pyramidâ€, which is the central contribution of the Feature Pyramid Networks (FPN) paper (candidate 2). While Mask R-CNN (candidate 4) introduced RoIAlign and the mask head, Mask R-CNN typically uses an FPN as the backbone in practice but does not itself introduce the feature-pyramid architecture. Other candidates (Faster R-CNN, SegFix, SOLO, HTC, PointRend, BMask R-CNN) do not primarily introduce the feature pyramid concept. Therefore the best match for a citation immediately following â€œfeature pyramidâ€ is the FPN paper.", "selected_title": "Feature pyramid networks for object detection"}, "selected_title": "Feature pyramid networks for object detection", "positive_title": "Feature pyramid networks for object detection", "is_correct": true, "reasoning": "The sentence refers specifically to extracting instance features â€œfrom the feature pyramidâ€ using a pooling operation such as RoIAlign and then performing pixel-wise classification with a mask head. The key phrase to match is â€œfeature pyramidâ€, which is the central contribution of the Feature Pyramid Networks (FPN) paper (candidate 2). While Mask R-CNN (candidate 4) introduced RoIAlign and the mask head, Mask R-CNN typically uses an FPN as the backbone in practice but does not itself introduce the feature-pyramid architecture. Other candidates (Faster R-CNN, SegFix, SOLO, HTC, PointRend, BMask R-CNN) do not primarily introduce the feature pyramid concept. Therefore the best match for a citation immediately following â€œfeature pyramidâ€ is the FPN paper."}
{"example_id": "blind_12385", "prompt": "You are an expert citation retrieval system as described in the paper \"Multi-Agent System for Reliable Citation Retrieval\".\nYour goal is to autonomously retrieve, verify, and recommend academic references given a query or document excerpt.\n\nTask:\nGiven a citation context from a scientific paper (where a citation is missing), identify the correct paper from a list of candidates.\nAnalyze the context to understand the specific claim, method, or result being cited.\nThen, evaluate each candidate paper to see if it matches the context.\nFinally, select the best matching paper.\n\nCitation Context:\nw-rank adaptation, we chose to build on compact architectures, based on reasonably-sized pre-trained language models (PLMs) such as BERT or BART, as considered in popular recent works TAPEX[CITATION] or OmniTab. Beyond scalability, such architectures, which do not require specific prompt design that could bias conclusions, offer easier comparison opportunities.\\footnote{We also note t\n\nCandidate Papers:\n1. Title: Qlora: Efficient finetuning of quantized llms\n   Abstract: QLoRA: Efficient Finetuning of Quantized LLMs: We present QLoRA, an efficient finetuning approach that reduces memory usage enough to finetune a 65B parameter model on a single 48GB GPU while preserving full 16-bit finetuning task performance. QLoRA backpropagates gradients through a frozen, 4-bit quantized pretrained language model into Low Rank Adapters~(LoRA). Our best model family, which we name Guanaco, outperforms all previous openly released models on the Vicuna benchmark, reaching 99.3% of the performance level of ChatGPT while only requiring 24 hours of finetuning on a single GPU. QLoRA introduces a number of innovations to save memory without sacrificing performance: (a) 4-bit NormalFloat (NF4), a new data type that is information theoretically optimal for normally distributed weights (b) double quantization to reduce the average memory footprint by quantizing the quantization constants, and (c) paged optimziers to manage memory spikes. We use QLoRA to finetune more than 1,000 models, providing a detailed analysis of instruction following and chatbot performance across 8 instruction datasets, multiple model types (LLaMA, T5), and model scales that would be infeasible to run with regular finetuning (e.g. 33B and 65B parameter models). Our results show that QLoRA finetuning on a small high-quality dataset leads to state-of-the-art results, even when using smaller models than the previous SoTA. We provide a detailed analysis of chatbot performance based on both human and GPT-4 evaluations showing that GPT-4 evaluations are a cheap and reasonable alternative to human evaluation. Furthermore, we find that current chatbot benchmarks are not trustworthy to accurately evaluate the performance levels of chatbots. A lemon-picked analysis demonstrates where Guanaco fails compared to ChatGPT. We release all of our models and code, including CUDA kernels for 4-bit training.\n\n2. Title: FORTAP: Using formulas for numerical-reasoning-aware table pretraining\n   Abstract: FORTAP: Using Formulas for Numerical-Reasoning-Aware Table Pretraining: Tables store rich numerical data, but numerical reasoning over tables is still a challenge. In this paper, we find that the spreadsheet formula, which performs calculations on numerical values in tables, is naturally a strong supervision of numerical reasoning. More importantly, large amounts of spreadsheets with expert-made formulae are available on the web and can be obtained easily. FORTAP is the first method for numerical-reasoning-aware table pretraining by leveraging large corpus of spreadsheet formulae. We design two formula pretraining tasks to explicitly guide FORTAP to learn numerical reference and calculation in semi-structured tables. FORTAP achieves state-of-the-art results on two representative downstream tasks, cell type classification and formula prediction, showing great potential of numerical-reasoning-aware pretraining.\n\n3. Title: OmniTab: Pretraining with natural and synthetic data for few-shot table-based question answering\n   Abstract: OmniTab: Pretraining with Natural and Synthetic Data for Few-shot Table-based Question Answering: The information in tables can be an important complement to text, making table-based question answering (QA) systems of great value. The intrinsic complexity of handling tables often adds an extra burden to both model design and data annotation. In this paper, we aim to develop a simple table-based QA model with minimal annotation effort. Motivated by the fact that table-based QA requires both alignment between questions and tables and the ability to perform complicated reasoning over multiple table elements, we propose an omnivorous pretraining approach that consumes both natural and synthetic data to endow models with these respective abilities. Specifically, given freely available tables, we leverage retrieval to pair them with relevant natural sentences for mask-based pretraining, and synthesize NL questions by converting SQL sampled from tables for pretraining with a QA loss. We perform extensive experiments in both few-shot and full settings, and the results clearly demonstrate the superiority of our model OmniTab, with the best multitasking approach achieving an absolute gain of 16.2% and 2.7% in 128-shot and full settings respectively, also establishing a new state-of-the-art on WikiTableQuestions. Detailed ablations and analyses reveal different characteristics of natural and synthetic data, shedding light on future directions in omnivorous pretraining. Code, pretraining data, and pretrained models are available at https://github.com/jzbjyb/OmniTab.\n\n4. Title: TaPas: Weakly supervised table parsing via pre-training\n   Abstract: TAPAS: Weakly Supervised Table Parsing via Pre-training: Answering natural language questions over tables is usually seen as a semantic parsing task. To alleviate the collection cost of full logical forms, one popular approach focuses on weak supervision consisting of denotations instead of logical forms. However, training semantic parsers from weak supervision poses difficulties, and in addition, the generated logical forms are only used as an intermediate step prior to retrieving the denotation. In this paper, we present TAPAS, an approach to question answering over tables without generating logical forms. TAPAS trains from weak supervision, and predicts the denotation by selecting table cells and optionally applying a corresponding aggregation operator to such selection. TAPAS extends BERT's architecture to encode tables as input, initializes from an effective joint pre-training of text segments and tables crawled from Wikipedia, and is trained end-to-end. We experiment with three different semantic parsing datasets, and find that TAPAS outperforms or rivals semantic parsing models by improving state-of-the-art accuracy on SQA from 55.1 to 67.2 and performing on par with the state-of-the-art on WIKISQL and WIKITQ, but with a simpler model architecture. We additionally find that transfer learning, which is trivial in our setting, from WIKISQL to WIKITQ, yields 48.7 accuracy, 4.2 points above the state-of-the-art.\n\n5. Title: A discrete hard EM approach for weakly supervised question answering\n   Abstract: A Discrete Hard EM Approach for Weakly Supervised Question Answering: Many question answering (QA) tasks only provide weak supervision for how the answer should be computed. For example, TriviaQA answers are entities that can be mentioned multiple times in supporting documents, while DROP answers can be computed by deriving many different equations from numbers in the reference text. In this paper, we show it is possible to convert such tasks into discrete latent variable learning problems with a precomputed, task-specific set of possible \"solutions\" (e.g. different mentions or equations) that contains one correct option. We then develop a hard EM learning scheme that computes gradients relative to the most likely solution at each update. Despite its simplicity, we show that this approach significantly outperforms previous methods on six QA tasks, including absolute gains of 2--10%, and achieves the state-of-the-art on five of them. Using hard updates instead of maximizing marginal likelihood is key to these results as it encourages the model to find the one correct answer, which we show through detailed qualitative analysis.\n\n6. Title: TAPEX: Table pre-training via learning a neural SQL executor\n   Abstract: TAPEX: Table Pre-training via Learning a Neural SQL Executor: Recent progress in language model pre-training has achieved a great success via leveraging large-scale unstructured textual data. However, it is still a challenge to apply pre-training on structured tabular data due to the absence of large-scale high-quality tabular data. In this paper, we propose TAPEX to show that table pre-training can be achieved by learning a neural SQL executor over a synthetic corpus, which is obtained by automatically synthesizing executable SQL queries and their execution outputs. TAPEX addresses the data scarcity challenge via guiding the language model to mimic a SQL executor on the diverse, large-scale and high-quality synthetic corpus. We evaluate TAPEX on four benchmark datasets. Experimental results demonstrate that TAPEX outperforms previous table pre-training approaches by a large margin and achieves new state-of-the-art results on all of them. This includes the improvements on the weakly-supervised WikiSQL denotation accuracy to 89.5% (+2.3%), the WikiTableQuestions denotation accuracy to 57.5% (+4.8%), the SQA denotation accuracy to 74.5% (+3.5%), and the TabFact accuracy to 84.2% (+3.2%). To our knowledge, this is the first work to exploit table pre-training via synthetic executable programs and to achieve new state-of-the-art results on various downstream tasks. Our code can be found at https://github.com/microsoft/Table-Pretraining.\n\n7. Title: Lora: Low-rank adaptation of large language models\n   Abstract: LoRA: Low-Rank Adaptation of Large Language Models: An important paradigm of natural language processing consists of large-scale pre-training on general domain data and adaptation to particular tasks or domains. As we pre-train larger models, full fine-tuning, which retrains all model parameters, becomes less feasible. Using GPT-3 175B as an example -- deploying independent instances of fine-tuned models, each with 175B parameters, is prohibitively expensive. We propose Low-Rank Adaptation, or LoRA, which freezes the pre-trained model weights and injects trainable rank decomposition matrices into each layer of the Transformer architecture, greatly reducing the number of trainable parameters for downstream tasks. Compared to GPT-3 175B fine-tuned with Adam, LoRA can reduce the number of trainable parameters by 10,000 times and the GPU memory requirement by 3 times. LoRA performs on-par or better than fine-tuning in model quality on RoBERTa, DeBERTa, GPT-2, and GPT-3, despite having fewer trainable parameters, a higher training throughput, and, unlike adapters, no additional inference latency. We also provide an empirical investigation into rank-deficiency in language model adaptation, which sheds light on the efficacy of LoRA. We release a package that facilitates the integration of LoRA with PyTorch models and provide our implementations and model checkpoints for RoBERTa, DeBERTa, and GPT-2 at https://github.com/microsoft/LoRA.\n\n8. Title: TAT-QA: A question answering benchmark on a hybrid of tabular and textual content in finance\n   Abstract: TAT-QA: A Question Answering Benchmark on a Hybrid of Tabular and Textual Content in Finance: Hybrid data combining both tabular and textual content (e.g., financial reports) are quite pervasive in the real world. However, Question Answering (QA) over such hybrid data is largely neglected in existing research. In this work, we extract samples from real financial reports to build a new large-scale QA dataset containing both Tabular And Textual data, named TAT-QA, where numerical reasoning is usually required to infer the answer, such as addition, subtraction, multiplication, division, counting, comparison/sorting, and the compositions. We further propose a novel QA model termed TAGOP, which is capable of reasoning over both tables and text. It adopts sequence tagging to extract relevant cells from the table along with relevant spans from the text to infer their semantics, and then applies symbolic reasoning over them with a set of aggregation operators to arrive at the final answer. TAGOPachieves 58.0% inF1, which is an 11.1% absolute increase over the previous best baseline model, according to our experiments on TAT-QA. But this result still lags far behind performance of expert human, i.e.90.8% in F1. It is demonstrated that our TAT-QA is very challenging and can serve as a benchmark for training and testing powerful QA models that address hybrid form data.\n\n9. Title: Time travel in llms: Tracing data contamination in large language models\n   Abstract: Time Travel in LLMs: Tracing Data Contamination in Large Language Models: Data contamination, i.e., the presence of test data from downstream tasks in the training data of large language models (LLMs), is a potential major issue in measuring LLMs' real effectiveness on other tasks. We propose a straightforward yet effective method for identifying data contamination within LLMs. At its core, our approach starts by identifying potential contamination at the instance level; using this information, our approach then assesses wider contamination at the partition level. To estimate contamination of individual instances, we employ \"guided instruction:\" a prompt consisting of the dataset name, partition type, and the random-length initial segment of a reference instance, asking the LLM to complete it. An instance is flagged as contaminated if the LLM's output either exactly or nearly matches the latter segment of the reference. To understand if an entire partition is contaminated, we propose two ideas. The first idea marks a dataset partition as contaminated if the average overlap score with the reference instances (as measured by ROUGE-L or BLEURT) is statistically significantly better with the completions from guided instruction compared to a \"general instruction\" that does not include the dataset and partition name. The second idea marks a dataset partition as contaminated if a classifier based on GPT-4 with few-shot in-context learning prompt marks multiple generated completions as exact/near-exact matches of the corresponding reference instances. Our best method achieves an accuracy between 92% and 100% in detecting if an LLM is contaminated with seven datasets, containing train and test/validation partitions, when contrasted with manual evaluation by human experts. Further, our findings indicate that GPT-4 is contaminated with AG News, WNLI, and XSum datasets.\n\n10. Title: Seq2sql: Generating structured queries from natural language using reinforcement learning\n   Abstract: Seq2SQL: Generating Structured Queries from Natural Language using Reinforcement Learning: A significant amount of the world's knowledge is stored in relational databases. However, the ability for users to retrieve facts from a database is limited due to a lack of understanding of query languages such as SQL. We propose Seq2SQL, a deep neural network for translating natural language questions to corresponding SQL queries. Our model leverages the structure of SQL queries to significantly reduce the output space of generated queries. Moreover, we use rewards from in-the-loop query execution over the database to learn a policy to generate unordered parts of the query, which we show are less suitable for optimization via cross entropy loss. In addition, we will publish WikiSQL, a dataset of 80654 hand-annotated examples of questions and SQL queries distributed across 24241 tables from Wikipedia. This dataset is required to train our model and is an order of magnitude larger than comparable datasets. By applying policy-based reinforcement learning with a query execution environment to WikiSQL, our model Seq2SQL outperforms attentional sequence to sequence models, improving execution accuracy from 35.9% to 59.4% and logical form accuracy from 23.4% to 48.3%.\n\nThink step-by-step about which candidate best matches the context. Summarize your reasoning, then provide the exact title that should fill the citation. Respond in JSON with fields:\n{\n  \"reasoning\": \"<your step-by-step analysis>\",\n  \"selected_title\": \"<exact candidate title>\"\n}", "response": {"reasoning": "The citation context names recent table-focused works that use compact, reasonably-sized pre-trained language models (PLMs) such as BERT or BART, and lists â€œTAPEX â€¦ or OmniTabâ€ as examples. From the candidates, both OmniTab (candidate 3) and TAPEX (candidate 6) are about table pretraining/QA. Candidate 6 (TAPEX) explicitly frames table pre-training for neural SQL execution and is a recent, popular work that builds on PLM pretraining (it is known to use seq2seq PLMs such as BART/T5 as the backbone), matching the contextâ€™s reference to compact PLM-based architectures. OmniTab is also relevant but the TAPEX paper more directly corresponds to the described PLM-based table pretraining example. Therefore the correct citation is TAPEX (candidate 6).", "selected_title": "TAPEX: Table Pre-training via Learning a Neural SQL Executor"}, "selected_title": "TAPEX: Table Pre-training via Learning a Neural SQL Executor", "positive_title": "TAPEX: Table pre-training via learning a neural SQL executor", "is_correct": true, "reasoning": "The citation context names recent table-focused works that use compact, reasonably-sized pre-trained language models (PLMs) such as BERT or BART, and lists â€œTAPEX â€¦ or OmniTabâ€ as examples. From the candidates, both OmniTab (candidate 3) and TAPEX (candidate 6) are about table pretraining/QA. Candidate 6 (TAPEX) explicitly frames table pre-training for neural SQL execution and is a recent, popular work that builds on PLM pretraining (it is known to use seq2seq PLMs such as BART/T5 as the backbone), matching the contextâ€™s reference to compact PLM-based architectures. OmniTab is also relevant but the TAPEX paper more directly corresponds to the described PLM-based table pretraining example. Therefore the correct citation is TAPEX (candidate 6)."}
{"example_id": "blind_15139", "prompt": "You are an expert citation retrieval system as described in the paper \"Multi-Agent System for Reliable Citation Retrieval\".\nYour goal is to autonomously retrieve, verify, and recommend academic references given a query or document excerpt.\n\nTask:\nGiven a citation context from a scientific paper (where a citation is missing), identify the correct paper from a list of candidates.\nAnalyze the context to understand the specific claim, method, or result being cited.\nThen, evaluate each candidate paper to see if it matches the context.\nFinally, select the best matching paper.\n\nCitation Context:\n>, objects, or scene graph) via pre-trained image encoder (object detector/classifier). Then, a series of innovations that employ visual attention over these fine-grained semantics[CITATION] are present to strengthen vision-language interaction. However, the capability of semantic comprehending in pre-trained detector/classifier is severely limited by the pre-defined semantic/class label\n\nCandidate Papers:\n1. Title: Attention on attention for image captioning\n   Abstract: Attention on Attention for Image Captioning: Attention mechanisms are widely used in current encoder/decoder frameworks of image captioning, where a weighted average on encoded vectors is generated at each time step to guide the caption decoding process. However, the decoder has little idea of whether or how well the attended vector and the given attention query are related, which could make the decoder give misled results. In this paper, we propose an Attention on Attention (AoA) module, which extends the conventional attention mechanisms to determine the relevance between attention results and queries. AoA first generates an information vector and an attention gate using the attention result and the current context, then adds another attention by applying element-wise multiplication to them and finally obtains the attended information, the expected useful knowledge. We apply AoA to both the encoder and the decoder of our image captioning model, which we name as AoA Network (AoANet). Experiments show that AoANet outperforms all previously published methods and achieves a new state-of-the-art performance of 129.8 CIDEr-D score on MS COCO Karpathy offline test split and 129.6 CIDEr-D (C40) score on the official online testing server. Code is available at https://github.com/husthuaan/AoANet.\n\n2. Title: Bottom-up and top-down attention for image captioning and visual question answering\n   Abstract: Bottom-Up and Top-Down Attention for Image Captioning and Visual Question Answering: Top-down visual attention mechanisms have been used extensively in image captioning and visual question answering (VQA) to enable deeper image understanding through fine-grained analysis and even multiple steps of reasoning. In this work, we propose a combined bottom-up and top-down attention mechanism that enables attention to be calculated at the level of objects and other salient image regions. This is the natural basis for attention to be considered. Within our approach, the bottom-up mechanism (based on Faster R-CNN) proposes image regions, each with an associated feature vector, while the top-down mechanism determines feature weightings. Applying this approach to image captioning, our results on the MSCOCO test server establish a new state-of-the-art for the task, achieving CIDEr / SPICE / BLEU-4 scores of 117.9, 21.5 and 36.9, respectively. Demonstrating the broad applicability of the method, applying the same approach to VQA we obtain first place in the 2017 VQA Challenge.\n\n3. Title: Exploring visual relationship for image captioning\n   Abstract: Exploring Visual Relationship for Image Captioning: It is always well believed that modeling relationships between objects would be helpful for representing and eventually describing an image. Nevertheless, there has not been evidence in support of the idea on image description generation. In this paper, we introduce a new design to explore the connections between objects for image captioning under the umbrella of attention-based encoder-decoder framework. Specifically, we present Graph Convolutional Networks plus Long Short-Term Memory (dubbed as GCN-LSTM) architecture that novelly integrates both semantic and spatial object relationships into image encoder. Technically, we build graphs over the detected objects in an image based on their spatial and semantic connections. The representations of each region proposed on objects are then refined by leveraging graph structure through GCN. With the learnt region-level features, our GCN-LSTM capitalizes on LSTM-based captioning framework with attention mechanism for sentence generation. Extensive experiments are conducted on COCO image captioning dataset, and superior results are reported when comparing to state-of-the-art approaches. More remarkably, GCN-LSTM increases CIDEr-D performance from 120.1% to 128.7% on COCO testing set.\n\n4. Title: Meshed-memory transformer for image captioning\n   Abstract: Meshed-Memory Transformer for Image Captioning: Transformer-based architectures represent the state of the art in sequence modeling tasks like machine translation and language understanding. Their applicability to multi-modal contexts like image captioning, however, is still largely under-explored. With the aim of filling this gap, we present M$^2$ - a Meshed Transformer with Memory for Image Captioning. The architecture improves both the image encoding and the language generation steps: it learns a multi-level representation of the relationships between image regions integrating learned a priori knowledge, and uses a mesh-like connectivity at decoding stage to exploit low- and high-level features. Experimentally, we investigate the performance of the M$^2$ Transformer and different fully-attentive models in comparison with recurrent ones. When tested on COCO, our proposal achieves a new state of the art in single-model and ensemble configurations on the \"Karpathy\" test split and on the online test server. We also assess its performances when describing objects unseen in the training set. Trained models and code for reproducing the experiments are publicly available at: https://github.com/aimagelab/meshed-memory-transformer.\n\n5. Title: Normalized and geometry-aware self-attention network for image captioning\n   Abstract: Normalized and Geometry-Aware Self-Attention Network for Image Captioning: Self-attention (SA) network has shown profound value in image captioning. In this paper, we improve SA from two aspects to promote the performance of image captioning. First, we propose Normalized Self-Attention (NSA), a reparameterization of SA that brings the benefits of normalization inside SA. While normalization is previously only applied outside SA, we introduce a novel normalization method and demonstrate that it is both possible and beneficial to perform it on the hidden activations inside SA. Second, to compensate for the major limit of Transformer that it fails to model the geometry structure of the input objects, we propose a class of Geometry-aware Self-Attention (GSA) that extends SA to explicitly and efficiently consider the relative geometry relations between the objects in the image. To construct our image captioning model, we combine the two modules and apply it to the vanilla self-attention network. We extensively evaluate our proposals on MS-COCO image captioning dataset and superior results are achieved when comparing to state-of-the-art approaches. Further experiments on three challenging tasks, i.e. video captioning, machine translation, and visual question answering, show the generality of our methods.\n\n6. Title: Deep captioning with multimodal recurrent neural networks (m-rnn)\n   Abstract: Deep Captioning with Multimodal Recurrent Neural Networks (m-RNN): In this paper, we present a multimodal Recurrent Neural Network (m-RNN) model for generating novel image captions. It directly models the probability distribution of generating a word given previous words and an image. Image captions are generated by sampling from this distribution. The model consists of two sub-networks: a deep recurrent neural network for sentences and a deep convolutional network for images. These two sub-networks interact with each other in a multimodal layer to form the whole m-RNN model. The effectiveness of our model is validated on four benchmark datasets (IAPR TC-12, Flickr 8K, Flickr 30K and MS COCO). Our model outperforms the state-of-the-art methods. In addition, we apply the m-RNN model to retrieval tasks for retrieving images or sentences, and achieves significant performance improvement over the state-of-the-art methods which directly optimize the ranking objective function for retrieval. The project page of this work is: www.stat.ucla.edu/~junhua.mao/m-RNN.html .\n\n7. Title: Auto-encoding scene graphs for image captioning\n   Abstract: Auto-Encoding Scene Graphs for Image Captioning: We propose Scene Graph Auto-Encoder (SGAE) that incorporates the language inductive bias into the encoder-decoder image captioning framework for more human-like captions. Intuitively, we humans use the inductive bias to compose collocations and contextual inference in discourse. For example, when we see the relation `person on bike', it is natural to replace `on' with `ride' and infer `person riding bike on a road' even the `road' is not evident. Therefore, exploiting such bias as a language prior is expected to help the conventional encoder-decoder models less likely overfit to the dataset bias and focus on reasoning. Specifically, we use the scene graph --- a directed graph ($\\mathcal{G}$) where an object node is connected by adjective nodes and relationship nodes --- to represent the complex structural layout of both image ($\\mathcal{I}$) and sentence ($\\mathcal{S}$). In the textual domain, we use SGAE to learn a dictionary ($\\mathcal{D}$) that helps to reconstruct sentences in the $\\mathcal{S}\\rightarrow \\mathcal{G} \\rightarrow \\mathcal{D} \\rightarrow \\mathcal{S}$ pipeline, where $\\mathcal{D}$ encodes the desired language prior; in the vision-language domain, we use the shared $\\mathcal{D}$ to guide the encoder-decoder in the $\\mathcal{I}\\rightarrow \\mathcal{G}\\rightarrow \\mathcal{D} \\rightarrow \\mathcal{S}$ pipeline. Thanks to the scene graph representation and shared dictionary, the inductive bias is transferred across domains in principle. We validate the effectiveness of SGAE on the challenging MS-COCO image captioning benchmark, e.g., our SGAE-based single-model achieves a new state-of-the-art $127.8$ CIDEr-D on the Karpathy split, and a competitive $125.5$ CIDEr-D (c40) on the official server even compared to other ensemble models.\n\n8. Title: Image captioning with semantic attention\n   Abstract: Image Captioning with Semantic Attention: Automatically generating a natural language description of an image has attracted interests recently both because of its importance in practical applications and because it connects two major artificial intelligence fields: computer vision and natural language processing. Existing approaches are either top-down, which start from a gist of an image and convert it into words, or bottom-up, which come up with words describing various aspects of an image and then combine them. In this paper, we propose a new algorithm that combines both approaches through a model of semantic attention. Our algorithm learns to selectively attend to semantic concept proposals and fuse them into hidden states and outputs of recurrent neural networks. The selection and fusion form a feedback connecting the top-down and bottom-up computation. We evaluate our algorithm on two public benchmarks: Microsoft COCO and Flickr30K. Experimental results show that our algorithm significantly outperforms the state-of-the-art approaches consistently across different evaluation metrics.\n\n9. Title: Neural machine translation by jointly learning to align and translate\n   Abstract: Neural Machine Translation by Jointly Learning to Align and Translate: Neural machine translation is a recently proposed approach to machine translation. Unlike the traditional statistical machine translation, the neural machine translation aims at building a single neural network that can be jointly tuned to maximize the translation performance. The models proposed recently for neural machine translation often belong to a family of encoder-decoders and consists of an encoder that encodes a source sentence into a fixed-length vector from which a decoder generates a translation. In this paper, we conjecture that the use of a fixed-length vector is a bottleneck in improving the performance of this basic encoder-decoder architecture, and propose to extend this by allowing a model to automatically (soft-)search for parts of a source sentence that are relevant to predicting a target word, without having to form these parts as a hard segment explicitly. With this new approach, we achieve a translation performance comparable to the existing state-of-the-art phrase-based system on the task of English-to-French translation. Furthermore, qualitative analysis reveals that the (soft-)alignments found by the model agree well with our intuition.\n\nThink step-by-step about which candidate best matches the context. Summarize your reasoning, then provide the exact title that should fill the citation. Respond in JSON with fields:\n{\n  \"reasoning\": \"<your step-by-step analysis>\",\n  \"selected_title\": \"<exact candidate title>\"\n}", "response": {"reasoning": "The citation context refers to using a pre-trained image encoder (object detector/classifier) to extract fine-grained semantics (objects, attributes, or scene graph) and then applying visual attention over those semantics to strengthen vision-language interaction. Candidate 2, \"Bottom-Up and Top-Down Attention for Image Captioning and Visual Question Answering,\" explicitly introduces a bottom-up mechanism based on Faster R-CNN that proposes object-level regions (i.e., a pre-trained object detector) and a top-down attention mechanism that attends over those object features. This paper is widely cited for employing visual attention over detector-produced, fine-grained semantics for captioning and VQA. Other candidates either propose generic attention improvements (e.g., AoA, self-attention variants) or focus on scene graphs/graph convolutions or language priors; they do not centrally introduce the combination of a pre-trained object detector + attention over object-level features. Therefore candidate 2 best matches the cited claim.", "selected_title": "Bottom-up and top-down attention for image captioning and visual question answering"}, "selected_title": "Bottom-up and top-down attention for image captioning and visual question answering", "positive_title": "Meshed-memory transformer for image captioning", "is_correct": false, "reasoning": "The citation context refers to using a pre-trained image encoder (object detector/classifier) to extract fine-grained semantics (objects, attributes, or scene graph) and then applying visual attention over those semantics to strengthen vision-language interaction. Candidate 2, \"Bottom-Up and Top-Down Attention for Image Captioning and Visual Question Answering,\" explicitly introduces a bottom-up mechanism based on Faster R-CNN that proposes object-level regions (i.e., a pre-trained object detector) and a top-down attention mechanism that attends over those object features. This paper is widely cited for employing visual attention over detector-produced, fine-grained semantics for captioning and VQA. Other candidates either propose generic attention improvements (e.g., AoA, self-attention variants) or focus on scene graphs/graph convolutions or language priors; they do not centrally introduce the combination of a pre-trained object detector + attention over object-level features. Therefore candidate 2 best matches the cited claim."}
