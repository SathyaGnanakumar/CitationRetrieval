[
  {
    "query_id": "q_0",
    "true_titles": [
      "{Attention is All you Need"
    ],
    "retrieved_titles": [
      "{Attention is All you Need",
      "{BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
      "{Neural Architecture Search with Reinforcement Learning",
      "{The Evolved Transformer",
      "{Energy and Policy Considerations for Deep Learning in NLP",
      "{Pay Less Attention with Lightweight and Dynamic Convolutions",
      "{The Evolved Transformer",
      "{Sequence to Sequence Learning with Neural Networks",
      "{Effective Approaches to Attention-based Neural Machine Translation",
      "{Neural Machine Translation by Jointly Learning to Align and Translate",
      "{Google's Neural Machine Translation System: Bridging the Gap between Human and Machine Translation",
      "{Neural Machine Translation in Linear Time",
      "{Convolutional Sequence to Sequence Learning",
      "{Depthwise Separable Convolutions for Neural Machine Translation",
      "{Pay Less Attention with Lightweight and Dynamic Convolutions",
      "{Attention is All you Need",
      "{Weighted Transformer Network for Machine Translation",
      "{Scaling Neural Machine Translation",
      "{The Best of Both Worlds: Combining Recent Advances in Neural Machine Translation",
      "{A Deep Reinforced Model for Abstractive Summarization"
    ],
    "paper_id": "paper_0",
    "context": "Introduction\nTransformer<|cite_0|> is widely used in natural language processing due to its high training efficiency and superior capa..."
  },
  {
    "query_id": "q_1",
    "true_titles": [
      "{BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"
    ],
    "retrieved_titles": [
      "{Attention is All you Need",
      "{BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
      "{Neural Architecture Search with Reinforcement Learning",
      "{The Evolved Transformer",
      "{Energy and Policy Considerations for Deep Learning in NLP",
      "{Pay Less Attention with Lightweight and Dynamic Convolutions",
      "{The Evolved Transformer",
      "{Sequence to Sequence Learning with Neural Networks",
      "{Effective Approaches to Attention-based Neural Machine Translation",
      "{Neural Machine Translation by Jointly Learning to Align and Translate",
      "{Google's Neural Machine Translation System: Bridging the Gap between Human and Machine Translation",
      "{Neural Machine Translation in Linear Time",
      "{Convolutional Sequence to Sequence Learning",
      "{Depthwise Separable Convolutions for Neural Machine Translation",
      "{Pay Less Attention with Lightweight and Dynamic Convolutions",
      "{Attention is All you Need",
      "{Weighted Transformer Network for Machine Translation",
      "{Scaling Neural Machine Translation",
      "{The Best of Both Worlds: Combining Recent Advances in Neural Machine Translation",
      "{A Deep Reinforced Model for Abstractive Summarization"
    ],
    "paper_id": "paper_0",
    "context": "ng long-distance dependencies. Building on top of them, modern state-of-the-art models, such as BERT<|cite_1|>, are able to learn powerful language representations from unlabeled text and even surpass..."
  },
  {
    "query_id": "q_2",
    "true_titles": [
      "{Neural Architecture Search with Reinforcement Learning"
    ],
    "retrieved_titles": [
      "{Attention is All you Need",
      "{BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
      "{Neural Architecture Search with Reinforcement Learning",
      "{The Evolved Transformer",
      "{Energy and Policy Considerations for Deep Learning in NLP",
      "{Pay Less Attention with Lightweight and Dynamic Convolutions",
      "{The Evolved Transformer",
      "{Sequence to Sequence Learning with Neural Networks",
      "{Effective Approaches to Attention-based Neural Machine Translation",
      "{Neural Machine Translation by Jointly Learning to Align and Translate",
      "{Google's Neural Machine Translation System: Bridging the Gap between Human and Machine Translation",
      "{Neural Machine Translation in Linear Time",
      "{Convolutional Sequence to Sequence Learning",
      "{Depthwise Separable Convolutions for Neural Machine Translation",
      "{Pay Less Attention with Lightweight and Dynamic Convolutions",
      "{Attention is All you Need",
      "{Weighted Transformer Network for Machine Translation",
      "{Scaling Neural Machine Translation",
      "{The Best of Both Worlds: Combining Recent Advances in Neural Machine Translation",
      "{A Deep Reinforced Model for Abstractive Summarization"
    ],
    "paper_id": "paper_0",
    "context": "tecture specialized for real-time NLP applications on the edge. Automatic neural architecture search<|cite_2|> is a choice for high accuracy model design, but the massive search cost (GPU hours and $C..."
  },
  {
    "query_id": "q_3",
    "true_titles": [
      "{Energy and Policy Considerations for Deep Learning in NLP"
    ],
    "retrieved_titles": [
      "{Attention is All you Need",
      "{BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
      "{Neural Architecture Search with Reinforcement Learning",
      "{The Evolved Transformer",
      "{Energy and Policy Considerations for Deep Learning in NLP",
      "{Pay Less Attention with Lightweight and Dynamic Convolutions",
      "{The Evolved Transformer",
      "{Sequence to Sequence Learning with Neural Networks",
      "{Effective Approaches to Attention-based Neural Machine Translation",
      "{Neural Machine Translation by Jointly Learning to Align and Translate",
      "{Google's Neural Machine Translation System: Bridging the Gap between Human and Machine Translation",
      "{Neural Machine Translation in Linear Time",
      "{Convolutional Sequence to Sequence Learning",
      "{Depthwise Separable Convolutions for Neural Machine Translation",
      "{Pay Less Attention with Lightweight and Dynamic Convolutions",
      "{Attention is All you Need",
      "{Weighted Transformer Network for Machine Translation",
      "{Scaling Neural Machine Translation",
      "{The Best of Both Worlds: Combining Recent Advances in Neural Machine Translation",
      "{A Deep Reinforced Model for Abstractive Summarization"
    ],
    "paper_id": "paper_0",
    "context": "gn, but the massive search cost (GPU hours and $CO_2$ emission) raises severe environmental concerns<|cite_3|>, shown in \\fig{fig:teaser:b}.\n\nIn this paper, we focus on the efficient inference for mob..."
  },
  {
    "query_id": "q_4",
    "true_titles": [
      "{Pay Less Attention with Lightweight and Dynamic Convolutions"
    ],
    "retrieved_titles": [
      "{Attention is All you Need",
      "{BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
      "{Neural Architecture Search with Reinforcement Learning",
      "{The Evolved Transformer",
      "{Energy and Policy Considerations for Deep Learning in NLP",
      "{Pay Less Attention with Lightweight and Dynamic Convolutions",
      "{The Evolved Transformer",
      "{Sequence to Sequence Learning with Neural Networks",
      "{Effective Approaches to Attention-based Neural Machine Translation",
      "{Neural Machine Translation by Jointly Learning to Align and Translate",
      "{Google's Neural Machine Translation System: Bridging the Gap between Human and Machine Translation",
      "{Neural Machine Translation in Linear Time",
      "{Convolutional Sequence to Sequence Learning",
      "{Depthwise Separable Convolutions for Neural Machine Translation",
      "{Pay Less Attention with Lightweight and Dynamic Convolutions",
      "{Attention is All you Need",
      "{Weighted Transformer Network for Machine Translation",
      "{Scaling Neural Machine Translation",
      "{The Best of Both Worlds: Combining Recent Advances in Neural Machine Translation",
      "{A Deep Reinforced Model for Abstractive Summarization"
    ],
    "paper_id": "paper_0",
    "context": "ttnshort dedicates \\textit{specialized} heads to model long and short distance contexts. Inspired by<|cite_4|>, \\attnshort introduces convolution in a parallel branch to capture \\textbf{local} depende..."
  }
]