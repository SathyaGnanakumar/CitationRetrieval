[
  {
    "query_id": "q_0",
    "true_titles": [
      "{Attention is All you Need"
    ],
    "retrieved_titles": [
      "{BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
      "{MetaPruning: Meta Learning for Automatic Neural Network Channel Pruning",
      "GAN Compression: Efficient Architectures for Interactive Conditional GANs",
      "{Learning Efficient Convolutional Networks through Network Slimming",
      "{Adaptive Attention Span in Transformers",
      "{Adaptive Attention Span in Transformers",
      "{Energy and Policy Considerations for Deep Learning in NLP",
      "{Neural Architecture Search with Reinforcement Learning",
      "{Neural Architecture Search with Reinforcement Learning",
      "{Neural Architecture Search with Reinforcement Learning",
      "{Neural Machine Translation in Linear Time",
      "{Quantizing Deep Convolutional Networks for Efficient Inference: A Whitepaper",
      "{Weighted Transformer Network for Machine Translation",
      "{Weighted Transformer Network for Machine Translation",
      "{Attention is All you Need",
      "{Attention is All you Need",
      "{Self-Attention with Relative Position Representations",
      "{Self-Attention with Relative Position Representations",
      "{ProxylessNAS: Direct Neural Architecture Search on Target Task and Hardware",
      "{FBNet: Hardware-Aware Efficient ConvNet Design via Differentiable Neural Architecture Search"
    ],
    "paper_id": "paper_0",
    "context": "Introduction\nTransformer<|cite_0|> is widely used in natural language processing due to its high training efficiency and superior capa..."
  },
  {
    "query_id": "q_1",
    "true_titles": [
      "{BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"
    ],
    "retrieved_titles": [
      "{BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
      "Generating Long Sequences with Sparse Transformers",
      "{Learning Transferable Architectures for Scalable Image Recognition",
      "{Sequence to Sequence Learning with Neural Networks",
      "{Augmenting self-attention with persistent memory",
      "{Neural Machine Translation in Linear Time",
      "{Neural Architecture Search with Reinforcement Learning",
      "{Neural Architecture Search with Reinforcement Learning",
      "{Neural Architecture Search with Reinforcement Learning",
      "{Trained Ternary Quantization",
      "{Self-Attention with Relative Position Representations",
      "{Self-Attention with Relative Position Representations",
      "{Channel Pruning for Accelerating Very Deep Neural Networks",
      "{FBNet: Hardware-Aware Efficient ConvNet Design via Differentiable Neural Architecture Search",
      "{Energy and Policy Considerations for Deep Learning in NLP",
      "{MetaPruning: Meta Learning for Automatic Neural Network Channel Pruning",
      "{AMC: AutoML for Model Compression and Acceleration on Mobile Devices",
      "{AMC: AutoML for Model Compression and Acceleration on Mobile Devices",
      "{Learning both Weights and Connections for Efficient Neural Networks",
      "{Pay Less Attention with Lightweight and Dynamic Convolutions"
    ],
    "paper_id": "paper_0",
    "context": "ng long-distance dependencies. Building on top of them, modern state-of-the-art models, such as BERT<|cite_1|>, are able to learn powerful language representations from unlabeled text and even surpass..."
  },
  {
    "query_id": "q_2",
    "true_titles": [
      "{Neural Architecture Search with Reinforcement Learning"
    ],
    "retrieved_titles": [
      "{ProxylessNAS: Direct Neural Architecture Search on Target Task and Hardware",
      "{Learning Efficient Convolutional Networks through Network Slimming",
      "{FBNet: Hardware-Aware Efficient ConvNet Design via Differentiable Neural Architecture Search",
      "{Efficient Neural Architecture Search via Parameter Sharing",
      "{Energy and Policy Considerations for Deep Learning in NLP",
      "{MetaPruning: Meta Learning for Automatic Neural Network Channel Pruning",
      "{MnasNet: Platform-Aware Neural Architecture Search for Mobile",
      "{HAQ: Hardware-Aware Automated Quantization with Mixed Precision",
      "{HAQ: Hardware-Aware Automated Quantization with Mixed Precision",
      "{Learning Transferable Architectures for Scalable Image Recognition",
      "{NetAdapt: Platform-Aware Neural Network Adaptation for Mobile Applications",
      "GAN Compression: Efficient Architectures for Interactive Conditional GANs",
      "{The Evolved Transformer",
      "{The Evolved Transformer",
      "{The Evolved Transformer",
      "{Neural Architecture Search with Reinforcement Learning",
      "{Neural Architecture Search with Reinforcement Learning",
      "{Neural Architecture Search with Reinforcement Learning",
      "{Scaling Neural Machine Translation",
      "{Scaling Neural Machine Translation"
    ],
    "paper_id": "paper_0",
    "context": "tecture specialized for real-time NLP applications on the edge. Automatic neural architecture search<|cite_2|> is a choice for high accuracy model design, but the massive search cost (GPU hours and $C..."
  },
  {
    "query_id": "q_3",
    "true_titles": [
      "{Energy and Policy Considerations for Deep Learning in NLP"
    ],
    "retrieved_titles": [
      "{FBNet: Hardware-Aware Efficient ConvNet Design via Differentiable Neural Architecture Search",
      "{ProxylessNAS: Direct Neural Architecture Search on Target Task and Hardware",
      "{AMC: AutoML for Model Compression and Acceleration on Mobile Devices",
      "{AMC: AutoML for Model Compression and Acceleration on Mobile Devices",
      "{Energy and Policy Considerations for Deep Learning in NLP",
      "{MnasNet: Platform-Aware Neural Architecture Search for Mobile",
      "{Scaling Neural Machine Translation",
      "{Scaling Neural Machine Translation",
      "{Efficient Neural Architecture Search via Parameter Sharing",
      "{NetAdapt: Platform-Aware Neural Network Adaptation for Mobile Applications",
      "{Depthwise Separable Convolutions for Neural Machine Translation",
      "{Learning Efficient Convolutional Networks through Network Slimming",
      "Point-Voxel CNN for efficient 3D deep learning",
      "{Learning Transferable Architectures for Scalable Image Recognition",
      "{HAQ: Hardware-Aware Automated Quantization with Mixed Precision",
      "{HAQ: Hardware-Aware Automated Quantization with Mixed Precision",
      "{Trained Ternary Quantization",
      "GAN Compression: Efficient Architectures for Interactive Conditional GANs",
      "{Deep Compression: Compressing Deep Neural Networks with Pruning, Trained Quantization and Huffman Coding",
      "{MetaPruning: Meta Learning for Automatic Neural Network Channel Pruning"
    ],
    "paper_id": "paper_0",
    "context": "gn, but the massive search cost (GPU hours and $CO_2$ emission) raises severe environmental concerns<|cite_3|>, shown in \\fig{fig:teaser:b}.\n\nIn this paper, we focus on the efficient inference for mob..."
  },
  {
    "query_id": "q_4",
    "true_titles": [
      "{Pay Less Attention with Lightweight and Dynamic Convolutions"
    ],
    "retrieved_titles": [
      "{Sequence to Sequence Learning with Neural Networks",
      "{Augmenting self-attention with persistent memory",
      "{Weighted Transformer Network for Machine Translation",
      "{Weighted Transformer Network for Machine Translation",
      "{MnasNet: Platform-Aware Neural Architecture Search for Mobile",
      "{Pay Less Attention with Lightweight and Dynamic Convolutions",
      "{Pay Less Attention with Lightweight and Dynamic Convolutions",
      "{Convolutional Sequence to Sequence Learning",
      "Generating Long Sequences with Sparse Transformers",
      "{Depthwise Separable Convolutions for Neural Machine Translation",
      "{Self-Attention with Relative Position Representations",
      "{Self-Attention with Relative Position Representations",
      "{Channel Pruning for Accelerating Very Deep Neural Networks",
      "Point-Voxel CNN for efficient 3D deep learning",
      "{Learning Efficient Convolutional Networks through Network Slimming",
      "{Quantizing Deep Convolutional Networks for Efficient Inference: A Whitepaper",
      "{Effective Approaches to Attention-based Neural Machine Translation",
      "{A Deep Reinforced Model for Abstractive Summarization",
      "{Adaptive Attention Span in Transformers",
      "{Adaptive Attention Span in Transformers"
    ],
    "paper_id": "paper_0",
    "context": "ttnshort dedicates \\textit{specialized} heads to model long and short distance contexts. Inspired by<|cite_4|>, \\attnshort introduces convolution in a parallel branch to capture \\textbf{local} depende..."
  }
]