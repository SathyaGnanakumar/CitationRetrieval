[
  {
    "query_id": "q_3",
    "paper_id": "paper_0",
    "true_title": "{Energy and Policy Considerations for Deep Learning in NLP",
    "context": "gn, but the massive search cost (GPU hours and $CO_2$ emission) raises severe environmental concerns<|cite_3|>, shown in \\fig{fig:teaser:b}.\n\nIn this paper, we focus on the efficient inference for mobile device",
    "retrieved_titles": [
      "{Learning Transferable Architectures for Scalable Image Recognition",
      "{Channel Pruning for Accelerating Very Deep Neural Networks",
      "{MnasNet: Platform-Aware Neural Architecture Search for Mobile",
      "{Trained Ternary Quantization",
      "{MetaPruning: Meta Learning for Automatic Neural Network Channel Pruning"
    ],
    "category": "not_in_top_k",
    "corpus_size": 51
  },
  {
    "query_id": "q_4",
    "paper_id": "paper_0",
    "true_title": "{Pay Less Attention with Lightweight and Dynamic Convolutions",
    "context": "ttnshort dedicates \\textit{specialized} heads to model long and short distance contexts. Inspired by<|cite_4|>, \\attnshort introduces convolution in a parallel branch to capture \\textbf{local} dependencies so t",
    "retrieved_titles": [
      "{Augmenting self-attention with persistent memory",
      "{Learning Transferable Architectures for Scalable Image Recognition",
      "{Channel Pruning for Accelerating Very Deep Neural Networks",
      "{Self-Attention with Relative Position Representations",
      "{Self-Attention with Relative Position Representations"
    ],
    "category": "not_in_top_k",
    "corpus_size": 51
  },
  {
    "query_id": "q_5",
    "paper_id": "paper_0",
    "true_title": "{The Evolved Transformer",
    "context": "hts, our manually-designed \\model achieves 0.5 higher BLEU than the AutoML-based Evolved Transformer<|cite_5|>, which requires more than 250 GPU years to search, emitting\nas much carbon as five cars in their li",
    "retrieved_titles": [
      "{Learning Transferable Architectures for Scalable Image Recognition",
      "{Channel Pruning for Accelerating Very Deep Neural Networks",
      "{Adaptive Attention Span in Transformers",
      "{Adaptive Attention Span in Transformers",
      "{The Best of Both Worlds: Combining Recent Advances in Neural Machine Translation"
    ],
    "category": "not_in_top_k",
    "corpus_size": 51
  },
  {
    "query_id": "q_7",
    "paper_id": "paper_0",
    "true_title": "{Neural Machine Translation in Linear Time",
    "context": "e-art performance. For instance, researchers have proposed highly-efficient convolution-based models<|cite_7|>. Convolution is an ideal primitive to model the local context information; however, it lacks the ab",
    "retrieved_titles": [
      "{Weighted Transformer Network for Machine Translation",
      "{Weighted Transformer Network for Machine Translation",
      "{Learning Transferable Architectures for Scalable Image Recognition",
      "{Neural Machine Translation by Jointly Learning to Align and Translate",
      "{A Deep Reinforced Model for Abstractive Summarization"
    ],
    "category": "not_in_top_k",
    "corpus_size": 51
  },
  {
    "query_id": "q_8",
    "paper_id": "paper_0",
    "true_title": "{Attention is All you Need",
    "context": "native, attention is able to capture global-context information by pairwise correlation. Transformer<|cite_8|> has demonstrated that it is possible to stack the self-attentions to achieve state-of-the-art perfo",
    "retrieved_titles": [
      "{Adaptive Attention Span in Transformers",
      "{Adaptive Attention Span in Transformers",
      "{Self-Attention with Relative Position Representations",
      "{Self-Attention with Relative Position Representations",
      "{Augmenting self-attention with persistent memory"
    ],
    "category": "not_in_top_k",
    "corpus_size": 51
  },
  {
    "query_id": "q_10",
    "paper_id": "paper_0",
    "true_title": "{Scaling Neural Machine Translation",
    "context": "t performance. Recently, there have been a lot of variants to the transformer<|cite_9|>. Among them,<|cite_10|> proposed to scale up the batch size;<|cite_11|> leverages the relative position representations;<|c",
    "retrieved_titles": [
      "{Neural Machine Translation by Jointly Learning to Align and Translate",
      "{Self-Attention with Relative Position Representations",
      "{Self-Attention with Relative Position Representations",
      "{MnasNet: Platform-Aware Neural Architecture Search for Mobile",
      "{Channel Pruning for Accelerating Very Deep Neural Networks"
    ],
    "category": "not_in_top_k",
    "corpus_size": 51
  },
  {
    "query_id": "q_16",
    "paper_id": "paper_0",
    "true_title": "{ProxylessNAS: Direct Neural Architecture Search on Target Task and Hardware",
    "context": "ce constraints into the optimization loop begins to emerge, such as MnasNet<|cite_15|>, ProxylessNAS<|cite_16|> and FBNet<|cite_17|>. In the NLP community, the evolved transformer<|cite_18|> adopts the neural ar",
    "retrieved_titles": [
      "{Learning Transferable Architectures for Scalable Image Recognition",
      "{Channel Pruning for Accelerating Very Deep Neural Networks",
      "{NetAdapt: Platform-Aware Neural Network Adaptation for Mobile Applications",
      "{Trained Ternary Quantization",
      "{FBNet: Hardware-Aware Efficient ConvNet Design via Differentiable Neural Architecture Search"
    ],
    "category": "not_in_top_k",
    "corpus_size": 51
  },
  {
    "query_id": "q_19",
    "paper_id": "paper_0",
    "true_title": "{Neural Architecture Search with Reinforcement Learning",
    "context": "17|>. In the NLP community, the evolved transformer<|cite_18|> adopts the neural architecture search<|cite_19|> to design basic blocks and finds a better \\#parameter-BLEU trade-off for the transformer. However, ",
    "retrieved_titles": [
      "{Learning Transferable Architectures for Scalable Image Recognition",
      "{FBNet: Hardware-Aware Efficient ConvNet Design via Differentiable Neural Architecture Search",
      "{The Evolved Transformer",
      "{The Evolved Transformer",
      "{The Evolved Transformer"
    ],
    "category": "not_in_top_k",
    "corpus_size": 51
  },
  {
    "query_id": "q_21",
    "paper_id": "paper_0",
    "true_title": "{Learning both Weights and Connections for Efficient Neural Networks",
    "context": "accelerate the existing large models. For instance, some have proposed to prune the separate neurons<|cite_21|> or the entire channels<|cite_22|>; others have proposed to quantize the network<|cite_23|> to accel",
    "retrieved_titles": [
      "{Weighted Transformer Network for Machine Translation",
      "{Weighted Transformer Network for Machine Translation",
      "{Channel Pruning for Accelerating Very Deep Neural Networks",
      "{Trained Ternary Quantization",
      "{Sequence to Sequence Learning with Neural Networks"
    ],
    "category": "not_in_top_k",
    "corpus_size": 51
  },
  {
    "query_id": "q_23",
    "paper_id": "paper_0",
    "true_title": "{Binarized Neural Networks: Training Deep Neural Networks with Weights and Activations Constrained to +1 or -1",
    "context": "e neurons<|cite_21|> or the entire channels<|cite_22|>; others have proposed to quantize the network<|cite_23|> to accelerate the model inference. \nRecently, AutoML has also been used to automate the model compr",
    "retrieved_titles": [
      "{HAQ: Hardware-Aware Automated Quantization with Mixed Precision",
      "{HAQ: Hardware-Aware Automated Quantization with Mixed Precision",
      "{Channel Pruning for Accelerating Very Deep Neural Networks",
      "{Sequence to Sequence Learning with Neural Networks",
      "{Learning Transferable Architectures for Scalable Image Recognition"
    ],
    "category": "not_in_top_k",
    "corpus_size": 51
  },
  {
    "query_id": "q_30",
    "paper_id": "paper_1",
    "true_title": "Inductive representation learning on large graphs",
    "context": "to learn graph representation,\n\nwhich include Graph Convolutional Network (GCN)<|cite_4|>, GraphSage<|cite_5|>, Graph Isomorphism Network (GIN)<|cite_6|> and Graph Attention Network (GAT)<|cite_7|>. Existing GN",
    "retrieved_titles": [
      "Semi-supervised classification with graph convolutional networks",
      "Semi-supervised classification with graph convolutional networks",
      "Incorporating syntactic and semantic information in word embeddings using graph convolutional networks",
      "Dynamically fused graph network for multi-hop reasoning",
      "Dynamically fused graph network for multi-hop reasoning"
    ],
    "category": "not_in_top_k",
    "corpus_size": 38
  },
  {
    "query_id": "q_33",
    "paper_id": "paper_1",
    "true_title": "Multi-hop reading comprehension across multiple documents by reasoning over heterogeneous graphs",
    "context": "ge passing algorithm is applied to obtain node representations from these summarized feature vectors<|cite_8|>.\n\nHowever, this early summarization strategy (summarization before GNN based representation learnin",
    "retrieved_titles": [
      "Neural message passing for quantum chemistry",
      "How powerful are graph neural networks?",
      "How powerful are graph neural networks?",
      "Dynamically fused graph network for multi-hop reasoning",
      "Dynamically fused graph network for multi-hop reasoning"
    ],
    "category": "not_in_top_k",
    "corpus_size": 38
  },
  {
    "query_id": "q_36",
    "paper_id": "paper_1",
    "true_title": "Hotpotqa: A dataset for diverse, explainable multi-hop question answering",
    "context": "am}.\n\nTo validate the effectiveness of the proposed GSN, we experiment on two NLP datasets: HotpotQA<|cite_11|> and fact extraction and verification data set provided by FEVER shared task 1.0<|cite_12|>. Both ta",
    "retrieved_titles": [
      "Semi-supervised classification with graph convolutional networks",
      "Semi-supervised classification with graph convolutional networks",
      "How powerful are graph neural networks?",
      "How powerful are graph neural networks?",
      "Dynamically fused graph network for multi-hop reasoning"
    ],
    "category": "not_in_top_k",
    "corpus_size": 38
  },
  {
    "query_id": "q_37",
    "paper_id": "paper_1",
    "true_title": "Fever: a large-scale dataset for fact extraction and verification",
    "context": " HotpotQA<|cite_11|> and fact extraction and verification data set provided by FEVER shared task 1.0<|cite_12|>. Both tasks require the model to have reasoning ability, and top performance has been achieved with",
    "retrieved_titles": [
      "Dynamically fused graph network for multi-hop reasoning",
      "Dynamically fused graph network for multi-hop reasoning",
      "Dynamically fused graph network for multi-hop reasoning",
      "Dynamically fused graph network for multi-hop reasoning",
      "How powerful are graph neural networks?"
    ],
    "category": "not_in_top_k",
    "corpus_size": 38
  },
  {
    "query_id": "q_38",
    "paper_id": "paper_1",
    "true_title": "Gear: Graph-based evidence aggregating and reasoning for fact verification",
    "context": "ve reasoning ability, and top performance has been achieved with early summarization followed by GNN<|cite_13|>. With thorough experiments, we show that the proposed GSN achieves better performance than standard",
    "retrieved_titles": [
      "Dynamically fused graph network for multi-hop reasoning",
      "Dynamically fused graph network for multi-hop reasoning",
      "Dynamically fused graph network for multi-hop reasoning",
      "Dynamically fused graph network for multi-hop reasoning",
      "How powerful are graph neural networks?"
    ],
    "category": "not_in_top_k",
    "corpus_size": 38
  },
  {
    "query_id": "q_40",
    "paper_id": "paper_1",
    "true_title": "Inductive representation learning on large graphs",
    "context": "have been proposed with different message passing algorithms. For example GCN<|cite_14|>, Graph Sage<|cite_15|>, GAT<|cite_16|>, GIN<|cite_17|>, etc. Our proposed GSN can be regarded as a variant of GNN. However",
    "retrieved_titles": [
      "Neural message passing for quantum chemistry",
      "Graph neural networks for social recommendation",
      "How powerful are graph neural networks?",
      "How powerful are graph neural networks?",
      "Dynamic coattention networks for question answering"
    ],
    "category": "not_in_top_k",
    "corpus_size": 38
  },
  {
    "query_id": "q_48",
    "paper_id": "paper_1",
    "true_title": "Improved semantic representations from tree-structured long short-term memory networks",
    "context": "e_22|> applied similar idea to derive word embeddings based on GCN. Furthermore, the tree-LSTM model<|cite_23|> and sent-LSTM<|cite_24|> model can also be regarded as implementation of GNN because they both expl",
    "retrieved_titles": [
      "Semi-supervised classification with graph convolutional networks",
      "Semi-supervised classification with graph convolutional networks",
      "Multi-hop reading comprehension across multiple documents by reasoning over heterogeneous graphs",
      "Multi-hop reading comprehension across multiple documents by reasoning over heterogeneous graphs",
      "Multi-hop reading comprehension across multiple documents by reasoning over heterogeneous graphs"
    ],
    "category": "not_in_top_k",
    "corpus_size": 38
  },
  {
    "query_id": "q_49",
    "paper_id": "paper_1",
    "true_title": "Sentence-state lstm for text representation",
    "context": "ea to derive word embeddings based on GCN. Furthermore, the tree-LSTM model<|cite_23|> and sent-LSTM<|cite_24|> model can also be regarded as implementation of GNN because they both explored recurrent message pa",
    "retrieved_titles": [
      "Semi-supervised classification with graph convolutional networks",
      "Semi-supervised classification with graph convolutional networks",
      "Neural message passing for quantum chemistry",
      "Multi-hop reading comprehension across multiple documents by reasoning over heterogeneous graphs",
      "Multi-hop reading comprehension across multiple documents by reasoning over heterogeneous graphs"
    ],
    "category": "not_in_top_k",
    "corpus_size": 38
  }
]