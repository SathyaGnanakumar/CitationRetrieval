query_id,paper_id,true_title,context,retrieved_titles,category,corpus_size
q_3,paper_0,{Energy and Policy Considerations for Deep Learning in NLP,"gn, but the massive search cost (GPU hours and $CO_2$ emission) raises severe environmental concerns<|cite_3|>, shown in \fig{fig:teaser:b}.

In this paper, we focus on the efficient inference for mobile device","['{Learning Transferable Architectures for Scalable Image Recognition', '{Channel Pruning for Accelerating Very Deep Neural Networks', '{MnasNet: Platform-Aware Neural Architecture Search for Mobile', '{Trained Ternary Quantization', '{MetaPruning: Meta Learning for Automatic Neural Network Channel Pruning']",not_in_top_k,51
q_4,paper_0,{Pay Less Attention with Lightweight and Dynamic Convolutions,"ttnshort dedicates \textit{specialized} heads to model long and short distance contexts. Inspired by<|cite_4|>, \attnshort introduces convolution in a parallel branch to capture \textbf{local} dependencies so t","['{Augmenting self-attention with persistent memory', '{Learning Transferable Architectures for Scalable Image Recognition', '{Channel Pruning for Accelerating Very Deep Neural Networks', '{Self-Attention with Relative Position Representations', '{Self-Attention with Relative Position Representations']",not_in_top_k,51
q_5,paper_0,{The Evolved Transformer,"hts, our manually-designed \model achieves 0.5 higher BLEU than the AutoML-based Evolved Transformer<|cite_5|>, which requires more than 250 GPU years to search, emitting
as much carbon as five cars in their li","['{Learning Transferable Architectures for Scalable Image Recognition', '{Channel Pruning for Accelerating Very Deep Neural Networks', '{Adaptive Attention Span in Transformers', '{Adaptive Attention Span in Transformers', '{The Best of Both Worlds: Combining Recent Advances in Neural Machine Translation']",not_in_top_k,51
q_7,paper_0,{Neural Machine Translation in Linear Time,"e-art performance. For instance, researchers have proposed highly-efficient convolution-based models<|cite_7|>. Convolution is an ideal primitive to model the local context information; however, it lacks the ab","['{Weighted Transformer Network for Machine Translation', '{Weighted Transformer Network for Machine Translation', '{Learning Transferable Architectures for Scalable Image Recognition', '{Neural Machine Translation by Jointly Learning to Align and Translate', '{A Deep Reinforced Model for Abstractive Summarization']",not_in_top_k,51
q_8,paper_0,{Attention is All you Need,"native, attention is able to capture global-context information by pairwise correlation. Transformer<|cite_8|> has demonstrated that it is possible to stack the self-attentions to achieve state-of-the-art perfo","['{Adaptive Attention Span in Transformers', '{Adaptive Attention Span in Transformers', '{Self-Attention with Relative Position Representations', '{Self-Attention with Relative Position Representations', '{Augmenting self-attention with persistent memory']",not_in_top_k,51
q_10,paper_0,{Scaling Neural Machine Translation,"t performance. Recently, there have been a lot of variants to the transformer<|cite_9|>. Among them,<|cite_10|> proposed to scale up the batch size;<|cite_11|> leverages the relative position representations;<|c","['{Neural Machine Translation by Jointly Learning to Align and Translate', '{Self-Attention with Relative Position Representations', '{Self-Attention with Relative Position Representations', '{MnasNet: Platform-Aware Neural Architecture Search for Mobile', '{Channel Pruning for Accelerating Very Deep Neural Networks']",not_in_top_k,51
q_16,paper_0,{ProxylessNAS: Direct Neural Architecture Search on Target Task and Hardware,"ce constraints into the optimization loop begins to emerge, such as MnasNet<|cite_15|>, ProxylessNAS<|cite_16|> and FBNet<|cite_17|>. In the NLP community, the evolved transformer<|cite_18|> adopts the neural ar","['{Learning Transferable Architectures for Scalable Image Recognition', '{Channel Pruning for Accelerating Very Deep Neural Networks', '{NetAdapt: Platform-Aware Neural Network Adaptation for Mobile Applications', '{Trained Ternary Quantization', '{FBNet: Hardware-Aware Efficient ConvNet Design via Differentiable Neural Architecture Search']",not_in_top_k,51
q_19,paper_0,{Neural Architecture Search with Reinforcement Learning,"17|>. In the NLP community, the evolved transformer<|cite_18|> adopts the neural architecture search<|cite_19|> to design basic blocks and finds a better \#parameter-BLEU trade-off for the transformer. However, ","['{Learning Transferable Architectures for Scalable Image Recognition', '{FBNet: Hardware-Aware Efficient ConvNet Design via Differentiable Neural Architecture Search', '{The Evolved Transformer', '{The Evolved Transformer', '{The Evolved Transformer']",not_in_top_k,51
q_21,paper_0,{Learning both Weights and Connections for Efficient Neural Networks,"accelerate the existing large models. For instance, some have proposed to prune the separate neurons<|cite_21|> or the entire channels<|cite_22|>; others have proposed to quantize the network<|cite_23|> to accel","['{Weighted Transformer Network for Machine Translation', '{Weighted Transformer Network for Machine Translation', '{Channel Pruning for Accelerating Very Deep Neural Networks', '{Trained Ternary Quantization', '{Sequence to Sequence Learning with Neural Networks']",not_in_top_k,51
q_23,paper_0,{Binarized Neural Networks: Training Deep Neural Networks with Weights and Activations Constrained to +1 or -1,"e neurons<|cite_21|> or the entire channels<|cite_22|>; others have proposed to quantize the network<|cite_23|> to accelerate the model inference. 
Recently, AutoML has also been used to automate the model compr","['{HAQ: Hardware-Aware Automated Quantization with Mixed Precision', '{HAQ: Hardware-Aware Automated Quantization with Mixed Precision', '{Channel Pruning for Accelerating Very Deep Neural Networks', '{Sequence to Sequence Learning with Neural Networks', '{Learning Transferable Architectures for Scalable Image Recognition']",not_in_top_k,51
q_30,paper_1,Inductive representation learning on large graphs,"to learn graph representation,

which include Graph Convolutional Network (GCN)<|cite_4|>, GraphSage<|cite_5|>, Graph Isomorphism Network (GIN)<|cite_6|> and Graph Attention Network (GAT)<|cite_7|>. Existing GN","['Semi-supervised classification with graph convolutional networks', 'Semi-supervised classification with graph convolutional networks', 'Incorporating syntactic and semantic information in word embeddings using graph convolutional networks', 'Dynamically fused graph network for multi-hop reasoning', 'Dynamically fused graph network for multi-hop reasoning']",not_in_top_k,38
q_33,paper_1,Multi-hop reading comprehension across multiple documents by reasoning over heterogeneous graphs,"ge passing algorithm is applied to obtain node representations from these summarized feature vectors<|cite_8|>.

However, this early summarization strategy (summarization before GNN based representation learnin","['Neural message passing for quantum chemistry', 'How powerful are graph neural networks?', 'How powerful are graph neural networks?', 'Dynamically fused graph network for multi-hop reasoning', 'Dynamically fused graph network for multi-hop reasoning']",not_in_top_k,38
q_36,paper_1,"Hotpotqa: A dataset for diverse, explainable multi-hop question answering","am}.

To validate the effectiveness of the proposed GSN, we experiment on two NLP datasets: HotpotQA<|cite_11|> and fact extraction and verification data set provided by FEVER shared task 1.0<|cite_12|>. Both ta","['Semi-supervised classification with graph convolutional networks', 'Semi-supervised classification with graph convolutional networks', 'How powerful are graph neural networks?', 'How powerful are graph neural networks?', 'Dynamically fused graph network for multi-hop reasoning']",not_in_top_k,38
q_37,paper_1,Fever: a large-scale dataset for fact extraction and verification," HotpotQA<|cite_11|> and fact extraction and verification data set provided by FEVER shared task 1.0<|cite_12|>. Both tasks require the model to have reasoning ability, and top performance has been achieved with","['Dynamically fused graph network for multi-hop reasoning', 'Dynamically fused graph network for multi-hop reasoning', 'Dynamically fused graph network for multi-hop reasoning', 'Dynamically fused graph network for multi-hop reasoning', 'How powerful are graph neural networks?']",not_in_top_k,38
q_38,paper_1,Gear: Graph-based evidence aggregating and reasoning for fact verification,"ve reasoning ability, and top performance has been achieved with early summarization followed by GNN<|cite_13|>. With thorough experiments, we show that the proposed GSN achieves better performance than standard","['Dynamically fused graph network for multi-hop reasoning', 'Dynamically fused graph network for multi-hop reasoning', 'Dynamically fused graph network for multi-hop reasoning', 'Dynamically fused graph network for multi-hop reasoning', 'How powerful are graph neural networks?']",not_in_top_k,38
q_40,paper_1,Inductive representation learning on large graphs,"have been proposed with different message passing algorithms. For example GCN<|cite_14|>, Graph Sage<|cite_15|>, GAT<|cite_16|>, GIN<|cite_17|>, etc. Our proposed GSN can be regarded as a variant of GNN. However","['Neural message passing for quantum chemistry', 'Graph neural networks for social recommendation', 'How powerful are graph neural networks?', 'How powerful are graph neural networks?', 'Dynamic coattention networks for question answering']",not_in_top_k,38
q_48,paper_1,Improved semantic representations from tree-structured long short-term memory networks,"e_22|> applied similar idea to derive word embeddings based on GCN. Furthermore, the tree-LSTM model<|cite_23|> and sent-LSTM<|cite_24|> model can also be regarded as implementation of GNN because they both expl","['Semi-supervised classification with graph convolutional networks', 'Semi-supervised classification with graph convolutional networks', 'Multi-hop reading comprehension across multiple documents by reasoning over heterogeneous graphs', 'Multi-hop reading comprehension across multiple documents by reasoning over heterogeneous graphs', 'Multi-hop reading comprehension across multiple documents by reasoning over heterogeneous graphs']",not_in_top_k,38
q_49,paper_1,Sentence-state lstm for text representation,"ea to derive word embeddings based on GCN. Furthermore, the tree-LSTM model<|cite_23|> and sent-LSTM<|cite_24|> model can also be regarded as implementation of GNN because they both explored recurrent message pa","['Semi-supervised classification with graph convolutional networks', 'Semi-supervised classification with graph convolutional networks', 'Neural message passing for quantum chemistry', 'Multi-hop reading comprehension across multiple documents by reasoning over heterogeneous graphs', 'Multi-hop reading comprehension across multiple documents by reasoning over heterogeneous graphs']",not_in_top_k,38
