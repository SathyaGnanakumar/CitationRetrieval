[
  {
    "query_id": "q_0",
    "true_titles": [
      "{Attention is All you Need"
    ],
    "retrieved_titles": [
      "{Adaptive Attention Span in Transformers",
      "{Adaptive Attention Span in Transformers",
      "{Trained Ternary Quantization",
      "{HAQ: Hardware-Aware Automated Quantization with Mixed Precision",
      "{HAQ: Hardware-Aware Automated Quantization with Mixed Precision",
      "{The Best of Both Worlds: Combining Recent Advances in Neural Machine Translation",
      "{Sequence to Sequence Learning with Neural Networks",
      "{Channel Pruning for Accelerating Very Deep Neural Networks",
      "{Learning Transferable Architectures for Scalable Image Recognition",
      "{Binarized Neural Networks: Training Deep Neural Networks with Weights and Activations Constrained to +1 or -1",
      "{Neural Architecture Search with Reinforcement Learning",
      "{Neural Architecture Search with Reinforcement Learning",
      "{Neural Architecture Search with Reinforcement Learning",
      "{Self-Attention with Relative Position Representations",
      "{Self-Attention with Relative Position Representations",
      "{Google's Neural Machine Translation System: Bridging the Gap between Human and Machine Translation",
      "{Attention is All you Need",
      "{Attention is All you Need",
      "{Convolutional Sequence to Sequence Learning",
      "{A Deep Reinforced Model for Abstractive Summarization"
    ],
    "paper_id": "paper_0",
    "context": "Introduction\nTransformer<|cite_0|> is widely used in natural language processing due to its high training efficiency and superior capa..."
  },
  {
    "query_id": "q_1",
    "true_titles": [
      "{BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"
    ],
    "retrieved_titles": [
      "{Learning Transferable Architectures for Scalable Image Recognition",
      "{BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
      "{Adaptive Attention Span in Transformers",
      "{Adaptive Attention Span in Transformers",
      "{Self-Attention with Relative Position Representations",
      "{Self-Attention with Relative Position Representations",
      "{Augmenting self-attention with persistent memory",
      "{Channel Pruning for Accelerating Very Deep Neural Networks",
      "{Neural Machine Translation by Jointly Learning to Align and Translate",
      "{Trained Ternary Quantization",
      "{NetAdapt: Platform-Aware Neural Network Adaptation for Mobile Applications",
      "{The Best of Both Worlds: Combining Recent Advances in Neural Machine Translation",
      "{Google's Neural Machine Translation System: Bridging the Gap between Human and Machine Translation",
      "{FBNet: Hardware-Aware Efficient ConvNet Design via Differentiable Neural Architecture Search",
      "{Efficient Neural Architecture Search via Parameter Sharing",
      "{Convolutional Sequence to Sequence Learning",
      "{Weighted Transformer Network for Machine Translation",
      "{Weighted Transformer Network for Machine Translation",
      "{Sequence to Sequence Learning with Neural Networks",
      "{Neural Architecture Search with Reinforcement Learning"
    ],
    "paper_id": "paper_0",
    "context": "ng long-distance dependencies. Building on top of them, modern state-of-the-art models, such as BERT<|cite_1|>, are able to learn powerful language representations from unlabeled text and even surpass..."
  },
  {
    "query_id": "q_2",
    "true_titles": [
      "{Neural Architecture Search with Reinforcement Learning"
    ],
    "retrieved_titles": [
      "{Learning Transferable Architectures for Scalable Image Recognition",
      "{Efficient Neural Architecture Search via Parameter Sharing",
      "{Learning Efficient Convolutional Networks through Network Slimming",
      "{ProxylessNAS: Direct Neural Architecture Search on Target Task and Hardware",
      "{Neural Architecture Search with Reinforcement Learning",
      "{Neural Architecture Search with Reinforcement Learning",
      "{Neural Architecture Search with Reinforcement Learning",
      "{Channel Pruning for Accelerating Very Deep Neural Networks",
      "{The Best of Both Worlds: Combining Recent Advances in Neural Machine Translation",
      "{FBNet: Hardware-Aware Efficient ConvNet Design via Differentiable Neural Architecture Search",
      "{Trained Ternary Quantization",
      "{Energy and Policy Considerations for Deep Learning in NLP",
      "{MnasNet: Platform-Aware Neural Architecture Search for Mobile",
      "{MetaPruning: Meta Learning for Automatic Neural Network Channel Pruning",
      "{Learning both Weights and Connections for Efficient Neural Networks",
      "{Neural Machine Translation by Jointly Learning to Align and Translate",
      "{A Deep Reinforced Model for Abstractive Summarization",
      "{HAQ: Hardware-Aware Automated Quantization with Mixed Precision",
      "{HAQ: Hardware-Aware Automated Quantization with Mixed Precision",
      "{Convolutional Sequence to Sequence Learning"
    ],
    "paper_id": "paper_0",
    "context": "tecture specialized for real-time NLP applications on the edge. Automatic neural architecture search<|cite_2|> is a choice for high accuracy model design, but the massive search cost (GPU hours and $C..."
  },
  {
    "query_id": "q_3",
    "true_titles": [
      "{Energy and Policy Considerations for Deep Learning in NLP"
    ],
    "retrieved_titles": [
      "{Learning Transferable Architectures for Scalable Image Recognition",
      "{Channel Pruning for Accelerating Very Deep Neural Networks",
      "{MnasNet: Platform-Aware Neural Architecture Search for Mobile",
      "{Trained Ternary Quantization",
      "{MetaPruning: Meta Learning for Automatic Neural Network Channel Pruning",
      "{Efficient Neural Architecture Search via Parameter Sharing",
      "{Learning Efficient Convolutional Networks through Network Slimming",
      "{Sequence to Sequence Learning with Neural Networks",
      "{Adaptive Attention Span in Transformers",
      "{Adaptive Attention Span in Transformers",
      "{FBNet: Hardware-Aware Efficient ConvNet Design via Differentiable Neural Architecture Search",
      "Point-Voxel CNN for efficient 3D deep learning",
      "{Quantizing Deep Convolutional Networks for Efficient Inference: A Whitepaper",
      "{Pay Less Attention with Lightweight and Dynamic Convolutions",
      "{Pay Less Attention with Lightweight and Dynamic Convolutions",
      "{Neural Architecture Search with Reinforcement Learning",
      "{Neural Architecture Search with Reinforcement Learning",
      "{Neural Architecture Search with Reinforcement Learning",
      "{Augmenting self-attention with persistent memory",
      "{ProxylessNAS: Direct Neural Architecture Search on Target Task and Hardware"
    ],
    "paper_id": "paper_0",
    "context": "gn, but the massive search cost (GPU hours and $CO_2$ emission) raises severe environmental concerns<|cite_3|>, shown in \\fig{fig:teaser:b}.\n\nIn this paper, we focus on the efficient inference for mob..."
  },
  {
    "query_id": "q_4",
    "true_titles": [
      "{Pay Less Attention with Lightweight and Dynamic Convolutions"
    ],
    "retrieved_titles": [
      "{Augmenting self-attention with persistent memory",
      "{Learning Transferable Architectures for Scalable Image Recognition",
      "{Channel Pruning for Accelerating Very Deep Neural Networks",
      "{Self-Attention with Relative Position Representations",
      "{Self-Attention with Relative Position Representations",
      "{Efficient Neural Architecture Search via Parameter Sharing",
      "{A Deep Reinforced Model for Abstractive Summarization",
      "{Adaptive Attention Span in Transformers",
      "{Adaptive Attention Span in Transformers",
      "{Trained Ternary Quantization",
      "{Quantizing Deep Convolutional Networks for Efficient Inference: A Whitepaper",
      "{The Best of Both Worlds: Combining Recent Advances in Neural Machine Translation",
      "{Sequence to Sequence Learning with Neural Networks",
      "{HAQ: Hardware-Aware Automated Quantization with Mixed Precision",
      "{HAQ: Hardware-Aware Automated Quantization with Mixed Precision",
      "{Learning Efficient Convolutional Networks through Network Slimming",
      "{Neural Machine Translation by Jointly Learning to Align and Translate",
      "{AMC: AutoML for Model Compression and Acceleration on Mobile Devices",
      "{AMC: AutoML for Model Compression and Acceleration on Mobile Devices",
      "{MnasNet: Platform-Aware Neural Architecture Search for Mobile"
    ],
    "paper_id": "paper_0",
    "context": "ttnshort dedicates \\textit{specialized} heads to model long and short distance contexts. Inspired by<|cite_4|>, \\attnshort introduces convolution in a parallel branch to capture \\textbf{local} depende..."
  },
  {
    "query_id": "q_5",
    "true_titles": [
      "{The Evolved Transformer"
    ],
    "retrieved_titles": [
      "{Learning Transferable Architectures for Scalable Image Recognition",
      "{Channel Pruning for Accelerating Very Deep Neural Networks",
      "{Adaptive Attention Span in Transformers",
      "{Adaptive Attention Span in Transformers",
      "{The Best of Both Worlds: Combining Recent Advances in Neural Machine Translation",
      "{Sequence to Sequence Learning with Neural Networks",
      "{MetaPruning: Meta Learning for Automatic Neural Network Channel Pruning",
      "{HAQ: Hardware-Aware Automated Quantization with Mixed Precision",
      "{HAQ: Hardware-Aware Automated Quantization with Mixed Precision",
      "{Self-Attention with Relative Position Representations",
      "{Self-Attention with Relative Position Representations",
      "{Weighted Transformer Network for Machine Translation",
      "{Weighted Transformer Network for Machine Translation",
      "{Pay Less Attention with Lightweight and Dynamic Convolutions",
      "{Pay Less Attention with Lightweight and Dynamic Convolutions",
      "{AMC: AutoML for Model Compression and Acceleration on Mobile Devices",
      "{AMC: AutoML for Model Compression and Acceleration on Mobile Devices",
      "{Learning Efficient Convolutional Networks through Network Slimming",
      "{Learning both Weights and Connections for Efficient Neural Networks",
      "{Efficient Neural Architecture Search via Parameter Sharing"
    ],
    "paper_id": "paper_0",
    "context": "hts, our manually-designed \\model achieves 0.5 higher BLEU than the AutoML-based Evolved Transformer<|cite_5|>, which requires more than 250 GPU years to search, emitting\nas much carbon as five cars i..."
  },
  {
    "query_id": "q_6",
    "true_titles": [
      "{Sequence to Sequence Learning with Neural Networks"
    ],
    "retrieved_titles": [
      "{Sequence to Sequence Learning with Neural Networks",
      "{Attention is All you Need",
      "{Attention is All you Need",
      "{Energy and Policy Considerations for Deep Learning in NLP",
      "{Trained Ternary Quantization",
      "{Self-Attention with Relative Position Representations",
      "{Self-Attention with Relative Position Representations",
      "{Channel Pruning for Accelerating Very Deep Neural Networks",
      "{Augmenting self-attention with persistent memory",
      "{The Best of Both Worlds: Combining Recent Advances in Neural Machine Translation",
      "{Learning Transferable Architectures for Scalable Image Recognition",
      "{The Evolved Transformer",
      "{The Evolved Transformer",
      "{The Evolved Transformer",
      "{NetAdapt: Platform-Aware Neural Network Adaptation for Mobile Applications",
      "{Neural Architecture Search with Reinforcement Learning",
      "{Neural Architecture Search with Reinforcement Learning",
      "{Neural Architecture Search with Reinforcement Learning",
      "{Convolutional Sequence to Sequence Learning",
      "{MnasNet: Platform-Aware Neural Architecture Search for Mobile"
    ],
    "paper_id": "paper_0",
    "context": "s.}\n\nRecurrent neural networks (RNNs) have prevailed various sequence modeling tasks for a long time<|cite_6|>. However, RNNs are not easy to  parallelize across the sequence due to its temporal depen..."
  },
  {
    "query_id": "q_7",
    "true_titles": [
      "{Neural Machine Translation in Linear Time"
    ],
    "retrieved_titles": [
      "{Weighted Transformer Network for Machine Translation",
      "{Weighted Transformer Network for Machine Translation",
      "{Learning Transferable Architectures for Scalable Image Recognition",
      "{Neural Machine Translation by Jointly Learning to Align and Translate",
      "{A Deep Reinforced Model for Abstractive Summarization",
      "{MnasNet: Platform-Aware Neural Architecture Search for Mobile",
      "{Sequence to Sequence Learning with Neural Networks",
      "{Self-Attention with Relative Position Representations",
      "{Self-Attention with Relative Position Representations",
      "{HAQ: Hardware-Aware Automated Quantization with Mixed Precision",
      "{HAQ: Hardware-Aware Automated Quantization with Mixed Precision",
      "{Augmenting self-attention with persistent memory",
      "{Adaptive Attention Span in Transformers",
      "{Adaptive Attention Span in Transformers",
      "GAN Compression: Efficient Architectures for Interactive Conditional GANs",
      "Point-Voxel CNN for efficient 3D deep learning",
      "{Pay Less Attention with Lightweight and Dynamic Convolutions",
      "{Pay Less Attention with Lightweight and Dynamic Convolutions",
      "{AMC: AutoML for Model Compression and Acceleration on Mobile Devices",
      "{AMC: AutoML for Model Compression and Acceleration on Mobile Devices"
    ],
    "paper_id": "paper_0",
    "context": "e-art performance. For instance, researchers have proposed highly-efficient convolution-based models<|cite_7|>. Convolution is an ideal primitive to model the local context information; however, it la..."
  },
  {
    "query_id": "q_8",
    "true_titles": [
      "{Attention is All you Need"
    ],
    "retrieved_titles": [
      "{Adaptive Attention Span in Transformers",
      "{Adaptive Attention Span in Transformers",
      "{Self-Attention with Relative Position Representations",
      "{Self-Attention with Relative Position Representations",
      "{Augmenting self-attention with persistent memory",
      "{Trained Ternary Quantization",
      "{Weighted Transformer Network for Machine Translation",
      "{Weighted Transformer Network for Machine Translation",
      "{The Best of Both Worlds: Combining Recent Advances in Neural Machine Translation",
      "{MetaPruning: Meta Learning for Automatic Neural Network Channel Pruning",
      "{Efficient Neural Architecture Search via Parameter Sharing",
      "{Pay Less Attention with Lightweight and Dynamic Convolutions",
      "{Pay Less Attention with Lightweight and Dynamic Convolutions",
      "{Energy and Policy Considerations for Deep Learning in NLP",
      "{Channel Pruning for Accelerating Very Deep Neural Networks",
      "{Learning Transferable Architectures for Scalable Image Recognition",
      "{Sequence to Sequence Learning with Neural Networks",
      "{Neural Machine Translation by Jointly Learning to Align and Translate",
      "{NetAdapt: Platform-Aware Neural Network Adaptation for Mobile Applications",
      "{Effective Approaches to Attention-based Neural Machine Translation"
    ],
    "paper_id": "paper_0",
    "context": "native, attention is able to capture global-context information by pairwise correlation. Transformer<|cite_8|> has demonstrated that it is possible to stack the self-attentions to achieve state-of-the..."
  },
  {
    "query_id": "q_9",
    "true_titles": [
      "{Weighted Transformer Network for Machine Translation"
    ],
    "retrieved_titles": [
      "{MnasNet: Platform-Aware Neural Architecture Search for Mobile",
      "{Adaptive Attention Span in Transformers",
      "{Adaptive Attention Span in Transformers",
      "{Neural Machine Translation by Jointly Learning to Align and Translate",
      "{Weighted Transformer Network for Machine Translation",
      "{Weighted Transformer Network for Machine Translation",
      "Point-Voxel CNN for efficient 3D deep learning",
      "{Learning Transferable Architectures for Scalable Image Recognition",
      "{AMC: AutoML for Model Compression and Acceleration on Mobile Devices",
      "{AMC: AutoML for Model Compression and Acceleration on Mobile Devices",
      "{Self-Attention with Relative Position Representations",
      "{Self-Attention with Relative Position Representations",
      "{Sequence to Sequence Learning with Neural Networks",
      "{Channel Pruning for Accelerating Very Deep Neural Networks",
      "{Google's Neural Machine Translation System: Bridging the Gap between Human and Machine Translation",
      "{Augmenting self-attention with persistent memory",
      "{HAQ: Hardware-Aware Automated Quantization with Mixed Precision",
      "{HAQ: Hardware-Aware Automated Quantization with Mixed Precision",
      "{The Best of Both Worlds: Combining Recent Advances in Neural Machine Translation",
      "{A Deep Reinforced Model for Abstractive Summarization"
    ],
    "paper_id": "paper_0",
    "context": "achieve state-of-the-art performance. Recently, there have been a lot of variants to the transformer<|cite_9|>. Among them,<|cite_10|> proposed to scale up the batch size;<|cite_11|> leverages the rel..."
  },
  {
    "query_id": "q_10",
    "true_titles": [
      "{Scaling Neural Machine Translation"
    ],
    "retrieved_titles": [
      "{Neural Machine Translation by Jointly Learning to Align and Translate",
      "{Self-Attention with Relative Position Representations",
      "{Self-Attention with Relative Position Representations",
      "{MnasNet: Platform-Aware Neural Architecture Search for Mobile",
      "{Channel Pruning for Accelerating Very Deep Neural Networks",
      "Point-Voxel CNN for efficient 3D deep learning",
      "{Sequence to Sequence Learning with Neural Networks",
      "{AMC: AutoML for Model Compression and Acceleration on Mobile Devices",
      "{AMC: AutoML for Model Compression and Acceleration on Mobile Devices",
      "{Learning Transferable Architectures for Scalable Image Recognition",
      "{Energy and Policy Considerations for Deep Learning in NLP",
      "{Google's Neural Machine Translation System: Bridging the Gap between Human and Machine Translation",
      "{HAQ: Hardware-Aware Automated Quantization with Mixed Precision",
      "{HAQ: Hardware-Aware Automated Quantization with Mixed Precision",
      "{Weighted Transformer Network for Machine Translation",
      "{Weighted Transformer Network for Machine Translation",
      "{Trained Ternary Quantization",
      "{Adaptive Attention Span in Transformers",
      "{Adaptive Attention Span in Transformers",
      "{The Best of Both Worlds: Combining Recent Advances in Neural Machine Translation"
    ],
    "paper_id": "paper_0",
    "context": "t performance. Recently, there have been a lot of variants to the transformer<|cite_9|>. Among them,<|cite_10|> proposed to scale up the batch size;<|cite_11|> leverages the relative position represen..."
  },
  {
    "query_id": "q_11",
    "true_titles": [
      "{Self-Attention with Relative Position Representations"
    ],
    "retrieved_titles": [
      "{Channel Pruning for Accelerating Very Deep Neural Networks",
      "{Self-Attention with Relative Position Representations",
      "{Self-Attention with Relative Position Representations",
      "Point-Voxel CNN for efficient 3D deep learning",
      "{Weighted Transformer Network for Machine Translation",
      "{Weighted Transformer Network for Machine Translation",
      "{Neural Machine Translation by Jointly Learning to Align and Translate",
      "{Learning Transferable Architectures for Scalable Image Recognition",
      "{Sequence to Sequence Learning with Neural Networks",
      "{HAQ: Hardware-Aware Automated Quantization with Mixed Precision",
      "{HAQ: Hardware-Aware Automated Quantization with Mixed Precision",
      "{Efficient Neural Architecture Search via Parameter Sharing",
      "{AMC: AutoML for Model Compression and Acceleration on Mobile Devices",
      "{AMC: AutoML for Model Compression and Acceleration on Mobile Devices",
      "{MetaPruning: Meta Learning for Automatic Neural Network Channel Pruning",
      "{Trained Ternary Quantization",
      "{The Best of Both Worlds: Combining Recent Advances in Neural Machine Translation",
      "{Adaptive Attention Span in Transformers",
      "{Adaptive Attention Span in Transformers",
      "{Google's Neural Machine Translation System: Bridging the Gap between Human and Machine Translation"
    ],
    "paper_id": "paper_0",
    "context": "f variants to the transformer<|cite_9|>. Among them,<|cite_10|> proposed to scale up the batch size;<|cite_11|> leverages the relative position representations;<|cite_12|> introduces the weighted mult..."
  },
  {
    "query_id": "q_12",
    "true_titles": [
      "{Weighted Transformer Network for Machine Translation"
    ],
    "retrieved_titles": [
      "{Weighted Transformer Network for Machine Translation",
      "{Weighted Transformer Network for Machine Translation",
      "{Channel Pruning for Accelerating Very Deep Neural Networks",
      "{Self-Attention with Relative Position Representations",
      "{Self-Attention with Relative Position Representations",
      "{Learning Transferable Architectures for Scalable Image Recognition",
      "{Neural Machine Translation by Jointly Learning to Align and Translate",
      "{Adaptive Attention Span in Transformers",
      "{Adaptive Attention Span in Transformers",
      "Point-Voxel CNN for efficient 3D deep learning",
      "{The Best of Both Worlds: Combining Recent Advances in Neural Machine Translation",
      "{Trained Ternary Quantization",
      "{Efficient Neural Architecture Search via Parameter Sharing",
      "{Sequence to Sequence Learning with Neural Networks",
      "{MetaPruning: Meta Learning for Automatic Neural Network Channel Pruning",
      "{Learning Efficient Convolutional Networks through Network Slimming",
      "{Attention is All you Need",
      "{Attention is All you Need",
      "{Google's Neural Machine Translation System: Bridging the Gap between Human and Machine Translation",
      "{AMC: AutoML for Model Compression and Acceleration on Mobile Devices"
    ],
    "paper_id": "paper_0",
    "context": "0|> proposed to scale up the batch size;<|cite_11|> leverages the relative position representations;<|cite_12|> introduces the weighted multi-head attention; \\change{<|cite_13|> applies adaptive masks..."
  },
  {
    "query_id": "q_13",
    "true_titles": [
      "{Adaptive Attention Span in Transformers"
    ],
    "retrieved_titles": [
      "{Self-Attention with Relative Position Representations",
      "{Self-Attention with Relative Position Representations",
      "{Adaptive Attention Span in Transformers",
      "{Adaptive Attention Span in Transformers",
      "{Channel Pruning for Accelerating Very Deep Neural Networks",
      "{Augmenting self-attention with persistent memory",
      "{The Best of Both Worlds: Combining Recent Advances in Neural Machine Translation",
      "{Learning Transferable Architectures for Scalable Image Recognition",
      "{MetaPruning: Meta Learning for Automatic Neural Network Channel Pruning",
      "{Trained Ternary Quantization",
      "{Efficient Neural Architecture Search via Parameter Sharing",
      "{MnasNet: Platform-Aware Neural Architecture Search for Mobile",
      "{Pay Less Attention with Lightweight and Dynamic Convolutions",
      "{Pay Less Attention with Lightweight and Dynamic Convolutions",
      "{Weighted Transformer Network for Machine Translation",
      "{Weighted Transformer Network for Machine Translation",
      "{Sequence to Sequence Learning with Neural Networks",
      "{Neural Machine Translation by Jointly Learning to Align and Translate",
      "{Google's Neural Machine Translation System: Bridging the Gap between Human and Machine Translation",
      "{Learning Efficient Convolutional Networks through Network Slimming"
    ],
    "paper_id": "paper_0",
    "context": "relative position representations;<|cite_12|> introduces the weighted multi-head attention; \\change{<|cite_13|> applies adaptive masks for long-range information on character-level language modeling w..."
  },
  {
    "query_id": "q_14",
    "true_titles": [
      "{Neural Architecture Search with Reinforcement Learning"
    ],
    "retrieved_titles": [
      "{FBNet: Hardware-Aware Efficient ConvNet Design via Differentiable Neural Architecture Search",
      "{Learning Transferable Architectures for Scalable Image Recognition",
      "{Efficient Neural Architecture Search via Parameter Sharing",
      "{MnasNet: Platform-Aware Neural Architecture Search for Mobile",
      "{ProxylessNAS: Direct Neural Architecture Search on Target Task and Hardware",
      "{The Evolved Transformer",
      "{The Evolved Transformer",
      "{The Evolved Transformer",
      "{Convolutional Sequence to Sequence Learning",
      "{HAQ: Hardware-Aware Automated Quantization with Mixed Precision",
      "{HAQ: Hardware-Aware Automated Quantization with Mixed Precision",
      "{The Best of Both Worlds: Combining Recent Advances in Neural Machine Translation",
      "{Channel Pruning for Accelerating Very Deep Neural Networks",
      "{MetaPruning: Meta Learning for Automatic Neural Network Channel Pruning",
      "{Neural Architecture Search with Reinforcement Learning",
      "{Neural Architecture Search with Reinforcement Learning",
      "{Neural Architecture Search with Reinforcement Learning",
      "{NetAdapt: Platform-Aware Neural Network Adaptation for Mobile Applications",
      "{Trained Ternary Quantization",
      "{Learning both Weights and Connections for Efficient Neural Networks"
    ],
    "paper_id": "paper_0",
    "context": "chitecture design space, automating the design with neural architecture search (NAS) becomes popular<|cite_14|>. To make the design efficient, integrating the hardware resource constraints into the op..."
  },
  {
    "query_id": "q_15",
    "true_titles": [
      "{MnasNet: Platform-Aware Neural Architecture Search for Mobile"
    ],
    "retrieved_titles": [
      "{NetAdapt: Platform-Aware Neural Network Adaptation for Mobile Applications",
      "{Energy and Policy Considerations for Deep Learning in NLP",
      "{Learning Transferable Architectures for Scalable Image Recognition",
      "{Channel Pruning for Accelerating Very Deep Neural Networks",
      "{FBNet: Hardware-Aware Efficient ConvNet Design via Differentiable Neural Architecture Search",
      "{HAQ: Hardware-Aware Automated Quantization with Mixed Precision",
      "{HAQ: Hardware-Aware Automated Quantization with Mixed Precision",
      "{Trained Ternary Quantization",
      "{Convolutional Sequence to Sequence Learning",
      "{The Best of Both Worlds: Combining Recent Advances in Neural Machine Translation",
      "{Neural Machine Translation by Jointly Learning to Align and Translate",
      "{Adaptive Attention Span in Transformers",
      "{Adaptive Attention Span in Transformers",
      "{Efficient Neural Architecture Search via Parameter Sharing",
      "{MnasNet: Platform-Aware Neural Architecture Search for Mobile",
      "{ProxylessNAS: Direct Neural Architecture Search on Target Task and Hardware",
      "{Deep Compression: Compressing Deep Neural Networks with Pruning, Trained Quantization and Huffman Coding",
      "{Self-Attention with Relative Position Representations",
      "{Self-Attention with Relative Position Representations",
      "Point-Voxel CNN for efficient 3D deep learning"
    ],
    "paper_id": "paper_0",
    "context": "ating the hardware resource constraints into the optimization loop begins to emerge, such as MnasNet<|cite_15|>, ProxylessNAS<|cite_16|> and FBNet<|cite_17|>. In the NLP community, the evolved transfo..."
  },
  {
    "query_id": "q_16",
    "true_titles": [
      "{ProxylessNAS: Direct Neural Architecture Search on Target Task and Hardware"
    ],
    "retrieved_titles": [
      "{Learning Transferable Architectures for Scalable Image Recognition",
      "{Channel Pruning for Accelerating Very Deep Neural Networks",
      "{NetAdapt: Platform-Aware Neural Network Adaptation for Mobile Applications",
      "{Trained Ternary Quantization",
      "{FBNet: Hardware-Aware Efficient ConvNet Design via Differentiable Neural Architecture Search",
      "{Convolutional Sequence to Sequence Learning",
      "{Neural Machine Translation by Jointly Learning to Align and Translate",
      "{Energy and Policy Considerations for Deep Learning in NLP",
      "{The Best of Both Worlds: Combining Recent Advances in Neural Machine Translation",
      "{Efficient Neural Architecture Search via Parameter Sharing",
      "{HAQ: Hardware-Aware Automated Quantization with Mixed Precision",
      "{HAQ: Hardware-Aware Automated Quantization with Mixed Precision",
      "{A Deep Reinforced Model for Abstractive Summarization",
      "{Self-Attention with Relative Position Representations",
      "{Self-Attention with Relative Position Representations",
      "{Google's Neural Machine Translation System: Bridging the Gap between Human and Machine Translation",
      "{MnasNet: Platform-Aware Neural Architecture Search for Mobile",
      "{Weighted Transformer Network for Machine Translation",
      "{Weighted Transformer Network for Machine Translation",
      "{Learning both Weights and Connections for Efficient Neural Networks"
    ],
    "paper_id": "paper_0",
    "context": "ce constraints into the optimization loop begins to emerge, such as MnasNet<|cite_15|>, ProxylessNAS<|cite_16|> and FBNet<|cite_17|>. In the NLP community, the evolved transformer<|cite_18|> adopts th..."
  },
  {
    "query_id": "q_17",
    "true_titles": [
      "{FBNet: Hardware-Aware Efficient ConvNet Design via Differentiable Neural Architecture Search"
    ],
    "retrieved_titles": [
      "{Learning Transferable Architectures for Scalable Image Recognition",
      "{FBNet: Hardware-Aware Efficient ConvNet Design via Differentiable Neural Architecture Search",
      "{Convolutional Sequence to Sequence Learning",
      "{The Best of Both Worlds: Combining Recent Advances in Neural Machine Translation",
      "{Efficient Neural Architecture Search via Parameter Sharing",
      "{Channel Pruning for Accelerating Very Deep Neural Networks",
      "{NetAdapt: Platform-Aware Neural Network Adaptation for Mobile Applications",
      "{Trained Ternary Quantization",
      "{MnasNet: Platform-Aware Neural Architecture Search for Mobile",
      "{The Evolved Transformer",
      "{The Evolved Transformer",
      "{The Evolved Transformer",
      "{Weighted Transformer Network for Machine Translation",
      "{Weighted Transformer Network for Machine Translation",
      "{Learning both Weights and Connections for Efficient Neural Networks",
      "{Energy and Policy Considerations for Deep Learning in NLP",
      "{Neural Machine Translation by Jointly Learning to Align and Translate",
      "{HAQ: Hardware-Aware Automated Quantization with Mixed Precision",
      "{HAQ: Hardware-Aware Automated Quantization with Mixed Precision",
      "{ProxylessNAS: Direct Neural Architecture Search on Target Task and Hardware"
    ],
    "paper_id": "paper_0",
    "context": "he optimization loop begins to emerge, such as MnasNet<|cite_15|>, ProxylessNAS<|cite_16|> and FBNet<|cite_17|>. In the NLP community, the evolved transformer<|cite_18|> adopts the neural architecture..."
  },
  {
    "query_id": "q_18",
    "true_titles": [
      "{The Evolved Transformer"
    ],
    "retrieved_titles": [
      "{Learning Transferable Architectures for Scalable Image Recognition",
      "{The Evolved Transformer",
      "{The Evolved Transformer",
      "{The Evolved Transformer",
      "{FBNet: Hardware-Aware Efficient ConvNet Design via Differentiable Neural Architecture Search",
      "{MnasNet: Platform-Aware Neural Architecture Search for Mobile",
      "{Efficient Neural Architecture Search via Parameter Sharing",
      "{The Best of Both Worlds: Combining Recent Advances in Neural Machine Translation",
      "{Channel Pruning for Accelerating Very Deep Neural Networks",
      "{Weighted Transformer Network for Machine Translation",
      "{Weighted Transformer Network for Machine Translation",
      "{Learning both Weights and Connections for Efficient Neural Networks",
      "{Convolutional Sequence to Sequence Learning",
      "{Energy and Policy Considerations for Deep Learning in NLP",
      "{HAQ: Hardware-Aware Automated Quantization with Mixed Precision",
      "{HAQ: Hardware-Aware Automated Quantization with Mixed Precision",
      "{AMC: AutoML for Model Compression and Acceleration on Mobile Devices",
      "{AMC: AutoML for Model Compression and Acceleration on Mobile Devices",
      "{Neural Machine Translation by Jointly Learning to Align and Translate",
      "{Trained Ternary Quantization"
    ],
    "paper_id": "paper_0",
    "context": "te_15|>, ProxylessNAS<|cite_16|> and FBNet<|cite_17|>. In the NLP community, the evolved transformer<|cite_18|> adopts the neural architecture search<|cite_19|> to design basic blocks and finds a bett..."
  },
  {
    "query_id": "q_19",
    "true_titles": [
      "{Neural Architecture Search with Reinforcement Learning"
    ],
    "retrieved_titles": [
      "{Learning Transferable Architectures for Scalable Image Recognition",
      "{FBNet: Hardware-Aware Efficient ConvNet Design via Differentiable Neural Architecture Search",
      "{The Evolved Transformer",
      "{The Evolved Transformer",
      "{The Evolved Transformer",
      "{Trained Ternary Quantization",
      "{Energy and Policy Considerations for Deep Learning in NLP",
      "{Channel Pruning for Accelerating Very Deep Neural Networks",
      "{MnasNet: Platform-Aware Neural Architecture Search for Mobile",
      "{Efficient Neural Architecture Search via Parameter Sharing",
      "{The Best of Both Worlds: Combining Recent Advances in Neural Machine Translation",
      "{Neural Machine Translation by Jointly Learning to Align and Translate",
      "{Convolutional Sequence to Sequence Learning",
      "{AMC: AutoML for Model Compression and Acceleration on Mobile Devices",
      "{AMC: AutoML for Model Compression and Acceleration on Mobile Devices",
      "{Weighted Transformer Network for Machine Translation",
      "{Weighted Transformer Network for Machine Translation",
      "{Learning both Weights and Connections for Efficient Neural Networks",
      "{HAQ: Hardware-Aware Automated Quantization with Mixed Precision",
      "{HAQ: Hardware-Aware Automated Quantization with Mixed Precision"
    ],
    "paper_id": "paper_0",
    "context": "17|>. In the NLP community, the evolved transformer<|cite_18|> adopts the neural architecture search<|cite_19|> to design basic blocks and finds a better \\#parameter-BLEU trade-off for the transformer..."
  },
  {
    "query_id": "q_20",
    "true_titles": [
      "Point-Voxel CNN for efficient 3D deep learning"
    ],
    "retrieved_titles": [
      "{Efficient Neural Architecture Search via Parameter Sharing",
      "{Learning Transferable Architectures for Scalable Image Recognition",
      "{FBNet: Hardware-Aware Efficient ConvNet Design via Differentiable Neural Architecture Search",
      "{Trained Ternary Quantization",
      "{Adaptive Attention Span in Transformers",
      "{Adaptive Attention Span in Transformers",
      "{Sequence to Sequence Learning with Neural Networks",
      "{Neural Machine Translation by Jointly Learning to Align and Translate",
      "{Channel Pruning for Accelerating Very Deep Neural Networks",
      "{Weighted Transformer Network for Machine Translation",
      "{Weighted Transformer Network for Machine Translation",
      "Point-Voxel CNN for efficient 3D deep learning",
      "{Google's Neural Machine Translation System: Bridging the Gap between Human and Machine Translation",
      "{Energy and Policy Considerations for Deep Learning in NLP",
      "{MnasNet: Platform-Aware Neural Architecture Search for Mobile",
      "{HAQ: Hardware-Aware Automated Quantization with Mixed Precision",
      "{HAQ: Hardware-Aware Automated Quantization with Mixed Precision",
      "{AMC: AutoML for Model Compression and Acceleration on Mobile Devices",
      "{AMC: AutoML for Model Compression and Acceleration on Mobile Devices",
      "{Quantizing Deep Convolutional Networks for Efficient Inference: A Whitepaper"
    ],
    "paper_id": "paper_0",
    "context": "most researchers.\n\n\\myparagraph{Model Acceleration.}\n\nApart from designing efficient models directly<|cite_20|>, another approach to achieve efficient inference is to compress and accelerate the exist..."
  },
  {
    "query_id": "q_21",
    "true_titles": [
      "{Learning both Weights and Connections for Efficient Neural Networks"
    ],
    "retrieved_titles": [
      "{Weighted Transformer Network for Machine Translation",
      "{Weighted Transformer Network for Machine Translation",
      "{Channel Pruning for Accelerating Very Deep Neural Networks",
      "{Trained Ternary Quantization",
      "{Sequence to Sequence Learning with Neural Networks",
      "{The Evolved Transformer",
      "{The Evolved Transformer",
      "{The Evolved Transformer",
      "{NetAdapt: Platform-Aware Neural Network Adaptation for Mobile Applications",
      "Point-Voxel CNN for efficient 3D deep learning",
      "{Efficient Neural Architecture Search via Parameter Sharing",
      "{Energy and Policy Considerations for Deep Learning in NLP",
      "{AMC: AutoML for Model Compression and Acceleration on Mobile Devices",
      "{AMC: AutoML for Model Compression and Acceleration on Mobile Devices",
      "{Neural Machine Translation by Jointly Learning to Align and Translate",
      "{Learning Transferable Architectures for Scalable Image Recognition",
      "{Augmenting self-attention with persistent memory",
      "{Attention is All you Need",
      "{Attention is All you Need",
      "{Google's Neural Machine Translation System: Bridging the Gap between Human and Machine Translation"
    ],
    "paper_id": "paper_0",
    "context": "accelerate the existing large models. For instance, some have proposed to prune the separate neurons<|cite_21|> or the entire channels<|cite_22|>; others have proposed to quantize the network<|cite_23..."
  },
  {
    "query_id": "q_22",
    "true_titles": [
      "{Channel Pruning for Accelerating Very Deep Neural Networks"
    ],
    "retrieved_titles": [
      "{Channel Pruning for Accelerating Very Deep Neural Networks",
      "{Weighted Transformer Network for Machine Translation",
      "{Weighted Transformer Network for Machine Translation",
      "{HAQ: Hardware-Aware Automated Quantization with Mixed Precision",
      "{HAQ: Hardware-Aware Automated Quantization with Mixed Precision",
      "{Learning Transferable Architectures for Scalable Image Recognition",
      "{Google's Neural Machine Translation System: Bridging the Gap between Human and Machine Translation",
      "{Augmenting self-attention with persistent memory",
      "{Self-Attention with Relative Position Representations",
      "{Self-Attention with Relative Position Representations",
      "{A Deep Reinforced Model for Abstractive Summarization",
      "{AMC: AutoML for Model Compression and Acceleration on Mobile Devices",
      "{AMC: AutoML for Model Compression and Acceleration on Mobile Devices",
      "{Sequence to Sequence Learning with Neural Networks",
      "{Energy and Policy Considerations for Deep Learning in NLP",
      "{Efficient Neural Architecture Search via Parameter Sharing",
      "{Trained Ternary Quantization",
      "{Neural Machine Translation by Jointly Learning to Align and Translate",
      "{The Best of Both Worlds: Combining Recent Advances in Neural Machine Translation",
      "{BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"
    ],
    "paper_id": "paper_0",
    "context": "ls. For instance, some have proposed to prune the separate neurons<|cite_21|> or the entire channels<|cite_22|>; others have proposed to quantize the network<|cite_23|> to accelerate the model inferen..."
  },
  {
    "query_id": "q_23",
    "true_titles": [
      "{Binarized Neural Networks: Training Deep Neural Networks with Weights and Activations Constrained to +1 or -1"
    ],
    "retrieved_titles": [
      "{HAQ: Hardware-Aware Automated Quantization with Mixed Precision",
      "{HAQ: Hardware-Aware Automated Quantization with Mixed Precision",
      "{Channel Pruning for Accelerating Very Deep Neural Networks",
      "{Sequence to Sequence Learning with Neural Networks",
      "{Learning Transferable Architectures for Scalable Image Recognition",
      "{AMC: AutoML for Model Compression and Acceleration on Mobile Devices",
      "{AMC: AutoML for Model Compression and Acceleration on Mobile Devices",
      "{Trained Ternary Quantization",
      "{Augmenting self-attention with persistent memory",
      "{Attention is All you Need",
      "{Attention is All you Need",
      "{Neural Machine Translation by Jointly Learning to Align and Translate",
      "{MnasNet: Platform-Aware Neural Architecture Search for Mobile",
      "{Google's Neural Machine Translation System: Bridging the Gap between Human and Machine Translation",
      "{A Deep Reinforced Model for Abstractive Summarization",
      "{Weighted Transformer Network for Machine Translation",
      "{Weighted Transformer Network for Machine Translation",
      "{Energy and Policy Considerations for Deep Learning in NLP",
      "{Self-Attention with Relative Position Representations",
      "{Self-Attention with Relative Position Representations"
    ],
    "paper_id": "paper_0",
    "context": "e neurons<|cite_21|> or the entire channels<|cite_22|>; others have proposed to quantize the network<|cite_23|> to accelerate the model inference. \nRecently, AutoML has also been used to automate the ..."
  },
  {
    "query_id": "q_24",
    "true_titles": [
      "{AMC: AutoML for Model Compression and Acceleration on Mobile Devices"
    ],
    "retrieved_titles": [
      "{AMC: AutoML for Model Compression and Acceleration on Mobile Devices",
      "{AMC: AutoML for Model Compression and Acceleration on Mobile Devices",
      "{The Best of Both Worlds: Combining Recent Advances in Neural Machine Translation",
      "{Learning Transferable Architectures for Scalable Image Recognition",
      "{Attention is All you Need",
      "{Attention is All you Need",
      "{Trained Ternary Quantization",
      "{MnasNet: Platform-Aware Neural Architecture Search for Mobile",
      "{HAQ: Hardware-Aware Automated Quantization with Mixed Precision",
      "{HAQ: Hardware-Aware Automated Quantization with Mixed Precision",
      "{Channel Pruning for Accelerating Very Deep Neural Networks",
      "{Adaptive Attention Span in Transformers",
      "{Adaptive Attention Span in Transformers",
      "{Sequence to Sequence Learning with Neural Networks",
      "{Augmenting self-attention with persistent memory",
      "{Energy and Policy Considerations for Deep Learning in NLP",
      "{A Deep Reinforced Model for Abstractive Summarization",
      "{Convolutional Sequence to Sequence Learning",
      "{Effective Approaches to Attention-based Neural Machine Translation",
      "{Google's Neural Machine Translation System: Bridging the Gap between Human and Machine Translation"
    ],
    "paper_id": "paper_0",
    "context": "l inference. \nRecently, AutoML has also been used to automate the model compression and acceleration<|cite_24|>. All these techniques are compressing existing models and are therefore orthogonal to ou..."
  },
  {
    "query_id": "q_25",
    "true_titles": [
      "Modeling polypharmacy side effects with graph convolutional networks"
    ],
    "retrieved_titles": [
      "Bidirectional attention flow for machine comprehension",
      "Bidirectional attention flow for machine comprehension",
      "Neural message passing for quantum chemistry",
      "Improved semantic representations from tree-structured long short-term memory networks",
      "Graph convolutional networks for text classification",
      "Graph neural networks for social recommendation",
      "Dynamic coattention networks for question answering",
      "Dynamically fused graph network for multi-hop reasoning",
      "Dynamically fused graph network for multi-hop reasoning",
      "Dynamically fused graph network for multi-hop reasoning",
      "Dynamically fused graph network for multi-hop reasoning",
      "Incorporating syntactic and semantic information in word embeddings using graph convolutional networks",
      "How powerful are graph neural networks?",
      "How powerful are graph neural networks?",
      "Exploiting semantics in neural machine translation with graph convolutional networks",
      "Aspect-based sentiment classification with aspect-specific graph convolutional networks",
      "Coarse-grain fine-grain coattention network for multi-evidence question answering",
      "Sentence-state lstm for text representation",
      "Question answering by reasoning across documents with graph convolutional networks",
      "Modeling polypharmacy side effects with graph convolutional networks"
    ],
    "paper_id": "paper_1",
    "context": " has attracted much attention recently,\n\nand have been applied to various tasks such as bio-medicine<|cite_0|>, computational chemistry<|cite_1|>, social networks<|cite_2|>, computer vision, and natur..."
  },
  {
    "query_id": "q_26",
    "true_titles": [
      "Neural message passing for quantum chemistry"
    ],
    "retrieved_titles": [
      "Graph convolutional networks for text classification",
      "Improved semantic representations from tree-structured long short-term memory networks",
      "How powerful are graph neural networks?",
      "How powerful are graph neural networks?",
      "Graph neural networks for social recommendation",
      "Bidirectional attention flow for machine comprehension",
      "Bidirectional attention flow for machine comprehension",
      "Dynamic coattention networks for question answering",
      "Neural message passing for quantum chemistry",
      "Incorporating syntactic and semantic information in word embeddings using graph convolutional networks",
      "Coarse-grain fine-grain coattention network for multi-evidence question answering",
      "Exploiting semantics in neural machine translation with graph convolutional networks",
      "Dynamically fused graph network for multi-hop reasoning",
      "Dynamically fused graph network for multi-hop reasoning",
      "Dynamically fused graph network for multi-hop reasoning",
      "Dynamically fused graph network for multi-hop reasoning",
      "Sentence-state lstm for text representation",
      "Aspect-based sentiment classification with aspect-specific graph convolutional networks",
      "Inductive representation learning on large graphs",
      "Inductive representation learning on large graphs"
    ],
    "paper_id": "paper_1",
    "context": "tly,\n\nand have been applied to various tasks such as bio-medicine<|cite_0|>, computational chemistry<|cite_1|>, social networks<|cite_2|>, computer vision, and natural language understanding<|cite_3|>..."
  },
  {
    "query_id": "q_27",
    "true_titles": [
      "Graph neural networks for social recommendation"
    ],
    "retrieved_titles": [
      "Improved semantic representations from tree-structured long short-term memory networks",
      "Graph convolutional networks for text classification",
      "How powerful are graph neural networks?",
      "How powerful are graph neural networks?",
      "Incorporating syntactic and semantic information in word embeddings using graph convolutional networks",
      "Bidirectional attention flow for machine comprehension",
      "Bidirectional attention flow for machine comprehension",
      "Select, answer and explain: Interpretable multi-hop reading comprehension over multiple documents",
      "Semi-supervised classification with graph convolutional networks",
      "Semi-supervised classification with graph convolutional networks",
      "Coarse-grain fine-grain coattention network for multi-evidence question answering",
      "Graph neural networks for social recommendation",
      "Graph convolutional encoders for syntax-aware neural machine translation",
      "Neural message passing for quantum chemistry",
      "Multi-hop reading comprehension across multiple documents by reasoning over heterogeneous graphs",
      "Multi-hop reading comprehension across multiple documents by reasoning over heterogeneous graphs",
      "Multi-hop reading comprehension across multiple documents by reasoning over heterogeneous graphs",
      "Dynamic coattention networks for question answering",
      "Dynamically fused graph network for multi-hop reasoning",
      "Dynamically fused graph network for multi-hop reasoning"
    ],
    "paper_id": "paper_1",
    "context": " to various tasks such as bio-medicine<|cite_0|>, computational chemistry<|cite_1|>, social networks<|cite_2|>, computer vision, and natural language understanding<|cite_3|>. GNN assumes structured gr..."
  },
  {
    "query_id": "q_28",
    "true_titles": [
      "Dynamically fused graph network for multi-hop reasoning"
    ],
    "retrieved_titles": [
      "Graph convolutional networks for text classification",
      "How powerful are graph neural networks?",
      "How powerful are graph neural networks?",
      "Improved semantic representations from tree-structured long short-term memory networks",
      "Graph neural networks for social recommendation",
      "Graph convolutional encoders for syntax-aware neural machine translation",
      "Coarse-grain fine-grain coattention network for multi-evidence question answering",
      "Neural message passing for quantum chemistry",
      "Dynamic coattention networks for question answering",
      "Dynamically fused graph network for multi-hop reasoning",
      "Dynamically fused graph network for multi-hop reasoning",
      "Dynamically fused graph network for multi-hop reasoning",
      "Dynamically fused graph network for multi-hop reasoning",
      "Bidirectional attention flow for machine comprehension",
      "Bidirectional attention flow for machine comprehension",
      "Select, answer and explain: Interpretable multi-hop reading comprehension over multiple documents",
      "Multi-hop reading comprehension across multiple documents by reasoning over heterogeneous graphs",
      "Multi-hop reading comprehension across multiple documents by reasoning over heterogeneous graphs",
      "Multi-hop reading comprehension across multiple documents by reasoning over heterogeneous graphs",
      "Semi-supervised classification with graph convolutional networks"
    ],
    "paper_id": "paper_1",
    "context": " chemistry<|cite_1|>, social networks<|cite_2|>, computer vision, and natural language understanding<|cite_3|>. GNN assumes structured graphical inputs, for example, molecule graphs, protein-protein i..."
  },
  {
    "query_id": "q_29",
    "true_titles": [
      "Semi-supervised classification with graph convolutional networks"
    ],
    "retrieved_titles": [
      "Dynamic coattention networks for question answering",
      "Exploiting semantics in neural machine translation with graph convolutional networks",
      "Incorporating syntactic and semantic information in word embeddings using graph convolutional networks",
      "Neural message passing for quantum chemistry",
      "How powerful are graph neural networks?",
      "How powerful are graph neural networks?",
      "Graph convolutional networks for text classification",
      "Graph neural networks for social recommendation",
      "Dynamically fused graph network for multi-hop reasoning",
      "Dynamically fused graph network for multi-hop reasoning",
      "Dynamically fused graph network for multi-hop reasoning",
      "Dynamically fused graph network for multi-hop reasoning",
      "Semi-supervised classification with graph convolutional networks",
      "Semi-supervised classification with graph convolutional networks",
      "Bidirectional attention flow for machine comprehension",
      "Bidirectional attention flow for machine comprehension",
      "Question answering by reasoning across documents with graph convolutional networks",
      "Aspect-based sentiment classification with aspect-specific graph convolutional networks",
      "Graph attention networks",
      "Graph attention networks"
    ],
    "paper_id": "paper_1",
    "context": "s have been proposed to learn graph representation,\n\nwhich include Graph Convolutional Network (GCN)<|cite_4|>, GraphSage<|cite_5|>, Graph Isomorphism Network (GIN)<|cite_6|> and Graph Attention Netwo..."
  },
  {
    "query_id": "q_30",
    "true_titles": [
      "Inductive representation learning on large graphs"
    ],
    "retrieved_titles": [
      "Semi-supervised classification with graph convolutional networks",
      "Semi-supervised classification with graph convolutional networks",
      "Incorporating syntactic and semantic information in word embeddings using graph convolutional networks",
      "Dynamically fused graph network for multi-hop reasoning",
      "Dynamically fused graph network for multi-hop reasoning",
      "Dynamically fused graph network for multi-hop reasoning",
      "Dynamically fused graph network for multi-hop reasoning",
      "Question answering by reasoning across documents with graph convolutional networks",
      "Graph convolution over pruned dependency trees improves relation extraction",
      "Aspect-based sentiment classification with aspect-specific graph convolutional networks",
      "Modeling polypharmacy side effects with graph convolutional networks",
      "Graph attention networks",
      "Graph attention networks",
      "Neural message passing for quantum chemistry",
      "Graph convolutional encoders for syntax-aware neural machine translation",
      "Exploiting semantics in neural machine translation with graph convolutional networks",
      "Multi-hop reading comprehension across multiple documents by reasoning over heterogeneous graphs",
      "Multi-hop reading comprehension across multiple documents by reasoning over heterogeneous graphs",
      "Multi-hop reading comprehension across multiple documents by reasoning over heterogeneous graphs",
      "Graph convolutional networks for text classification"
    ],
    "paper_id": "paper_1",
    "context": "to learn graph representation,\n\nwhich include Graph Convolutional Network (GCN)<|cite_4|>, GraphSage<|cite_5|>, Graph Isomorphism Network (GIN)<|cite_6|> and Graph Attention Network (GAT)<|cite_7|>. E..."
  },
  {
    "query_id": "q_31",
    "true_titles": [
      "How powerful are graph neural networks?"
    ],
    "retrieved_titles": [
      "How powerful are graph neural networks?",
      "How powerful are graph neural networks?",
      "Dynamically fused graph network for multi-hop reasoning",
      "Dynamically fused graph network for multi-hop reasoning",
      "Dynamically fused graph network for multi-hop reasoning",
      "Dynamically fused graph network for multi-hop reasoning",
      "Semi-supervised classification with graph convolutional networks",
      "Semi-supervised classification with graph convolutional networks",
      "Graph convolutional encoders for syntax-aware neural machine translation",
      "Aspect-based sentiment classification with aspect-specific graph convolutional networks",
      "Incorporating syntactic and semantic information in word embeddings using graph convolutional networks",
      "Coarse-grain fine-grain coattention network for multi-evidence question answering",
      "Graph convolutional networks for text classification",
      "Graph convolution over pruned dependency trees improves relation extraction",
      "Modeling polypharmacy side effects with graph convolutional networks",
      "Question answering by reasoning across documents with graph convolutional networks",
      "Exploiting semantics in neural machine translation with graph convolutional networks",
      "Dynamic coattention networks for question answering",
      "Neural message passing for quantum chemistry",
      "Inductive representation learning on large graphs"
    ],
    "paper_id": "paper_1",
    "context": "de Graph Convolutional Network (GCN)<|cite_4|>, GraphSage<|cite_5|>, Graph Isomorphism Network (GIN)<|cite_6|> and Graph Attention Network (GAT)<|cite_7|>. Existing GNN variants assume features of eac..."
  },
  {
    "query_id": "q_32",
    "true_titles": [
      "Graph attention networks"
    ],
    "retrieved_titles": [
      "How powerful are graph neural networks?",
      "How powerful are graph neural networks?",
      "Dynamically fused graph network for multi-hop reasoning",
      "Dynamically fused graph network for multi-hop reasoning",
      "Dynamically fused graph network for multi-hop reasoning",
      "Dynamically fused graph network for multi-hop reasoning",
      "Graph neural networks for social recommendation",
      "Semi-supervised classification with graph convolutional networks",
      "Semi-supervised classification with graph convolutional networks",
      "Hierarchical graph network for multi-hop question answering",
      "Hierarchical graph network for multi-hop question answering",
      "Neural message passing for quantum chemistry",
      "Coarse-grain fine-grain coattention network for multi-evidence question answering",
      "Bidirectional attention flow for machine comprehension",
      "Bidirectional attention flow for machine comprehension",
      "Graph attention networks",
      "Graph attention networks",
      "Multi-hop reading comprehension across multiple documents by reasoning over heterogeneous graphs",
      "Multi-hop reading comprehension across multiple documents by reasoning over heterogeneous graphs",
      "Multi-hop reading comprehension across multiple documents by reasoning over heterogeneous graphs"
    ],
    "paper_id": "paper_1",
    "context": "|>, GraphSage<|cite_5|>, Graph Isomorphism Network (GIN)<|cite_6|> and Graph Attention Network (GAT)<|cite_7|>. Existing GNN variants assume features of each node to be a vector, which is initialized ..."
  },
  {
    "query_id": "q_33",
    "true_titles": [
      "Multi-hop reading comprehension across multiple documents by reasoning over heterogeneous graphs"
    ],
    "retrieved_titles": [
      "Neural message passing for quantum chemistry",
      "How powerful are graph neural networks?",
      "How powerful are graph neural networks?",
      "Dynamically fused graph network for multi-hop reasoning",
      "Dynamically fused graph network for multi-hop reasoning",
      "Dynamically fused graph network for multi-hop reasoning",
      "Dynamically fused graph network for multi-hop reasoning",
      "Dynamic coattention networks for question answering",
      "Bidirectional attention flow for machine comprehension",
      "Bidirectional attention flow for machine comprehension",
      "Semi-supervised classification with graph convolutional networks",
      "Semi-supervised classification with graph convolutional networks",
      "Graph neural networks for social recommendation",
      "Graph convolutional networks for text classification",
      "Inductive representation learning on large graphs",
      "Inductive representation learning on large graphs",
      "Aspect-based sentiment classification with aspect-specific graph convolutional networks",
      "Coarse-grain fine-grain coattention network for multi-evidence question answering",
      "Graph convolution over pruned dependency trees improves relation extraction",
      "Graph convolutional encoders for syntax-aware neural machine translation"
    ],
    "paper_id": "paper_1",
    "context": "ge passing algorithm is applied to obtain node representations from these summarized feature vectors<|cite_8|>.\n\nHowever, this early summarization strategy (summarization before GNN based representati..."
  },
  {
    "query_id": "q_34",
    "true_titles": [
      "Bidirectional attention flow for machine comprehension"
    ],
    "retrieved_titles": [
      "How powerful are graph neural networks?",
      "How powerful are graph neural networks?",
      "Dynamically fused graph network for multi-hop reasoning",
      "Dynamically fused graph network for multi-hop reasoning",
      "Dynamically fused graph network for multi-hop reasoning",
      "Dynamically fused graph network for multi-hop reasoning",
      "Question answering by reasoning across documents with graph convolutional networks",
      "Multi-hop reading comprehension across multiple documents by reasoning over heterogeneous graphs",
      "Multi-hop reading comprehension across multiple documents by reasoning over heterogeneous graphs",
      "Multi-hop reading comprehension across multiple documents by reasoning over heterogeneous graphs",
      "Semi-supervised classification with graph convolutional networks",
      "Semi-supervised classification with graph convolutional networks",
      "Graph neural networks for social recommendation",
      "Select, answer and explain: Interpretable multi-hop reading comprehension over multiple documents",
      "Incorporating syntactic and semantic information in word embeddings using graph convolutional networks",
      "Inductive representation learning on large graphs",
      "Inductive representation learning on large graphs",
      "Bidirectional attention flow for machine comprehension",
      "Bidirectional attention flow for machine comprehension",
      "Coarse-grain fine-grain coattention network for multi-evidence question answering"
    ],
    "paper_id": "paper_1",
    "context": "egy (summarization before GNN based representation learning) could bring inevitable information loss<|cite_9|>, and result in\ninformation flow bottleneck thus less powerful reasoning ability among gra..."
  },
  {
    "query_id": "q_35",
    "true_titles": [
      "Dynamic coattention networks for question answering"
    ],
    "retrieved_titles": [
      "Question answering by reasoning across documents with graph convolutional networks",
      "Dynamically fused graph network for multi-hop reasoning",
      "Dynamically fused graph network for multi-hop reasoning",
      "Dynamically fused graph network for multi-hop reasoning",
      "Dynamically fused graph network for multi-hop reasoning",
      "Select, answer and explain: Interpretable multi-hop reading comprehension over multiple documents",
      "Graph convolution over pruned dependency trees improves relation extraction",
      "Bidirectional attention flow for machine comprehension",
      "Bidirectional attention flow for machine comprehension",
      "Multi-hop reading comprehension across multiple documents by reasoning over heterogeneous graphs",
      "Multi-hop reading comprehension across multiple documents by reasoning over heterogeneous graphs",
      "Multi-hop reading comprehension across multiple documents by reasoning over heterogeneous graphs",
      "Incorporating syntactic and semantic information in word embeddings using graph convolutional networks",
      "Coarse-grain fine-grain coattention network for multi-evidence question answering",
      "Semi-supervised classification with graph convolutional networks",
      "Semi-supervised classification with graph convolutional networks",
      "Dynamic coattention networks for question answering",
      "Hierarchical graph network for multi-hop question answering",
      "Hierarchical graph network for multi-hop question answering",
      "Aspect-based sentiment classification with aspect-specific graph convolutional networks"
    ],
    "paper_id": "paper_1",
    "context": "as a way to encode query-aware contextual information based on affinity matrix between two sequences<|cite_10|>. In the context of this paper, the advantage of co-attention is that it can encode neigh..."
  },
  {
    "query_id": "q_36",
    "true_titles": [
      "Hotpotqa: A dataset for diverse, explainable multi-hop question answering"
    ],
    "retrieved_titles": [
      "Semi-supervised classification with graph convolutional networks",
      "Semi-supervised classification with graph convolutional networks",
      "How powerful are graph neural networks?",
      "How powerful are graph neural networks?",
      "Dynamically fused graph network for multi-hop reasoning",
      "Dynamically fused graph network for multi-hop reasoning",
      "Dynamically fused graph network for multi-hop reasoning",
      "Dynamically fused graph network for multi-hop reasoning",
      "Dynamic coattention networks for question answering",
      "Gear: Graph-based evidence aggregating and reasoning for fact verification",
      "Gear: Graph-based evidence aggregating and reasoning for fact verification",
      "Fever: a large-scale dataset for fact extraction and verification",
      "Select, answer and explain: Interpretable multi-hop reading comprehension over multiple documents",
      "Graph convolution over pruned dependency trees improves relation extraction",
      "Incorporating syntactic and semantic information in word embeddings using graph convolutional networks",
      "Graph convolutional encoders for syntax-aware neural machine translation",
      "Bidirectional attention flow for machine comprehension",
      "Bidirectional attention flow for machine comprehension",
      "Multi-hop reading comprehension across multiple documents by reasoning over heterogeneous graphs",
      "Multi-hop reading comprehension across multiple documents by reasoning over heterogeneous graphs"
    ],
    "paper_id": "paper_1",
    "context": "am}.\n\nTo validate the effectiveness of the proposed GSN, we experiment on two NLP datasets: HotpotQA<|cite_11|> and fact extraction and verification data set provided by FEVER shared task 1.0<|cite_12..."
  },
  {
    "query_id": "q_37",
    "true_titles": [
      "Fever: a large-scale dataset for fact extraction and verification"
    ],
    "retrieved_titles": [
      "Dynamically fused graph network for multi-hop reasoning",
      "Dynamically fused graph network for multi-hop reasoning",
      "Dynamically fused graph network for multi-hop reasoning",
      "Dynamically fused graph network for multi-hop reasoning",
      "How powerful are graph neural networks?",
      "How powerful are graph neural networks?",
      "Gear: Graph-based evidence aggregating and reasoning for fact verification",
      "Gear: Graph-based evidence aggregating and reasoning for fact verification",
      "Semi-supervised classification with graph convolutional networks",
      "Semi-supervised classification with graph convolutional networks",
      "Neural message passing for quantum chemistry",
      "Exploiting semantics in neural machine translation with graph convolutional networks",
      "Bidirectional attention flow for machine comprehension",
      "Bidirectional attention flow for machine comprehension",
      "Graph neural networks for social recommendation",
      "Coarse-grain fine-grain coattention network for multi-evidence question answering",
      "Question answering by reasoning across documents with graph convolutional networks",
      "Dynamic coattention networks for question answering",
      "Multi-hop reading comprehension across multiple documents by reasoning over heterogeneous graphs",
      "Multi-hop reading comprehension across multiple documents by reasoning over heterogeneous graphs"
    ],
    "paper_id": "paper_1",
    "context": " HotpotQA<|cite_11|> and fact extraction and verification data set provided by FEVER shared task 1.0<|cite_12|>. Both tasks require the model to have reasoning ability, and top performance has been ac..."
  },
  {
    "query_id": "q_38",
    "true_titles": [
      "Gear: Graph-based evidence aggregating and reasoning for fact verification"
    ],
    "retrieved_titles": [
      "Dynamically fused graph network for multi-hop reasoning",
      "Dynamically fused graph network for multi-hop reasoning",
      "Dynamically fused graph network for multi-hop reasoning",
      "Dynamically fused graph network for multi-hop reasoning",
      "How powerful are graph neural networks?",
      "How powerful are graph neural networks?",
      "Dynamic coattention networks for question answering",
      "Coarse-grain fine-grain coattention network for multi-evidence question answering",
      "Bidirectional attention flow for machine comprehension",
      "Bidirectional attention flow for machine comprehension",
      "Neural message passing for quantum chemistry",
      "Multi-hop reading comprehension across multiple documents by reasoning over heterogeneous graphs",
      "Multi-hop reading comprehension across multiple documents by reasoning over heterogeneous graphs",
      "Multi-hop reading comprehension across multiple documents by reasoning over heterogeneous graphs",
      "Select, answer and explain: Interpretable multi-hop reading comprehension over multiple documents",
      "Semi-supervised classification with graph convolutional networks",
      "Semi-supervised classification with graph convolutional networks",
      "Graph convolutional networks for text classification",
      "Question answering by reasoning across documents with graph convolutional networks",
      "Exploiting semantics in neural machine translation with graph convolutional networks"
    ],
    "paper_id": "paper_1",
    "context": "ve reasoning ability, and top performance has been achieved with early summarization followed by GNN<|cite_13|>. With thorough experiments, we show that the proposed GSN achieves better performance th..."
  },
  {
    "query_id": "q_39",
    "true_titles": [
      "Semi-supervised classification with graph convolutional networks"
    ],
    "retrieved_titles": [
      "How powerful are graph neural networks?",
      "How powerful are graph neural networks?",
      "Neural message passing for quantum chemistry",
      "Dynamically fused graph network for multi-hop reasoning",
      "Dynamically fused graph network for multi-hop reasoning",
      "Dynamically fused graph network for multi-hop reasoning",
      "Dynamically fused graph network for multi-hop reasoning",
      "Graph neural networks for social recommendation",
      "Dynamic coattention networks for question answering",
      "Multi-hop reading comprehension across multiple documents by reasoning over heterogeneous graphs",
      "Multi-hop reading comprehension across multiple documents by reasoning over heterogeneous graphs",
      "Multi-hop reading comprehension across multiple documents by reasoning over heterogeneous graphs",
      "Bidirectional attention flow for machine comprehension",
      "Bidirectional attention flow for machine comprehension",
      "Graph convolutional encoders for syntax-aware neural machine translation",
      "Exploiting semantics in neural machine translation with graph convolutional networks",
      "Semi-supervised classification with graph convolutional networks",
      "Semi-supervised classification with graph convolutional networks",
      "Graph convolution over pruned dependency trees improves relation extraction",
      "Graph convolutional networks for text classification"
    ],
    "paper_id": "paper_1",
    "context": "\nMultiple GNN variants have been proposed with different message passing algorithms. For example GCN<|cite_14|>, Graph Sage<|cite_15|>, GAT<|cite_16|>, GIN<|cite_17|>, etc. Our proposed GSN can be reg..."
  },
  {
    "query_id": "q_40",
    "true_titles": [
      "Inductive representation learning on large graphs"
    ],
    "retrieved_titles": [
      "Neural message passing for quantum chemistry",
      "Graph neural networks for social recommendation",
      "How powerful are graph neural networks?",
      "How powerful are graph neural networks?",
      "Dynamic coattention networks for question answering",
      "Dynamically fused graph network for multi-hop reasoning",
      "Dynamically fused graph network for multi-hop reasoning",
      "Dynamically fused graph network for multi-hop reasoning",
      "Dynamically fused graph network for multi-hop reasoning",
      "Semi-supervised classification with graph convolutional networks",
      "Semi-supervised classification with graph convolutional networks",
      "Multi-hop reading comprehension across multiple documents by reasoning over heterogeneous graphs",
      "Multi-hop reading comprehension across multiple documents by reasoning over heterogeneous graphs",
      "Multi-hop reading comprehension across multiple documents by reasoning over heterogeneous graphs",
      "Bidirectional attention flow for machine comprehension",
      "Bidirectional attention flow for machine comprehension",
      "Exploiting semantics in neural machine translation with graph convolutional networks",
      "Graph convolutional encoders for syntax-aware neural machine translation",
      "Graph convolution over pruned dependency trees improves relation extraction",
      "Incorporating syntactic and semantic information in word embeddings using graph convolutional networks"
    ],
    "paper_id": "paper_1",
    "context": "have been proposed with different message passing algorithms. For example GCN<|cite_14|>, Graph Sage<|cite_15|>, GAT<|cite_16|>, GIN<|cite_17|>, etc. Our proposed GSN can be regarded as a variant of G..."
  },
  {
    "query_id": "q_41",
    "true_titles": [
      "Graph attention networks"
    ],
    "retrieved_titles": [
      "Dynamically fused graph network for multi-hop reasoning",
      "Dynamically fused graph network for multi-hop reasoning",
      "Dynamically fused graph network for multi-hop reasoning",
      "Dynamically fused graph network for multi-hop reasoning",
      "Semi-supervised classification with graph convolutional networks",
      "Semi-supervised classification with graph convolutional networks",
      "How powerful are graph neural networks?",
      "How powerful are graph neural networks?",
      "Graph convolution over pruned dependency trees improves relation extraction",
      "Neural message passing for quantum chemistry",
      "Graph neural networks for social recommendation",
      "Multi-hop reading comprehension across multiple documents by reasoning over heterogeneous graphs",
      "Multi-hop reading comprehension across multiple documents by reasoning over heterogeneous graphs",
      "Multi-hop reading comprehension across multiple documents by reasoning over heterogeneous graphs",
      "Graph convolutional encoders for syntax-aware neural machine translation",
      "Dynamic coattention networks for question answering",
      "Graph convolutional networks for text classification",
      "Modeling polypharmacy side effects with graph convolutional networks",
      "Select, answer and explain: Interpretable multi-hop reading comprehension over multiple documents",
      "Graph attention networks"
    ],
    "paper_id": "paper_1",
    "context": "ed with different message passing algorithms. For example GCN<|cite_14|>, Graph Sage<|cite_15|>, GAT<|cite_16|>, GIN<|cite_17|>, etc. Our proposed GSN can be regarded as a variant of GNN. However, GSN..."
  },
  {
    "query_id": "q_42",
    "true_titles": [
      "How powerful are graph neural networks?"
    ],
    "retrieved_titles": [
      "How powerful are graph neural networks?",
      "How powerful are graph neural networks?",
      "Dynamically fused graph network for multi-hop reasoning",
      "Dynamically fused graph network for multi-hop reasoning",
      "Dynamically fused graph network for multi-hop reasoning",
      "Dynamically fused graph network for multi-hop reasoning",
      "Semi-supervised classification with graph convolutional networks",
      "Semi-supervised classification with graph convolutional networks",
      "Neural message passing for quantum chemistry",
      "Graph neural networks for social recommendation",
      "Graph convolution over pruned dependency trees improves relation extraction",
      "Graph convolutional encoders for syntax-aware neural machine translation",
      "Dynamic coattention networks for question answering",
      "Inductive representation learning on large graphs",
      "Inductive representation learning on large graphs",
      "Multi-hop reading comprehension across multiple documents by reasoning over heterogeneous graphs",
      "Multi-hop reading comprehension across multiple documents by reasoning over heterogeneous graphs",
      "Multi-hop reading comprehension across multiple documents by reasoning over heterogeneous graphs",
      "Select, answer and explain: Interpretable multi-hop reading comprehension over multiple documents",
      "Graph convolutional networks for text classification"
    ],
    "paper_id": "paper_1",
    "context": "t message passing algorithms. For example GCN<|cite_14|>, Graph Sage<|cite_15|>, GAT<|cite_16|>, GIN<|cite_17|>, etc. Our proposed GSN can be regarded as a variant of GNN. However, GSN differs from pr..."
  },
  {
    "query_id": "q_43",
    "true_titles": [
      "Graph convolutional encoders for syntax-aware neural machine translation"
    ],
    "retrieved_titles": [
      "Improved semantic representations from tree-structured long short-term memory networks",
      "Graph convolutional encoders for syntax-aware neural machine translation",
      "Incorporating syntactic and semantic information in word embeddings using graph convolutional networks",
      "Graph neural networks for social recommendation",
      "Aspect-based sentiment classification with aspect-specific graph convolutional networks",
      "Exploiting semantics in neural machine translation with graph convolutional networks",
      "Graph convolutional networks for text classification",
      "How powerful are graph neural networks?",
      "How powerful are graph neural networks?",
      "Question answering by reasoning across documents with graph convolutional networks",
      "Semi-supervised classification with graph convolutional networks",
      "Semi-supervised classification with graph convolutional networks",
      "Graph convolution over pruned dependency trees improves relation extraction",
      "Neural message passing for quantum chemistry",
      "Dynamic coattention networks for question answering",
      "Dynamically fused graph network for multi-hop reasoning",
      "Dynamically fused graph network for multi-hop reasoning",
      "Dynamically fused graph network for multi-hop reasoning",
      "Dynamically fused graph network for multi-hop reasoning",
      "Graph attention networks"
    ],
    "paper_id": "paper_1",
    "context": "oup usually builds graphs from parsing trees or develops graph-like Recurrent Neural Networks (RNN).<|cite_18|> and<|cite_19|> explored building graphs from syntactic or semantic parsing trees and ins..."
  },
  {
    "query_id": "q_44",
    "true_titles": [
      "Exploiting semantics in neural machine translation with graph convolutional networks"
    ],
    "retrieved_titles": [
      "Improved semantic representations from tree-structured long short-term memory networks",
      "Graph convolutional encoders for syntax-aware neural machine translation",
      "Semi-supervised classification with graph convolutional networks",
      "Semi-supervised classification with graph convolutional networks",
      "Question answering by reasoning across documents with graph convolutional networks",
      "Incorporating syntactic and semantic information in word embeddings using graph convolutional networks",
      "Graph neural networks for social recommendation",
      "Aspect-based sentiment classification with aspect-specific graph convolutional networks",
      "Exploiting semantics in neural machine translation with graph convolutional networks",
      "Graph convolutional networks for text classification",
      "Graph attention networks",
      "Graph attention networks",
      "How powerful are graph neural networks?",
      "How powerful are graph neural networks?",
      "Graph convolution over pruned dependency trees improves relation extraction",
      "Neural message passing for quantum chemistry",
      "Dynamic coattention networks for question answering",
      "Coarse-grain fine-grain coattention network for multi-evidence question answering",
      "Dynamically fused graph network for multi-hop reasoning",
      "Dynamically fused graph network for multi-hop reasoning"
    ],
    "paper_id": "paper_1",
    "context": "lds graphs from parsing trees or develops graph-like Recurrent Neural Networks (RNN).<|cite_18|> and<|cite_19|> explored building graphs from syntactic or semantic parsing trees and inserted a GCN bas..."
  },
  {
    "query_id": "q_45",
    "true_titles": [
      "Graph convolution over pruned dependency trees improves relation extraction"
    ],
    "retrieved_titles": [
      "Graph convolutional encoders for syntax-aware neural machine translation",
      "Graph convolution over pruned dependency trees improves relation extraction",
      "Exploiting semantics in neural machine translation with graph convolutional networks",
      "Dynamically fused graph network for multi-hop reasoning",
      "Dynamically fused graph network for multi-hop reasoning",
      "Dynamically fused graph network for multi-hop reasoning",
      "Dynamically fused graph network for multi-hop reasoning",
      "Dynamic coattention networks for question answering",
      "Semi-supervised classification with graph convolutional networks",
      "Semi-supervised classification with graph convolutional networks",
      "How powerful are graph neural networks?",
      "How powerful are graph neural networks?",
      "Bidirectional attention flow for machine comprehension",
      "Bidirectional attention flow for machine comprehension",
      "Incorporating syntactic and semantic information in word embeddings using graph convolutional networks",
      "Improved semantic representations from tree-structured long short-term memory networks",
      "Aspect-based sentiment classification with aspect-specific graph convolutional networks",
      "Question answering by reasoning across documents with graph convolutional networks",
      "Graph attention networks",
      "Graph attention networks"
    ],
    "paper_id": "paper_1",
    "context": " inserted a GCN based sub-network to the encoder of sequence-to-sequence machine translation models.<|cite_20|> applied GCN on pruned syntactic dependency trees for relation extraction.<|cite_21|> pro..."
  },
  {
    "query_id": "q_46",
    "true_titles": [
      "Aspect-based sentiment classification with aspect-specific graph convolutional networks"
    ],
    "retrieved_titles": [
      "Graph convolution over pruned dependency trees improves relation extraction",
      "Graph convolutional encoders for syntax-aware neural machine translation",
      "Aspect-based sentiment classification with aspect-specific graph convolutional networks",
      "Improved semantic representations from tree-structured long short-term memory networks",
      "Gear: Graph-based evidence aggregating and reasoning for fact verification",
      "Gear: Graph-based evidence aggregating and reasoning for fact verification",
      "Bidirectional attention flow for machine comprehension",
      "Bidirectional attention flow for machine comprehension",
      "Graph convolutional networks for text classification",
      "How powerful are graph neural networks?",
      "How powerful are graph neural networks?",
      "Select, answer and explain: Interpretable multi-hop reading comprehension over multiple documents",
      "Dynamic coattention networks for question answering",
      "Multi-hop reading comprehension across multiple documents by reasoning over heterogeneous graphs",
      "Multi-hop reading comprehension across multiple documents by reasoning over heterogeneous graphs",
      "Multi-hop reading comprehension across multiple documents by reasoning over heterogeneous graphs",
      "Dynamically fused graph network for multi-hop reasoning",
      "Dynamically fused graph network for multi-hop reasoning",
      "Dynamically fused graph network for multi-hop reasoning",
      "Dynamically fused graph network for multi-hop reasoning"
    ],
    "paper_id": "paper_1",
    "context": "slation models.<|cite_20|> applied GCN on pruned syntactic dependency trees for relation extraction.<|cite_21|> proposed to use GCN over syntactic dependency trees for aspect-based sentiment classific..."
  },
  {
    "query_id": "q_47",
    "true_titles": [
      "Incorporating syntactic and semantic information in word embeddings using graph convolutional networks"
    ],
    "retrieved_titles": [
      "Graph convolutional encoders for syntax-aware neural machine translation",
      "Aspect-based sentiment classification with aspect-specific graph convolutional networks",
      "Graph convolution over pruned dependency trees improves relation extraction",
      "Incorporating syntactic and semantic information in word embeddings using graph convolutional networks",
      "Dynamically fused graph network for multi-hop reasoning",
      "Dynamically fused graph network for multi-hop reasoning",
      "Dynamically fused graph network for multi-hop reasoning",
      "Dynamically fused graph network for multi-hop reasoning",
      "Graph attention networks",
      "Graph attention networks",
      "Multi-hop reading comprehension across multiple documents by reasoning over heterogeneous graphs",
      "Multi-hop reading comprehension across multiple documents by reasoning over heterogeneous graphs",
      "Multi-hop reading comprehension across multiple documents by reasoning over heterogeneous graphs",
      "Bidirectional attention flow for machine comprehension",
      "Bidirectional attention flow for machine comprehension",
      "Dynamic coattention networks for question answering",
      "Graph convolutional networks for text classification",
      "Gear: Graph-based evidence aggregating and reasoning for fact verification",
      "Gear: Graph-based evidence aggregating and reasoning for fact verification",
      "How powerful are graph neural networks?"
    ],
    "paper_id": "paper_1",
    "context": "_21|> proposed to use GCN over syntactic dependency trees for aspect-based sentiment classification.<|cite_22|> applied similar idea to derive word embeddings based on GCN. Furthermore, the tree-LSTM ..."
  },
  {
    "query_id": "q_48",
    "true_titles": [
      "Improved semantic representations from tree-structured long short-term memory networks"
    ],
    "retrieved_titles": [
      "Semi-supervised classification with graph convolutional networks",
      "Semi-supervised classification with graph convolutional networks",
      "Multi-hop reading comprehension across multiple documents by reasoning over heterogeneous graphs",
      "Multi-hop reading comprehension across multiple documents by reasoning over heterogeneous graphs",
      "Multi-hop reading comprehension across multiple documents by reasoning over heterogeneous graphs",
      "How powerful are graph neural networks?",
      "How powerful are graph neural networks?",
      "Neural message passing for quantum chemistry",
      "Question answering by reasoning across documents with graph convolutional networks",
      "Aspect-based sentiment classification with aspect-specific graph convolutional networks",
      "Dynamic coattention networks for question answering",
      "Incorporating syntactic and semantic information in word embeddings using graph convolutional networks",
      "Select, answer and explain: Interpretable multi-hop reading comprehension over multiple documents",
      "Coarse-grain fine-grain coattention network for multi-evidence question answering",
      "Graph neural networks for social recommendation",
      "Dynamically fused graph network for multi-hop reasoning",
      "Dynamically fused graph network for multi-hop reasoning",
      "Dynamically fused graph network for multi-hop reasoning",
      "Dynamically fused graph network for multi-hop reasoning",
      "Inductive representation learning on large graphs"
    ],
    "paper_id": "paper_1",
    "context": "e_22|> applied similar idea to derive word embeddings based on GCN. Furthermore, the tree-LSTM model<|cite_23|> and sent-LSTM<|cite_24|> model can also be regarded as implementation of GNN because the..."
  },
  {
    "query_id": "q_49",
    "true_titles": [
      "Sentence-state lstm for text representation"
    ],
    "retrieved_titles": [
      "Semi-supervised classification with graph convolutional networks",
      "Semi-supervised classification with graph convolutional networks",
      "Neural message passing for quantum chemistry",
      "Multi-hop reading comprehension across multiple documents by reasoning over heterogeneous graphs",
      "Multi-hop reading comprehension across multiple documents by reasoning over heterogeneous graphs",
      "Multi-hop reading comprehension across multiple documents by reasoning over heterogeneous graphs",
      "How powerful are graph neural networks?",
      "How powerful are graph neural networks?",
      "Question answering by reasoning across documents with graph convolutional networks",
      "Dynamic coattention networks for question answering",
      "Incorporating syntactic and semantic information in word embeddings using graph convolutional networks",
      "Select, answer and explain: Interpretable multi-hop reading comprehension over multiple documents",
      "Coarse-grain fine-grain coattention network for multi-evidence question answering",
      "Graph neural networks for social recommendation",
      "Improved semantic representations from tree-structured long short-term memory networks",
      "Dynamically fused graph network for multi-hop reasoning",
      "Dynamically fused graph network for multi-hop reasoning",
      "Dynamically fused graph network for multi-hop reasoning",
      "Dynamically fused graph network for multi-hop reasoning",
      "Inductive representation learning on large graphs"
    ],
    "paper_id": "paper_1",
    "context": "ea to derive word embeddings based on GCN. Furthermore, the tree-LSTM model<|cite_23|> and sent-LSTM<|cite_24|> model can also be regarded as implementation of GNN because they both explored recurrent..."
  }
]