{"example_id": "blind_964", "prompt": "You are an expert citation retrieval system as described in the paper \"Multi-Agent System for Reliable Citation Retrieval\".\nYour goal is to autonomously retrieve, verify, and recommend academic references given a query or document excerpt.\n\nTask:\nGiven a citation context from a scientific paper (where a citation is missing), identify the correct paper from a list of candidates.\nAnalyze the context to understand the specific claim, method, or result being cited.\nThen, evaluate each candidate paper to see if it matches the context.\nFinally, select the best matching paper.\n\nCitation Context:\nn backbones with language models with intermediate networks and billions of data pairs, exhibiting superior results on vision-language tasks. OFA, Uni-Perceivers, and Unified-IO[CITATION] also introduce unified training architectures for different modalities with competitive performance to uni-modal methods. Recently, inspired by the powerful GPT-4, many efforts have been d\n\nCandidate Papers:\n1. Title: Microsoft coco captions: Data collection and evaluation server\n   Abstract: Microsoft COCO Captions: Data Collection and Evaluation Server: In this paper we describe the Microsoft COCO Caption dataset and evaluation server. When completed, the dataset will contain over one and a half million captions describing over 330,000 images. For the training and validation images, five independent human generated captions will be provided. To ensure consistency in evaluation of automatic caption generation algorithms, an evaluation server is used. The evaluation server receives candidate captions and scores them using several popular metrics, including BLEU, METEOR, ROUGE and CIDEr. Instructions for using the evaluation server are provided.\n\n2. Title: Photorealistic Text-to-Image Diffusion Models with Deep Language Understanding\n   Abstract: Photorealistic Text-to-Image Diffusion Models with Deep Language Understanding: We present Imagen, a text-to-image diffusion model with an unprecedented degree of photorealism and a deep level of language understanding. Imagen builds on the power of large transformer language models in understanding text and hinges on the strength of diffusion models in high-fidelity image generation. Our key discovery is that generic large language models (e.g. T5), pretrained on text-only corpora, are surprisingly effective at encoding text for image synthesis: increasing the size of the language model in Imagen boosts both sample fidelity and image-text alignment much more than increasing the size of the image diffusion model. Imagen achieves a new state-of-the-art FID score of 7.27 on the COCO dataset, without ever training on COCO, and human raters find Imagen samples to be on par with the COCO data itself in image-text alignment. To assess text-to-image models in greater depth, we introduce DrawBench, a comprehensive and challenging benchmark for text-to-image models. With DrawBench, we compare Imagen with recent methods including VQ-GAN+CLIP, Latent Diffusion Models, and DALL-E 2, and find that human raters prefer Imagen over other models in side-by-side comparisons, both in terms of sample quality and image-text alignment. See https://imagen.research.google/ for an overview of the results.\n\n3. Title: Unified-io: A unified model for vision, language, and multi-modal tasks\n   Abstract: Unified-IO: A Unified Model for Vision, Language, and Multi-Modal Tasks: We propose Unified-IO, a model that performs a large variety of AI tasks spanning classical computer vision tasks, including pose estimation, object detection, depth estimation and image generation, vision-and-language tasks such as region captioning and referring expression, to natural language processing tasks such as question answering and paraphrasing. Developing a single unified model for such a large variety of tasks poses unique challenges due to the heterogeneous inputs and outputs pertaining to each task, including RGB images, per-pixel maps, binary masks, bounding boxes, and language. We achieve this unification by homogenizing every supported input and output into a sequence of discrete vocabulary tokens. This common representation across all tasks allows us to train a single transformer-based architecture, jointly on over 90 diverse datasets in the vision and language fields. Unified-IO is the first model capable of performing all 7 tasks on the GRIT benchmark and produces strong results across 16 diverse benchmarks like NYUv2-Depth, ImageNet, VQA2.0, OK-VQA, Swig, VizWizGround, BoolQ, and SciTail, with no task-specific fine-tuning. Code and demos for Unified-IO are available at: https://unified-io.allenai.org.\n\n4. Title: Ofa: Unifying architectures, tasks, and modalities through a simple sequence-to-sequence learning framework\n   Abstract: OFA: Unifying Architectures, Tasks, and Modalities Through a Simple Sequence-to-Sequence Learning Framework: In this work, we pursue a unified paradigm for multimodal pretraining to break the scaffolds of complex task/modality-specific customization. We propose OFA, a Task-Agnostic and Modality-Agnostic framework that supports Task Comprehensiveness. OFA unifies a diverse set of cross-modal and unimodal tasks, including image generation, visual grounding, image captioning, image classification, language modeling, etc., in a simple sequence-to-sequence learning framework. OFA follows the instruction-based learning in both pretraining and finetuning stages, requiring no extra task-specific layers for downstream tasks. In comparison with the recent state-of-the-art vision & language models that rely on extremely large cross-modal datasets, OFA is pretrained on only 20M publicly available image-text pairs. Despite its simplicity and relatively small-scale training data, OFA achieves new SOTAs in a series of cross-modal tasks while attaining highly competitive performances on uni-modal tasks. Our further analysis indicates that OFA can also effectively transfer to unseen tasks and unseen domains. Our code and models are publicly available at https://github.com/OFA-Sys/OFA.\n\n5. Title: High-Resolution Image Synthesis with Latent Diffusion Models\n   Abstract: High-Resolution Image Synthesis with Latent Diffusion Models: By decomposing the image formation process into a sequential application of denoising autoencoders, diffusion models (DMs) achieve state-of-the-art synthesis results on image data and beyond. Additionally, their formulation allows for a guiding mechanism to control the image generation process without retraining. However, since these models typically operate directly in pixel space, optimization of powerful DMs often consumes hundreds of GPU days and inference is expensive due to sequential evaluations. To enable DM training on limited computational resources while retaining their quality and flexibility, we apply them in the latent space of powerful pretrained autoencoders. In contrast to previous work, training diffusion models on such a representation allows for the first time to reach a near-optimal point between complexity reduction and detail preservation, greatly boosting visual fidelity. By introducing cross-attention layers into the model architecture, we turn diffusion models into powerful and flexible generators for general conditioning inputs such as text or bounding boxes and high-resolution synthesis becomes possible in a convolutional manner. Our latent diffusion models (LDMs) achieve a new state of the art for image inpainting and highly competitive performance on various tasks, including unconditional image generation, semantic scene synthesis, and super-resolution, while significantly reducing computational requirements compared to pixel-based DMs. Code is available at https://github.com/CompVis/latent-diffusion .\n\n6. Title: Generative adversarial networks\n   Abstract: Generative Adversarial Networks: We propose a new framework for estimating generative models via an adversarial process, in which we simultaneously train two models: a generative model G that captures the data distribution, and a discriminative model D that estimates the probability that a sample came from the training data rather than G. The training procedure for G is to maximize the probability of D making a mistake. This framework corresponds to a minimax two-player game. In the space of arbitrary functions G and D, a unique solution exists, with G recovering the training data distribution and D equal to 1/2 everywhere. In the case where G and D are defined by multilayer perceptrons, the entire system can be trained with backpropagation. There is no need for any Markov chains or unrolled approximate inference networks during either training or generation of samples. Experiments demonstrate the potential of the framework through qualitative and quantitative evaluation of the generated samples.\n\n7. Title: Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models\n   Abstract: BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models: The cost of vision-and-language pre-training has become increasingly prohibitive due to end-to-end training of large-scale models. This paper proposes BLIP-2, a generic and efficient pre-training strategy that bootstraps vision-language pre-training from off-the-shelf frozen pre-trained image encoders and frozen large language models. BLIP-2 bridges the modality gap with a lightweight Querying Transformer, which is pre-trained in two stages. The first stage bootstraps vision-language representation learning from a frozen image encoder. The second stage bootstraps vision-to-language generative learning from a frozen language model. BLIP-2 achieves state-of-the-art performance on various vision-language tasks, despite having significantly fewer trainable parameters than existing methods. For example, our model outperforms Flamingo80B by 8.7% on zero-shot VQAv2 with 54x fewer trainable parameters. We also demonstrate the model's emerging capabilities of zero-shot image-to-text generation that can follow natural language instructions.\n\n8. Title: CGOF++: Controllable 3D Face Synthesis with Conditional Generative Occupancy Fields\n   Abstract: CGOF++: Controllable 3D Face Synthesis with Conditional Generative Occupancy Fields: Capitalizing on the recent advances in image generation models, existing controllable face image synthesis methods are able to generate high-fidelity images with some levels of controllability, e.g., controlling the shapes, expressions, textures, and poses of the generated face images. However, previous methods focus on controllable 2D image generative models, which are prone to producing inconsistent face images under large expression and pose changes. In this paper, we propose a new NeRF-based conditional 3D face synthesis framework, which enables 3D controllability over the generated face images by imposing explicit 3D conditions from 3D face priors. At its core is a conditional Generative Occupancy Field (cGOF++) that effectively enforces the shape of the generated face to conform to a given 3D Morphable Model (3DMM) mesh, built on top of EG3D [1], a recent tri-plane-based generative model. To achieve accurate control over fine-grained 3D face shapes of the synthesized images, we additionally incorporate a 3D landmark loss as well as a volume warping loss into our synthesis framework. Experiments validate the effectiveness of the proposed method, which is able to generate high-fidelity face images and shows more precise 3D controllability than state-of-the-art 2D-based controllable face synthesis methods.\n\n9. Title: Hierarchical Text-Conditional Image Generation with CLIP Latents\n   Abstract: Hierarchical Text-Conditional Image Generation with CLIP Latents: Contrastive models like CLIP have been shown to learn robust representations of images that capture both semantics and style. To leverage these representations for image generation, we propose a two-stage model: a prior that generates a CLIP image embedding given a text caption, and a decoder that generates an image conditioned on the image embedding. We show that explicitly generating image representations improves image diversity with minimal loss in photorealism and caption similarity. Our decoders conditioned on image representations can also produce variations of an image that preserve both its semantics and style, while varying the non-essential details absent from the image representation. Moreover, the joint embedding space of CLIP enables language-guided image manipulations in a zero-shot fashion. We use diffusion models for the decoder and experiment with both autoregressive and diffusion models for the prior, finding that the latter are computationally more efficient and produce higher-quality samples.\n\n10. Title: Inverting generative adversarial renderer for face reconstruction\n   Abstract: Inverting Generative Adversarial Renderer for Face Reconstruction: Given a monocular face image as input, 3D face geometry reconstruction aims to recover a corresponding 3D face mesh. Recently, both optimization-based and learning-based face reconstruction methods have taken advantage of the emerging differentiable renderer and shown promising results. However, the differentiable renderer, mainly based on graphics rules, simplifies the realistic mechanism of the illumination, reflection, \\etc, of the real world, thus cannot produce realistic images. This brings a lot of domain-shift noise to the optimization or training process. In this work, we introduce a novel Generative Adversarial Renderer (GAR) and propose to tailor its inverted version to the general fitting pipeline, to tackle the above problem. Specifically, the carefully designed neural renderer takes a face normal map and a latent code representing other factors as inputs and renders a realistic face image. Since the GAR learns to model the complicated real-world image, instead of relying on the simplified graphics rules, it is capable of producing realistic images, which essentially inhibits the domain-shift noise in training and optimization. Equipped with the elaborated GAR, we further proposed a novel approach to predict 3D face parameters, in which we first obtain fine initial parameters via Renderer Inverting and then refine it with gradient-based optimizers. Extensive experiments have been conducted to demonstrate the effectiveness of the proposed generative adversarial renderer and the novel optimization-based face reconstruction framework. Our method achieves state-of-the-art performances on multiple face reconstruction datasets.\n\nThink step-by-step about which candidate best matches the context. Summarize your reasoning, then provide the exact title that should fill the citation. Respond in JSON with fields:\n{\n  \"reasoning\": \"<your step-by-step analysis>\",\n  \"selected_title\": \"<exact candidate title>\"\n}", "response": {"reasoning": "The citation context discusses unified training architectures for different modalities, specifically mentioning OFA, Uni-Perceivers, and Unified-IO, which are models that integrate various tasks and modalities in AI. Among the candidate papers, the one titled 'Unified-IO: A Unified Model for Vision, Language, and Multi-Modal Tasks' directly aligns with the context as it describes a model that performs a variety of AI tasks across vision and language, emphasizing its unified approach. The paper titled 'OFA: Unifying Architectures, Tasks, and Modalities Through a Simple Sequence-to-Sequence Learning Framework' also fits well, as it discusses a unified framework for multimodal pretraining. However, the context specifically mentions 'Unified-IO' as one of the models, making it the most relevant choice. The other candidates either focus on specific tasks or do not address the unified training aspect as directly as Unified-IO does.", "selected_title": "Unified-IO: A Unified Model for Vision, Language, and Multi-Modal Tasks"}, "selected_title": "Unified-IO: A Unified Model for Vision, Language, and Multi-Modal Tasks", "positive_title": "Unified-io: A unified model for vision, language, and multi-modal tasks", "is_correct": true, "reasoning": "The citation context discusses unified training architectures for different modalities, specifically mentioning OFA, Uni-Perceivers, and Unified-IO, which are models that integrate various tasks and modalities in AI. Among the candidate papers, the one titled 'Unified-IO: A Unified Model for Vision, Language, and Multi-Modal Tasks' directly aligns with the context as it describes a model that performs a variety of AI tasks across vision and language, emphasizing its unified approach. The paper titled 'OFA: Unifying Architectures, Tasks, and Modalities Through a Simple Sequence-to-Sequence Learning Framework' also fits well, as it discusses a unified framework for multimodal pretraining. However, the context specifically mentions 'Unified-IO' as one of the models, making it the most relevant choice. The other candidates either focus on specific tasks or do not address the unified training aspect as directly as Unified-IO does."}
{"example_id": "blind_15855", "prompt": "You are an expert citation retrieval system as described in the paper \"Multi-Agent System for Reliable Citation Retrieval\".\nYour goal is to autonomously retrieve, verify, and recommend academic references given a query or document excerpt.\n\nTask:\nGiven a citation context from a scientific paper (where a citation is missing), identify the correct paper from a list of candidates.\nAnalyze the context to understand the specific claim, method, or result being cited.\nThen, evaluate each candidate paper to see if it matches the context.\nFinally, select the best matching paper.\n\nCitation Context:\nce speed and energy efficiency on the edge, weights and activations of neural networks need to be quantized to low precision. Prior works use the same QBN for all layers[CITATION], which would introduce a huge decrease in accuracy as bit goes down. A possible solution here is to use mixed-precision quantization, where higher precision is used for more sensitive lay\n\nCandidate Papers:\n1. Title: Towards accurate binary convolutional neural network\n   Abstract: Towards Accurate Binary Convolutional Neural Network: We introduce a novel scheme to train binary convolutional neural networks (CNNs) -- CNNs with weights and activations constrained to {-1,+1} at run-time. It has been known that using binary weights and activations drastically reduce memory size and accesses, and can replace arithmetic operations with more efficient bitwise operations, leading to much faster test-time inference and lower power consumption. However, previous works on binarizing CNNs usually result in severe prediction accuracy degradation. In this paper, we address this issue with two major innovations: (1) approximating full-precision weights with the linear combination of multiple binary weight bases; (2) employing multiple binary activations to alleviate information loss. The implementation of the resulting binary CNN, denoted as ABC-Net, is shown to achieve much closer performance to its full-precision counterpart, and even reach the comparable prediction accuracy on ImageNet and forest trail datasets, given adequate binary weight bases and activations.\n\n2. Title: Dorefa-net: Training low bitwidth convolutional neural networks with low bitwidth gradients\n   Abstract: DoReFa-Net: Training Low Bitwidth Convolutional Neural Networks with Low Bitwidth Gradients: We propose DoReFa-Net, a method to train convolutional neural networks that have low bitwidth weights and activations using low bitwidth parameter gradients. In particular, during backward pass, parameter gradients are stochastically quantized to low bitwidth numbers before being propagated to convolutional layers. As convolutions during forward/backward passes can now operate on low bitwidth weights and activations/gradients respectively, DoReFa-Net can use bit convolution kernels to accelerate both training and inference. Moreover, as bit convolutions can be efficiently implemented on CPU, FPGA, ASIC and GPU, DoReFa-Net opens the way to accelerate training of low bitwidth neural network on these hardware. Our experiments on SVHN and ImageNet datasets prove that DoReFa-Net can achieve comparable prediction accuracy as 32-bit counterparts. For example, a DoReFa-Net derived from AlexNet that has 1-bit weights, 2-bit activations, can be trained from scratch using 6-bit gradients to get 46.1\\% top-1 accuracy on ImageNet validation set. The DoReFa-Net AlexNet model is released publicly.\n\n3. Title: Mobilenetv2: Inverted residuals and linear bottlenecks\n   Abstract: MobileNetV2: Inverted Residuals and Linear Bottlenecks: In this paper we describe a new mobile architecture, MobileNetV2, that improves the state of the art performance of mobile models on multiple tasks and benchmarks as well as across a spectrum of different model sizes. We also describe efficient ways of applying these mobile models to object detection in a novel framework we call SSDLite. Additionally, we demonstrate how to build mobile semantic segmentation models through a reduced form of DeepLabv3 which we call Mobile DeepLabv3.   The MobileNetV2 architecture is based on an inverted residual structure where the input and output of the residual block are thin bottleneck layers opposite to traditional residual models which use expanded representations in the input an MobileNetV2 uses lightweight depthwise convolutions to filter features in the intermediate expansion layer. Additionally, we find that it is important to remove non-linearities in the narrow layers in order to maintain representational power. We demonstrate that this improves performance and provide an intuition that led to this design. Finally, our approach allows decoupling of the input/output domains from the expressiveness of the transformation, which provides a convenient framework for further analysis. We measure our performance on Imagenet classification, COCO object detection, VOC image segmentation. We evaluate the trade-offs between accuracy, and number of operations measured by multiply-adds (MAdd), as well as the number of parameters\n\n4. Title: Deep compression: Compressing deep neural networks with pruning, trained quantization and huffman coding\n   Abstract: Deep Compression: Compressing Deep Neural Networks with Pruning, Trained Quantization and Huffman Coding: Neural networks are both computationally intensive and memory intensive, making them difficult to deploy on embedded systems with limited hardware resources. To address this limitation, we introduce \"deep compression\", a three stage pipeline: pruning, trained quantization and Huffman coding, that work together to reduce the storage requirement of neural networks by 35x to 49x without affecting their accuracy. Our method first prunes the network by learning only the important connections. Next, we quantize the weights to enforce weight sharing, finally, we apply Huffman coding. After the first two steps we retrain the network to fine tune the remaining connections and the quantized centroids. Pruning, reduces the number of connections by 9x to 13x; Quantization then reduces the number of bits that represent each connection from 32 to 5. On the ImageNet dataset, our method reduced the storage required by AlexNet by 35x, from 240MB to 6.9MB, without loss of accuracy. Our method reduced the size of VGG-16 by 49x from 552MB to 11.3MB, again with no loss of accuracy. This allows fitting the model into on-chip SRAM cache rather than off-chip DRAM memory. Our compression method also facilitates the use of complex neural networks in mobile applications where application size and download bandwidth are constrained. Benchmarked on CPU, GPU and mobile GPU, compressed network has 3x to 4x layerwise speedup and 3x to 7x better energy efficiency.\n\n5. Title: Xnor-net: Imagenet classification using binary convolutional neural networks\n   Abstract: XNOR-Net: ImageNet Classification Using Binary Convolutional Neural Networks: We propose two efficient approximations to standard convolutional neural networks: Binary-Weight-Networks and XNOR-Networks. In Binary-Weight-Networks, the filters are approximated with binary values resulting in 32x memory saving. In XNOR-Networks, both the filters and the input to convolutional layers are binary. XNOR-Networks approximate convolutions using primarily binary operations. This results in 58x faster convolutional operations and 32x memory savings. XNOR-Nets offer the possibility of running state-of-the-art networks on CPUs (rather than GPUs) in real-time. Our binary networks are simple, accurate, efficient, and work on challenging visual tasks. We evaluate our approach on the ImageNet classification task. The classification accuracy with a Binary-Weight-Network version of AlexNet is only 2.9% less than the full-precision AlexNet (in top-1 measure). We compare our method with recent network binarization methods, BinaryConnect and BinaryNets, and outperform these methods by large margins on ImageNet, more than 16% in top-1 accuracy.\n\n6. Title: Hawq-v2: Hessian aware trace-weighted quantization of neural networks\n   Abstract: HAWQ-V2: Hessian Aware trace-Weighted Quantization of Neural Networks: Quantization is an effective method for reducing memory footprint and inference time of Neural Networks, e.g., for efficient inference in the cloud, especially at the edge. However, ultra low precision quantization could lead to significant degradation in model generalization. A promising method to address this is to perform mixed-precision quantization, where more sensitive layers are kept at higher precision. However, the search space for a mixed-precision quantization is exponential in the number of layers. Recent work has proposed HAWQ, a novel Hessian based framework, with the aim of reducing this exponential search space by using second-order information. While promising, this prior work has three major limitations: (i) HAWQV1 only uses the top Hessian eigenvalue as a measure of sensitivity and do not consider the rest of the Hessian spectrum; (ii) HAWQV1 approach only provides relative sensitivity of different layers and therefore requires a manual selection of the mixed-precision setting; and (iii) HAWQV1 does not consider mixed-precision activation quantization. Here, we present HAWQV2 which addresses these shortcomings. For (i), we perform a theoretical analysis showing that a better sensitivity metric is to compute the average of all of the Hessian eigenvalues. For (ii), we develop a Pareto frontier based method for selecting the exact bit precision of different layers without any manual selection. For (iii), we extend the Hessian analysis to mixed-precision activation quantization. We have found this to be very beneficial for object detection. We show that HAWQV2 achieves new state-of-the-art results for a wide range of tasks.\n\n7. Title: Hawq: Hessian aware quantization of neural networks with mixed-precision\n   Abstract: HAWQ: Hessian AWare Quantization of Neural Networks with Mixed-Precision: Model size and inference speed/power have become a major challenge in the deployment of Neural Networks for many applications. A promising approach to address these problems is quantization. However, uniformly quantizing a model to ultra low precision leads to significant accuracy degradation. A novel solution for this is to use mixed-precision quantization, as some parts of the network may allow lower precision as compared to other layers. However, there is no systematic way to determine the precision of different layers. A brute force approach is not feasible for deep networks, as the search space for mixed-precision is exponential in the number of layers. Another challenge is a similar factorial complexity for determining block-wise fine-tuning order when quantizing the model to a target precision. Here, we introduce Hessian AWare Quantization (HAWQ), a novel second-order quantization method to address these problems. HAWQ allows for the automatic selection of the relative quantization precision of each layer, based on the layer's Hessian spectrum. Moreover, HAWQ provides a deterministic fine-tuning order for quantizing layers, based on second-order information. We show the results of our method on Cifar-10 using ResNet20, and on ImageNet using Inception-V3, ResNet50 and SqueezeNext models. Comparing HAWQ with state-of-the-art shows that we can achieve similar/better accuracy with $8\\times$ activation compression ratio on ResNet20, as compared to DNAS~\\cite{wu2018mixed}, and up to $1\\%$ higher accuracy with up to $14\\%$ smaller models on ResNet50 and Inception-V3, compared to recently proposed methods of RVQuant~\\cite{park2018value} and HAQ~\\cite{wang2018haq}. Furthermore, we show that we can quantize SqueezeNext to just 1MB model size while achieving above $68\\%$ top1 accuracy on ImageNet.\n\n8. Title: Autoq: Automated kernel-wise neural network quantization\n   Abstract: AutoQ: Automated Kernel-Wise Neural Network Quantization: Network quantization is one of the most hardware friendly techniques to enable the deployment of convolutional neural networks (CNNs) on low-power mobile devices. Recent network quantization techniques quantize each weight kernel in a convolutional layer independently for higher inference accuracy, since the weight kernels in a layer exhibit different variances and hence have different amounts of redundancy. The quantization bitwidth or bit number (QBN) directly decides the inference accuracy, latency, energy and hardware overhead. To effectively reduce the redundancy and accelerate CNN inferences, various weight kernels should be quantized with different QBNs. However, prior works use only one QBN to quantize each convolutional layer or the entire CNN, because the design space of searching a QBN for each weight kernel is too large. The hand-crafted heuristic of the kernel-wise QBN search is so sophisticated that domain experts can obtain only sub-optimal results. It is difficult for even deep reinforcement learning (DRL) Deep Deterministic Policy Gradient (DDPG)-based agents to find a kernel-wise QBN configuration that can achieve reasonable inference accuracy. In this paper, we propose a hierarchical-DRL-based kernel-wise network quantization technique, AutoQ, to automatically search a QBN for each weight kernel, and choose another QBN for each activation layer. Compared to the models quantized by the state-of-the-art DRL-based schemes, on average, the same models quantized by AutoQ reduce the inference latency by 54.06\\%, and decrease the inference energy consumption by 50.69\\%, while achieving the same inference accuracy.\n\n9. Title: Quantization and training of neural networks for efficient integer-arithmetic-only inference\n   Abstract: Quantization and Training of Neural Networks for Efficient Integer-Arithmetic-Only Inference: The rising popularity of intelligent mobile devices and the daunting computational cost of deep learning-based models call for efficient and accurate on-device inference schemes. We propose a quantization scheme that allows inference to be carried out using integer-only arithmetic, which can be implemented more efficiently than floating point inference on commonly available integer-only hardware. We also co-design a training procedure to preserve end-to-end model accuracy post quantization. As a result, the proposed quantization scheme improves the tradeoff between accuracy and on-device latency. The improvements are significant even on MobileNets, a model family known for run-time efficiency, and are demonstrated in ImageNet classification and COCO detection on popular CPUs.\n\n10. Title: Designing neural network architectures using reinforcement learning\n   Abstract: Designing Neural Network Architectures using Reinforcement Learning: At present, designing convolutional neural network (CNN) architectures requires both human expertise and labor. New architectures are handcrafted by careful experimentation or modified from a handful of existing networks. We introduce MetaQNN, a meta-modeling algorithm based on reinforcement learning to automatically generate high-performing CNN architectures for a given learning task. The learning agent is trained to sequentially choose CNN layers using $Q$-learning with an $\\epsilon$-greedy exploration strategy and experience replay. The agent explores a large but finite space of possible architectures and iteratively discovers designs with improved performance on the learning task. On image classification benchmarks, the agent-designed networks (consisting of only standard convolution, pooling, and fully-connected layers) beat existing networks designed with the same layer types and are competitive against the state-of-the-art methods that use more complex layer types. We also outperform existing meta-modeling approaches for network design on image classification tasks.\n\nThink step-by-step about which candidate best matches the context. Summarize your reasoning, then provide the exact title that should fill the citation. Respond in JSON with fields:\n{\n  \"reasoning\": \"<your step-by-step analysis>\",\n  \"selected_title\": \"<exact candidate title>\"\n}", "response": {"reasoning": "The citation context discusses the need for quantization of weights and activations in neural networks to improve speed and energy efficiency, while highlighting the issue of accuracy degradation when using a uniform quantization bit number (QBN) across all layers. It suggests that mixed-precision quantization could be a solution, where different layers use different precision levels based on their sensitivity. Among the candidate papers, several focus on quantization methods. The most relevant ones are HAWQ-V2 and HAWQ, which specifically address mixed-precision quantization and the challenges associated with it. HAWQ-V2 improves upon the original HAWQ by addressing limitations and providing a systematic approach to mixed-precision quantization. This aligns closely with the context's emphasis on mixed-precision quantization as a solution to the accuracy degradation problem. Therefore, HAWQ-V2 is the best match for the citation context.", "selected_title": "Hawq-v2: Hessian aware trace-weighted quantization of neural networks"}, "selected_title": "Hawq-v2: Hessian aware trace-weighted quantization of neural networks", "positive_title": "Quantization and training of neural networks for efficient integer-arithmetic-only inference", "is_correct": false, "reasoning": "The citation context discusses the need for quantization of weights and activations in neural networks to improve speed and energy efficiency, while highlighting the issue of accuracy degradation when using a uniform quantization bit number (QBN) across all layers. It suggests that mixed-precision quantization could be a solution, where different layers use different precision levels based on their sensitivity. Among the candidate papers, several focus on quantization methods. The most relevant ones are HAWQ-V2 and HAWQ, which specifically address mixed-precision quantization and the challenges associated with it. HAWQ-V2 improves upon the original HAWQ by addressing limitations and providing a systematic approach to mixed-precision quantization. This aligns closely with the context's emphasis on mixed-precision quantization as a solution to the accuracy degradation problem. Therefore, HAWQ-V2 is the best match for the citation context."}
{"example_id": "blind_7759", "prompt": "You are an expert citation retrieval system as described in the paper \"Multi-Agent System for Reliable Citation Retrieval\".\nYour goal is to autonomously retrieve, verify, and recommend academic references given a query or document excerpt.\n\nTask:\nGiven a citation context from a scientific paper (where a citation is missing), identify the correct paper from a list of candidates.\nAnalyze the context to understand the specific claim, method, or result being cited.\nThen, evaluate each candidate paper to see if it matches the context.\nFinally, select the best matching paper.\n\nCitation Context:\nea} \\end{center} \\end{figure} RGB and depth data have been used for semantic and instance segmentation for some time, but there are only few works on panoptic segmentation exploiting both modalities.[CITATION] still apply RGB images for panoptic segmentation, using depth only to estimate camera poses and to generate a 3D map. process depth and colour in two separate branches of a deep neural netw\n\nCandidate Papers:\n1. Title: Cmt-deeplab: Clustering mask transformers for panoptic segmentation\n   Abstract: CMT-DeepLab: Clustering Mask Transformers for Panoptic Segmentation: We propose Clustering Mask Transformer (CMT-DeepLab), a transformer-based framework for panoptic segmentation designed around clustering. It rethinks the existing transformer architectures used in segmentation and detection; CMT-DeepLab considers the object queries as cluster centers, which fill the role of grouping the pixels when applied to segmentation. The clustering is computed with an alternating procedure, by first assigning pixels to the clusters by their feature affinity, and then updating the cluster centers and pixel features. Together, these operations comprise the Clustering Mask Transformer (CMT) layer, which produces cross-attention that is denser and more consistent with the final segmentation task. CMT-DeepLab improves the performance over prior art significantly by 4.4% PQ, achieving a new state-of-the-art of 55.7% PQ on the COCO test-dev set.\n\n2. Title: Panoptic feature pyramid networks\n   Abstract: Panoptic Feature Pyramid Networks: The recently introduced panoptic segmentation task has renewed our community's interest in unifying the tasks of instance segmentation (for thing classes) and semantic segmentation (for stuff classes). However, current state-of-the-art methods for this joint task use separate and dissimilar networks for instance and semantic segmentation, without performing any shared computation. In this work, we aim to unify these methods at the architectural level, designing a single network for both tasks. Our approach is to endow Mask R-CNN, a popular instance segmentation method, with a semantic segmentation branch using a shared Feature Pyramid Network (FPN) backbone. Surprisingly, this simple baseline not only remains effective for instance segmentation, but also yields a lightweight, top-performing method for semantic segmentation. In this work, we perform a detailed study of this minimally extended version of Mask R-CNN with FPN, which we refer to as Panoptic FPN, and show it is a robust and accurate baseline for both tasks. Given its effectiveness and conceptual simplicity, we hope our method can serve as a strong baseline and aid future research in panoptic segmentation.\n\n3. Title: Attention-guided unified network for panoptic segmentation\n   Abstract: Attention-guided Unified Network for Panoptic Segmentation: This paper studies panoptic segmentation, a recently proposed task which segments foreground (FG) objects at the instance level as well as background (BG) contents at the semantic level. Existing methods mostly dealt with these two problems separately, but in this paper, we reveal the underlying relationship between them, in particular, FG objects provide complementary cues to assist BG understanding. Our approach, named the Attention-guided Unified Network (AUNet), is a unified framework with two branches for FG and BG segmentation simultaneously. Two sources of attentions are added to the BG branch, namely, RPN and FG segmentation mask to provide object-level and pixel-level attentions, respectively. Our approach is generalized to different backbones with consistent accuracy gain in both FG and BG segmentation, and also sets new state-of-the-arts both in the MS-COCO (46.5% PQ) and Cityscapes (59.0% PQ) benchmarks.\n\n4. Title: Efficient multi-task rgb-d scene analysis for indoor environments\n   Abstract: Efficient Multi-Task RGB-D Scene Analysis for Indoor Environments: Semantic scene understanding is essential for mobile agents acting in various environments. Although semantic segmentation already provides a lot of information, details about individual objects as well as the general scene are missing but required for many real-world applications. However, solving multiple tasks separately is expensive and cannot be accomplished in real time given limited computing and battery capabilities on a mobile platform. In this paper, we propose an efficient multi-task approach for RGB-D scene analysis~(EMSANet) that simultaneously performs semantic and instance segmentation~(panoptic segmentation), instance orientation estimation, and scene classification. We show that all tasks can be accomplished using a single neural network in real time on a mobile platform without diminishing performance - by contrast, the individual tasks are able to benefit from each other. In order to evaluate our multi-task approach, we extend the annotations of the common RGB-D indoor datasets NYUv2 and SUNRGB-D for instance segmentation and orientation estimation. To the best of our knowledge, we are the first to provide results in such a comprehensive multi-task setting for indoor scene analysis on NYUv2 and SUNRGB-D.\n\n5. Title: Panopticfusion: Online volumetric semantic mapping at the level of stuff and things\n   Abstract: PanopticFusion: Online Volumetric Semantic Mapping at the Level of Stuff and Things: We propose PanopticFusion, a novel online volumetric semantic mapping system at the level of stuff and things. In contrast to previous semantic mapping systems, PanopticFusion is able to densely predict class labels of a background region (stuff) and individually segment arbitrary foreground objects (things). In addition, our system has the capability to reconstruct a large-scale scene and extract a labeled mesh thanks to its use of a spatially hashed volumetric map representation. Our system first predicts pixel-wise panoptic labels (class labels for stuff regions and instance IDs for thing regions) for incoming RGB frames by fusing 2D semantic and instance segmentation outputs. The predicted panoptic labels are integrated into the volumetric map together with depth measurements while keeping the consistency of the instance IDs, which could vary frame to frame, by referring to the 3D map at that moment. In addition, we construct a fully connected conditional random field (CRF) model with respect to panoptic labels for map regularization. For online CRF inference, we propose a novel unary potential approximation and a map division strategy.   We evaluated the performance of our system on the ScanNet (v2) dataset. PanopticFusion outperformed or compared with state-of-the-art offline 3D DNN methods in both semantic and instance segmentation benchmarks. Also, we demonstrate a promising augmented reality application using a 3D panoptic map generated by the proposed system.\n\n6. Title: Scenegraphfusion: Incremental 3d scene graph prediction from rgb-d sequences\n   Abstract: SceneGraphFusion: Incremental 3D Scene Graph Prediction from RGB-D Sequences: Scene graphs are a compact and explicit representation successfully used in a variety of 2D scene understanding tasks. This work proposes a method to incrementally build up semantic scene graphs from a 3D environment given a sequence of RGB-D frames. To this end, we aggregate PointNet features from primitive scene components by means of a graph neural network. We also propose a novel attention mechanism well suited for partial and missing graph data present in such an incremental reconstruction scenario. Although our proposed method is designed to run on submaps of the scene, we show it also transfers to entire 3D scenes. Experiments show that our approach outperforms 3D scene graph prediction methods by a large margin and its accuracy is on par with other 3D semantic and panoptic segmentation methods while running at 35 Hz.\n\n7. Title: Fully convolutional networks for panoptic segmentation\n   Abstract: Fully Convolutional Networks for Panoptic Segmentation: In this paper, we present a conceptually simple, strong, and efficient framework for panoptic segmentation, called Panoptic FCN. Our approach aims to represent and predict foreground things and background stuff in a unified fully convolutional pipeline. In particular, Panoptic FCN encodes each object instance or stuff category into a specific kernel weight with the proposed kernel generator and produces the prediction by convolving the high-resolution feature directly. With this approach, instance-aware and semantically consistent properties for things and stuff can be respectively satisfied in a simple generate-kernel-then-segment workflow. Without extra boxes for localization or instance separation, the proposed approach outperforms previous box-based and -free models with high efficiency on COCO, Cityscapes, and Mapillary Vistas datasets with single scale input. Our code is made publicly available at https://github.com/Jia-Research-Lab/PanopticFCN.\n\n8. Title: Intra-batch supervision for panoptic segmentation on high-resolution images\n   Abstract: Intra-Batch Supervision for Panoptic Segmentation on High-Resolution Images: Unified panoptic segmentation methods are achieving state-of-the-art results on several datasets. To achieve these results on high-resolution datasets, these methods apply crop-based training. In this work, we find that, although crop-based training is advantageous in general, it also has a harmful side-effect. Specifically, it limits the ability of unified networks to discriminate between large object instances, causing them to make predictions that are confused between multiple instances. To solve this, we propose Intra-Batch Supervision (IBS), which improves a network's ability to discriminate between instances by introducing additional supervision using multiple images from the same batch. We show that, with our IBS, we successfully address the confusion problem and consistently improve the performance of unified networks. For the high-resolution Cityscapes and Mapillary Vistas datasets, we achieve improvements of up to +2.5 on the Panoptic Quality for thing classes, and even more considerable gains of up to +5.8 on both the pixel accuracy and pixel precision, which we identify as better metrics to capture the confusion problem.\n\nThink step-by-step about which candidate best matches the context. Summarize your reasoning, then provide the exact title that should fill the citation. Respond in JSON with fields:\n{\n  \"reasoning\": \"<your step-by-step analysis>\",\n  \"selected_title\": \"<exact candidate title>\"\n}", "response": {"reasoning": "The citation context discusses the use of RGB and depth data for panoptic segmentation, specifically mentioning that depth is used only for estimating camera poses and generating a 3D map, while RGB images are primarily used for panoptic segmentation. Among the candidate papers, the one that best aligns with this context is 'Efficient Multi-Task RGB-D Scene Analysis for Indoor Environments'. This paper explicitly mentions the simultaneous performance of semantic and instance segmentation (panoptic segmentation) using RGB-D data, which directly relates to the context's focus on using both RGB and depth data for segmentation tasks. Other candidates either focus on panoptic segmentation without emphasizing the RGB-D aspect or do not mention the specific use of depth data in the same way. Therefore, this candidate is the most relevant to the citation context.", "selected_title": "Efficient multi-task rgb-d scene analysis for indoor environments"}, "selected_title": "Efficient multi-task rgb-d scene analysis for indoor environments", "positive_title": "Panopticfusion: Online volumetric semantic mapping at the level of stuff and things", "is_correct": false, "reasoning": "The citation context discusses the use of RGB and depth data for panoptic segmentation, specifically mentioning that depth is used only for estimating camera poses and generating a 3D map, while RGB images are primarily used for panoptic segmentation. Among the candidate papers, the one that best aligns with this context is 'Efficient Multi-Task RGB-D Scene Analysis for Indoor Environments'. This paper explicitly mentions the simultaneous performance of semantic and instance segmentation (panoptic segmentation) using RGB-D data, which directly relates to the context's focus on using both RGB and depth data for segmentation tasks. Other candidates either focus on panoptic segmentation without emphasizing the RGB-D aspect or do not mention the specific use of depth data in the same way. Therefore, this candidate is the most relevant to the citation context."}
{"example_id": "blind_12146", "prompt": "You are an expert citation retrieval system as described in the paper \"Multi-Agent System for Reliable Citation Retrieval\".\nYour goal is to autonomously retrieve, verify, and recommend academic references given a query or document excerpt.\n\nTask:\nGiven a citation context from a scientific paper (where a citation is missing), identify the correct paper from a list of candidates.\nAnalyze the context to understand the specific claim, method, or result being cited.\nThen, evaluate each candidate paper to see if it matches the context.\nFinally, select the best matching paper.\n\nCitation Context:\nnt using transformers. Empirical research on both formal languages and synthetic functions has uncovered an intriguing array of learning biases, but theoretical understanding is lacking. For instance,[CITATION] experimentally argued that heldout generalization is biased towards low-degree polynomials and provided empirical evidence that transformers prefer to represent functions of \\emph{low sens\n\nCandidate Papers:\n1. Title: Generalization on the Unseen, Logic Reasoning and Degree Curriculum\n   Abstract: Generalization on the Unseen, Logic Reasoning and Degree Curriculum: This paper considers the learning of logical (Boolean) functions with focus on the generalization on the unseen (GOTU) setting, a strong case of out-of-distribution generalization. This is motivated by the fact that the rich combinatorial nature of data in certain reasoning tasks (e.g., arithmetic/logic) makes representative data sampling challenging, and learning successfully under GOTU gives a first vignette of an 'extrapolating' or 'reasoning' learner. We then study how different network architectures trained by (S)GD perform under GOTU and provide both theoretical and experimental evidence that for a class of network models including instances of Transformers, random features models, and diagonal linear networks, a min-degree-interpolator is learned on the unseen. We also provide evidence that other instances with larger learning rates or mean-field networks reach leaky min-degree solutions. These findings lead to two implications: (1) we provide an explanation to the length generalization problem (e.g., Anil et al. 2022); (2) we introduce a curriculum learning algorithm called Degree-Curriculum that learns monomials more efficiently by incrementing supports.\n\n2. Title: Theoretical Limitations of Self-Attention in Neural Sequence Models\n   Abstract: Theoretical Limitations of Self-Attention in Neural Sequence Models: Transformers are emerging as the new workhorse of NLP, showing great success across tasks. Unlike LSTMs, transformers process input sequences entirely through self-attention. Previous work has suggested that the computational capabilities of self-attention to process hierarchical structures are limited. In this work, we mathematically investigate the computational power of self-attention to model formal languages. Across both soft and hard attention, we show strong theoretical limitations of the computational abilities of self-attention, finding that it cannot model periodic finite-state languages, nor hierarchical structure, unless the number of layers or heads increases with input length. These limitations seem surprising given the practical success of self-attention and the prominent role assigned to hierarchical structure in linguistics, suggesting that natural language can be approximated well with models that are too weak for the formal languages typically assumed in theoretical linguistics.\n\n3. Title: Formal language recognition by hard attention transformers: Perspectives from circuit complexity\n   Abstract: Formal Language Recognition by Hard Attention Transformers: Perspectives from Circuit Complexity: This paper analyzes three formal models of Transformer encoders that differ in the form of their self-attention mechanism: unique hard attention (UHAT); generalized unique hard attention (GUHAT), which generalizes UHAT; and averaging hard attention (AHAT). We show that UHAT and GUHAT Transformers, viewed as string acceptors, can only recognize formal languages in the complexity class AC$^0$, the class of languages recognizable by families of Boolean circuits of constant depth and polynomial size. This upper bound subsumes Hahn's (2020) results that GUHAT cannot recognize the DYCK languages or the PARITY language, since those languages are outside AC$^0$ (Furst et al., 1984). In contrast, the non-AC$^0$ languages MAJORITY and DYCK-1 are recognizable by AHAT networks, implying that AHAT can recognize languages that UHAT and GUHAT cannot.\n\n4. Title: Average-Hard Attention Transformers are Constant-Depth Uniform Threshold\nCircuits\n   Abstract: Average-Hard Attention Transformers are Constant-Depth Uniform Threshold Circuits: Transformers have emerged as a widely used neural network model for various natural language processing tasks. Previous research explored their relationship with constant-depth threshold circuits, making two assumptions: average-hard attention and logarithmic precision for internal computations relative to input length. Merrill et al. (2022) prove that average-hard attention transformers recognize languages that fall within the complexity class TC0, denoting the set of languages that can be recognized by constant-depth polynomial-size threshold circuits. Likewise, Merrill and Sabharwal (2023) show that log-precision transformers recognize languages within the class of uniform TC0. This shows that both transformer models can be simulated by constant-depth threshold circuits, with the latter being more robust due to generating a uniform circuit family. Our paper shows that the first result can be extended to yield uniform circuits as well.\n\n5. Title: Transformers as Algorithms: Generalization and Stability in In-context Learning\n   Abstract: Transformers as Algorithms: Generalization and Stability in In-context Learning: In-context learning (ICL) is a type of prompting where a transformer model operates on a sequence of (input, output) examples and performs inference on-the-fly. In this work, we formalize in-context learning as an algorithm learning problem where a transformer model implicitly constructs a hypothesis function at inference-time. We first explore the statistical aspects of this abstraction through the lens of multitask learning: We obtain generalization bounds for ICL when the input prompt is (1) a sequence of i.i.d. (input, label) pairs or (2) a trajectory arising from a dynamical system. The crux of our analysis is relating the excess risk to the stability of the algorithm implemented by the transformer. We characterize when transformer/attention architecture provably obeys the stability condition and also provide empirical verification. For generalization on unseen tasks, we identify an inductive bias phenomenon in which the transfer learning risk is governed by the task complexity and the number of MTL tasks in a highly predictable manner. Finally, we provide numerical evaluations that (1) demonstrate transformers can indeed implement near-optimal algorithms on classical regression problems with i.i.d. and dynamic data, (2) provide insights on stability, and (3) verify our theoretical predictions.\n\n6. Title: Overcoming a Theoretical Limitation of Self-Attention\n   Abstract: Overcoming a Theoretical Limitation of Self-Attention: Although transformers are remarkably effective for many tasks, there are some surprisingly easy-looking regular languages that they struggle with. Hahn shows that for languages where acceptance depends on a single input symbol, a transformer's classification decisions become less and less confident (that is, with cross-entropy approaching 1 bit per string) as input strings get longer and longer. We examine this limitation using two languages: PARITY, the language of bit strings with an odd number of 1s, and FIRST, the language of bit strings starting with a 1. We demonstrate three ways of overcoming the limitation suggested by Hahn's lemma. First, we settle an open question by constructing a transformer that recognizes PARITY with perfect accuracy, and similarly for FIRST. Second, we use layer normalization to bring the cross-entropy of both models arbitrarily close to zero. Third, when transformers need to focus on a single position, as for FIRST, we find that they can fail to generalize to longer strings; we offer a simple remedy to this problem that also improves length generalization in machine translation.\n\n7. Title: Thinking like transformers\n   Abstract: Thinking Like Transformers: What is the computational model behind a Transformer? Where recurrent neural networks have direct parallels in finite state machines, allowing clear discussion and thought around architecture variants or trained models, Transformers have no such familiar parallel. In this paper we aim to change that, proposing a computational model for the transformer-encoder in the form of a programming language. We map the basic components of a transformer-encoder -- attention and feed-forward computation -- into simple primitives, around which we form a programming language: the Restricted Access Sequence Processing Language (RASP). We show how RASP can be used to program solutions to tasks that could conceivably be learned by a Transformer, and how a Transformer can be trained to mimic a RASP solution. In particular, we provide RASP programs for histograms, sorting, and Dyck-languages. We further use our model to relate their difficulty in terms of the number of required layers and attention heads: analyzing a RASP program implies a maximum number of heads and layers necessary to encode a task in a transformer. Finally, we see how insights gained from our abstraction might be used to explain phenomena seen in recent works.\n\n8. Title: Inductive biases and variable creation in self-attention mechanisms\n   Abstract: Inductive Biases and Variable Creation in Self-Attention Mechanisms: Self-attention, an architectural motif designed to model long-range interactions in sequential data, has driven numerous recent breakthroughs in natural language processing and beyond. This work provides a theoretical analysis of the inductive biases of self-attention modules. Our focus is to rigorously establish which functions and long-range dependencies self-attention blocks prefer to represent. Our main result shows that bounded-norm Transformer networks \"create sparse variables\": a single self-attention head can represent a sparse function of the input sequence, with sample complexity scaling only logarithmically with the context length. To support our analysis, we present synthetic experiments to probe the sample complexity of learning sparse Boolean functions with Transformers.\n\nThink step-by-step about which candidate best matches the context. Summarize your reasoning, then provide the exact title that should fill the citation. Respond in JSON with fields:\n{\n  \"reasoning\": \"<your step-by-step analysis>\",\n  \"selected_title\": \"<exact candidate title>\"\n}", "response": {"reasoning": "The citation context discusses empirical research on learning biases in transformers, specifically mentioning that heldout generalization is biased towards low-degree polynomials. The context implies a focus on the theoretical understanding of how transformers generalize, particularly in relation to polynomial functions. Among the candidate papers, the first one, 'Generalization on the Unseen, Logic Reasoning and Degree Curriculum', directly addresses generalization in the context of transformers and provides empirical evidence related to learning biases, including the mention of low-degree solutions. This aligns closely with the context's emphasis on empirical evidence and theoretical understanding of generalization biases. The other candidates either focus on limitations of self-attention, formal language recognition, or do not directly address the specific claim about low-degree polynomials and heldout generalization. Therefore, the first candidate is the best match.", "selected_title": "Generalization on the Unseen, Logic Reasoning and Degree Curriculum"}, "selected_title": "Generalization on the Unseen, Logic Reasoning and Degree Curriculum", "positive_title": "Generalization on the Unseen, Logic Reasoning and Degree Curriculum", "is_correct": true, "reasoning": "The citation context discusses empirical research on learning biases in transformers, specifically mentioning that heldout generalization is biased towards low-degree polynomials. The context implies a focus on the theoretical understanding of how transformers generalize, particularly in relation to polynomial functions. Among the candidate papers, the first one, 'Generalization on the Unseen, Logic Reasoning and Degree Curriculum', directly addresses generalization in the context of transformers and provides empirical evidence related to learning biases, including the mention of low-degree solutions. This aligns closely with the context's emphasis on empirical evidence and theoretical understanding of generalization biases. The other candidates either focus on limitations of self-attention, formal language recognition, or do not directly address the specific claim about low-degree polynomials and heldout generalization. Therefore, the first candidate is the best match."}
{"example_id": "blind_17016", "prompt": "You are an expert citation retrieval system as described in the paper \"Multi-Agent System for Reliable Citation Retrieval\".\nYour goal is to autonomously retrieve, verify, and recommend academic references given a query or document excerpt.\n\nTask:\nGiven a citation context from a scientific paper (where a citation is missing), identify the correct paper from a list of candidates.\nAnalyze the context to understand the specific claim, method, or result being cited.\nThen, evaluate each candidate paper to see if it matches the context.\nFinally, select the best matching paper.\n\nCitation Context:\nodels, the Pretraining-Finetuning paradigm takes a more dominant role compared to training from scratch across various tasks, including computer vision, natural language processing[CITATION], and speech recognition. Under the paradigm of Pretraining-Finetuing, advanced parameter-efficient finetuning~(PEFT) methods have emerged. Compared to full finetuning, PEFT method\n\nCandidate Papers:\n1. Title: An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale\n   Abstract: An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale: While the Transformer architecture has become the de-facto standard for natural language processing tasks, its applications to computer vision remain limited. In vision, attention is either applied in conjunction with convolutional networks, or used to replace certain components of convolutional networks while keeping their overall structure in place. We show that this reliance on CNNs is not necessary and a pure transformer applied directly to sequences of image patches can perform very well on image classification tasks. When pre-trained on large amounts of data and transferred to multiple mid-sized or small image recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision Transformer (ViT) attains excellent results compared to state-of-the-art convolutional networks while requiring substantially fewer computational resources to train.\n\n2. Title: Fine-Tuning can Distort Pretrained Features and Underperform Out-of-Distribution\n   Abstract: Fine-Tuning can Distort Pretrained Features and Underperform Out-of-Distribution: When transferring a pretrained model to a downstream task, two popular methods are full fine-tuning (updating all the model parameters) and linear probing (updating only the last linear layer -- the \"head\"). It is well known that fine-tuning leads to better accuracy in-distribution (ID). However, in this paper, we find that fine-tuning can achieve worse accuracy than linear probing out-of-distribution (OOD) when the pretrained features are good and the distribution shift is large. On 10 distribution shift datasets (Breeds-Living17, Breeds-Entity30, DomainNet, CIFAR $\\to$ STL, CIFAR10.1, FMoW, ImageNetV2, ImageNet-R, ImageNet-A, ImageNet-Sketch), fine-tuning obtains on average 2% higher accuracy ID but 7% lower accuracy OOD than linear probing. We show theoretically that this tradeoff between ID and OOD accuracy arises even in a simple setting: fine-tuning overparameterized two-layer linear networks. We prove that the OOD error of fine-tuning is high when we initialize with a fixed or random head -- this is because while fine-tuning learns the head, the lower layers of the neural network change simultaneously and distort the pretrained features. Our analysis suggests that the easy two-step strategy of linear probing then full fine-tuning (LP-FT), sometimes used as a fine-tuning heuristic, combines the benefits of both fine-tuning and linear probing. Empirically, LP-FT outperforms both fine-tuning and linear probing on the above datasets (1% better ID, 10% better OOD than full fine-tuning).\n\n3. Title: TWINS: A Fine-Tuning Framework for Improved Transferability of Adversarial Robustness and Generalization\n   Abstract: TWINS: A Fine-Tuning Framework for Improved Transferability of Adversarial Robustness and Generalization: Recent years have seen the ever-increasing importance of pre-trained models and their downstream training in deep learning research and applications. At the same time, the defense for adversarial examples has been mainly investigated in the context of training from random initialization on simple classification tasks. To better exploit the potential of pre-trained models in adversarial robustness, this paper focuses on the fine-tuning of an adversarially pre-trained model in various classification tasks. Existing research has shown that since the robust pre-trained model has already learned a robust feature extractor, the crucial question is how to maintain the robustness in the pre-trained model when learning the downstream task. We study the model-based and data-based approaches for this goal and find that the two common approaches cannot achieve the objective of improving both generalization and adversarial robustness. Thus, we propose a novel statistics-based approach, Two-WIng NormliSation (TWINS) fine-tuning framework, which consists of two neural networks where one of them keeps the population means and variances of pre-training data in the batch normalization layers. Besides the robust information transfer, TWINS increases the effective learning rate without hurting the training stability since the relationship between a weight norm and its gradient norm in standard batch normalization layer is broken, resulting in a faster escape from the sub-optimal initialization and alleviating the robust overfitting. Finally, TWINS is shown to be effective on a wide range of image classification datasets in terms of both generalization and robustness. Our code is available at https://github.com/ziquanliu/CVPR2023-TWINS.\n\n4. Title: Adversarial robustness: From self-supervised pre-training to fine-tuning\n   Abstract: Adversarial Robustness: From Self-Supervised Pre-Training to Fine-Tuning: Pretrained models from self-supervision are prevalently used in fine-tuning downstream tasks faster or for better accuracy. However, gaining robustness from pretraining is left unexplored. We introduce adversarial training into self-supervision, to provide general-purpose robust pre-trained models for the first time. We find these robust pre-trained models can benefit the subsequent fine-tuning in two ways: i) boosting final model robustness; ii) saving the computation cost, if proceeding towards adversarial fine-tuning. We conduct extensive experiments to demonstrate that the proposed framework achieves large performance margins (eg, 3.83% on robust accuracy and 1.3% on standard accuracy, on the CIFAR-10 dataset), compared with the conventional end-to-end adversarial training baseline. Moreover, we find that different self-supervised pre-trained models have a diverse adversarial vulnerability. It inspires us to ensemble several pretraining tasks, which boosts robustness more. Our ensemble strategy contributes to a further improvement of 3.59% on robust accuracy, while maintaining a slightly higher standard accuracy on CIFAR-10. Our codes are available at https://github.com/TAMU-VITA/Adv-SS-Pretraining.\n\n5. Title: Parameter-efficient transfer learning for NLP\n   Abstract: Parameter-Efficient Transfer Learning for NLP: Fine-tuning large pre-trained models is an effective transfer mechanism in NLP. However, in the presence of many downstream tasks, fine-tuning is parameter inefficient: an entire new model is required for every task. As an alternative, we propose transfer with adapter modules. Adapter modules yield a compact and extensible model; they add only a few trainable parameters per task, and new tasks can be added without revisiting previous ones. The parameters of the original network remain fixed, yielding a high degree of parameter sharing. To demonstrate adapter's effectiveness, we transfer the recently proposed BERT Transformer model to 26 diverse text classification tasks, including the GLUE benchmark. Adapters attain near state-of-the-art performance, whilst adding only a few parameters per task. On GLUE, we attain within 0.4% of the performance of full fine-tuning, adding only 3.6% parameters per task. By contrast, fine-tuning trains 100% of the parameters per task.\n\n6. Title: AutoLoRa: A Parameter-Free Automated Robust Fine-Tuning Framework\n   Abstract: AutoLoRa: A Parameter-Free Automated Robust Fine-Tuning Framework: Robust Fine-Tuning (RFT) is a low-cost strategy to obtain adversarial robustness in downstream applications, without requiring a lot of computational resources and collecting significant amounts of data. This paper uncovers an issue with the existing RFT, where optimizing both adversarial and natural objectives through the feature extractor (FE) yields significantly divergent gradient directions. This divergence introduces instability in the optimization process, thereby hindering the attainment of adversarial robustness and rendering RFT highly sensitive to hyperparameters. To mitigate this issue, we propose a low-rank (LoRa) branch that disentangles RFT into two distinct components: optimizing natural objectives via the LoRa branch and adversarial objectives via the FE. Besides, we introduce heuristic strategies for automating the scheduling of the learning rate and the scalars of loss terms. Extensive empirical evaluations demonstrate that our proposed automated RFT disentangled via the LoRa branch (AutoLoRa) achieves new state-of-the-art results across a range of downstream tasks. AutoLoRa holds significant practical utility, as it automatically converts a pre-trained FE into an adversarially robust model for downstream tasks without the need for searching hyperparameters.\n\n7. Title: Robust pre-training by adversarial contrastive learning\n   Abstract: Robust Pre-Training by Adversarial Contrastive Learning: Recent work has shown that, when integrated with adversarial training, self-supervised pre-training can lead to state-of-the-art robustness In this work, we improve robustness-aware self-supervised pre-training by learning representations that are consistent under both data augmentations and adversarial perturbations. Our approach leverages a recent contrastive learning framework, which learns representations by maximizing feature consistency under differently augmented views. This fits particularly well with the goal of adversarial robustness, as one cause of adversarial fragility is the lack of feature invariance, i.e., small input perturbations can result in undesirable large changes in features or even predicted labels. We explore various options to formulate the contrastive task, and demonstrate that by injecting adversarial perturbations, contrastive pre-training can lead to models that are both label-efficient and robust. We empirically evaluate the proposed Adversarial Contrastive Learning (ACL) and show it can consistently outperform existing methods. For example on the CIFAR-10 dataset, ACL outperforms the previous state-of-the-art unsupervised robust pre-training approach by 2.99% on robust accuracy and 2.14% on standard accuracy. We further demonstrate that ACL pre-training can improve semi-supervised adversarial training, even when only a few labeled examples are available. Our codes and pre-trained models have been released at: https://github.com/VITA-Group/Adversarial-Contrastive-Learning.\n\n8. Title: Attention is all you need\n   Abstract: Attention Is All You Need: The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.\n\n9. Title: Visual prompt tuning\n   Abstract: Visual Prompt Tuning: The current modus operandi in adapting pre-trained models involves updating all the backbone parameters, ie, full fine-tuning. This paper introduces Visual Prompt Tuning (VPT) as an efficient and effective alternative to full fine-tuning for large-scale Transformer models in vision. Taking inspiration from recent advances in efficiently tuning large language models, VPT introduces only a small amount (less than 1% of model parameters) of trainable parameters in the input space while keeping the model backbone frozen. Via extensive experiments on a wide variety of downstream recognition tasks, we show that VPT achieves significant performance gains compared to other parameter efficient tuning protocols. Most importantly, VPT even outperforms full fine-tuning in many cases across model capacities and training data scales, while reducing per-task storage cost.\n\nThink step-by-step about which candidate best matches the context. Summarize your reasoning, then provide the exact title that should fill the citation. Respond in JSON with fields:\n{\n  \"reasoning\": \"<your step-by-step analysis>\",\n  \"selected_title\": \"<exact candidate title>\"\n}", "response": {"reasoning": "The citation context discusses the Pretraining-Finetuning paradigm and mentions advanced parameter-efficient fine-tuning (PEFT) methods. The context emphasizes the importance of these methods in various tasks, including computer vision and natural language processing. Among the candidate papers, the one that directly addresses parameter-efficient fine-tuning methods is 'Parameter-efficient transfer learning for NLP', which discusses the use of adapter modules to achieve efficient transfer learning in NLP tasks. Additionally, 'Visual prompt tuning' also focuses on an efficient alternative to full fine-tuning for large-scale models in vision, which aligns with the context's mention of PEFT methods. However, the first paper specifically addresses the parameter efficiency aspect in NLP, which is a broader application of the discussed paradigm. Therefore, it is the most relevant to the context provided.", "selected_title": "Parameter-efficient transfer learning for NLP"}, "selected_title": "Parameter-efficient transfer learning for NLP", "positive_title": "Attention is all you need", "is_correct": false, "reasoning": "The citation context discusses the Pretraining-Finetuning paradigm and mentions advanced parameter-efficient fine-tuning (PEFT) methods. The context emphasizes the importance of these methods in various tasks, including computer vision and natural language processing. Among the candidate papers, the one that directly addresses parameter-efficient fine-tuning methods is 'Parameter-efficient transfer learning for NLP', which discusses the use of adapter modules to achieve efficient transfer learning in NLP tasks. Additionally, 'Visual prompt tuning' also focuses on an efficient alternative to full fine-tuning for large-scale models in vision, which aligns with the context's mention of PEFT methods. However, the first paper specifically addresses the parameter efficiency aspect in NLP, which is a broader application of the discussed paradigm. Therefore, it is the most relevant to the context provided."}
{"example_id": "blind_5213", "prompt": "You are an expert citation retrieval system as described in the paper \"Multi-Agent System for Reliable Citation Retrieval\".\nYour goal is to autonomously retrieve, verify, and recommend academic references given a query or document excerpt.\n\nTask:\nGiven a citation context from a scientific paper (where a citation is missing), identify the correct paper from a list of candidates.\nAnalyze the context to understand the specific claim, method, or result being cited.\nThen, evaluate each candidate paper to see if it matches the context.\nFinally, select the best matching paper.\n\nCitation Context:\nrd in Natural Language Processing (NLP), recent literature commences introducing transformer-like networks into various computer vision tasks, including image recognition, object detection[CITATION], segmentation, visual question answering (VQA). The Vision Transformer (ViT) directly feeds image patches into a transformer for image classification, which removes\n\nCandidate Papers:\n1. Title: One-shot object detection with co-attention and co-excitation\n   Abstract: One-Shot Object Detection with Co-Attention and Co-Excitation: This paper aims to tackle the challenging problem of one-shot object detection. Given a query image patch whose class label is not included in the training data, the goal of the task is to detect all instances of the same class in a target image. To this end, we develop a novel {\\em co-attention and co-excitation} (CoAE) framework that makes contributions in three key technical aspects. First, we propose to use the non-local operation to explore the co-attention embodied in each query-target pair and yield region proposals accounting for the one-shot situation. Second, we formulate a squeeze-and-co-excitation scheme that can adaptively emphasize correlated feature channels to help uncover relevant proposals and eventually the target objects. Third, we design a margin-based ranking loss for implicitly learning a metric to predict the similarity of a region proposal to the underlying query, no matter its class label is seen or unseen in training. The resulting model is therefore a two-stage detector that yields a strong baseline on both VOC and MS-COCO under one-shot setting of detecting objects from both seen and never-seen classes. Codes are available at https://github.com/timy90022/One-Shot-Object-Detection.\n\n2. Title: Lstd: A low-shot transfer detector for object detection\n   Abstract: LSTD: A Low-Shot Transfer Detector for Object Detection: Recent advances in object detection are mainly driven by deep learning with large-scale detection benchmarks. However, the fully-annotated training set is often limited for a target detection task, which may deteriorate the performance of deep detectors. To address this challenge, we propose a novel low-shot transfer detector (LSTD) in this paper, where we leverage rich source-domain knowledge to construct an effective target-domain detector with very few training examples. The main contributions are described as follows. First, we design a flexible deep architecture of LSTD to alleviate transfer difficulties in low-shot detection. This architecture can integrate the advantages of both SSD and Faster RCNN in a unified deep framework. Second, we introduce a novel regularized transfer learning framework for low-shot detection, where the transfer knowledge (TK) and background depression (BD) regularizations are proposed to leverage object knowledge respectively from source and target domains, in order to further enhance fine-tuning with a few target images. Finally, we examine our LSTD on a number of challenging low-shot detection experiments, where LSTD outperforms other state-of-the-art approaches. The results demonstrate that LSTD is a preferable deep detector for low-shot scenarios.\n\n3. Title: Faster R-CNN: Towards real-time object detection with region proposal networks\n   Abstract: Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks: State-of-the-art object detection networks depend on region proposal algorithms to hypothesize object locations. Advances like SPPnet and Fast R-CNN have reduced the running time of these detection networks, exposing region proposal computation as a bottleneck. In this work, we introduce a Region Proposal Network (RPN) that shares full-image convolutional features with the detection network, thus enabling nearly cost-free region proposals. An RPN is a fully convolutional network that simultaneously predicts object bounds and objectness scores at each position. The RPN is trained end-to-end to generate high-quality region proposals, which are used by Fast R-CNN for detection. We further merge RPN and Fast R-CNN into a single network by sharing their convolutional features---using the recently popular terminology of neural networks with 'attention' mechanisms, the RPN component tells the unified network where to look. For the very deep VGG-16 model, our detection system has a frame rate of 5fps (including all steps) on a GPU, while achieving state-of-the-art object detection accuracy on PASCAL VOC 2007, 2012, and MS COCO datasets with only 300 proposals per image. In ILSVRC and COCO 2015 competitions, Faster R-CNN and RPN are the foundations of the 1st-place winning entries in several tracks. Code has been made publicly available.\n\n4. Title: Os2d: One-stage one-shot object detection by matching anchor features\n   Abstract: OS2D: One-Stage One-Shot Object Detection by Matching Anchor Features: In this paper, we consider the task of one-shot object detection, which consists in detecting objects defined by a single demonstration. Differently from the standard object detection, the classes of objects used for training and testing do not overlap. We build the one-stage system that performs localization and recognition jointly. We use dense correlation matching of learned local features to find correspondences, a feed-forward geometric transformation model to align features and bilinear resampling of the correlation tensor to compute the detection score of the aligned features. All the components are differentiable, which allows end-to-end training. Experimental evaluation on several challenging domains (retail products, 3D objects, buildings and logos) shows that our method can detect unseen classes (e.g., toothpaste when trained on groceries) and outperforms several baselines by a significant margin. Our code is available online: https://github.com/aosokin/os2d .\n\n5. Title: End-to-end object detection with transformers\n   Abstract: End-to-End Object Detection with Transformers: We present a new method that views object detection as a direct set prediction problem. Our approach streamlines the detection pipeline, effectively removing the need for many hand-designed components like a non-maximum suppression procedure or anchor generation that explicitly encode our prior knowledge about the task. The main ingredients of the new framework, called DEtection TRansformer or DETR, are a set-based global loss that forces unique predictions via bipartite matching, and a transformer encoder-decoder architecture. Given a fixed small set of learned object queries, DETR reasons about the relations of the objects and the global image context to directly output the final set of predictions in parallel. The new model is conceptually simple and does not require a specialized library, unlike many other modern detectors. DETR demonstrates accuracy and run-time performance on par with the well-established and highly-optimized Faster RCNN baseline on the challenging COCO object detection dataset. Moreover, DETR can be easily generalized to produce panoptic segmentation in a unified manner. We show that it significantly outperforms competitive baselines. Training code and pretrained models are available at https://github.com/facebookresearch/detr.\n\n6. Title: An image is worth 16x16 words: Transformers for image recognition at scale\n   Abstract: An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale: While the Transformer architecture has become the de-facto standard for natural language processing tasks, its applications to computer vision remain limited. In vision, attention is either applied in conjunction with convolutional networks, or used to replace certain components of convolutional networks while keeping their overall structure in place. We show that this reliance on CNNs is not necessary and a pure transformer applied directly to sequences of image patches can perform very well on image classification tasks. When pre-trained on large amounts of data and transferred to multiple mid-sized or small image recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision Transformer (ViT) attains excellent results compared to state-of-the-art convolutional networks while requiring substantially fewer computational resources to train.\n\n7. Title: LXMERT: Learning cross-modality encoder representations from transformers\n   Abstract: LXMERT: Learning Cross-Modality Encoder Representations from Transformers: Vision-and-language reasoning requires an understanding of visual concepts, language semantics, and, most importantly, the alignment and relationships between these two modalities. We thus propose the LXMERT (Learning Cross-Modality Encoder Representations from Transformers) framework to learn these vision-and-language connections. In LXMERT, we build a large-scale Transformer model that consists of three encoders: an object relationship encoder, a language encoder, and a cross-modality encoder. Next, to endow our model with the capability of connecting vision and language semantics, we pre-train the model with large amounts of image-and-sentence pairs, via five diverse representative pre-training tasks: masked language modeling, masked object prediction (feature regression and label classification), cross-modality matching, and image question answering. These tasks help in learning both intra-modality and cross-modality relationships. After fine-tuning from our pre-trained parameters, our model achieves the state-of-the-art results on two visual question answering datasets (i.e., VQA and GQA). We also show the generalizability of our pre-trained cross-modality model by adapting it to a challenging visual-reasoning task, NLVR2, and improve the previous best result by 22% absolute (54% to 76%). Lastly, we demonstrate detailed ablation studies to prove that both our novel model components and pre-training strategies significantly contribute to our strong results; and also present several attention visualizations for the different encoders. Code and pre-trained models publicly available at: https://github.com/airsplay/lxmert\n\n8. Title: Cross-modal self-attention network for referring image segmentation\n   Abstract: Cross-Modal Self-Attention Network for Referring Image Segmentation: We consider the problem of referring image segmentation. Given an input image and a natural language expression, the goal is to segment the object referred by the language expression in the image. Existing works in this area treat the language expression and the input image separately in their representations. They do not sufficiently capture long-range correlations between these two modalities. In this paper, we propose a cross-modal self-attention (CMSA) module that effectively captures the long-range dependencies between linguistic and visual features. Our model can adaptively focus on informative words in the referring expression and important regions in the input image. In addition, we propose a gated multi-level fusion module to selectively integrate self-attentive cross-modal features corresponding to different levels in the image. This module controls the information flow of features at different levels. We validate the proposed approach on four evaluation datasets. Our proposed approach consistently outperforms existing state-of-the-art methods.\n\nThink step-by-step about which candidate best matches the context. Summarize your reasoning, then provide the exact title that should fill the citation. Respond in JSON with fields:\n{\n  \"reasoning\": \"<your step-by-step analysis>\",\n  \"selected_title\": \"<exact candidate title>\"\n}", "response": {"reasoning": "The citation context discusses the introduction of transformer-like networks into various computer vision tasks, specifically mentioning the Vision Transformer (ViT) and its application to image classification. Among the candidate papers, the one that directly addresses the use of transformers in image recognition is candidate 6, titled 'An image is worth 16x16 words: Transformers for image recognition at scale.' This paper specifically discusses the application of the transformer architecture to image classification tasks, which aligns perfectly with the context provided. Other candidates focus on object detection or other methodologies that do not directly relate to the transformer architecture's application in image recognition as described in the context. Therefore, candidate 6 is the best match.", "selected_title": "An image is worth 16x16 words: Transformers for image recognition at scale"}, "selected_title": "An image is worth 16x16 words: Transformers for image recognition at scale", "positive_title": "End-to-end object detection with transformers", "is_correct": false, "reasoning": "The citation context discusses the introduction of transformer-like networks into various computer vision tasks, specifically mentioning the Vision Transformer (ViT) and its application to image classification. Among the candidate papers, the one that directly addresses the use of transformers in image recognition is candidate 6, titled 'An image is worth 16x16 words: Transformers for image recognition at scale.' This paper specifically discusses the application of the transformer architecture to image classification tasks, which aligns perfectly with the context provided. Other candidates focus on object detection or other methodologies that do not directly relate to the transformer architecture's application in image recognition as described in the context. Therefore, candidate 6 is the best match."}
{"example_id": "blind_9272", "prompt": "You are an expert citation retrieval system as described in the paper \"Multi-Agent System for Reliable Citation Retrieval\".\nYour goal is to autonomously retrieve, verify, and recommend academic references given a query or document excerpt.\n\nTask:\nGiven a citation context from a scientific paper (where a citation is missing), identify the correct paper from a list of candidates.\nAnalyze the context to understand the specific claim, method, or result being cited.\nThen, evaluate each candidate paper to see if it matches the context.\nFinally, select the best matching paper.\n\nCitation Context:\numan feedback has been explored extensively, often being used in robotics tasks to extrapolate reward signal beyond an initial preference set. Recent work in NLP has explored implementations[CITATION], objectives, and even alternatives for RLHF, but have generally overlooked or dismissed length increases. Our work is largely orthogonal to these directions, using the issue of\n\nCandidate Papers:\n1. Title: Fine-Grained Human Feedback Gives Better Rewards for Language Model Training\n   Abstract: Fine-Grained Human Feedback Gives Better Rewards for Language Model Training: Language models (LMs) often exhibit undesirable text generation behaviors, including generating false, toxic, or irrelevant outputs. Reinforcement learning from human feedback (RLHF) - where human preference judgments on LM outputs are transformed into a learning signal - has recently shown promise in addressing these issues. However, such holistic feedback conveys limited information on long text outputs; it does not indicate which aspects of the outputs influenced user preference; e.g., which parts contain what type(s) of errors. In this paper, we use fine-grained human feedback (e.g., which sentence is false, which sub-sentence is irrelevant) as an explicit training signal. We introduce Fine-Grained RLHF, a framework that enables training and learning from reward functions that are fine-grained in two respects: (1) density, providing a reward after every segment (e.g., a sentence) is generated; and (2) incorporating multiple reward models associated with different feedback types (e.g., factual incorrectness, irrelevance, and information incompleteness). We conduct experiments on detoxification and long-form question answering to illustrate how learning with such reward functions leads to improved performance, supported by both automatic and human evaluation. Additionally, we show that LM behaviors can be customized using different combinations of fine-grained reward models. We release all data, collected human feedback, and codes at https://FineGrainedRLHF.github.io.\n\n2. Title: Secrets of RLHF in Large Language Models Part I: PPO\n   Abstract: Secrets of RLHF in Large Language Models Part I: PPO: Large language models (LLMs) have formulated a blueprint for the advancement of artificial general intelligence. Its primary objective is to function as a human-centric (helpful, honest, and harmless) assistant. Alignment with humans assumes paramount significance, and reinforcement learning with human feedback (RLHF) emerges as the pivotal technological paradigm underpinning this pursuit. Current technical routes usually include \\textbf{reward models} to measure human preferences, \\textbf{Proximal Policy Optimization} (PPO) to optimize policy model outputs, and \\textbf{process supervision} to improve step-by-step reasoning capabilities. However, due to the challenges of reward design, environment interaction, and agent training, coupled with huge trial and error cost of large language models, there is a significant barrier for AI researchers to motivate the development of technical alignment and safe landing of LLMs. The stable training of RLHF has still been a puzzle. In the first report, we dissect the framework of RLHF, re-evaluate the inner workings of PPO, and explore how the parts comprising PPO algorithms impact policy agent training. We identify policy constraints being the key factor for the effective implementation of the PPO algorithm. Therefore, we explore the PPO-max, an advanced version of PPO algorithm, to efficiently improve the training stability of the policy model. Based on our main results, we perform a comprehensive analysis of RLHF abilities compared with SFT models and ChatGPT. The absence of open-source implementations has posed significant challenges to the investigation of LLMs alignment. Therefore, we are eager to release technical reports, reward models and PPO codes, aiming to make modest contributions to the advancement of LLMs.\n\n3. Title: Defining and Characterizing Reward Hacking\n   Abstract: Defining and Characterizing Reward Hacking: We provide the first formal definition of reward hacking, a phenomenon where optimizing an imperfect proxy reward function, $\\mathcal{\\tilde{R}}$, leads to poor performance according to the true reward function, $\\mathcal{R}$. We say that a proxy is unhackable if increasing the expected proxy return can never decrease the expected true return. Intuitively, it might be possible to create an unhackable proxy by leaving some terms out of the reward function (making it \"narrower\") or overlooking fine-grained distinctions between roughly equivalent outcomes, but we show this is usually not the case. A key insight is that the linearity of reward (in state-action visit counts) makes unhackability a very strong condition. In particular, for the set of all stochastic policies, two reward functions can only be unhackable if one of them is constant. We thus turn our attention to deterministic policies and finite sets of stochastic policies, where non-trivial unhackable pairs always exist, and establish necessary and sufficient conditions for the existence of simplifications, an important special case of unhackability. Our results reveal a tension between using reward functions to specify narrow tasks and aligning AI systems with human values.\n\n4. Title: Learning to summarize from human feedback\n   Abstract: Learning to summarize from human feedback: As language models become more powerful, training and evaluation are increasingly bottlenecked by the data and metrics used for a particular task. For example, summarization models are often trained to predict human reference summaries and evaluated using ROUGE, but both of these metrics are rough proxies for what we really care about -- summary quality. In this work, we show that it is possible to significantly improve summary quality by training a model to optimize for human preferences. We collect a large, high-quality dataset of human comparisons between summaries, train a model to predict the human-preferred summary, and use that model as a reward function to fine-tune a summarization policy using reinforcement learning. We apply our method to a version of the TL;DR dataset of Reddit posts and find that our models significantly outperform both human reference summaries and much larger models fine-tuned with supervised learning alone. Our models also transfer to CNN/DM news articles, producing summaries nearly as good as the human reference without any news-specific fine-tuning. We conduct extensive analyses to understand our human feedback dataset and fine-tuned models We establish that our reward model generalizes to new datasets, and that optimizing our reward model results in better summaries than optimizing ROUGE according to humans. We hope the evidence from our paper motivates machine learning researchers to pay closer attention to how their training loss affects the model behavior they actually want.\n\n5. Title: Aligning Robot and Human Representations\n   Abstract: Aligning Robot and Human Representations: To act in the world, robots rely on a representation of salient task aspects: for example, to carry a coffee mug, a robot may consider movement efficiency or mug orientation in its behavior. However, if we want robots to act for and with people, their representations must not be just functional but also reflective of what humans care about, i.e. they must be aligned. We observe that current learning approaches suffer from representation misalignment, where the robot's learned representation does not capture the human's representation. We suggest that because humans are the ultimate evaluator of robot performance, we must explicitly focus our efforts on aligning learned representations with humans, in addition to learning the downstream task. We advocate that current representation learning approaches in robotics should be studied from the perspective of how well they accomplish the objective of representation alignment. We mathematically define the problem, identify its key desiderata, and situate current methods within this formalism. We conclude by suggesting future directions for exploring open challenges.\n\n6. Title: Event Representations for Automated Story Generation with Deep Neural Nets\n   Abstract: Event Representations for Automated Story Generation with Deep Neural Nets: Automated story generation is the problem of automatically selecting a sequence of events, actions, or words that can be told as a story. We seek to develop a system that can generate stories by learning everything it needs to know from textual story corpora. To date, recurrent neural networks that learn language models at character, word, or sentence levels have had little success generating coherent stories. We explore the question of event representations that provide a mid-level of abstraction between words and sentences in order to retain the semantic information of the original data while minimizing event sparsity. We present a technique for preprocessing textual story data into event sequences. We then present a technique for automated story generation whereby we decompose the problem into the generation of successive events (event2event) and the generation of natural language sentences from events (event2sentence). We give empirical results comparing different event representations and their effects on event successor generation and the translation of events to natural language.\n\n7. Title: Controlling Output Length in Neural Encoder-Decoders\n   Abstract: Controlling Output Length in Neural Encoder-Decoders: Neural encoder-decoder models have shown great success in many sequence generation tasks. However, previous work has not investigated situations in which we would like to control the length of encoder-decoder outputs. This capability is crucial for applications such as text summarization, in which we have to generate concise summaries with a desired length. In this paper, we propose methods for controlling the output sequence length for neural encoder-decoder models: two decoding-based methods and two learning-based methods. Results show that our learning-based methods have the capability to control length without degrading summary quality in a summarization task.\n\n8. Title: SLiC-HF: Sequence Likelihood Calibration with Human Feedback\n   Abstract: SLiC-HF: Sequence Likelihood Calibration with Human Feedback: Learning from human feedback has been shown to be effective at aligning language models with human preferences. Past work has often relied on Reinforcement Learning from Human Feedback (RLHF), which optimizes the language model using reward scores assigned from a reward model trained on human preference data. In this work we show how the recently introduced Sequence Likelihood Calibration (SLiC), can also be used to effectively learn from human preferences (SLiC-HF). Furthermore, we demonstrate this can be done with human feedback data collected for a different model, similar to off-policy, offline RL data. Automatic and human evaluation experiments on the TL;DR summarization task show that SLiC-HF significantly improves supervised fine-tuning baselines. Furthermore, SLiC-HF presents a competitive alternative to the PPO RLHF implementation used in past work while being much simpler to implement, easier to tune and more computationally efficient in practice.\n\n9. Title: Reward Gaming in Conditional Text Generation\n   Abstract: Reward Gaming in Conditional Text Generation: To align conditional text generation model outputs with desired behaviors, there has been an increasing focus on training the model using reinforcement learning (RL) with reward functions learned from human annotations. Under this framework, we identify three common cases where high rewards are incorrectly assigned to undesirable patterns: noise-induced spurious correlation, naturally occurring spurious correlation, and covariate shift. We show that even though learned metrics achieve high performance on the distribution of the data used to train the reward function, the undesirable patterns may be amplified during RL training of the text generation model. While there has been discussion about reward gaming in the RL or safety community, in this discussion piece, we would like to highlight reward gaming in the natural language generation (NLG) community using concrete conditional text generation examples and discuss potential fixes and areas for future work.\n\nThink step-by-step about which candidate best matches the context. Summarize your reasoning, then provide the exact title that should fill the citation. Respond in JSON with fields:\n{\n  \"reasoning\": \"<your step-by-step analysis>\",\n  \"selected_title\": \"<exact candidate title>\"\n}", "response": {"reasoning": "The citation context discusses the exploration of human feedback in robotics and NLP, particularly in relation to reinforcement learning from human feedback (RLHF) and mentions that recent work has overlooked or dismissed length increases. Among the candidate papers, the one that directly addresses the control of output length in neural encoder-decoder models is 'Controlling Output Length in Neural Encoder-Decoders'. This paper proposes methods for controlling output length, which aligns with the context's mention of length increases being overlooked. Other candidates focus on various aspects of RLHF but do not specifically address the issue of output length control, making this paper the most relevant choice.", "selected_title": "Controlling Output Length in Neural Encoder-Decoders"}, "selected_title": "Controlling Output Length in Neural Encoder-Decoders", "positive_title": "Secrets of RLHF in Large Language Models Part I: PPO", "is_correct": false, "reasoning": "The citation context discusses the exploration of human feedback in robotics and NLP, particularly in relation to reinforcement learning from human feedback (RLHF) and mentions that recent work has overlooked or dismissed length increases. Among the candidate papers, the one that directly addresses the control of output length in neural encoder-decoder models is 'Controlling Output Length in Neural Encoder-Decoders'. This paper proposes methods for controlling output length, which aligns with the context's mention of length increases being overlooked. Other candidates focus on various aspects of RLHF but do not specifically address the issue of output length control, making this paper the most relevant choice."}
{"example_id": "blind_9506", "prompt": "You are an expert citation retrieval system as described in the paper \"Multi-Agent System for Reliable Citation Retrieval\".\nYour goal is to autonomously retrieve, verify, and recommend academic references given a query or document excerpt.\n\nTask:\nGiven a citation context from a scientific paper (where a citation is missing), identify the correct paper from a list of candidates.\nAnalyze the context to understand the specific claim, method, or result being cited.\nThen, evaluate each candidate paper to see if it matches the context.\nFinally, select the best matching paper.\n\nCitation Context:\nng approach for leveraging online human-object interaction videos without explicit pose estimation to improve dexterity in robotic manipulation, sharing a similar motivation as MVP and VC-1[CITATION].\n\nCandidate Papers:\n1. Title: Dexmv: Imitation learning for dexterous manipulation from human videos\n   Abstract: DexMV: Imitation Learning for Dexterous Manipulation from Human Videos: While significant progress has been made on understanding hand-object interactions in computer vision, it is still very challenging for robots to perform complex dexterous manipulation. In this paper, we propose a new platform and pipeline DexMV (Dexterous Manipulation from Videos) for imitation learning. We design a platform with: (i) a simulation system for complex dexterous manipulation tasks with a multi-finger robot hand and (ii) a computer vision system to record large-scale demonstrations of a human hand conducting the same tasks. In our novel pipeline, we extract 3D hand and object poses from videos, and propose a novel demonstration translation method to convert human motion to robot demonstrations. We then apply and benchmark multiple imitation learning algorithms with the demonstrations. We show that the demonstrations can indeed improve robot learning by a large margin and solve the complex tasks which reinforcement learning alone cannot solve. More details can be found in the project page: https://yzqin.github.io/dexmv\n\n2. Title: Rrl: Resnet as representation for reinforcement learning\n   Abstract: RRL: Resnet as representation for Reinforcement Learning: The ability to autonomously learn behaviors via direct interactions in uninstrumented environments can lead to generalist robots capable of enhancing productivity or providing care in unstructured settings like homes. Such uninstrumented settings warrant operations only using the robot's proprioceptive sensor such as onboard cameras, joint encoders, etc which can be challenging for policy learning owing to the high dimensionality and partial observability issues. We propose RRL: Resnet as representation for Reinforcement Learning -- a straightforward yet effective approach that can learn complex behaviors directly from proprioceptive inputs. RRL fuses features extracted from pre-trained Resnet into the standard reinforcement learning pipeline and delivers results comparable to learning directly from the state. In a simulated dexterous manipulation benchmark, where the state of the art methods fail to make significant progress, RRL delivers contact rich behaviors. The appeal of RRL lies in its simplicity in bringing together progress from the fields of Representation Learning, Imitation Learning, and Reinforcement Learning. Its effectiveness in learning behaviors directly from visual inputs with performance and sample efficiency matching learning directly from the state, even in complex high dimensional domains, is far from obvious.\n\n3. Title: R3m: A universal visual representation for robot manipulation\n   Abstract: R3M: A Universal Visual Representation for Robot Manipulation: We study how visual representations pre-trained on diverse human video data can enable data-efficient learning of downstream robotic manipulation tasks. Concretely, we pre-train a visual representation using the Ego4D human video dataset using a combination of time-contrastive learning, video-language alignment, and an L1 penalty to encourage sparse and compact representations. The resulting representation, R3M, can be used as a frozen perception module for downstream policy learning. Across a suite of 12 simulated robot manipulation tasks, we find that R3M improves task success by over 20% compared to training from scratch and by over 10% compared to state-of-the-art visual representations like CLIP and MoCo. Furthermore, R3M enables a Franka Emika Panda arm to learn a range of manipulation tasks in a real, cluttered apartment given just 20 demonstrations. Code and pre-trained models are available at https://tinyurl.com/robotr3m.\n\n4. Title: GNFactor: Multi-Task Real Robot Learning with Generalizable Neural Feature Fields\n   Abstract: GNFactor: Multi-Task Real Robot Learning with Generalizable Neural Feature Fields: It is a long-standing problem in robotics to develop agents capable of executing diverse manipulation tasks from visual observations in unstructured real-world environments. To achieve this goal, the robot needs to have a comprehensive understanding of the 3D structure and semantics of the scene. In this work, we present $\\textbf{GNFactor}$, a visual behavior cloning agent for multi-task robotic manipulation with $\\textbf{G}$eneralizable $\\textbf{N}$eural feature $\\textbf{F}$ields. GNFactor jointly optimizes a generalizable neural field (GNF) as a reconstruction module and a Perceiver Transformer as a decision-making module, leveraging a shared deep 3D voxel representation. To incorporate semantics in 3D, the reconstruction module utilizes a vision-language foundation model ($\\textit{e.g.}$, Stable Diffusion) to distill rich semantic information into the deep 3D voxel. We evaluate GNFactor on 3 real robot tasks and perform detailed ablations on 10 RLBench tasks with a limited number of demonstrations. We observe a substantial improvement of GNFactor over current state-of-the-art methods in seen and unseen tasks, demonstrating the strong generalization ability of GNFactor. Our project website is https://yanjieze.com/GNFactor/ .\n\n5. Title: Masked visual pre-training for motor control\n   Abstract: Masked Visual Pre-training for Motor Control: This paper shows that self-supervised visual pre-training from real-world images is effective for learning motor control tasks from pixels. We first train the visual representations by masked modeling of natural images. We then freeze the visual encoder and train neural network controllers on top with reinforcement learning. We do not perform any task-specific fine-tuning of the encoder; the same visual representations are used for all motor control tasks. To the best of our knowledge, this is the first self-supervised model to exploit real-world images at scale for motor control. To accelerate progress in learning from pixels, we contribute a benchmark suite of hand-designed tasks varying in movements, scenes, and robots. Without relying on labels, state-estimation, or expert demonstrations, we consistently outperform supervised encoders by up to 80% absolute success rate, sometimes even matching the oracle state performance. We also find that in-the-wild images, e.g., from YouTube or Egocentric videos, lead to better visual representations for various manipulation tasks than ImageNet images.\n\n6. Title: Vrl3: A data-driven framework for visual deep reinforcement learning\n   Abstract: VRL3: A Data-Driven Framework for Visual Deep Reinforcement Learning: We propose VRL3, a powerful data-driven framework with a simple design for solving challenging visual deep reinforcement learning (DRL) tasks. We analyze a number of major obstacles in taking a data-driven approach, and present a suite of design principles, novel findings, and critical insights about data-driven visual DRL. Our framework has three stages: in stage 1, we leverage non-RL datasets (e.g. ImageNet) to learn task-agnostic visual representations; in stage 2, we use offline RL data (e.g. a limited number of expert demonstrations) to convert the task-agnostic representations into more powerful task-specific representations; in stage 3, we fine-tune the agent with online RL. On a set of challenging hand manipulation tasks with sparse reward and realistic visual inputs, compared to the previous SOTA, VRL3 achieves an average of 780% better sample efficiency. And on the hardest task, VRL3 is 1220% more sample efficient (2440% when using a wider encoder) and solves the task with only 10% of the computation. These significant results clearly demonstrate the great potential of data-driven deep reinforcement learning.\n\n7. Title: Time-contrastive networks: Self-supervised learning from video\n   Abstract: Time-Contrastive Networks: Self-Supervised Learning from Video: We propose a self-supervised approach for learning representations and robotic behaviors entirely from unlabeled videos recorded from multiple viewpoints, and study how this representation can be used in two robotic imitation settings: imitating object interactions from videos of humans, and imitating human poses. Imitation of human behavior requires a viewpoint-invariant representation that captures the relationships between end-effectors (hands or robot grippers) and the environment, object attributes, and body pose. We train our representations using a metric learning loss, where multiple simultaneous viewpoints of the same observation are attracted in the embedding space, while being repelled from temporal neighbors which are often visually similar but functionally different. In other words, the model simultaneously learns to recognize what is common between different-looking images, and what is different between similar-looking images. This signal causes our model to discover attributes that do not change across viewpoint, but do change across time, while ignoring nuisance variables such as occlusions, motion blur, lighting and background. We demonstrate that this representation can be used by a robot to directly mimic human poses without an explicit correspondence, and that it can be used as a reward function within a reinforcement learning algorithm. While representations are learned from an unlabeled collection of task-related videos, robot behaviors such as pouring are learned by watching a single 3rd-person demonstration by a human. Reward functions obtained by following the human demonstrations under the learned representation enable efficient reinforcement learning that is practical for real-world robotic systems. Video results, open-source code and dataset are available at https://sermanet.github.io/imitate\n\n8. Title: Where are we in the search for an Artificial Visual Cortex for Embodied Intelligence?\n   Abstract: Where are we in the search for an Artificial Visual Cortex for Embodied Intelligence?: We present the largest and most comprehensive empirical study of pre-trained visual representations (PVRs) or visual 'foundation models' for Embodied AI. First, we curate CortexBench, consisting of 17 different tasks spanning locomotion, navigation, dexterous, and mobile manipulation. Next, we systematically evaluate existing PVRs and find that none are universally dominant. To study the effect of pre-training data size and diversity, we combine over 4,000 hours of egocentric videos from 7 different sources (over 4.3M images) and ImageNet to train different-sized vision transformers using Masked Auto-Encoding (MAE) on slices of this data. Contrary to inferences from prior work, we find that scaling dataset size and diversity does not improve performance universally (but does so on average). Our largest model, named VC-1, outperforms all prior PVRs on average but does not universally dominate either. Next, we show that task- or domain-specific adaptation of VC-1 leads to substantial gains, with VC-1 (adapted) achieving competitive or superior performance than the best known results on all of the benchmarks in CortexBench. Finally, we present real-world hardware experiments, in which VC-1 and VC-1 (adapted) outperform the strongest pre-existing PVR. Overall, this paper presents no new techniques but a rigorous systematic evaluation, a broad set of findings about PVRs (that in some cases, refute those made in narrow domains in prior work), and open-sourced code and models (that required over 10,000 GPU-hours to train) for the benefit of the research community.\n\n9. Title: Data-efficient deep reinforcement learning for dexterous manipulation\n   Abstract: Data-efficient Deep Reinforcement Learning for Dexterous Manipulation: Deep learning and reinforcement learning methods have recently been used to solve a variety of problems in continuous control domains. An obvious application of these techniques is dexterous manipulation tasks in robotics which are difficult to solve using traditional control theory or hand-engineered approaches. One example of such a task is to grasp an object and precisely stack it on another. Solving this difficult and practically relevant problem in the real world is an important long-term goal for the field of robotics. Here we take a step towards this goal by examining the problem in simulation and providing models and techniques aimed at solving it. We introduce two extensions to the Deep Deterministic Policy Gradient algorithm (DDPG), a model-free Q-learning based method, which make it significantly more data-efficient and scalable. Our results show that by making extensive use of off-policy data and replay, it is possible to find control policies that robustly grasp objects and stack them. Further, our results hint that it may soon be feasible to train successful stacking policies by collecting interactions on real robots.\n\nThink step-by-step about which candidate best matches the context. Summarize your reasoning, then provide the exact title that should fill the citation. Respond in JSON with fields:\n{\n  \"reasoning\": \"<your step-by-step analysis>\",\n  \"selected_title\": \"<exact candidate title>\"\n}", "response": {"reasoning": "The citation context discusses an approach for improving dexterity in robotic manipulation using online human-object interaction videos without explicit pose estimation. The context mentions a motivation similar to MVP and VC-1, which suggests a focus on leveraging visual representations from videos for robotic tasks. Among the candidate papers, the one titled 'R3M: A Universal Visual Representation for Robot Manipulation' stands out as it specifically addresses the use of visual representations pre-trained on human video data to enhance learning in robotic manipulation tasks. This aligns closely with the context's emphasis on improving dexterity through video-based learning. Other candidates, while relevant to robotics and manipulation, do not focus as directly on the use of human videos for improving dexterity in the same way. Therefore, the best match for the citation is the paper that discusses the use of visual representations from human videos for robotic manipulation.", "selected_title": "R3M: A Universal Visual Representation for Robot Manipulation"}, "selected_title": "R3M: A Universal Visual Representation for Robot Manipulation", "positive_title": "Where are we in the search for an Artificial Visual Cortex for Embodied Intelligence?", "is_correct": false, "reasoning": "The citation context discusses an approach for improving dexterity in robotic manipulation using online human-object interaction videos without explicit pose estimation. The context mentions a motivation similar to MVP and VC-1, which suggests a focus on leveraging visual representations from videos for robotic tasks. Among the candidate papers, the one titled 'R3M: A Universal Visual Representation for Robot Manipulation' stands out as it specifically addresses the use of visual representations pre-trained on human video data to enhance learning in robotic manipulation tasks. This aligns closely with the context's emphasis on improving dexterity through video-based learning. Other candidates, while relevant to robotics and manipulation, do not focus as directly on the use of human videos for improving dexterity in the same way. Therefore, the best match for the citation is the paper that discusses the use of visual representations from human videos for robotic manipulation."}
{"example_id": "blind_10513", "prompt": "You are an expert citation retrieval system as described in the paper \"Multi-Agent System for Reliable Citation Retrieval\".\nYour goal is to autonomously retrieve, verify, and recommend academic references given a query or document excerpt.\n\nTask:\nGiven a citation context from a scientific paper (where a citation is missing), identify the correct paper from a list of candidates.\nAnalyze the context to understand the specific claim, method, or result being cited.\nThen, evaluate each candidate paper to see if it matches the context.\nFinally, select the best matching paper.\n\nCitation Context:\na transformer to directly regress the parameters of 3D models given conditions. Besides, applying diffusion models to directly generate 3D assets has also been widely researched.[CITATION] lead the SOTA of current 3D generation methods by first generating high-quality 3D shapes with DiT and then producing detailed textures with material diffusion models. As mesh is a crucial component\n\nCandidate Papers:\n1. Title: Denoising diffusion probabilistic models\n   Abstract: Denoising Diffusion Probabilistic Models: We present high quality image synthesis results using diffusion probabilistic models, a class of latent variable models inspired by considerations from nonequilibrium thermodynamics. Our best results are obtained by training on a weighted variational bound designed according to a novel connection between diffusion probabilistic models and denoising score matching with Langevin dynamics, and our models naturally admit a progressive lossy decompression scheme that can be interpreted as a generalization of autoregressive decoding. On the unconditional CIFAR10 dataset, we obtain an Inception score of 9.46 and a state-of-the-art FID score of 3.17. On 256x256 LSUN, we obtain sample quality similar to ProgressiveGAN. Our implementation is available at https://github.com/hojonathanho/diffusion\n\n2. Title: CLAY: A Controllable Large-scale Generative Model for Creating High-quality 3D Assets\n   Abstract: CLAY: A Controllable Large-scale Generative Model for Creating High-quality 3D Assets: In the realm of digital creativity, our potential to craft intricate 3D worlds from imagination is often hampered by the limitations of existing digital tools, which demand extensive expertise and efforts. To narrow this disparity, we introduce CLAY, a 3D geometry and material generator designed to effortlessly transform human imagination into intricate 3D digital structures. CLAY supports classic text or image inputs as well as 3D-aware controls from diverse primitives (multi-view images, voxels, bounding boxes, point clouds, implicit representations, etc). At its core is a large-scale generative model composed of a multi-resolution Variational Autoencoder (VAE) and a minimalistic latent Diffusion Transformer (DiT), to extract rich 3D priors directly from a diverse range of 3D geometries. Specifically, it adopts neural fields to represent continuous and complete surfaces and uses a geometry generative module with pure transformer blocks in latent space. We present a progressive training scheme to train CLAY on an ultra large 3D model dataset obtained through a carefully designed processing pipeline, resulting in a 3D native geometry generator with 1.5 billion parameters. For appearance generation, CLAY sets out to produce physically-based rendering (PBR) textures by employing a multi-view material diffusion model that can generate 2K resolution textures with diffuse, roughness, and metallic modalities. We demonstrate using CLAY for a range of controllable 3D asset creations, from sketchy conceptual designs to production ready assets with intricate details. Even first time users can easily use CLAY to bring their vivid 3D imaginations to life, unleashing unlimited creativity.\n\n3. Title: PivotMesh: Generic 3D Mesh Generation via Pivot Vertices Guidance\n   Abstract: PivotMesh: Generic 3D Mesh Generation via Pivot Vertices Guidance: Generating compact and sharply detailed 3D meshes poses a significant challenge for current 3D generative models. Different from extracting dense meshes from neural representation, some recent works try to model the native mesh distribution (i.e., a set of triangles), which generates more compact results as humans crafted. However, due to the complexity and variety of mesh topology, these methods are typically limited to small datasets with specific categories and are hard to extend. In this paper, we introduce a generic and scalable mesh generation framework PivotMesh, which makes an initial attempt to extend the native mesh generation to large-scale datasets. We employ a transformer-based auto-encoder to encode meshes into discrete tokens and decode them from face level to vertex level hierarchically. Subsequently, to model the complex typology, we first learn to generate pivot vertices as coarse mesh representation and then generate the complete mesh tokens with the same auto-regressive Transformer. This reduces the difficulty compared with directly modeling the mesh distribution and further improves the model controllability. PivotMesh demonstrates its versatility by effectively learning from both small datasets like Shapenet, and large-scale datasets like Objaverse and Objaverse-xl. Extensive experiments indicate that PivotMesh can generate compact and sharp 3D meshes across various categories, highlighting its great potential for native mesh modeling.\n\n4. Title: MeshLRM: Large Reconstruction Model for High-Quality Mesh\n   Abstract: MeshLRM: Large Reconstruction Model for High-Quality Mesh: We propose MeshLRM, a novel LRM-based approach that can reconstruct a high-quality mesh from merely four input images in less than one second. Different from previous large reconstruction models (LRMs) that focus on NeRF-based reconstruction, MeshLRM incorporates differentiable mesh extraction and rendering within the LRM framework. This allows for end-to-end mesh reconstruction by fine-tuning a pre-trained NeRF LRM with mesh rendering. Moreover, we improve the LRM architecture by simplifying several complex designs in previous LRMs. MeshLRM's NeRF initialization is sequentially trained with low- and high-resolution images; this new LRM training strategy enables significantly faster convergence and thereby leads to better quality with less compute. Our approach achieves state-of-the-art mesh reconstruction from sparse-view inputs and also allows for many downstream applications, including text-to-3D and single-image-to-3D generation. Project page: https://sarahweiii.github.io/meshlrm/\n\n5. Title: PolyDiff: Generating 3D Polygonal Meshes with Diffusion Models\n   Abstract: PolyDiff: Generating 3D Polygonal Meshes with Diffusion Models: We introduce PolyDiff, the first diffusion-based approach capable of directly generating realistic and diverse 3D polygonal meshes. In contrast to methods that use alternate 3D shape representations (e.g. implicit representations), our approach is a discrete denoising diffusion probabilistic model that operates natively on the polygonal mesh data structure. This enables learning of both the geometric properties of vertices and the topological characteristics of faces. Specifically, we treat meshes as quantized triangle soups, progressively corrupted with categorical noise in the forward diffusion phase. In the reverse diffusion phase, a transformer-based denoising network is trained to revert the noising process, restoring the original mesh structure. At inference, new meshes can be generated by applying this denoising network iteratively, starting with a completely noisy triangle soup. Consequently, our model is capable of producing high-quality 3D polygonal meshes, ready for integration into downstream 3D workflows. Our extensive experimental analysis shows that PolyDiff achieves a significant advantage (avg. FID and JSD improvement of 18.2 and 5.8 respectively) over current state-of-the-art methods.\n\n6. Title: Attention is all you need\n   Abstract: Attention Is All You Need: The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.\n\n7. Title: MeshAnything: Artist-Created Mesh Generation with Autoregressive Transformers\n   Abstract: MeshAnything: Artist-Created Mesh Generation with Autoregressive Transformers: Recently, 3D assets created via reconstruction and generation have matched the quality of manually crafted assets, highlighting their potential for replacement. However, this potential is largely unrealized because these assets always need to be converted to meshes for 3D industry applications, and the meshes produced by current mesh extraction methods are significantly inferior to Artist-Created Meshes (AMs), i.e., meshes created by human artists. Specifically, current mesh extraction methods rely on dense faces and ignore geometric features, leading to inefficiencies, complicated post-processing, and lower representation quality. To address these issues, we introduce MeshAnything, a model that treats mesh extraction as a generation problem, producing AMs aligned with specified shapes. By converting 3D assets in any 3D representation into AMs, MeshAnything can be integrated with various 3D asset production methods, thereby enhancing their application across the 3D industry. The architecture of MeshAnything comprises a VQ-VAE and a shape-conditioned decoder-only transformer. We first learn a mesh vocabulary using the VQ-VAE, then train the shape-conditioned decoder-only transformer on this vocabulary for shape-conditioned autoregressive mesh generation. Our extensive experiments show that our method generates AMs with hundreds of times fewer faces, significantly improving storage, rendering, and simulation efficiencies, while achieving precision comparable to previous methods.\n\n8. Title: Lrm: Large reconstruction model for single image to 3d\n   Abstract: LRM: Large Reconstruction Model for Single Image to 3D: We propose the first Large Reconstruction Model (LRM) that predicts the 3D model of an object from a single input image within just 5 seconds. In contrast to many previous methods that are trained on small-scale datasets such as ShapeNet in a category-specific fashion, LRM adopts a highly scalable transformer-based architecture with 500 million learnable parameters to directly predict a neural radiance field (NeRF) from the input image. We train our model in an end-to-end manner on massive multi-view data containing around 1 million objects, including both synthetic renderings from Objaverse and real captures from MVImgNet. This combination of a high-capacity model and large-scale training data empowers our model to be highly generalizable and produce high-quality 3D reconstructions from various testing inputs, including real-world in-the-wild captures and images created by generative models. Video demos and interactable 3D meshes can be found on our LRM project webpage: https://yiconghong.me/LRM.\n\nThink step-by-step about which candidate best matches the context. Summarize your reasoning, then provide the exact title that should fill the citation. Respond in JSON with fields:\n{\n  \"reasoning\": \"<your step-by-step analysis>\",\n  \"selected_title\": \"<exact candidate title>\"\n}", "response": {"reasoning": "The citation context discusses the use of diffusion models for generating 3D assets and mentions a state-of-the-art (SOTA) method that involves generating high-quality 3D shapes with a Diffusion Transformer (DiT) and producing detailed textures with material diffusion models. Among the candidate papers, the one that directly addresses the generation of 3D polygonal meshes using diffusion models is 'PolyDiff: Generating 3D Polygonal Meshes with Diffusion Models'. This paper specifically highlights the use of a diffusion-based approach to generate realistic 3D meshes, which aligns closely with the context's focus on diffusion models and 3D asset generation. Other candidates, while relevant to 3D generation, do not emphasize the diffusion model aspect as strongly as PolyDiff does. Therefore, PolyDiff is the best match for the citation context.", "selected_title": "PolyDiff: Generating 3D Polygonal Meshes with Diffusion Models"}, "selected_title": "PolyDiff: Generating 3D Polygonal Meshes with Diffusion Models", "positive_title": "CLAY: A Controllable Large-scale Generative Model for Creating High-quality 3D Assets", "is_correct": false, "reasoning": "The citation context discusses the use of diffusion models for generating 3D assets and mentions a state-of-the-art (SOTA) method that involves generating high-quality 3D shapes with a Diffusion Transformer (DiT) and producing detailed textures with material diffusion models. Among the candidate papers, the one that directly addresses the generation of 3D polygonal meshes using diffusion models is 'PolyDiff: Generating 3D Polygonal Meshes with Diffusion Models'. This paper specifically highlights the use of a diffusion-based approach to generate realistic 3D meshes, which aligns closely with the context's focus on diffusion models and 3D asset generation. Other candidates, while relevant to 3D generation, do not emphasize the diffusion model aspect as strongly as PolyDiff does. Therefore, PolyDiff is the best match for the citation context."}
{"example_id": "blind_16646", "prompt": "You are an expert citation retrieval system as described in the paper \"Multi-Agent System for Reliable Citation Retrieval\".\nYour goal is to autonomously retrieve, verify, and recommend academic references given a query or document excerpt.\n\nTask:\nGiven a citation context from a scientific paper (where a citation is missing), identify the correct paper from a list of candidates.\nAnalyze the context to understand the specific claim, method, or result being cited.\nThen, evaluate each candidate paper to see if it matches the context.\nFinally, select the best matching paper.\n\nCitation Context:\nke a novel yet different path to achieve editing by modulating the initial noise. There are also some novel methods who offer customized text control by inverting images into textual tokens[CITATION]. \\subsection{High-Fidelity Inversion of GANs} Unlike the natural inversion capability exists in diffusion models, GANs need to do inversion by encoder, optimization\n\nCandidate Papers:\n1. Title: Paint by example: Exemplar-based image editing with diffusion models\n   Abstract: Paint by Example: Exemplar-based Image Editing with Diffusion Models: Language-guided image editing has achieved great success recently. In this paper, for the first time, we investigate exemplar-guided image editing for more precise control. We achieve this goal by leveraging self-supervised training to disentangle and re-organize the source image and the exemplar. However, the naive approach will cause obvious fusing artifacts. We carefully analyze it and propose an information bottleneck and strong augmentations to avoid the trivial solution of directly copying and pasting the exemplar image. Meanwhile, to ensure the controllability of the editing process, we design an arbitrary shape mask for the exemplar image and leverage the classifier-free guidance to increase the similarity to the exemplar image. The whole framework involves a single forward of the diffusion model without any iterative optimization. We demonstrate that our method achieves an impressive performance and enables controllable editing on in-the-wild images with high fidelity.\n\n2. Title: Null-text inversion for editing real images using guided diffusion models\n   Abstract: Null-text Inversion for Editing Real Images using Guided Diffusion Models: Recent text-guided diffusion models provide powerful image generation capabilities. Currently, a massive effort is given to enable the modification of these images using text only as means to offer intuitive and versatile editing. To edit a real image using these state-of-the-art tools, one must first invert the image with a meaningful text prompt into the pretrained model's domain. In this paper, we introduce an accurate inversion technique and thus facilitate an intuitive text-based modification of the image. Our proposed inversion consists of two novel key components: (i) Pivotal inversion for diffusion models. While current methods aim at mapping random noise samples to a single input image, we use a single pivotal noise vector for each timestamp and optimize around it. We demonstrate that a direct inversion is inadequate on its own, but does provide a good anchor for our optimization. (ii) NULL-text optimization, where we only modify the unconditional textual embedding that is used for classifier-free guidance, rather than the input text embedding. This allows for keeping both the model weights and the conditional embedding intact and hence enables applying prompt-based editing while avoiding the cumbersome tuning of the model's weights. Our Null-text inversion, based on the publicly available Stable Diffusion model, is extensively evaluated on a variety of images and prompt editing, showing high-fidelity editing of real images.\n\n3. Title: Diffusion models already have a semantic latent space\n   Abstract: Diffusion Models already have a Semantic Latent Space: Diffusion models achieve outstanding generative performance in various domains. Despite their great success, they lack semantic latent space which is essential for controlling the generative process. To address the problem, we propose asymmetric reverse process (Asyrp) which discovers the semantic latent space in frozen pretrained diffusion models. Our semantic latent space, named h-space, has nice properties for accommodating semantic image manipulation: homogeneity, linearity, robustness, and consistency across timesteps. In addition, we introduce a principled design of the generative process for versatile editing and quality boost ing by quantifiable measures: editing strength of an interval and quality deficiency at a timestep. Our method is applicable to various architectures (DDPM++, iD- DPM, and ADM) and datasets (CelebA-HQ, AFHQ-dog, LSUN-church, LSUN- bedroom, and METFACES). Project page: https://kwonminki.github.io/Asyrp/\n\n4. Title: Repaint: Inpainting using denoising diffusion probabilistic models\n   Abstract: RePaint: Inpainting using Denoising Diffusion Probabilistic Models: Free-form inpainting is the task of adding new content to an image in the regions specified by an arbitrary binary mask. Most existing approaches train for a certain distribution of masks, which limits their generalization capabilities to unseen mask types. Furthermore, training with pixel-wise and perceptual losses often leads to simple textural extensions towards the missing areas instead of semantically meaningful generation. In this work, we propose RePaint: A Denoising Diffusion Probabilistic Model (DDPM) based inpainting approach that is applicable to even extreme masks. We employ a pretrained unconditional DDPM as the generative prior. To condition the generation process, we only alter the reverse diffusion iterations by sampling the unmasked regions using the given image information. Since this technique does not modify or condition the original DDPM network itself, the model produces high-quality and diverse output images for any inpainting form. We validate our method for both faces and general-purpose image inpainting using standard and extreme masks.   RePaint outperforms state-of-the-art Autoregressive, and GAN approaches for at least five out of six mask distributions.   Github Repository: git.io/RePaint\n\n5. Title: Glide: Towards photorealistic image generation and editing with text-guided diffusion models\n   Abstract: GLIDE: Towards Photorealistic Image Generation and Editing with Text-Guided Diffusion Models: Diffusion models have recently been shown to generate high-quality synthetic images, especially when paired with a guidance technique to trade off diversity for fidelity. We explore diffusion models for the problem of text-conditional image synthesis and compare two different guidance strategies: CLIP guidance and classifier-free guidance. We find that the latter is preferred by human evaluators for both photorealism and caption similarity, and often produces photorealistic samples. Samples from a 3.5 billion parameter text-conditional diffusion model using classifier-free guidance are favored by human evaluators to those from DALL-E, even when the latter uses expensive CLIP reranking. Additionally, we find that our models can be fine-tuned to perform image inpainting, enabling powerful text-driven image editing. We train a smaller model on a filtered dataset and release the code and weights at https://github.com/openai/glide-text2im.\n\n6. Title: Imagic: Text-based real image editing with diffusion models\n   Abstract: Imagic: Text-Based Real Image Editing with Diffusion Models: Text-conditioned image editing has recently attracted considerable interest. However, most methods are currently either limited to specific editing types (e.g., object overlay, style transfer), or apply to synthetically generated images, or require multiple input images of a common object. In this paper we demonstrate, for the very first time, the ability to apply complex (e.g., non-rigid) text-guided semantic edits to a single real image. For example, we can change the posture and composition of one or multiple objects inside an image, while preserving its original characteristics. Our method can make a standing dog sit down or jump, cause a bird to spread its wings, etc. -- each within its single high-resolution natural image provided by the user. Contrary to previous work, our proposed method requires only a single input image and a target text (the desired edit). It operates on real images, and does not require any additional inputs (such as image masks or additional views of the object). Our method, which we call \"Imagic\", leverages a pre-trained text-to-image diffusion model for this task. It produces a text embedding that aligns with both the input image and the target text, while fine-tuning the diffusion model to capture the image-specific appearance. We demonstrate the quality and versatility of our method on numerous inputs from various domains, showcasing a plethora of high quality complex semantic image edits, all within a single unified framework.\n\n7. Title: Prompt-to-prompt image editing with cross attention control\n   Abstract: Prompt-to-Prompt Image Editing with Cross Attention Control: Recent large-scale text-driven synthesis models have attracted much attention thanks to their remarkable capabilities of generating highly diverse images that follow given text prompts. Such text-based synthesis methods are particularly appealing to humans who are used to verbally describe their intent. Therefore, it is only natural to extend the text-driven image synthesis to text-driven image editing. Editing is challenging for these generative models, since an innate property of an editing technique is to preserve most of the original image, while in the text-based models, even a small modification of the text prompt often leads to a completely different outcome. State-of-the-art methods mitigate this by requiring the users to provide a spatial mask to localize the edit, hence, ignoring the original structure and content within the masked region. In this paper, we pursue an intuitive prompt-to-prompt editing framework, where the edits are controlled by text only. To this end, we analyze a text-conditioned model in depth and observe that the cross-attention layers are the key to controlling the relation between the spatial layout of the image to each word in the prompt. With this observation, we present several applications which monitor the image synthesis by editing the textual prompt only. This includes localized editing by replacing a word, global editing by adding a specification, and even delicately controlling the extent to which a word is reflected in the image. We present our results over diverse images and prompts, demonstrating high-quality synthesis and fidelity to the edited prompts.\n\n8. Title: Denoising diffusion probabilistic models\n   Abstract: Denoising Diffusion Probabilistic Models: We present high quality image synthesis results using diffusion probabilistic models, a class of latent variable models inspired by considerations from nonequilibrium thermodynamics. Our best results are obtained by training on a weighted variational bound designed according to a novel connection between diffusion probabilistic models and denoising score matching with Langevin dynamics, and our models naturally admit a progressive lossy decompression scheme that can be interpreted as a generalization of autoregressive decoding. On the unconditional CIFAR10 dataset, we obtain an Inception score of 9.46 and a state-of-the-art FID score of 3.17. On 256x256 LSUN, we obtain sample quality similar to ProgressiveGAN. Our implementation is available at https://github.com/hojonathanho/diffusion\n\n9. Title: An image is worth one word: Personalizing text-to-image generation using textual inversion\n   Abstract: An Image is Worth One Word: Personalizing Text-to-Image Generation using Textual Inversion: Text-to-image models offer unprecedented freedom to guide creation through natural language. Yet, it is unclear how such freedom can be exercised to generate images of specific unique concepts, modify their appearance, or compose them in new roles and novel scenes. In other words, we ask: how can we use language-guided models to turn our cat into a painting, or imagine a new product based on our favorite toy? Here we present a simple approach that allows such creative freedom. Using only 3-5 images of a user-provided concept, like an object or a style, we learn to represent it through new \"words\" in the embedding space of a frozen text-to-image model. These \"words\" can be composed into natural language sentences, guiding personalized creation in an intuitive way. Notably, we find evidence that a single word embedding is sufficient for capturing unique and varied concepts. We compare our approach to a wide range of baselines, and demonstrate that it can more faithfully portray the concepts across a range of applications and tasks.   Our code, data and new words will be available at: https://textual-inversion.github.io\n\nThink step-by-step about which candidate best matches the context. Summarize your reasoning, then provide the exact title that should fill the citation. Respond in JSON with fields:\n{\n  \"reasoning\": \"<your step-by-step analysis>\",\n  \"selected_title\": \"<exact candidate title>\"\n}", "response": {"reasoning": "The citation context discusses novel methods for text control in image editing, specifically mentioning the inversion of images into textual tokens. Among the candidate papers, the second one, 'Null-text inversion for editing real images using guided diffusion models', directly addresses the concept of inversion in the context of image editing using guided diffusion models. It describes a pivotal inversion technique that allows for intuitive text-based modifications, which aligns closely with the claim made in the citation context about modulating initial noise and customizing text control. The other candidates either focus on different aspects of image editing or do not emphasize the inversion process as a key component. Therefore, the second paper is the most relevant and appropriate choice for the citation.", "selected_title": "Null-text inversion for editing real images using guided diffusion models"}, "selected_title": "Null-text inversion for editing real images using guided diffusion models", "positive_title": "An image is worth one word: Personalizing text-to-image generation using textual inversion", "is_correct": false, "reasoning": "The citation context discusses novel methods for text control in image editing, specifically mentioning the inversion of images into textual tokens. Among the candidate papers, the second one, 'Null-text inversion for editing real images using guided diffusion models', directly addresses the concept of inversion in the context of image editing using guided diffusion models. It describes a pivotal inversion technique that allows for intuitive text-based modifications, which aligns closely with the claim made in the citation context about modulating initial noise and customizing text control. The other candidates either focus on different aspects of image editing or do not emphasize the inversion process as a key component. Therefore, the second paper is the most relevant and appropriate choice for the citation."}
{"example_id": "blind_13939", "prompt": "You are an expert citation retrieval system as described in the paper \"Multi-Agent System for Reliable Citation Retrieval\".\nYour goal is to autonomously retrieve, verify, and recommend academic references given a query or document excerpt.\n\nTask:\nGiven a citation context from a scientific paper (where a citation is missing), identify the correct paper from a list of candidates.\nAnalyze the context to understand the specific claim, method, or result being cited.\nThen, evaluate each candidate paper to see if it matches the context.\nFinally, select the best matching paper.\n\nCitation Context:\nof work (SentenceBERT, BERT-flow, SimCSE) explored intermediate training to improve STS tasks. All of them applied pre-fine tuning on NLI datasets. More recently, EFL[CITATION] proposed a task transformation paradigm, improving single sentence tasks with less labels using rich sentence-pair datasets. \\textbf{Adapter Tuning:} Adapter tuning has emerged as a novel parameter\n\nCandidate Papers:\n1. Title: P-Tuning v2: Prompt Tuning Can Be Comparable to Fine-tuning Universally Across Scales and Tasks\n   Abstract: P-Tuning v2: Prompt Tuning Can Be Comparable to Fine-tuning Universally Across Scales and Tasks: Prompt tuning, which only tunes continuous prompts with a frozen language model, substantially reduces per-task storage and memory usage at training. However, in the context of NLU, prior work reveals that prompt tuning does not perform well for normal-sized pretrained models. We also find that existing methods of prompt tuning cannot handle hard sequence labeling tasks, indicating a lack of universality. We present a novel empirical finding that properly optimized prompt tuning can be universally effective across a wide range of model scales and NLU tasks. It matches the performance of finetuning while having only 0.1%-3% tuned parameters. Our method P-Tuning v2 is an implementation of Deep Prompt Tuning \\cite{li2021prefix,qin2021learning} optimized and adapted for NLU. Given the universality and simplicity of P-Tuning v2, we believe it can serve as an alternative to finetuning and a strong baseline for future research.Our code and data are released at https://github.com/THUDM/P-tuning-v2.\n\n2. Title: Entailment as Few-Shot Learner\n   Abstract: Entailment as Few-Shot Learner: Large pre-trained language models (LMs) have demonstrated remarkable ability as few-shot learners. However, their success hinges largely on scaling model parameters to a degree that makes it challenging to train and serve. In this paper, we propose a new approach, named as EFL, that can turn small LMs into better few-shot learners. The key idea of this approach is to reformulate potential NLP task into an entailment one, and then fine-tune the model with as little as 8 examples. We further demonstrate our proposed method can be: (i) naturally combined with an unsupervised contrastive learning-based data augmentation method; (ii) easily extended to multilingual few-shot learning. A systematic evaluation on 18 standard NLP tasks demonstrates that this approach improves the various existing SOTA few-shot learning methods by 12\\%, and yields competitive few-shot performance with 500 times larger models, such as GPT-3.\n\n3. Title: Language models are few-shot learners\n   Abstract: Language Models are Few-Shot Learners: Recent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training on a large corpus of text followed by fine-tuning on a specific task. While typically task-agnostic in architecture, this method still requires task-specific fine-tuning datasets of thousands or tens of thousands of examples. By contrast, humans can generally perform a new language task from only a few examples or from simple instructions - something which current NLP systems still largely struggle to do. Here we show that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art fine-tuning approaches. Specifically, we train GPT-3, an autoregressive language model with 175 billion parameters, 10x more than any previous non-sparse language model, and test its performance in the few-shot setting. For all tasks, GPT-3 is applied without any gradient updates or fine-tuning, with tasks and few-shot demonstrations specified purely via text interaction with the model. GPT-3 achieves strong performance on many NLP datasets, including translation, question-answering, and cloze tasks, as well as several tasks that require on-the-fly reasoning or domain adaptation, such as unscrambling words, using a novel word in a sentence, or performing 3-digit arithmetic. At the same time, we also identify some datasets where GPT-3's few-shot learning still struggles, as well as some datasets where GPT-3 faces methodological issues related to training on large web corpora. Finally, we find that GPT-3 can generate samples of news articles which human evaluators have difficulty distinguishing from articles written by humans. We discuss broader societal impacts of this finding and of GPT-3 in general.\n\n4. Title: Exploiting cloze questions for few shot text classification and natural language inference\n   Abstract: Exploiting Cloze Questions for Few Shot Text Classification and Natural Language Inference: Some NLP tasks can be solved in a fully unsupervised fashion by providing a pretrained language model with \"task descriptions\" in natural language (e.g., Radford et al., 2019). While this approach underperforms its supervised counterpart, we show in this work that the two ideas can be combined: We introduce Pattern-Exploiting Training (PET), a semi-supervised training procedure that reformulates input examples as cloze-style phrases to help language models understand a given task. These phrases are then used to assign soft labels to a large set of unlabeled examples. Finally, standard supervised training is performed on the resulting training set. For several tasks and languages, PET outperforms supervised training and strong semi-supervised approaches in low-resource settings by a large margin.\n\n5. Title: BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\n   Abstract: BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding: We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models, BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications.   BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5% (7.7% point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).\n\n6. Title: Mad-x: An adapter-based framework for multi-task cross-lingual transfer\n   Abstract: MAD-X: An Adapter-Based Framework for Multi-Task Cross-Lingual Transfer: The main goal behind state-of-the-art pre-trained multilingual models such as multilingual BERT and XLM-R is enabling and bootstrapping NLP applications in low-resource languages through zero-shot or few-shot cross-lingual transfer. However, due to limited model capacity, their transfer performance is the weakest exactly on such low-resource languages and languages unseen during pre-training. We propose MAD-X, an adapter-based framework that enables high portability and parameter-efficient transfer to arbitrary tasks and languages by learning modular language and task representations. In addition, we introduce a novel invertible adapter architecture and a strong baseline method for adapting a pre-trained multilingual model to a new language. MAD-X outperforms the state of the art in cross-lingual transfer across a representative set of typologically diverse languages on named entity recognition and causal commonsense reasoning, and achieves competitive results on question answering. Our code and adapters are available at AdapterHub.ml\n\n7. Title: Sentence-bert: Sentence embeddings using siamese bert-networks\n   Abstract: Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks: BERT (Devlin et al., 2018) and RoBERTa (Liu et al., 2019) has set a new state-of-the-art performance on sentence-pair regression tasks like semantic textual similarity (STS). However, it requires that both sentences are fed into the network, which causes a massive computational overhead: Finding the most similar pair in a collection of 10,000 sentences requires about 50 million inference computations (~65 hours) with BERT. The construction of BERT makes it unsuitable for semantic similarity search as well as for unsupervised tasks like clustering.   In this publication, we present Sentence-BERT (SBERT), a modification of the pretrained BERT network that use siamese and triplet network structures to derive semantically meaningful sentence embeddings that can be compared using cosine-similarity. This reduces the effort for finding the most similar pair from 65 hours with BERT / RoBERTa to about 5 seconds with SBERT, while maintaining the accuracy from BERT.   We evaluate SBERT and SRoBERTa on common STS tasks and transfer learning tasks, where it outperforms other state-of-the-art sentence embeddings methods.\n\n8. Title: The power of scale for parameter-efficient prompt tuning\n   Abstract: The Power of Scale for Parameter-Efficient Prompt Tuning: In this work, we explore \"prompt tuning\", a simple yet effective mechanism for learning \"soft prompts\" to condition frozen language models to perform specific downstream tasks. Unlike the discrete text prompts used by GPT-3, soft prompts are learned through backpropagation and can be tuned to incorporate signal from any number of labeled examples. Our end-to-end learned approach outperforms GPT-3's \"few-shot\" learning by a large margin. More remarkably, through ablations on model size using T5, we show that prompt tuning becomes more competitive with scale: as models exceed billions of parameters, our method \"closes the gap\" and matches the strong performance of model tuning (where all model weights are tuned). This finding is especially relevant in that large models are costly to share and serve, and the ability to reuse one frozen model for multiple downstream tasks can ease this burden. Our method can be seen as a simplification of the recently proposed \"prefix tuning\" of Li and Liang (2021), and we provide a comparison to this and other similar approaches. Finally, we show that conditioning a frozen model with soft prompts confers benefits in robustness to domain transfer, as compared to full model tuning.\n\nThink step-by-step about which candidate best matches the context. Summarize your reasoning, then provide the exact title that should fill the citation. Respond in JSON with fields:\n{\n  \"reasoning\": \"<your step-by-step analysis>\",\n  \"selected_title\": \"<exact candidate title>\"\n}", "response": {"reasoning": "The citation context discusses recent advancements in sentence transformation paradigms, specifically mentioning a method called EFL that improves single sentence tasks with fewer labels using rich sentence-pair datasets. Among the candidate papers, the second one titled 'Entailment as Few-Shot Learner' directly introduces a method called EFL, which aligns perfectly with the context's mention of EFL proposing a task transformation paradigm. The other candidates either focus on different aspects of NLP or do not mention EFL or similar task transformation paradigms. Therefore, the best match for the citation context is the second candidate.", "selected_title": "Entailment as Few-Shot Learner"}, "selected_title": "Entailment as Few-Shot Learner", "positive_title": "Entailment as Few-Shot Learner", "is_correct": true, "reasoning": "The citation context discusses recent advancements in sentence transformation paradigms, specifically mentioning a method called EFL that improves single sentence tasks with fewer labels using rich sentence-pair datasets. Among the candidate papers, the second one titled 'Entailment as Few-Shot Learner' directly introduces a method called EFL, which aligns perfectly with the context's mention of EFL proposing a task transformation paradigm. The other candidates either focus on different aspects of NLP or do not mention EFL or similar task transformation paradigms. Therefore, the best match for the citation context is the second candidate."}
{"example_id": "blind_4010", "prompt": "You are an expert citation retrieval system as described in the paper \"Multi-Agent System for Reliable Citation Retrieval\".\nYour goal is to autonomously retrieve, verify, and recommend academic references given a query or document excerpt.\n\nTask:\nGiven a citation context from a scientific paper (where a citation is missing), identify the correct paper from a list of candidates.\nAnalyze the context to understand the specific claim, method, or result being cited.\nThen, evaluate each candidate paper to see if it matches the context.\nFinally, select the best matching paper.\n\nCitation Context:\nimal bit widths. In this paper, we leverage transformer explainability to estimate the ViT layer sensitivity for allocating mixed precision for quantization. Furthermore, we adopt and improve the RepQ[CITATION] framework for efficient mixed precision PTQ. \\subsection{Explainability for Transformers} The majority of prior research on explainability has concentrated on CNNs, with a main focus on gradient me\n\nCandidate Papers:\n1. Title: Analyzing Multi-Head Self-Attention: Specialized Heads Do the Heavy Lifting, the Rest Can Be Pruned\n   Abstract: Analyzing Multi-Head Self-Attention: Specialized Heads Do the Heavy Lifting, the Rest Can Be Pruned: Multi-head self-attention is a key component of the Transformer, a state-of-the-art architecture for neural machine translation. In this work we evaluate the contribution made by individual attention heads in the encoder to the overall performance of the model and analyze the roles played by them. We find that the most important and confident heads play consistent and often linguistically-interpretable roles. When pruning heads using a method based on stochastic gates and a differentiable relaxation of the L0 penalty, we observe that specialized heads are last to be pruned. Our novel pruning method removes the vast majority of heads without seriously affecting performance. For example, on the English-Russian WMT dataset, pruning 38 out of 48 encoder heads results in a drop of only 0.15 BLEU.\n\n2. Title: Hawq-v2: Hessian aware trace-weighted quantization of neural networks\n   Abstract: HAWQ-V2: Hessian Aware trace-Weighted Quantization of Neural Networks: Quantization is an effective method for reducing memory footprint and inference time of Neural Networks, e.g., for efficient inference in the cloud, especially at the edge. However, ultra low precision quantization could lead to significant degradation in model generalization. A promising method to address this is to perform mixed-precision quantization, where more sensitive layers are kept at higher precision. However, the search space for a mixed-precision quantization is exponential in the number of layers. Recent work has proposed HAWQ, a novel Hessian based framework, with the aim of reducing this exponential search space by using second-order information. While promising, this prior work has three major limitations: (i) HAWQV1 only uses the top Hessian eigenvalue as a measure of sensitivity and do not consider the rest of the Hessian spectrum; (ii) HAWQV1 approach only provides relative sensitivity of different layers and therefore requires a manual selection of the mixed-precision setting; and (iii) HAWQV1 does not consider mixed-precision activation quantization. Here, we present HAWQV2 which addresses these shortcomings. For (i), we perform a theoretical analysis showing that a better sensitivity metric is to compute the average of all of the Hessian eigenvalues. For (ii), we develop a Pareto frontier based method for selecting the exact bit precision of different layers without any manual selection. For (iii), we extend the Hessian analysis to mixed-precision activation quantization. We have found this to be very beneficial for object detection. We show that HAWQV2 achieves new state-of-the-art results for a wide range of tasks.\n\n3. Title: Haq: Hardware-aware automated quantization with mixed precision\n   Abstract: HAQ: Hardware-Aware Automated Quantization with Mixed Precision: Model quantization is a widely used technique to compress and accelerate deep neural network (DNN) inference. Emergent DNN hardware accelerators begin to support mixed precision (1-8 bits) to further improve the computation efficiency, which raises a great challenge to find the optimal bitwidth for each layer: it requires domain experts to explore the vast design space trading off among accuracy, latency, energy, and model size, which is both time-consuming and sub-optimal. Conventional quantization algorithm ignores the different hardware architectures and quantizes all the layers in a uniform way. In this paper, we introduce the Hardware-Aware Automated Quantization (HAQ) framework which leverages the reinforcement learning to automatically determine the quantization policy, and we take the hardware accelerator's feedback in the design loop. Rather than relying on proxy signals such as FLOPs and model size, we employ a hardware simulator to generate direct feedback signals (latency and energy) to the RL agent. Compared with conventional methods, our framework is fully automated and can specialize the quantization policy for different neural network architectures and hardware architectures. Our framework effectively reduced the latency by 1.4-1.95x and the energy consumption by 1.9x with negligible loss of accuracy compared with the fixed bitwidth (8 bits) quantization. Our framework reveals that the optimal policies on different hardware architectures (i.e., edge and cloud architectures) under different resource constraints (i.e., latency, energy and model size) are drastically different. We interpreted the implication of different quantization policies, which offer insights for both neural network architecture design and hardware architecture design.\n\n4. Title: Patch-wise Mixed-Precision Quantization of Vision Transformer\n   Abstract: Patch-wise Mixed-Precision Quantization of Vision Transformer: As emerging hardware begins to support mixed bit-width arithmetic computation, mixed-precision quantization is widely used to reduce the complexity of neural networks. However, Vision Transformers (ViTs) require complex self-attention computation to guarantee the learning of powerful feature representations, which makes mixed-precision quantization of ViTs still challenging. In this paper, we propose a novel patch-wise mixed-precision quantization (PMQ) for efficient inference of ViTs. Specifically, we design a lightweight global metric, which is faster than existing methods, to measure the sensitivity of each component in ViTs to quantization errors. Moreover, we also introduce a pareto frontier approach to automatically allocate the optimal bit-precision according to the sensitivity. To further reduce the computational complexity of self-attention in inference stage, we propose a patch-wise module to reallocate bit-width of patches in each layer. Extensive experiments on the ImageNet dataset shows that our method greatly reduces the search cost and facilitates the application of mixed-precision quantization to ViTs.\n\n5. Title: Segmenter: Transformer for semantic segmentation\n   Abstract: Segmenter: Transformer for Semantic Segmentation: Image segmentation is often ambiguous at the level of individual image patches and requires contextual information to reach label consensus. In this paper we introduce Segmenter, a transformer model for semantic segmentation. In contrast to convolution-based methods, our approach allows to model global context already at the first layer and throughout the network. We build on the recent Vision Transformer (ViT) and extend it to semantic segmentation. To do so, we rely on the output embeddings corresponding to image patches and obtain class labels from these embeddings with a point-wise linear decoder or a mask transformer decoder. We leverage models pre-trained for image classification and show that we can fine-tune them on moderate sized datasets available for semantic segmentation. The linear decoder allows to obtain excellent results already, but the performance can be further improved by a mask transformer generating class masks. We conduct an extensive ablation study to show the impact of the different parameters, in particular the performance is better for large models and small patch sizes. Segmenter attains excellent results for semantic segmentation. It outperforms the state of the art on both ADE20K and Pascal Context datasets and is competitive on Cityscapes.\n\n6. Title: Repq-vit: Scale reparameterization for post-training quantization of vision transformers\n   Abstract: RepQ-ViT: Scale Reparameterization for Post-Training Quantization of Vision Transformers: Post-training quantization (PTQ), which only requires a tiny dataset for calibration without end-to-end retraining, is a light and practical model compression technique. Recently, several PTQ schemes for vision transformers (ViTs) have been presented; unfortunately, they typically suffer from non-trivial accuracy degradation, especially in low-bit cases. In this paper, we propose RepQ-ViT, a novel PTQ framework for ViTs based on quantization scale reparameterization, to address the above issues. RepQ-ViT decouples the quantization and inference processes, where the former employs complex quantizers and the latter employs scale-reparameterized simplified quantizers. This ensures both accurate quantization and efficient inference, which distinguishes it from existing approaches that sacrifice quantization performance to meet the target hardware. More specifically, we focus on two components with extreme distributions: post-LayerNorm activations with severe inter-channel variation and post-Softmax activations with power-law features, and initially apply channel-wise quantization and log$\\sqrt{2}$ quantization, respectively. Then, we reparameterize the scales to hardware-friendly layer-wise quantization and log2 quantization for inference, with only slight accuracy or computational costs. Extensive experiments are conducted on multiple vision tasks with different model variants, proving that RepQ-ViT, without hyperparameters and expensive reconstruction procedures, can outperform existing strong baselines and encouragingly improve the accuracy of 4-bit PTQ of ViTs to a usable level. Code is available at https://github.com/zkkli/RepQ-ViT.\n\n7. Title: Efficientformer: Vision transformers at mobilenet speed\n   Abstract: EfficientFormer: Vision Transformers at MobileNet Speed: Vision Transformers (ViT) have shown rapid progress in computer vision tasks, achieving promising results on various benchmarks. However, due to the massive number of parameters and model design, \\textit{e.g.}, attention mechanism, ViT-based models are generally times slower than lightweight convolutional networks. Therefore, the deployment of ViT for real-time applications is particularly challenging, especially on resource-constrained hardware such as mobile devices. Recent efforts try to reduce the computation complexity of ViT through network architecture search or hybrid design with MobileNet block, yet the inference speed is still unsatisfactory. This leads to an important question: can transformers run as fast as MobileNet while obtaining high performance? To answer this, we first revisit the network architecture and operators used in ViT-based models and identify inefficient designs. Then we introduce a dimension-consistent pure transformer (without MobileNet blocks) as a design paradigm. Finally, we perform latency-driven slimming to get a series of final models dubbed EfficientFormer. Extensive experiments show the superiority of EfficientFormer in performance and speed on mobile devices. Our fastest model, EfficientFormer-L1, achieves $79.2\\%$ top-1 accuracy on ImageNet-1K with only $1.6$ ms inference latency on iPhone 12 (compiled with CoreML), which runs as fast as MobileNetV2$\\times 1.4$ ($1.6$ ms, $74.7\\%$ top-1), and our largest model, EfficientFormer-L7, obtains $83.3\\%$ accuracy with only $7.0$ ms latency. Our work proves that properly designed transformers can reach extremely low latency on mobile devices while maintaining high performance.\n\n8. Title: Transformer interpretability beyond attention visualization\n   Abstract: Transformer Interpretability Beyond Attention Visualization: Self-attention techniques, and specifically Transformers, are dominating the field of text processing and are becoming increasingly popular in computer vision classification tasks. In order to visualize the parts of the image that led to a certain classification, existing methods either rely on the obtained attention maps or employ heuristic propagation along the attention graph. In this work, we propose a novel way to compute relevancy for Transformer networks. The method assigns local relevance based on the Deep Taylor Decomposition principle and then propagates these relevancy scores through the layers. This propagation involves attention layers and skip connections, which challenge existing methods. Our solution is based on a specific formulation that is shown to maintain the total relevancy across layers. We benchmark our method on very recent visual Transformer networks, as well as on a text classification problem, and demonstrate a clear advantage over the existing explainability methods.\n\n9. Title: Hawq: Hessian aware quantization of neural networks with mixed-precision\n   Abstract: HAWQ: Hessian AWare Quantization of Neural Networks with Mixed-Precision: Model size and inference speed/power have become a major challenge in the deployment of Neural Networks for many applications. A promising approach to address these problems is quantization. However, uniformly quantizing a model to ultra low precision leads to significant accuracy degradation. A novel solution for this is to use mixed-precision quantization, as some parts of the network may allow lower precision as compared to other layers. However, there is no systematic way to determine the precision of different layers. A brute force approach is not feasible for deep networks, as the search space for mixed-precision is exponential in the number of layers. Another challenge is a similar factorial complexity for determining block-wise fine-tuning order when quantizing the model to a target precision. Here, we introduce Hessian AWare Quantization (HAWQ), a novel second-order quantization method to address these problems. HAWQ allows for the automatic selection of the relative quantization precision of each layer, based on the layer's Hessian spectrum. Moreover, HAWQ provides a deterministic fine-tuning order for quantizing layers, based on second-order information. We show the results of our method on Cifar-10 using ResNet20, and on ImageNet using Inception-V3, ResNet50 and SqueezeNext models. Comparing HAWQ with state-of-the-art shows that we can achieve similar/better accuracy with $8\\times$ activation compression ratio on ResNet20, as compared to DNAS~\\cite{wu2018mixed}, and up to $1\\%$ higher accuracy with up to $14\\%$ smaller models on ResNet50 and Inception-V3, compared to recently proposed methods of RVQuant~\\cite{park2018value} and HAQ~\\cite{wang2018haq}. Furthermore, we show that we can quantize SqueezeNext to just 1MB model size while achieving above $68\\%$ top1 accuracy on ImageNet.\n\nThink step-by-step about which candidate best matches the context. Summarize your reasoning, then provide the exact title that should fill the citation. Respond in JSON with fields:\n{\n  \"reasoning\": \"<your step-by-step analysis>\",\n  \"selected_title\": \"<exact candidate title>\"\n}", "response": {"reasoning": "The citation context discusses leveraging transformer explainability to estimate the sensitivity of ViT layers for mixed precision quantization, specifically mentioning the improvement of the RepQ framework for efficient mixed precision post-training quantization (PTQ). Among the candidate papers, the one titled 'RepQ-ViT: Scale Reparameterization for Post-Training Quantization of Vision Transformers' directly addresses the RepQ framework and focuses on post-training quantization for Vision Transformers, which aligns perfectly with the context's emphasis on improving mixed precision PTQ. Other candidates, while related to quantization and transformers, do not specifically mention the RepQ framework or focus on the same aspects of mixed precision quantization for ViTs. Therefore, the best match is the paper that explicitly discusses RepQ and its application to Vision Transformers.", "selected_title": "Repq-vit: Scale reparameterization for post-training quantization of vision transformers"}, "selected_title": "Repq-vit: Scale reparameterization for post-training quantization of vision transformers", "positive_title": "Repq-vit: Scale reparameterization for post-training quantization of vision transformers", "is_correct": true, "reasoning": "The citation context discusses leveraging transformer explainability to estimate the sensitivity of ViT layers for mixed precision quantization, specifically mentioning the improvement of the RepQ framework for efficient mixed precision post-training quantization (PTQ). Among the candidate papers, the one titled 'RepQ-ViT: Scale Reparameterization for Post-Training Quantization of Vision Transformers' directly addresses the RepQ framework and focuses on post-training quantization for Vision Transformers, which aligns perfectly with the context's emphasis on improving mixed precision PTQ. Other candidates, while related to quantization and transformers, do not specifically mention the RepQ framework or focus on the same aspects of mixed precision quantization for ViTs. Therefore, the best match is the paper that explicitly discusses RepQ and its application to Vision Transformers."}
{"example_id": "blind_5482", "prompt": "You are an expert citation retrieval system as described in the paper \"Multi-Agent System for Reliable Citation Retrieval\".\nYour goal is to autonomously retrieve, verify, and recommend academic references given a query or document excerpt.\n\nTask:\nGiven a citation context from a scientific paper (where a citation is missing), identify the correct paper from a list of candidates.\nAnalyze the context to understand the specific claim, method, or result being cited.\nThen, evaluate each candidate paper to see if it matches the context.\nFinally, select the best matching paper.\n\nCitation Context:\noned on given human poses. Text2Human proposes to generate human images conditioned on texts and poses. We focus on human video generation. For the human content manipulation, pose transfer[CITATION] is a popular topic. Pose transfer deals with video data. The task is to transfer the poses from the reference videos to the source video. Chan \\etal take human poses as inputs and the desi\n\nCandidate Papers:\n1. Title: Phenaki: Variable length video generation from open domain textual description\n   Abstract: Phenaki: Variable Length Video Generation From Open Domain Textual Description: We present Phenaki, a model capable of realistic video synthesis, given a sequence of textual prompts. Generating videos from text is particularly challenging due to the computational cost, limited quantities of high quality text-video data and variable length of videos. To address these issues, we introduce a new model for learning video representation which compresses the video to a small representation of discrete tokens. This tokenizer uses causal attention in time, which allows it to work with variable-length videos. To generate video tokens from text we are using a bidirectional masked transformer conditioned on pre-computed text tokens. The generated video tokens are subsequently de-tokenized to create the actual video. To address data issues, we demonstrate how joint training on a large corpus of image-text pairs as well as a smaller number of video-text examples can result in generalization beyond what is available in the video datasets. Compared to the previous video generation methods, Phenaki can generate arbitrary long videos conditioned on a sequence of prompts (i.e. time variable text or a story) in open domain. To the best of our knowledge, this is the first time a paper studies generating videos from time variable prompts. In addition, compared to the per-frame baselines, the proposed video encoder-decoder computes fewer tokens per video but results in better spatio-temporal consistency.\n\n2. Title: Liquid warping gan: A unified framework for human motion imitation, appearance transfer and novel view synthesis\n   Abstract: Liquid Warping GAN: A Unified Framework for Human Motion Imitation, Appearance Transfer and Novel View Synthesis: We tackle the human motion imitation, appearance transfer, and novel view synthesis within a unified framework, which means that the model once being trained can be used to handle all these tasks. The existing task-specific methods mainly use 2D keypoints (pose) to estimate the human body structure. However, they only expresses the position information with no abilities to characterize the personalized shape of the individual person and model the limbs rotations. In this paper, we propose to use a 3D body mesh recovery module to disentangle the pose and shape, which can not only model the joint location and rotation but also characterize the personalized body shape. To preserve the source information, such as texture, style, color, and face identity, we propose a Liquid Warping GAN with Liquid Warping Block (LWB) that propagates the source information in both image and feature spaces, and synthesizes an image with respect to the reference. Specifically, the source features are extracted by a denoising convolutional auto-encoder for characterizing the source identity well. Furthermore, our proposed method is able to support a more flexible warping from multiple sources. In addition, we build a new dataset, namely Impersonator (iPER) dataset, for the evaluation of human motion imitation, appearance transfer, and novel view synthesis. Extensive experiments demonstrate the effectiveness of our method in several aspects, such as robustness in occlusion case and preserving face identity, shape consistency and clothes details. All codes and datasets are available on https://svip-lab.github.io/project/impersonator.html\n\n3. Title: Generative adversarial networks\n   Abstract: Generative Adversarial Networks: We propose a new framework for estimating generative models via an adversarial process, in which we simultaneously train two models: a generative model G that captures the data distribution, and a discriminative model D that estimates the probability that a sample came from the training data rather than G. The training procedure for G is to maximize the probability of D making a mistake. This framework corresponds to a minimax two-player game. In the space of arbitrary functions G and D, a unique solution exists, with G recovering the training data distribution and D equal to 1/2 everywhere. In the case where G and D are defined by multilayer perceptrons, the entire system can be trained with backpropagation. There is no need for any Markov chains or unrolled approximate inference networks during either training or generation of samples. Experiments demonstrate the potential of the framework through qualitative and quantitative evaluation of the generated samples.\n\n4. Title: Make-a-video: Text-to-video generation without text-video data\n   Abstract: Make-A-Video: Text-to-Video Generation without Text-Video Data: We propose Make-A-Video -- an approach for directly translating the tremendous recent progress in Text-to-Image (T2I) generation to Text-to-Video (T2V). Our intuition is simple: learn what the world looks like and how it is described from paired text-image data, and learn how the world moves from unsupervised video footage. Make-A-Video has three advantages: (1) it accelerates training of the T2V model (it does not need to learn visual and multimodal representations from scratch), (2) it does not require paired text-video data, and (3) the generated videos inherit the vastness (diversity in aesthetic, fantastical depictions, etc.) of today's image generation models. We design a simple yet effective way to build on T2I models with novel and effective spatial-temporal modules. First, we decompose the full temporal U-Net and attention tensors and approximate them in space and time. Second, we design a spatial temporal pipeline to generate high resolution and frame rate videos with a video decoder, interpolation model and two super resolution models that can enable various applications besides T2V. In all aspects, spatial and temporal resolution, faithfulness to text, and quality, Make-A-Video sets the new state-of-the-art in text-to-video generation, as determined by both qualitative and quantitative measures.\n\n5. Title: High-resolution image synthesis with latent diffusion models\n   Abstract: High-Resolution Image Synthesis with Latent Diffusion Models: By decomposing the image formation process into a sequential application of denoising autoencoders, diffusion models (DMs) achieve state-of-the-art synthesis results on image data and beyond. Additionally, their formulation allows for a guiding mechanism to control the image generation process without retraining. However, since these models typically operate directly in pixel space, optimization of powerful DMs often consumes hundreds of GPU days and inference is expensive due to sequential evaluations. To enable DM training on limited computational resources while retaining their quality and flexibility, we apply them in the latent space of powerful pretrained autoencoders. In contrast to previous work, training diffusion models on such a representation allows for the first time to reach a near-optimal point between complexity reduction and detail preservation, greatly boosting visual fidelity. By introducing cross-attention layers into the model architecture, we turn diffusion models into powerful and flexible generators for general conditioning inputs such as text or bounding boxes and high-resolution synthesis becomes possible in a convolutional manner. Our latent diffusion models (LDMs) achieve a new state of the art for image inpainting and highly competitive performance on various tasks, including unconditional image generation, semantic scene synthesis, and super-resolution, while significantly reducing computational requirements compared to pixel-based DMs. Code is available at https://github.com/CompVis/latent-diffusion .\n\n6. Title: Photorealistic Text-to-Image Diffusion Models with Deep Language Understanding\n   Abstract: Photorealistic Text-to-Image Diffusion Models with Deep Language Understanding: We present Imagen, a text-to-image diffusion model with an unprecedented degree of photorealism and a deep level of language understanding. Imagen builds on the power of large transformer language models in understanding text and hinges on the strength of diffusion models in high-fidelity image generation. Our key discovery is that generic large language models (e.g. T5), pretrained on text-only corpora, are surprisingly effective at encoding text for image synthesis: increasing the size of the language model in Imagen boosts both sample fidelity and image-text alignment much more than increasing the size of the image diffusion model. Imagen achieves a new state-of-the-art FID score of 7.27 on the COCO dataset, without ever training on COCO, and human raters find Imagen samples to be on par with the COCO data itself in image-text alignment. To assess text-to-image models in greater depth, we introduce DrawBench, a comprehensive and challenging benchmark for text-to-image models. With DrawBench, we compare Imagen with recent methods including VQ-GAN+CLIP, Latent Diffusion Models, and DALL-E 2, and find that human raters prefer Imagen over other models in side-by-side comparisons, both in terms of sample quality and image-text alignment. See https://imagen.research.google/ for an overview of the results.\n\n7. Title: Denoising diffusion probabilistic models\n   Abstract: Denoising Diffusion Probabilistic Models: We present high quality image synthesis results using diffusion probabilistic models, a class of latent variable models inspired by considerations from nonequilibrium thermodynamics. Our best results are obtained by training on a weighted variational bound designed according to a novel connection between diffusion probabilistic models and denoising score matching with Langevin dynamics, and our models naturally admit a progressive lossy decompression scheme that can be interpreted as a generalization of autoregressive decoding. On the unconditional CIFAR10 dataset, we obtain an Inception score of 9.46 and a state-of-the-art FID score of 3.17. On 256x256 LSUN, we obtain sample quality similar to ProgressiveGAN. Our implementation is available at https://github.com/hojonathanho/diffusion\n\n8. Title: Bailando: 3D Dance Generation by Actor-Critic GPT with Choreographic Memory\n   Abstract: Bailando: 3D Dance Generation by Actor-Critic GPT with Choreographic Memory: Driving 3D characters to dance following a piece of music is highly challenging due to the spatial constraints applied to poses by choreography norms. In addition, the generated dance sequence also needs to maintain temporal coherency with different music genres. To tackle these challenges, we propose a novel music-to-dance framework, Bailando, with two powerful components: 1) a choreographic memory that learns to summarize meaningful dancing units from 3D pose sequence to a quantized codebook, 2) an actor-critic Generative Pre-trained Transformer (GPT) that composes these units to a fluent dance coherent to the music. With the learned choreographic memory, dance generation is realized on the quantized units that meet high choreography standards, such that the generated dancing sequences are confined within the spatial constraints. To achieve synchronized alignment between diverse motion tempos and music beats, we introduce an actor-critic-based reinforcement learning scheme to the GPT with a newly-designed beat-align reward function. Extensive experiments on the standard benchmark demonstrate that our proposed framework achieves state-of-the-art performance both qualitatively and quantitatively. Notably, the learned choreographic memory is shown to discover human-interpretable dancing-style poses in an unsupervised manner.\n\n9. Title: Zero-shot text-to-image generation\n   Abstract: Zero-Shot Text-to-Image Generation: Text-to-image generation has traditionally focused on finding better modeling assumptions for training on a fixed dataset. These assumptions might involve complex architectures, auxiliary losses, or side information such as object part labels or segmentation masks supplied during training. We describe a simple approach for this task based on a transformer that autoregressively models the text and image tokens as a single stream of data. With sufficient data and scale, our approach is competitive with previous domain-specific models when evaluated in a zero-shot fashion.\n\n10. Title: Neural discrete representation learning\n   Abstract: Neural Discrete Representation Learning: Learning useful representations without supervision remains a key challenge in machine learning. In this paper, we propose a simple yet powerful generative model that learns such discrete representations. Our model, the Vector Quantised-Variational AutoEncoder (VQ-VAE), differs from VAEs in two key ways: the encoder network outputs discrete, rather than continuous, codes; and the prior is learnt rather than static. In order to learn a discrete latent representation, we incorporate ideas from vector quantisation (VQ). Using the VQ method allows the model to circumvent issues of \"posterior collapse\" -- where the latents are ignored when they are paired with a powerful autoregressive decoder -- typically observed in the VAE framework. Pairing these representations with an autoregressive prior, the model can generate high quality images, videos, and speech as well as doing high quality speaker conversion and unsupervised learning of phonemes, providing further evidence of the utility of the learnt representations.\n\nThink step-by-step about which candidate best matches the context. Summarize your reasoning, then provide the exact title that should fill the citation. Respond in JSON with fields:\n{\n  \"reasoning\": \"<your step-by-step analysis>\",\n  \"selected_title\": \"<exact candidate title>\"\n}", "response": {"reasoning": "The citation context discusses human video generation and specifically mentions 'pose transfer' as a popular topic that deals with transferring poses from reference videos to source videos. Among the candidate papers, the one that most closely aligns with this context is 'Liquid warping gan: A unified framework for human motion imitation, appearance transfer and novel view synthesis'. This paper explicitly addresses human motion imitation and appearance transfer, which are directly related to pose transfer in video data. The other candidates focus on text-to-video generation or image synthesis, which do not directly address the specific claim about pose transfer in the context of human video generation. Therefore, the best matching paper is the one that discusses a unified framework for handling human motion and appearance, which includes pose transfer.", "selected_title": "Liquid warping gan: A unified framework for human motion imitation, appearance transfer and novel view synthesis"}, "selected_title": "Liquid warping gan: A unified framework for human motion imitation, appearance transfer and novel view synthesis", "positive_title": "Liquid warping gan: A unified framework for human motion imitation, appearance transfer and novel view synthesis", "is_correct": true, "reasoning": "The citation context discusses human video generation and specifically mentions 'pose transfer' as a popular topic that deals with transferring poses from reference videos to source videos. Among the candidate papers, the one that most closely aligns with this context is 'Liquid warping gan: A unified framework for human motion imitation, appearance transfer and novel view synthesis'. This paper explicitly addresses human motion imitation and appearance transfer, which are directly related to pose transfer in video data. The other candidates focus on text-to-video generation or image synthesis, which do not directly address the specific claim about pose transfer in the context of human video generation. Therefore, the best matching paper is the one that discusses a unified framework for handling human motion and appearance, which includes pose transfer."}
{"example_id": "blind_8838", "prompt": "You are an expert citation retrieval system as described in the paper \"Multi-Agent System for Reliable Citation Retrieval\".\nYour goal is to autonomously retrieve, verify, and recommend academic references given a query or document excerpt.\n\nTask:\nGiven a citation context from a scientific paper (where a citation is missing), identify the correct paper from a list of candidates.\nAnalyze the context to understand the specific claim, method, or result being cited.\nThen, evaluate each candidate paper to see if it matches the context.\nFinally, select the best matching paper.\n\nCitation Context:\nas the dominant architecture in numerous natural language processing (NLP) tasks. The transformative impact of the transformer has been further propelled by the emergence of models like GPT[CITATION], catapulting the popularity of this architecture to new heights. Meanwhile, the versatility of transformers extends beyond NLP, encompassing diverse domains such as vision, audio,\n\nCandidate Papers:\n1. Title: Post-training quantization for vision transformer\n   Abstract: Post-Training Quantization for Vision Transformer: Recently, transformer has achieved remarkable performance on a variety of computer vision applications. Compared with mainstream convolutional neural networks, vision transformers are often of sophisticated architectures for extracting powerful feature representations, which are more difficult to be developed on mobile devices. In this paper, we present an effective post-training quantization algorithm for reducing the memory storage and computational costs of vision transformers. Basically, the quantization task can be regarded as finding the optimal low-bit quantization intervals for weights and inputs, respectively. To preserve the functionality of the attention mechanism, we introduce a ranking loss into the conventional quantization objective that aims to keep the relative order of the self-attention results after quantization. Moreover, we thoroughly analyze the relationship between quantization loss of different layers and the feature diversity, and explore a mixed-precision quantization scheme by exploiting the nuclear norm of each attention map and output feature. The effectiveness of the proposed method is verified on several benchmark models and datasets, which outperforms the state-of-the-art post-training quantization algorithms. For instance, we can obtain an 81.29\\% top-1 accuracy using DeiT-B model on ImageNet dataset with about 8-bit quantization.\n\n2. Title: Vatt: Transformers for multimodal self-supervised learning from raw video, audio and text\n   Abstract: VATT: Transformers for Multimodal Self-Supervised Learning from Raw Video, Audio and Text: We present a framework for learning multimodal representations from unlabeled data using convolution-free Transformer architectures. Specifically, our Video-Audio-Text Transformer (VATT) takes raw signals as inputs and extracts multimodal representations that are rich enough to benefit a variety of downstream tasks. We train VATT end-to-end from scratch using multimodal contrastive losses and evaluate its performance by the downstream tasks of video action recognition, audio event classification, image classification, and text-to-video retrieval. Furthermore, we study a modality-agnostic, single-backbone Transformer by sharing weights among the three modalities. We show that the convolution-free VATT outperforms state-of-the-art ConvNet-based architectures in the downstream tasks. Especially, VATT's vision Transformer achieves the top-1 accuracy of 82.1% on Kinetics-400, 83.6% on Kinetics-600, 72.7% on Kinetics-700, and 41.1% on Moments in Time, new records while avoiding supervised pre-training. Transferring to image classification leads to 78.7% top-1 accuracy on ImageNet compared to 64.7% by training the same Transformer from scratch, showing the generalizability of our model despite the domain gap between videos and images. VATT's audio Transformer also sets a new record on waveform-based audio event recognition by achieving the mAP of 39.4% on AudioSet without any supervised pre-training. VATT's source code is publicly available.\n\n3. Title: Ptq4vit: Post-training quantization for vision transformers with twin uniform quantization\n   Abstract: PTQ4ViT: Post-training quantization for vision transformers with twin uniform quantization: Quantization is one of the most effective methods to compress neural networks, which has achieved great success on convolutional neural networks (CNNs). Recently, vision transformers have demonstrated great potential in computer vision. However, previous post-training quantization methods performed not well on vision transformer, resulting in more than 1% accuracy drop even in 8-bit quantization. Therefore, we analyze the problems of quantization on vision transformers. We observe the distributions of activation values after softmax and GELU functions are quite different from the Gaussian distribution. We also observe that common quantization metrics, such as MSE and cosine distance, are inaccurate to determine the optimal scaling factor. In this paper, we propose the twin uniform quantization method to reduce the quantization error on these activation values. And we propose to use a Hessian guided metric to evaluate different scaling factors, which improves the accuracy of calibration at a small cost. To enable the fast quantization of vision transformers, we develop an efficient framework, PTQ4ViT. Experiments show the quantized vision transformers achieve near-lossless prediction accuracy (less than 0.5% drop at 8-bit quantization) on the ImageNet classification task.\n\n4. Title: Language models are few-shot learners\n   Abstract: Language Models are Few-Shot Learners: Recent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training on a large corpus of text followed by fine-tuning on a specific task. While typically task-agnostic in architecture, this method still requires task-specific fine-tuning datasets of thousands or tens of thousands of examples. By contrast, humans can generally perform a new language task from only a few examples or from simple instructions - something which current NLP systems still largely struggle to do. Here we show that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art fine-tuning approaches. Specifically, we train GPT-3, an autoregressive language model with 175 billion parameters, 10x more than any previous non-sparse language model, and test its performance in the few-shot setting. For all tasks, GPT-3 is applied without any gradient updates or fine-tuning, with tasks and few-shot demonstrations specified purely via text interaction with the model. GPT-3 achieves strong performance on many NLP datasets, including translation, question-answering, and cloze tasks, as well as several tasks that require on-the-fly reasoning or domain adaptation, such as unscrambling words, using a novel word in a sentence, or performing 3-digit arithmetic. At the same time, we also identify some datasets where GPT-3's few-shot learning still struggles, as well as some datasets where GPT-3 faces methodological issues related to training on large web corpora. Finally, we find that GPT-3 can generate samples of news articles which human evaluators have difficulty distinguishing from articles written by humans. We discuss broader societal impacts of this finding and of GPT-3 in general.\n\n5. Title: Fp8 quantization: The power of the exponent\n   Abstract: FP8 Quantization: The Power of the Exponent: When quantizing neural networks for efficient inference, low-bit integers are the go-to format for efficiency. However, low-bit floating point numbers have an extra degree of freedom, assigning some bits to work on an exponential scale instead. This paper in-depth investigates this benefit of the floating point format for neural network inference. We detail the choices that can be made for the FP8 format, including the important choice of the number of bits for the mantissa and exponent, and show analytically in which settings these choices give better performance. Then we show how these findings translate to real networks, provide an efficient implementation for FP8 simulation, and a new algorithm that enables the learning of both the scale parameters and the number of exponent bits in the FP8 format. Our chief conclusion is that when doing post-training quantization for a wide range of networks, the FP8 format is better than INT8 in terms of accuracy, and the choice of the number of exponent bits is driven by the severity of outliers in the network. We also conduct experiments with quantization-aware training where the difference in formats disappears as the network is trained to reduce the effect of outliers.\n\n6. Title: Zeroq: A novel zero shot quantization framework\n   Abstract: ZeroQ: A Novel Zero Shot Quantization Framework: Quantization is a promising approach for reducing the inference time and memory footprint of neural networks. However, most existing quantization methods require access to the original training dataset for retraining during quantization. This is often not possible for applications with sensitive or proprietary data, e.g., due to privacy and security concerns. Existing zero-shot quantization methods use different heuristics to address this, but they result in poor performance, especially when quantizing to ultra-low precision. Here, we propose ZeroQ , a novel zero-shot quantization framework to address this. ZeroQ enables mixed-precision quantization without any access to the training or validation data. This is achieved by optimizing for a Distilled Dataset, which is engineered to match the statistics of batch normalization across different layers of the network. ZeroQ supports both uniform and mixed-precision quantization. For the latter, we introduce a novel Pareto frontier based method to automatically determine the mixed-precision bit setting for all layers, with no manual search involved. We extensively test our proposed method on a diverse set of models, including ResNet18/50/152, MobileNetV2, ShuffleNet, SqueezeNext, and InceptionV3 on ImageNet, as well as RetinaNet-ResNet50 on the Microsoft COCO dataset. In particular, we show that ZeroQ can achieve 1.71\\% higher accuracy on MobileNetV2, as compared to the recently proposed DFQ method. Importantly, ZeroQ has a very low computational overhead, and it can finish the entire quantization process in less than 30s (0.5\\% of one epoch training time of ResNet50 on ImageNet). We have open-sourced the ZeroQ framework\\footnote{https://github.com/amirgholami/ZeroQ}.\n\n7. Title: Integer or Floating Point? New Outlooks for Low-Bit Quantization on Large Language Models\n   Abstract: Integer or Floating Point? New Outlooks for Low-Bit Quantization on Large Language Models: Efficient deployment of large language models (LLMs) necessitates low-bit quantization to minimize model size and inference cost. While low-bit integer formats (e.g., INT8/INT4) have been the conventional choice, emerging low-bit floating-point formats (e.g., FP8/FP4) offer a compelling alternative and are gaining support from cutting-edge hardware, such as NVIDIA's H100 GPU. However, the superiority of low-bit INT versus FP formats for quantization on LLMs remains unclear. In this study, we conduct a comparative analysis of INT and FP quantization with the same bit-width, revealing that the optimal quantization format varies across different layers due to the complexity and diversity of tensor distribution. Consequently, we advocate the Mixture of Formats Quantization (MoFQ), which selects the optimal format on a layer-wise basis. This simple yet effective approach achieves state-of-the-art results in both weight-only (W-only) and weight-activation (WA) post-training quantization scenarios when tested on LLaMA across various tasks. In 4-bit W-only quantization, MoFQ surpasses GPTQ without complex hyperparameter tuning and with an order of magnitude faster quantization speed. While in 8-bit WA quantization, MoFQ significantly outperforms INT/FP-only methods, achieving performance close to the full precision model. Notably, MoFQ incurs no hardware overhead compared to INT/FP-only quantization, as the bit-width remains unchanged.\n\n8. Title: Smoothquant: Accurate and efficient post-training quantization for large language models\n   Abstract: SmoothQuant: Accurate and Efficient Post-Training Quantization for Large Language Models: Large language models (LLMs) show excellent performance but are compute- and memory-intensive. Quantization can reduce memory and accelerate inference. However, existing methods cannot maintain accuracy and hardware efficiency at the same time. We propose SmoothQuant, a training-free, accuracy-preserving, and general-purpose post-training quantization (PTQ) solution to enable 8-bit weight, 8-bit activation (W8A8) quantization for LLMs. Based on the fact that weights are easy to quantize while activations are not, SmoothQuant smooths the activation outliers by offline migrating the quantization difficulty from activations to weights with a mathematically equivalent transformation. SmoothQuant enables an INT8 quantization of both weights and activations for all the matrix multiplications in LLMs, including OPT, BLOOM, GLM, MT-NLG, Llama-1/2, Falcon, Mistral, and Mixtral models. We demonstrate up to 1.56x speedup and 2x memory reduction for LLMs with negligible loss in accuracy. SmoothQuant enables serving 530B LLM within a single node. Our work offers a turn-key solution that reduces hardware costs and democratizes LLMs. Code is available at https://github.com/mit-han-lab/smoothquant.\n\n9. Title: Data-Free Quantization Through Weight Equalization and Bias Correction\n   Abstract: Data-Free Quantization Through Weight Equalization and Bias Correction: We introduce a data-free quantization method for deep neural networks that does not require fine-tuning or hyperparameter selection. It achieves near-original model performance on common computer vision architectures and tasks. 8-bit fixed-point quantization is essential for efficient inference on modern deep learning hardware. However, quantizing models to run in 8-bit is a non-trivial task, frequently leading to either significant performance reduction or engineering time spent on training a network to be amenable to quantization. Our approach relies on equalizing the weight ranges in the network by making use of a scale-equivariance property of activation functions. In addition the method corrects biases in the error that are introduced during quantization. This improves quantization accuracy performance, and can be applied to many common computer vision architectures with a straight forward API call. For common architectures, such as the MobileNet family, we achieve state-of-the-art quantized model performance. We further show that the method also extends to other computer vision architectures and tasks such as semantic segmentation and object detection.\n\n10. Title: Understanding and Overcoming the Challenges of Efficient Transformer Quantization\n   Abstract: Understanding and Overcoming the Challenges of Efficient Transformer Quantization: Transformer-based architectures have become the de-facto standard models for a wide range of Natural Language Processing tasks. However, their memory footprint and high latency are prohibitive for efficient deployment and inference on resource-limited devices. In this work, we explore quantization for transformers. We show that transformers have unique quantization challenges -- namely, high dynamic activation ranges that are difficult to represent with a low bit fixed-point format. We establish that these activations contain structured outliers in the residual connections that encourage specific attention patterns, such as attending to the special separator token. To combat these challenges, we present three solutions based on post-training quantization and quantization-aware training, each with a different set of compromises for accuracy, model size, and ease of use. In particular, we introduce a novel quantization scheme -- per-embedding-group quantization. We demonstrate the effectiveness of our methods on the GLUE benchmark using BERT, establishing state-of-the-art results for post-training quantization. Finally, we show that transformer weights and embeddings can be quantized to ultra-low bit-widths, leading to significant memory savings with a minimum accuracy loss. Our source code is available at~\\url{https://github.com/qualcomm-ai-research/transformer-quantization}.\n\nThink step-by-step about which candidate best matches the context. Summarize your reasoning, then provide the exact title that should fill the citation. Respond in JSON with fields:\n{\n  \"reasoning\": \"<your step-by-step analysis>\",\n  \"selected_title\": \"<exact candidate title>\"\n}", "response": {"reasoning": "The citation context discusses the transformative impact of the transformer architecture in natural language processing (NLP) tasks and mentions models like GPT that have propelled its popularity. Among the candidate papers, the one titled 'Language models are few-shot learners' directly addresses the advancements in NLP through the use of large language models, specifically mentioning GPT-3, which aligns with the context's reference to GPT. The other candidates focus on quantization techniques for vision transformers or multimodal learning, which do not directly relate to the claim about the impact of transformers in NLP. Therefore, the best match is the paper that discusses the capabilities and performance of GPT-3 in NLP tasks.", "selected_title": "Language models are few-shot learners"}, "selected_title": "Language models are few-shot learners", "positive_title": "Language models are few-shot learners", "is_correct": true, "reasoning": "The citation context discusses the transformative impact of the transformer architecture in natural language processing (NLP) tasks and mentions models like GPT that have propelled its popularity. Among the candidate papers, the one titled 'Language models are few-shot learners' directly addresses the advancements in NLP through the use of large language models, specifically mentioning GPT-3, which aligns with the context's reference to GPT. The other candidates focus on quantization techniques for vision transformers or multimodal learning, which do not directly relate to the claim about the impact of transformers in NLP. Therefore, the best match is the paper that discusses the capabilities and performance of GPT-3 in NLP tasks."}
{"example_id": "blind_8701", "prompt": "You are an expert citation retrieval system as described in the paper \"Multi-Agent System for Reliable Citation Retrieval\".\nYour goal is to autonomously retrieve, verify, and recommend academic references given a query or document excerpt.\n\nTask:\nGiven a citation context from a scientific paper (where a citation is missing), identify the correct paper from a list of candidates.\nAnalyze the context to understand the specific claim, method, or result being cited.\nThen, evaluate each candidate paper to see if it matches the context.\nFinally, select the best matching paper.\n\nCitation Context:\nained classifier; they demonstrate that a large fraction of images are misclassified. Eykholt \\etal consider physical attacks on stop sign classifiers using images cropped from video frames[CITATION]. They successfully fool classifiers using both norm bounded perturbations, and also sparse perturbations using carefully placed stickers. Stop signs attacks on object detectors are considered in<|cit\n\nCandidate Papers:\n1. Title: Robust physical-world attacks on deep learning models\n   Abstract: Robust Physical-World Attacks on Deep Learning Models: Recent studies show that the state-of-the-art deep neural networks (DNNs) are vulnerable to adversarial examples, resulting from small-magnitude perturbations added to the input. Given that that emerging physical systems are using DNNs in safety-critical situations, adversarial examples could mislead these systems and cause dangerous situations.Therefore, understanding adversarial examples in the physical world is an important step towards developing resilient learning algorithms. We propose a general attack algorithm,Robust Physical Perturbations (RP2), to generate robust visual adversarial perturbations under different physical conditions. Using the real-world case of road sign classification, we show that adversarial examples generated using RP2 achieve high targeted misclassification rates against standard-architecture road sign classifiers in the physical world under various environmental conditions, including viewpoints. Due to the current lack of a standardized testing method, we propose a two-stage evaluation methodology for robust physical adversarial examples consisting of lab and field tests. Using this methodology, we evaluate the efficacy of physical adversarial manipulations on real objects. Witha perturbation in the form of only black and white stickers,we attack a real stop sign, causing targeted misclassification in 100% of the images obtained in lab settings, and in 84.8%of the captured video frames obtained on a moving vehicle(field test) for the target classifier.\n\n2. Title: Adversarial t-shirt! evading person detectors in a physical world\n   Abstract: Adversarial T-shirt! Evading Person Detectors in A Physical World: It is known that deep neural networks (DNNs) are vulnerable to adversarial attacks. The so-called physical adversarial examples deceive DNN-based decisionmakers by attaching adversarial patches to real objects. However, most of the existing works on physical adversarial attacks focus on static objects such as glass frames, stop signs and images attached to cardboard. In this work, we proposed adversarial T-shirts, a robust physical adversarial example for evading person detectors even if it could undergo non-rigid deformation due to a moving person's pose changes. To the best of our knowledge, this is the first work that models the effect of deformation for designing physical adversarial examples with respect to-rigid objects such as T-shirts. We show that the proposed method achieves74% and 57% attack success rates in the digital and physical worlds respectively against YOLOv2. In contrast, the state-of-the-art physical attack method to fool a person detector only achieves 18% attack success rate. Furthermore, by leveraging min-max optimization, we extend our method to the ensemble attack setting against two object detectors YOLO-v2 and Faster R-CNN simultaneously.\n\n3. Title: AdvHat: Real-world adversarial attack on ArcFace Face ID system\n   Abstract: AdvHat: Real-world adversarial attack on ArcFace Face ID system: In this paper we propose a novel easily reproducible technique to attack the best public Face ID system ArcFace in different shooting conditions. To create an attack, we print the rectangular paper sticker on a common color printer and put it on the hat. The adversarial sticker is prepared with a novel algorithm for off-plane transformations of the image which imitates sticker location on the hat. Such an approach confuses the state-of-the-art public Face ID model LResNet100E-IR, ArcFace@ms1m-refine-v2 and is transferable to other Face ID models.\n\n4. Title: Adversarial examples for semantic segmentation and object detection\n   Abstract: Adversarial Examples for Semantic Segmentation and Object Detection: It has been well demonstrated that adversarial examples, i.e., natural images with visually imperceptible perturbations added, generally exist for deep networks to fail on image classification. In this paper, we extend adversarial examples to semantic segmentation and object detection which are much more difficult. Our observation is that both segmentation and detection are based on classifying multiple targets on an image (e.g., the basic target is a pixel or a receptive field in segmentation, and an object proposal in detection), which inspires us to optimize a loss function over a set of pixels/proposals for generating adversarial perturbations. Based on this idea, we propose a novel algorithm named Dense Adversary Generation (DAG), which generates a large family of adversarial examples, and applies to a wide range of state-of-the-art deep networks for segmentation and detection. We also find that the adversarial perturbations can be transferred across networks with different training data, based on different architectures, and even for different recognition tasks. In particular, the transferability across networks with the same architecture is more significant than in other cases. Besides, summing up heterogeneous perturbations often leads to better transfer performance, which provides an effective method of black-box adversarial attack.\n\n5. Title: Adversarial patch\n   Abstract: Adversarial Patch: We present a method to create universal, robust, targeted adversarial image patches in the real world. The patches are universal because they can be used to attack any scene, robust because they work under a wide variety of transformations, and targeted because they can cause a classifier to output any target class. These adversarial patches can be printed, added to any scene, photographed, and presented to image classifiers; even when the patches are small, they cause the classifiers to ignore the other items in the scene and report a chosen target class.   To reproduce the results from the paper, our code is available at https://github.com/tensorflow/cleverhans/tree/master/examples/adversarial_patch\n\n6. Title: Transferable Adversarial Attacks for Image and Video Object Detection\n   Abstract: Transferable Adversarial Attacks for Image and Video Object Detection: Adversarial examples have been demonstrated to threaten many computer vision tasks including object detection. However, the existing attacking methods for object detection have two limitations: poor transferability, which denotes that the generated adversarial examples have low success rate to attack other kinds of detection methods, and high computation cost, which means that they need more time to generate an adversarial image, and therefore are difficult to deal with the video data. To address these issues, we utilize a generative mechanism to obtain the adversarial image and video. In this way, the processing time is reduced. To enhance the transferability, we destroy the feature maps extracted from the feature network, which usually constitutes the basis of object detectors. The proposed method is based on the Generative Adversarial Network (GAN) framework, where we combine the high-level class loss and low-level feature loss to jointly train the adversarial example generator. A series of experiments conducted on PASCAL VOC and ImageNet VID datasets show that our method can efficiently generate image and video adversarial examples, and more importantly, these adversarial examples have better transferability, and thus, are able to simultaneously attack two kinds of representative object detection models: proposal based models like Faster-RCNN, and regression based models like SSD.\n\n7. Title: Fooling automated surveillance cameras: adversarial patches to attack person detection\n   Abstract: Fooling automated surveillance cameras: adversarial patches to attack person detection: Adversarial attacks on machine learning models have seen increasing interest in the past years. By making only subtle changes to the input of a convolutional neural network, the output of the network can be swayed to output a completely different result. The first attacks did this by changing pixel values of an input image slightly to fool a classifier to output the wrong class. Other approaches have tried to learn \"patches\" that can be applied to an object to fool detectors and classifiers. Some of these approaches have also shown that these attacks are feasible in the real-world, i.e. by modifying an object and filming it with a video camera. However, all of these approaches target classes that contain almost no intra-class variety (e.g. stop signs). The known structure of the object is then used to generate an adversarial patch on top of it.   In this paper, we present an approach to generate adversarial patches to targets with lots of intra-class variety, namely persons. The goal is to generate a patch that is able successfully hide a person from a person detector. An attack that could for instance be used maliciously to circumvent surveillance systems, intruders can sneak around undetected by holding a small cardboard plate in front of their body aimed towards the surveillance camera.   From our results we can see that our system is able significantly lower the accuracy of a person detector. Our approach also functions well in real-life scenarios where the patch is filmed by a camera. To the best of our knowledge we are the first to attempt this kind of attack on targets with a high level of intra-class variety like persons.\n\n8. Title: Focal loss for dense object detection\n   Abstract: Focal Loss for Dense Object Detection: The highest accuracy object detectors to date are based on a two-stage approach popularized by R-CNN, where a classifier is applied to a sparse set of candidate object locations. In contrast, one-stage detectors that are applied over a regular, dense sampling of possible object locations have the potential to be faster and simpler, but have trailed the accuracy of two-stage detectors thus far. In this paper, we investigate why this is the case. We discover that the extreme foreground-background class imbalance encountered during training of dense detectors is the central cause. We propose to address this class imbalance by reshaping the standard cross entropy loss such that it down-weights the loss assigned to well-classified examples. Our novel Focal Loss focuses training on a sparse set of hard examples and prevents the vast number of easy negatives from overwhelming the detector during training. To evaluate the effectiveness of our loss, we design and train a simple dense detector we call RetinaNet. Our results show that when trained with the focal loss, RetinaNet is able to match the speed of previous one-stage detectors while surpassing the accuracy of all existing state-of-the-art two-stage detectors. Code is at: https://github.com/facebookresearch/Detectron.\n\n9. Title: Mask r-cnn\n   Abstract: Mask R-CNN: We present a conceptually simple, flexible, and general framework for object instance segmentation. Our approach efficiently detects objects in an image while simultaneously generating a high-quality segmentation mask for each instance. The method, called Mask R-CNN, extends Faster R-CNN by adding a branch for predicting an object mask in parallel with the existing branch for bounding box recognition. Mask R-CNN is simple to train and adds only a small overhead to Faster R-CNN, running at 5 fps. Moreover, Mask R-CNN is easy to generalize to other tasks, e.g., allowing us to estimate human poses in the same framework. We show top results in all three tracks of the COCO suite of challenges, including instance segmentation, bounding-box object detection, and person keypoint detection. Without bells and whistles, Mask R-CNN outperforms all existing, single-model entries on every task, including the COCO 2016 challenge winners. We hope our simple and effective approach will serve as a solid baseline and help ease future research in instance-level recognition. Code has been made available at: https://github.com/facebookresearch/Detectron\n\n10. Title: Physical adversarial examples for object detectors\n   Abstract: Physical Adversarial Examples for Object Detectors: Deep neural networks (DNNs) are vulnerable to adversarial examples-maliciously crafted inputs that cause DNNs to make incorrect predictions. Recent work has shown that these attacks generalize to the physical domain, to create perturbations on physical objects that fool image classifiers under a variety of real-world conditions. Such attacks pose a risk to deep learning models used in safety-critical cyber-physical systems. In this work, we extend physical attacks to more challenging object detection models, a broader class of deep learning algorithms widely used to detect and label multiple objects within a scene. Improving upon a previous physical attack on image classifiers, we create perturbed physical objects that are either ignored or mislabeled by object detection models. We implement a Disappearance Attack, in which we cause a Stop sign to \"disappear\" according to the detector-either by covering thesign with an adversarial Stop sign poster, or by adding adversarial stickers onto the sign. In a video recorded in a controlled lab environment, the state-of-the-art YOLOv2 detector failed to recognize these adversarial Stop signs in over 85% of the video frames. In an outdoor experiment, YOLO was fooled by the poster and sticker attacks in 72.5% and 63.5% of the video frames respectively. We also use Faster R-CNN, a different object detection model, to demonstrate the transferability of our adversarial perturbations. The created poster perturbation is able to fool Faster R-CNN in 85.9% of the video frames in a controlled lab environment, and 40.2% of the video frames in an outdoor environment. Finally, we present preliminary results with a new Creation Attack, where in innocuous physical stickers fool a model into detecting nonexistent objects.\n\nThink step-by-step about which candidate best matches the context. Summarize your reasoning, then provide the exact title that should fill the citation. Respond in JSON with fields:\n{\n  \"reasoning\": \"<your step-by-step analysis>\",\n  \"selected_title\": \"<exact candidate title>\"\n}", "response": {"reasoning": "The citation context discusses physical attacks on stop sign classifiers, specifically mentioning the use of perturbations and stickers to fool classifiers. Among the candidate papers, the one titled 'Physical adversarial examples for object detectors' directly addresses physical adversarial attacks on object detection models, including a specific focus on stop signs. It describes methods that involve covering or altering stop signs with adversarial stickers and posters, which aligns perfectly with the context provided. Other candidates either focus on different types of objects or do not specifically mention stop signs or similar physical attacks. Therefore, the best match is the paper that explicitly discusses the adversarial manipulation of stop signs in the context of object detection.", "selected_title": "Physical adversarial examples for object detectors"}, "selected_title": "Physical adversarial examples for object detectors", "positive_title": "Robust physical-world attacks on deep learning models", "is_correct": false, "reasoning": "The citation context discusses physical attacks on stop sign classifiers, specifically mentioning the use of perturbations and stickers to fool classifiers. Among the candidate papers, the one titled 'Physical adversarial examples for object detectors' directly addresses physical adversarial attacks on object detection models, including a specific focus on stop signs. It describes methods that involve covering or altering stop signs with adversarial stickers and posters, which aligns perfectly with the context provided. Other candidates either focus on different types of objects or do not specifically mention stop signs or similar physical attacks. Therefore, the best match is the paper that explicitly discusses the adversarial manipulation of stop signs in the context of object detection."}
{"example_id": "blind_19751", "prompt": "You are an expert citation retrieval system as described in the paper \"Multi-Agent System for Reliable Citation Retrieval\".\nYour goal is to autonomously retrieve, verify, and recommend academic references given a query or document excerpt.\n\nTask:\nGiven a citation context from a scientific paper (where a citation is missing), identify the correct paper from a list of candidates.\nAnalyze the context to understand the specific claim, method, or result being cited.\nThen, evaluate each candidate paper to see if it matches the context.\nFinally, select the best matching paper.\n\nCitation Context:\nrt in controlling the size and location of the generated objects. Another very recent research direction investigates \\emph{localization-controllable} diffusion models. In particular, GLIGEN[CITATION] integrates a new adapter layer into each Transformer block of a pre-trained diffusion model to achieve size and location control. Although GLIGEN can generate objects according to a user-specified si\n\nCandidate Papers:\n1. Title: Hierarchical text-conditional image generation with clip latents\n   Abstract: Hierarchical Text-Conditional Image Generation with CLIP Latents: Contrastive models like CLIP have been shown to learn robust representations of images that capture both semantics and style. To leverage these representations for image generation, we propose a two-stage model: a prior that generates a CLIP image embedding given a text caption, and a decoder that generates an image conditioned on the image embedding. We show that explicitly generating image representations improves image diversity with minimal loss in photorealism and caption similarity. Our decoders conditioned on image representations can also produce variations of an image that preserve both its semantics and style, while varying the non-essential details absent from the image representation. Moreover, the joint embedding space of CLIP enables language-guided image manipulations in a zero-shot fashion. We use diffusion models for the decoder and experiment with both autoregressive and diffusion models for the prior, finding that the latter are computationally more efficient and produce higher-quality samples.\n\n2. Title: Denoising diffusion implicit models\n   Abstract: Denoising Diffusion Implicit Models: Denoising diffusion probabilistic models (DDPMs) have achieved high quality image generation without adversarial training, yet they require simulating a Markov chain for many steps to produce a sample. To accelerate sampling, we present denoising diffusion implicit models (DDIMs), a more efficient class of iterative implicit probabilistic models with the same training procedure as DDPMs. In DDPMs, the generative process is defined as the reverse of a Markovian diffusion process. We construct a class of non-Markovian diffusion processes that lead to the same training objective, but whose reverse process can be much faster to sample from. We empirically demonstrate that DDIMs can produce high quality samples $10 \\times$ to $50 \\times$ faster in terms of wall-clock time compared to DDPMs, allow us to trade off computation for sample quality, and can perform semantically meaningful image interpolation directly in the latent space.\n\n3. Title: T2i-adapter: Learning adapters to dig out more controllable ability for text-to-image diffusion models\n   Abstract: T2I-Adapter: Learning Adapters to Dig out More Controllable Ability for Text-to-Image Diffusion Models: The incredible generative ability of large-scale text-to-image (T2I) models has demonstrated strong power of learning complex structures and meaningful semantics. However, relying solely on text prompts cannot fully take advantage of the knowledge learned by the model, especially when flexible and accurate controlling (e.g., color and structure) is needed. In this paper, we aim to ``dig out\" the capabilities that T2I models have implicitly learned, and then explicitly use them to control the generation more granularly. Specifically, we propose to learn simple and lightweight T2I-Adapters to align internal knowledge in T2I models with external control signals, while freezing the original large T2I models. In this way, we can train various adapters according to different conditions, achieving rich control and editing effects in the color and structure of the generation results. Further, the proposed T2I-Adapters have attractive properties of practical value, such as composability and generalization ability. Extensive experiments demonstrate that our T2I-Adapter has promising generation quality and a wide range of applications.\n\n4. Title: Photorealistic text-to-image diffusion models with deep language understanding\n   Abstract: Photorealistic Text-to-Image Diffusion Models with Deep Language Understanding: We present Imagen, a text-to-image diffusion model with an unprecedented degree of photorealism and a deep level of language understanding. Imagen builds on the power of large transformer language models in understanding text and hinges on the strength of diffusion models in high-fidelity image generation. Our key discovery is that generic large language models (e.g. T5), pretrained on text-only corpora, are surprisingly effective at encoding text for image synthesis: increasing the size of the language model in Imagen boosts both sample fidelity and image-text alignment much more than increasing the size of the image diffusion model. Imagen achieves a new state-of-the-art FID score of 7.27 on the COCO dataset, without ever training on COCO, and human raters find Imagen samples to be on par with the COCO data itself in image-text alignment. To assess text-to-image models in greater depth, we introduce DrawBench, a comprehensive and challenging benchmark for text-to-image models. With DrawBench, we compare Imagen with recent methods including VQ-GAN+CLIP, Latent Diffusion Models, and DALL-E 2, and find that human raters prefer Imagen over other models in side-by-side comparisons, both in terms of sample quality and image-text alignment. See https://imagen.research.google/ for an overview of the results.\n\n5. Title: Composer: Creative and controllable image synthesis with composable conditions\n   Abstract: Composer: Creative and Controllable Image Synthesis with Composable Conditions: Recent large-scale generative models learned on big data are capable of synthesizing incredible images yet suffer from limited controllability. This work offers a new generation paradigm that allows flexible control of the output image, such as spatial layout and palette, while maintaining the synthesis quality and model creativity. With compositionality as the core idea, we first decompose an image into representative factors, and then train a diffusion model with all these factors as the conditions to recompose the input. At the inference stage, the rich intermediate representations work as composable elements, leading to a huge design space (i.e., exponentially proportional to the number of decomposed factors) for customizable content creation. It is noteworthy that our approach, which we call Composer, supports various levels of conditions, such as text description as the global information, depth map and sketch as the local guidance, color histogram for low-level details, etc. Besides improving controllability, we confirm that Composer serves as a general framework and facilitates a wide range of classical generative tasks without retraining. Code and models will be made available.\n\n6. Title: Vector-quantized image modeling with improved VQGAN\n   Abstract: Vector-quantized Image Modeling with Improved VQGAN: Pretraining language models with next-token prediction on massive text corpora has delivered phenomenal zero-shot, few-shot, transfer learning and multi-tasking capabilities on both generative and discriminative language tasks. Motivated by this success, we explore a Vector-quantized Image Modeling (VIM) approach that involves pretraining a Transformer to predict rasterized image tokens autoregressively. The discrete image tokens are encoded from a learned Vision-Transformer-based VQGAN (ViT-VQGAN). We first propose multiple improvements over vanilla VQGAN from architecture to codebook learning, yielding better efficiency and reconstruction fidelity. The improved ViT-VQGAN further improves vector-quantized image modeling tasks, including unconditional, class-conditioned image generation and unsupervised representation learning. When trained on ImageNet at \\(256\\times256\\) resolution, we achieve Inception Score (IS) of 175.1 and Fr'echet Inception Distance (FID) of 4.17, a dramatic improvement over the vanilla VQGAN, which obtains 70.6 and 17.04 for IS and FID, respectively. Based on ViT-VQGAN and unsupervised pretraining, we further evaluate the pretrained Transformer by averaging intermediate features, similar to Image GPT (iGPT). This ImageNet-pretrained VIM-L significantly beats iGPT-L on linear-probe accuracy from 60.3% to 73.2% for a similar model size. VIM-L also outperforms iGPT-XL which is trained with extra web image data and larger model size.\n\n7. Title: GLIGEN: Open-Set Grounded Text-to-Image Generation\n   Abstract: GLIGEN: Open-Set Grounded Text-to-Image Generation: Large-scale text-to-image diffusion models have made amazing advances. However, the status quo is to use text input alone, which can impede controllability. In this work, we propose GLIGEN, Grounded-Language-to-Image Generation, a novel approach that builds upon and extends the functionality of existing pre-trained text-to-image diffusion models by enabling them to also be conditioned on grounding inputs. To preserve the vast concept knowledge of the pre-trained model, we freeze all of its weights and inject the grounding information into new trainable layers via a gated mechanism. Our model achieves open-world grounded text2img generation with caption and bounding box condition inputs, and the grounding ability generalizes well to novel spatial configurations and concepts. GLIGEN's zero-shot performance on COCO and LVIS outperforms that of existing supervised layout-to-image baselines by a large margin.\n\n8. Title: Deep Unsupervised Learning using Nonequilibrium Thermodynamics\n   Abstract: Deep Unsupervised Learning using Nonequilibrium Thermodynamics: A central problem in machine learning involves modeling complex data-sets using highly flexible families of probability distributions in which learning, sampling, inference, and evaluation are still analytically or computationally tractable. Here, we develop an approach that simultaneously achieves both flexibility and tractability. The essential idea, inspired by non-equilibrium statistical physics, is to systematically and slowly destroy structure in a data distribution through an iterative forward diffusion process. We then learn a reverse diffusion process that restores structure in data, yielding a highly flexible and tractable generative model of the data. This approach allows us to rapidly learn, sample from, and evaluate probabilities in deep generative models with thousands of layers or time steps, as well as to compute conditional and posterior probabilities under the learned model. We additionally release an open source reference implementation of the algorithm.\n\nThink step-by-step about which candidate best matches the context. Summarize your reasoning, then provide the exact title that should fill the citation. Respond in JSON with fields:\n{\n  \"reasoning\": \"<your step-by-step analysis>\",\n  \"selected_title\": \"<exact candidate title>\"\n}", "response": {"reasoning": "The citation context discusses a method for controlling the size and location of generated objects using a model called GLIGEN, which integrates a new adapter layer into each Transformer block of a pre-trained diffusion model. Among the candidate papers, the one titled 'GLIGEN: Open-Set Grounded Text-to-Image Generation' directly mentions GLIGEN and describes its functionality, including the use of grounding inputs to enhance controllability in text-to-image generation. This aligns perfectly with the context's focus on localization-controllable diffusion models and the specific mention of GLIGEN's capabilities. The other candidates do not mention GLIGEN or focus on similar methods of size and location control in the same way, making them less relevant.", "selected_title": "GLIGEN: Open-Set Grounded Text-to-Image Generation"}, "selected_title": "GLIGEN: Open-Set Grounded Text-to-Image Generation", "positive_title": "GLIGEN: Open-Set Grounded Text-to-Image Generation", "is_correct": true, "reasoning": "The citation context discusses a method for controlling the size and location of generated objects using a model called GLIGEN, which integrates a new adapter layer into each Transformer block of a pre-trained diffusion model. Among the candidate papers, the one titled 'GLIGEN: Open-Set Grounded Text-to-Image Generation' directly mentions GLIGEN and describes its functionality, including the use of grounding inputs to enhance controllability in text-to-image generation. This aligns perfectly with the context's focus on localization-controllable diffusion models and the specific mention of GLIGEN's capabilities. The other candidates do not mention GLIGEN or focus on similar methods of size and location control in the same way, making them less relevant."}
{"example_id": "blind_6280", "prompt": "You are an expert citation retrieval system as described in the paper \"Multi-Agent System for Reliable Citation Retrieval\".\nYour goal is to autonomously retrieve, verify, and recommend academic references given a query or document excerpt.\n\nTask:\nGiven a citation context from a scientific paper (where a citation is missing), identify the correct paper from a list of candidates.\nAnalyze the context to understand the specific claim, method, or result being cited.\nThen, evaluate each candidate paper to see if it matches the context.\nFinally, select the best matching paper.\n\nCitation Context:\nntered at the centroid of each class with a certain $\\ell_p$ radius; 2) the convexity of samples in the high dimensional latent space. We evaluate our metric on the AudioSet and Kinetic-Sounds dataset[CITATION]. The results indicate that multi-modal models are only more robust measured by class-wise metrics for a limited number of classes. We also observe the point-wise robustness of classification results\n\nCandidate Papers:\n1. Title: Certified adversarial robustness via randomized smoothing\n   Abstract: Certified Adversarial Robustness via Randomized Smoothing: We show how to turn any classifier that classifies well under Gaussian noise into a new classifier that is certifiably robust to adversarial perturbations under the $\\ell_2$ norm. This \"randomized smoothing\" technique has been proposed recently in the literature, but existing guarantees are loose. We prove a tight robustness guarantee in $\\ell_2$ norm for smoothing with Gaussian noise. We use randomized smoothing to obtain an ImageNet classifier with e.g. a certified top-1 accuracy of 49% under adversarial perturbations with $\\ell_2$ norm less than 0.5 (=127/255). No certified defense has been shown feasible on ImageNet except for smoothing. On smaller-scale datasets where competing approaches to certified $\\ell_2$ robustness are viable, smoothing delivers higher certified accuracies. Our strong empirical results suggest that randomized smoothing is a promising direction for future research into adversarially robust classification. Code and models are available at http://github.com/locuslab/smoothing.\n\n2. Title: Look, listen and learn\n   Abstract: Look, Listen and Learn: We consider the question: what can be learnt by looking at and listening to a large number of unlabelled videos? There is a valuable, but so far untapped, source of information contained in the video itself -- the correspondence between the visual and the audio streams, and we introduce a novel \"Audio-Visual Correspondence\" learning task that makes use of this. Training visual and audio networks from scratch, without any additional supervision other than the raw unconstrained videos themselves, is shown to successfully solve this task, and, more interestingly, result in good visual and audio representations. These features set the new state-of-the-art on two sound classification benchmarks, and perform on par with the state-of-the-art self-supervised approaches on ImageNet classification. We also demonstrate that the network is able to localize objects in both modalities, as well as perform fine-grained recognition tasks.\n\n3. Title: Provable defenses against adversarial examples via the convex outer adversarial polytope\n   Abstract: Provable defenses against adversarial examples via the convex outer adversarial polytope: We propose a method to learn deep ReLU-based classifiers that are provably robust against norm-bounded adversarial perturbations on the training data. For previously unseen examples, the approach is guaranteed to detect all adversarial examples, though it may flag some non-adversarial examples as well. The basic idea is to consider a convex outer approximation of the set of activations reachable through a norm-bounded perturbation, and we develop a robust optimization procedure that minimizes the worst case loss over this outer region (via a linear program). Crucially, we show that the dual problem to this linear program can be represented itself as a deep network similar to the backpropagation network, leading to very efficient optimization approaches that produce guaranteed bounds on the robust loss. The end result is that by executing a few more forward and backward passes through a slightly modified version of the original network (though possibly with much larger batch sizes), we can learn a classifier that is provably robust to any norm-bounded adversarial attack. We illustrate the approach on a number of tasks to train classifiers with robust adversarial guarantees (e.g. for MNIST, we produce a convolutional classifier that provably has less than 5.8% test error for any adversarial attack with bounded $\\ell_\\infty$ norm less than $\\epsilon = 0.1$), and code for all experiments in the paper is available at https://github.com/locuslab/convex_adversarial.\n\n4. Title: On evaluating adversarial robustness\n   Abstract: On Evaluating Adversarial Robustness: Correctly evaluating defenses against adversarial examples has proven to be extremely difficult. Despite the significant amount of recent work attempting to design defenses that withstand adaptive attacks, few have succeeded; most papers that propose defenses are quickly shown to be incorrect.   We believe a large contributing factor is the difficulty of performing security evaluations. In this paper, we discuss the methodological foundations, review commonly accepted best practices, and suggest new methods for evaluating defenses to adversarial examples. We hope that both researchers developing defenses as well as readers and reviewers who wish to understand the completeness of an evaluation consider our advice in order to avoid common pitfalls.\n\n5. Title: Towards deep learning models resistant to adversarial attacks\n   Abstract: Towards Deep Learning Models Resistant to Adversarial Attacks: Recent work has demonstrated that deep neural networks are vulnerable to adversarial examples---inputs that are almost indistinguishable from natural data and yet classified incorrectly by the network. In fact, some of the latest findings suggest that the existence of adversarial attacks may be an inherent weakness of deep learning models. To address this problem, we study the adversarial robustness of neural networks through the lens of robust optimization. This approach provides us with a broad and unifying view on much of the prior work on this topic. Its principled nature also enables us to identify methods for both training and attacking neural networks that are reliable and, in a certain sense, universal. In particular, they specify a concrete security guarantee that would protect against any adversary. These methods let us train networks with significantly improved resistance to a wide range of adversarial attacks. They also suggest the notion of security against a first-order adversary as a natural and broad security guarantee. We believe that robustness against such well-defined classes of adversaries is an important stepping stone towards fully resistant deep learning models. Code and pre-trained models are available at https://github.com/MadryLab/mnist_challenge and https://github.com/MadryLab/cifar10_challenge.\n\n6. Title: Universal adversarial perturbations\n   Abstract: Universal adversarial perturbations: Given a state-of-the-art deep neural network classifier, we show the existence of a universal (image-agnostic) and very small perturbation vector that causes natural images to be misclassified with high probability. We propose a systematic algorithm for computing universal perturbations, and show that state-of-the-art deep neural networks are highly vulnerable to such perturbations, albeit being quasi-imperceptible to the human eye. We further empirically analyze these universal perturbations and show, in particular, that they generalize very well across neural networks. The surprising existence of universal perturbations reveals important geometric correlations among the high-dimensional decision boundary of classifiers. It further outlines potential security breaches with the existence of single directions in the input space that adversaries can possibly exploit to break a classifier on most natural images.\n\n7. Title: Empir: Ensembles of mixed precision deep networks for increased robustness against adversarial attacks\n   Abstract: EMPIR: Ensembles of Mixed Precision Deep Networks for Increased Robustness against Adversarial Attacks: Ensuring robustness of Deep Neural Networks (DNNs) is crucial to their adoption in safety-critical applications such as self-driving cars, drones, and healthcare. Notably, DNNs are vulnerable to adversarial attacks in which small input perturbations can produce catastrophic misclassifications. In this work, we propose EMPIR, ensembles of quantized DNN models with different numerical precisions, as a new approach to increase robustness against adversarial attacks. EMPIR is based on the observation that quantized neural networks often demonstrate much higher robustness to adversarial attacks than full precision networks, but at the cost of a substantial loss in accuracy on the original (unperturbed) inputs. EMPIR overcomes this limitation to achieve the 'best of both worlds', i.e., the higher unperturbed accuracies of the full precision models combined with the higher robustness of the low precision models, by composing them in an ensemble. Further, as low precision DNN models have significantly lower computational and storage requirements than full precision models, EMPIR models only incur modest compute and memory overheads compared to a single full-precision model (<25% in our evaluations). We evaluate EMPIR across a suite of DNNs for 3 different image recognition tasks (MNIST, CIFAR-10 and ImageNet) and under 4 different adversarial attacks. Our results indicate that EMPIR boosts the average adversarial accuracies by 42.6%, 15.2% and 10.5% for the DNN models trained on the MNIST, CIFAR-10 and ImageNet datasets respectively, when compared to single full-precision models, without sacrificing accuracy on the unperturbed inputs.\n\n8. Title: Ucf101: A dataset of 101 human actions classes from videos in the wild\n   Abstract: UCF101: A Dataset of 101 Human Actions Classes From Videos in The Wild: We introduce UCF101 which is currently the largest dataset of human actions. It consists of 101 action classes, over 13k clips and 27 hours of video data. The database consists of realistic user uploaded videos containing camera motion and cluttered background. Additionally, we provide baseline action recognition results on this new dataset using standard bag of words approach with overall performance of 44.5%. To the best of our knowledge, UCF101 is currently the most challenging dataset of actions due to its large number of classes, large number of clips and also unconstrained nature of such clips.\n\nThink step-by-step about which candidate best matches the context. Summarize your reasoning, then provide the exact title that should fill the citation. Respond in JSON with fields:\n{\n  \"reasoning\": \"<your step-by-step analysis>\",\n  \"selected_title\": \"<exact candidate title>\"\n}", "response": {"reasoning": "The citation context discusses evaluating a metric on the AudioSet and Kinetic-Sounds datasets, focusing on the robustness of multi-modal models and their performance across different classes. The context implies a connection to audio-visual learning or classification tasks. Among the candidate papers, 'Look, listen and learn' directly addresses the learning from audio and visual data, specifically mentioning sound classification benchmarks, which aligns with the datasets mentioned in the context. Other candidates primarily focus on adversarial robustness and do not relate to the multi-modal aspect or the specific datasets mentioned. Therefore, 'Look, listen and learn' is the best match for the citation context.", "selected_title": "Look, listen and learn"}, "selected_title": "Look, listen and learn", "positive_title": "Look, listen and learn", "is_correct": true, "reasoning": "The citation context discusses evaluating a metric on the AudioSet and Kinetic-Sounds datasets, focusing on the robustness of multi-modal models and their performance across different classes. The context implies a connection to audio-visual learning or classification tasks. Among the candidate papers, 'Look, listen and learn' directly addresses the learning from audio and visual data, specifically mentioning sound classification benchmarks, which aligns with the datasets mentioned in the context. Other candidates primarily focus on adversarial robustness and do not relate to the multi-modal aspect or the specific datasets mentioned. Therefore, 'Look, listen and learn' is the best match for the citation context."}
{"example_id": "blind_9124", "prompt": "You are an expert citation retrieval system as described in the paper \"Multi-Agent System for Reliable Citation Retrieval\".\nYour goal is to autonomously retrieve, verify, and recommend academic references given a query or document excerpt.\n\nTask:\nGiven a citation context from a scientific paper (where a citation is missing), identify the correct paper from a list of candidates.\nAnalyze the context to understand the specific claim, method, or result being cited.\nThen, evaluate each candidate paper to see if it matches the context.\nFinally, select the best matching paper.\n\nCitation Context:\nrefore, most of the existing effective defenses are conducted in the inference phase. For example, motivated by classic detection and preprocessing defenses in the vision domain,[CITATION] explores how these methods can be directly transferred to this scenario and propose perplexity-based detection, paraphrasing as baseline defenses for GCG attack. Besides, prop\n\nCandidate Papers:\n1. Title: Certifying LLM Safety against Adversarial Prompting\n   Abstract: Certifying LLM Safety against Adversarial Prompting: Large language models (LLMs) are vulnerable to adversarial attacks that add malicious tokens to an input prompt to bypass the safety guardrails of an LLM and cause it to produce harmful content. In this work, we introduce erase-and-check, the first framework for defending against adversarial prompts with certifiable safety guarantees. Given a prompt, our procedure erases tokens individually and inspects the resulting subsequences using a safety filter. Our safety certificate guarantees that harmful prompts are not mislabeled as safe due to an adversarial attack up to a certain size. We implement the safety filter in two ways, using Llama 2 and DistilBERT, and compare the performance of erase-and-check for the two cases. We defend against three attack modes: i) adversarial suffix, where an adversarial sequence is appended at the end of a harmful prompt; ii) adversarial insertion, where the adversarial sequence is inserted anywhere in the middle of the prompt; and iii) adversarial infusion, where adversarial tokens are inserted at arbitrary positions in the prompt, not necessarily as a contiguous block. Our experimental results demonstrate that this procedure can obtain strong certified safety guarantees on harmful prompts while maintaining good empirical performance on safe prompts. Additionally, we propose three efficient empirical defenses: i) RandEC, a randomized subsampling version of erase-and-check; ii) GreedyEC, which greedily erases tokens that maximize the softmax score of the harmful class; and iii) GradEC, which uses gradient information to optimize tokens to erase. We demonstrate their effectiveness against adversarial prompts generated by the Greedy Coordinate Gradient (GCG) attack algorithm. The code for our experiments is available at https://github.com/aounon/certified-llm-safety.\n\n2. Title: Baseline Defenses for Adversarial Attacks Against Aligned Language Models\n   Abstract: Baseline Defenses for Adversarial Attacks Against Aligned Language Models: As Large Language Models quickly become ubiquitous, it becomes critical to understand their security vulnerabilities. Recent work shows that text optimizers can produce jailbreaking prompts that bypass moderation and alignment. Drawing from the rich body of work on adversarial machine learning, we approach these attacks with three questions: What threat models are practically useful in this domain? How do baseline defense techniques perform in this new domain? How does LLM security differ from computer vision?   We evaluate several baseline defense strategies against leading adversarial attacks on LLMs, discussing the various settings in which each is feasible and effective. Particularly, we look at three types of defenses: detection (perplexity based), input preprocessing (paraphrase and retokenization), and adversarial training. We discuss white-box and gray-box settings and discuss the robustness-performance trade-off for each of the defenses considered. We find that the weakness of existing discrete optimizers for text, combined with the relatively high costs of optimization, makes standard adaptive attacks more challenging for LLMs. Future research will be needed to uncover whether more powerful optimizers can be developed, or whether the strength of filtering and preprocessing defenses is greater in the LLMs domain than it has been in computer vision.\n\n3. Title: On the (Statistical) Detection of Adversarial Examples\n   Abstract: On the (Statistical) Detection of Adversarial Examples: Machine Learning (ML) models are applied in a variety of tasks such as network intrusion detection or Malware classification. Yet, these models are vulnerable to a class of malicious inputs known as adversarial examples. These are slightly perturbed inputs that are classified incorrectly by the ML model. The mitigation of these adversarial inputs remains an open problem. As a step towards understanding adversarial examples, we show that they are not drawn from the same distribution than the original data, and can thus be detected using statistical tests. Using thus knowledge, we introduce a complimentary approach to identify specific inputs that are adversarial. Specifically, we augment our ML model with an additional output, in which the model is trained to classify all adversarial inputs. We evaluate our approach on multiple adversarial example crafting methods (including the fast gradient sign and saliency map methods) with several datasets. The statistical test flags sample sets containing adversarial inputs confidently at sample sizes between 10 and 100 data points. Furthermore, our augmented model either detects adversarial examples as outliers with high accuracy (> 80%) or increases the adversary's cost - the perturbation added - by more than 150%. In this way, we show that statistical properties of adversarial examples are essential to their detection.\n\n4. Title: Gradient-based Adversarial Attacks against Text Transformers\n   Abstract: Gradient-based Adversarial Attacks against Text Transformers: We propose the first general-purpose gradient-based attack against transformer models. Instead of searching for a single adversarial example, we search for a distribution of adversarial examples parameterized by a continuous-valued matrix, hence enabling gradient-based optimization. We empirically demonstrate that our white-box attack attains state-of-the-art attack performance on a variety of natural language tasks. Furthermore, we show that a powerful black-box transfer attack, enabled by sampling from the adversarial distribution, matches or exceeds existing methods, while only requiring hard-label outputs.\n\n5. Title: Training language models to follow instructions with human feedback\n   Abstract: Training language models to follow instructions with human feedback: Making language models bigger does not inherently make them better at following a user's intent. For example, large language models can generate outputs that are untruthful, toxic, or simply not helpful to the user. In other words, these models are not aligned with their users. In this paper, we show an avenue for aligning language models with user intent on a wide range of tasks by fine-tuning with human feedback. Starting with a set of labeler-written prompts and prompts submitted through the OpenAI API, we collect a dataset of labeler demonstrations of the desired model behavior, which we use to fine-tune GPT-3 using supervised learning. We then collect a dataset of rankings of model outputs, which we use to further fine-tune this supervised model using reinforcement learning from human feedback. We call the resulting models InstructGPT. In human evaluations on our prompt distribution, outputs from the 1.3B parameter InstructGPT model are preferred to outputs from the 175B GPT-3, despite having 100x fewer parameters. Moreover, InstructGPT models show improvements in truthfulness and reductions in toxic output generation while having minimal performance regressions on public NLP datasets. Even though InstructGPT still makes simple mistakes, our results show that fine-tuning with human feedback is a promising direction for aligning language models with human intent.\n\n6. Title: Judging LLM-as-a-judge with MT-Bench and Chatbot Arena\n   Abstract: Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena: Evaluating large language model (LLM) based chat assistants is challenging due to their broad capabilities and the inadequacy of existing benchmarks in measuring human preferences. To address this, we explore using strong LLMs as judges to evaluate these models on more open-ended questions. We examine the usage and limitations of LLM-as-a-judge, including position, verbosity, and self-enhancement biases, as well as limited reasoning ability, and propose solutions to mitigate some of them. We then verify the agreement between LLM judges and human preferences by introducing two benchmarks: MT-bench, a multi-turn question set; and Chatbot Arena, a crowdsourced battle platform. Our results reveal that strong LLM judges like GPT-4 can match both controlled and crowdsourced human preferences well, achieving over 80% agreement, the same level of agreement between humans. Hence, LLM-as-a-judge is a scalable and explainable way to approximate human preferences, which are otherwise very expensive to obtain. Additionally, we show our benchmark and traditional benchmarks complement each other by evaluating several variants of LLaMA and Vicuna. The MT-bench questions, 3K expert votes, and 30K conversations with human preferences are publicly available at https://github.com/lm-sys/FastChat/tree/main/fastchat/llm_judge.\n\n7. Title: RAIN: Your Language Models Can Align Themselves without Finetuning\n   Abstract: RAIN: Your Language Models Can Align Themselves without Finetuning: Large language models (LLMs) often demonstrate inconsistencies with human preferences. Previous research typically gathered human preference data and then aligned the pre-trained models using reinforcement learning or instruction tuning, a.k.a. the finetuning step. In contrast, aligning frozen LLMs without requiring alignment data is more appealing. This work explores the potential of the latter setting. We discover that by integrating self-evaluation and rewind mechanisms, unaligned LLMs can directly produce responses consistent with human preferences via self-boosting. We introduce a novel inference method, Rewindable Auto-regressive INference (RAIN), that allows pre-trained LLMs to evaluate their own generation and use the evaluation results to guide rewind and generation for AI safety. Notably, RAIN operates without the need of extra data for model alignment and abstains from any training, gradient computation, or parameter updates. Experimental results evaluated by GPT-4 and humans demonstrate the effectiveness of RAIN: on the HH dataset, RAIN improves the harmlessness rate of LLaMA 30B from 82% of vanilla inference to 97%, while maintaining the helpfulness rate. On the TruthfulQA dataset, RAIN improves the truthfulness of the already-well-aligned LLaMA-2-chat 13B model by 5%.\n\n8. Title: Universal and Transferable Adversarial Attacks on Aligned Language Models\n   Abstract: Universal and Transferable Adversarial Attacks on Aligned Language Models: Because \"out-of-the-box\" large language models are capable of generating a great deal of objectionable content, recent work has focused on aligning these models in an attempt to prevent undesirable generation. While there has been some success at circumventing these measures -- so-called \"jailbreaks\" against LLMs -- these attacks have required significant human ingenuity and are brittle in practice. In this paper, we propose a simple and effective attack method that causes aligned language models to generate objectionable behaviors. Specifically, our approach finds a suffix that, when attached to a wide range of queries for an LLM to produce objectionable content, aims to maximize the probability that the model produces an affirmative response (rather than refusing to answer). However, instead of relying on manual engineering, our approach automatically produces these adversarial suffixes by a combination of greedy and gradient-based search techniques, and also improves over past automatic prompt generation methods.   Surprisingly, we find that the adversarial prompts generated by our approach are quite transferable, including to black-box, publicly released LLMs. Specifically, we train an adversarial attack suffix on multiple prompts (i.e., queries asking for many different types of objectionable content), as well as multiple models (in our case, Vicuna-7B and 13B). When doing so, the resulting attack suffix is able to induce objectionable content in the public interfaces to ChatGPT, Bard, and Claude, as well as open source LLMs such as LLaMA-2-Chat, Pythia, Falcon, and others. In total, this work significantly advances the state-of-the-art in adversarial attacks against aligned language models, raising important questions about how such systems can be prevented from producing objectionable information. Code is available at github.com/llm-attacks/llm-attacks.\n\n9. Title: Towards deep learning models resistant to adversarial attacks\n   Abstract: Towards Deep Learning Models Resistant to Adversarial Attacks: Recent work has demonstrated that deep neural networks are vulnerable to adversarial examples---inputs that are almost indistinguishable from natural data and yet classified incorrectly by the network. In fact, some of the latest findings suggest that the existence of adversarial attacks may be an inherent weakness of deep learning models. To address this problem, we study the adversarial robustness of neural networks through the lens of robust optimization. This approach provides us with a broad and unifying view on much of the prior work on this topic. Its principled nature also enables us to identify methods for both training and attacking neural networks that are reliable and, in a certain sense, universal. In particular, they specify a concrete security guarantee that would protect against any adversary. These methods let us train networks with significantly improved resistance to a wide range of adversarial attacks. They also suggest the notion of security against a first-order adversary as a natural and broad security guarantee. We believe that robustness against such well-defined classes of adversaries is an important stepping stone towards fully resistant deep learning models. Code and pre-trained models are available at https://github.com/MadryLab/mnist_challenge and https://github.com/MadryLab/cifar10_challenge.\n\nThink step-by-step about which candidate best matches the context. Summarize your reasoning, then provide the exact title that should fill the citation. Respond in JSON with fields:\n{\n  \"reasoning\": \"<your step-by-step analysis>\",\n  \"selected_title\": \"<exact candidate title>\"\n}", "response": {"reasoning": "The citation context discusses existing defenses against adversarial attacks, specifically mentioning perplexity-based detection and paraphrasing as baseline defenses for the GCG attack. Among the candidate papers, the second one titled 'Baseline Defenses for Adversarial Attacks Against Aligned Language Models' directly addresses baseline defense techniques, including detection methods (perplexity-based) and input preprocessing (paraphrase), which aligns closely with the context's focus on transferring classic detection and preprocessing defenses to the scenario of GCG attacks. The first paper, while discussing adversarial prompts, does not specifically mention the baseline defenses or the methods highlighted in the context. The other candidates do not focus on the specific defenses mentioned in the citation context. Therefore, the second paper is the best match.", "selected_title": "Baseline Defenses for Adversarial Attacks Against Aligned Language Models"}, "selected_title": "Baseline Defenses for Adversarial Attacks Against Aligned Language Models", "positive_title": "Baseline Defenses for Adversarial Attacks Against Aligned Language Models", "is_correct": true, "reasoning": "The citation context discusses existing defenses against adversarial attacks, specifically mentioning perplexity-based detection and paraphrasing as baseline defenses for the GCG attack. Among the candidate papers, the second one titled 'Baseline Defenses for Adversarial Attacks Against Aligned Language Models' directly addresses baseline defense techniques, including detection methods (perplexity-based) and input preprocessing (paraphrase), which aligns closely with the context's focus on transferring classic detection and preprocessing defenses to the scenario of GCG attacks. The first paper, while discussing adversarial prompts, does not specifically mention the baseline defenses or the methods highlighted in the context. The other candidates do not focus on the specific defenses mentioned in the citation context. Therefore, the second paper is the best match."}
{"example_id": "blind_2317", "prompt": "You are an expert citation retrieval system as described in the paper \"Multi-Agent System for Reliable Citation Retrieval\".\nYour goal is to autonomously retrieve, verify, and recommend academic references given a query or document excerpt.\n\nTask:\nGiven a citation context from a scientific paper (where a citation is missing), identify the correct paper from a list of candidates.\nAnalyze the context to understand the specific claim, method, or result being cited.\nThen, evaluate each candidate paper to see if it matches the context.\nFinally, select the best matching paper.\n\nCitation Context:\nis precisely due to its real-time high-quality rendering capabilities that 3DGS has been applied to various domains, including autonomous driving, content generation, and 4D dynamic scenes[CITATION], among others. Despite these advancements, 3DGS still has some drawbacks, such as the storage of Gaussians and handling multi-resolution, and so on. Several works have enhanced 3DGS by improving Gau\n\nCandidate Papers:\n1. Title: 3d gaussian splatting for real-time radiance field rendering\n   Abstract: 3D Gaussian Splatting for Real-Time Radiance Field Rendering: Radiance Field methods have recently revolutionized novel-view synthesis of scenes captured with multiple photos or videos. However, achieving high visual quality still requires neural networks that are costly to train and render, while recent faster methods inevitably trade off speed for quality. For unbounded and complete scenes (rather than isolated objects) and 1080p resolution rendering, no current method can achieve real-time display rates. We introduce three key elements that allow us to achieve state-of-the-art visual quality while maintaining competitive training times and importantly allow high-quality real-time (>= 30 fps) novel-view synthesis at 1080p resolution. First, starting from sparse points produced during camera calibration, we represent the scene with 3D Gaussians that preserve desirable properties of continuous volumetric radiance fields for scene optimization while avoiding unnecessary computation in empty space; Second, we perform interleaved optimization/density control of the 3D Gaussians, notably optimizing anisotropic covariance to achieve an accurate representation of the scene; Third, we develop a fast visibility-aware rendering algorithm that supports anisotropic splatting and both accelerates training and allows realtime rendering. We demonstrate state-of-the-art visual quality and real-time rendering on several established datasets.\n\n2. Title: Mip-nerf 360: Unbounded anti-aliased neural radiance fields\n   Abstract: Mip-NeRF 360: Unbounded Anti-Aliased Neural Radiance Fields: Though neural radiance fields (NeRF) have demonstrated impressive view synthesis results on objects and small bounded regions of space, they struggle on \"unbounded\" scenes, where the camera may point in any direction and content may exist at any distance. In this setting, existing NeRF-like models often produce blurry or low-resolution renderings (due to the unbalanced detail and scale of nearby and distant objects), are slow to train, and may exhibit artifacts due to the inherent ambiguity of the task of reconstructing a large scene from a small set of images. We present an extension of mip-NeRF (a NeRF variant that addresses sampling and aliasing) that uses a non-linear scene parameterization, online distillation, and a novel distortion-based regularizer to overcome the challenges presented by unbounded scenes. Our model, which we dub \"mip-NeRF 360\" as we target scenes in which the camera rotates 360 degrees around a point, reduces mean-squared error by 57% compared to mip-NeRF, and is able to produce realistic synthesized views and detailed depth maps for highly intricate, unbounded real-world scenes.\n\n3. Title: Mega-nerf: Scalable construction of large-scale nerfs for virtual fly-throughs\n   Abstract: Mega-NeRF: Scalable Construction of Large-Scale NeRFs for Virtual Fly-Throughs: We use neural radiance fields (NeRFs) to build interactive 3D environments from large-scale visual captures spanning buildings or even multiple city blocks collected primarily from drones. In contrast to single object scenes (on which NeRFs are traditionally evaluated), our scale poses multiple challenges including (1) the need to model thousands of images with varying lighting conditions, each of which capture only a small subset of the scene, (2) prohibitively large model capacities that make it infeasible to train on a single GPU, and (3) significant challenges for fast rendering that would enable interactive fly-throughs.   To address these challenges, we begin by analyzing visibility statistics for large-scale scenes, motivating a sparse network structure where parameters are specialized to different regions of the scene. We introduce a simple geometric clustering algorithm for data parallelism that partitions training images (or rather pixels) into different NeRF submodules that can be trained in parallel.   We evaluate our approach on existing datasets (Quad 6k and UrbanScene3D) as well as against our own drone footage, improving training speed by 3x and PSNR by 12%. We also evaluate recent NeRF fast renderers on top of Mega-NeRF and introduce a novel method that exploits temporal coherence. Our technique achieves a 40x speedup over conventional NeRF rendering while remaining within 0.8 db in PSNR quality, exceeding the fidelity of existing fast renderers.\n\n4. Title: Unisim: A neural closed-loop sensor simulator\n   Abstract: UniSim: A Neural Closed-Loop Sensor Simulator: Rigorously testing autonomy systems is essential for making safe self-driving vehicles (SDV) a reality. It requires one to generate safety critical scenarios beyond what can be collected safely in the world, as many scenarios happen rarely on public roads. To accurately evaluate performance, we need to test the SDV on these scenarios in closed-loop, where the SDV and other actors interact with each other at each timestep. Previously recorded driving logs provide a rich resource to build these new scenarios from, but for closed loop evaluation, we need to modify the sensor data based on the new scene configuration and the SDV's decisions, as actors might be added or removed and the trajectories of existing actors and the SDV will differ from the original log. In this paper, we present UniSim, a neural sensor simulator that takes a single recorded log captured by a sensor-equipped vehicle and converts it into a realistic closed-loop multi-sensor simulation. UniSim builds neural feature grids to reconstruct both the static background and dynamic actors in the scene, and composites them together to simulate LiDAR and camera data at new viewpoints, with actors added or removed and at new placements. To better handle extrapolated views, we incorporate learnable priors for dynamic objects, and leverage a convolutional network to complete unseen regions. Our experiments show UniSim can simulate realistic sensor data with small domain gap on downstream tasks. With UniSim, we demonstrate closed-loop evaluation of an autonomy system on safety-critical scenarios as if it were in the real world.\n\n5. Title: Instant neural graphics primitives with a multiresolution hash encoding\n   Abstract: Instant Neural Graphics Primitives with a Multiresolution Hash Encoding: Neural graphics primitives, parameterized by fully connected neural networks, can be costly to train and evaluate. We reduce this cost with a versatile new input encoding that permits the use of a smaller network without sacrificing quality, thus significantly reducing the number of floating point and memory access operations: a small neural network is augmented by a multiresolution hash table of trainable feature vectors whose values are optimized through stochastic gradient descent. The multiresolution structure allows the network to disambiguate hash collisions, making for a simple architecture that is trivial to parallelize on modern GPUs. We leverage this parallelism by implementing the whole system using fully-fused CUDA kernels with a focus on minimizing wasted bandwidth and compute operations. We achieve a combined speedup of several orders of magnitude, enabling training of high-quality neural graphics primitives in a matter of seconds, and rendering in tens of milliseconds at a resolution of ${1920\\!\\times\\!1080}$.\n\n6. Title: Neural point-based graphics\n   Abstract: Neural Point-Based Graphics: We present a new point-based approach for modeling the appearance of real scenes. The approach uses a raw point cloud as the geometric representation of a scene, and augments each point with a learnable neural descriptor that encodes local geometry and appearance. A deep rendering network is learned in parallel with the descriptors, so that new views of the scene can be obtained by passing the rasterizations of a point cloud from new viewpoints through this network. The input rasterizations use the learned descriptors as point pseudo-colors. We show that the proposed approach can be used for modeling complex scenes and obtaining their photorealistic views, while avoiding explicit surface estimation and meshing. In particular, compelling results are obtained for scene scanned using hand-held commodity RGB-D sensors as well as standard RGB cameras even in the presence of objects that are challenging for standard mesh-based modeling.\n\n7. Title: Monosdf: Exploring monocular geometric cues for neural implicit surface reconstruction\n   Abstract: MonoSDF: Exploring Monocular Geometric Cues for Neural Implicit Surface Reconstruction: In recent years, neural implicit surface reconstruction methods have become popular for multi-view 3D reconstruction. In contrast to traditional multi-view stereo methods, these approaches tend to produce smoother and more complete reconstructions due to the inductive smoothness bias of neural networks. State-of-the-art neural implicit methods allow for high-quality reconstructions of simple scenes from many input views. Yet, their performance drops significantly for larger and more complex scenes and scenes captured from sparse viewpoints. This is caused primarily by the inherent ambiguity in the RGB reconstruction loss that does not provide enough constraints, in particular in less-observed and textureless areas. Motivated by recent advances in the area of monocular geometry prediction, we systematically explore the utility these cues provide for improving neural implicit surface reconstruction. We demonstrate that depth and normal cues, predicted by general-purpose monocular estimators, significantly improve reconstruction quality and optimization time. Further, we analyse and investigate multiple design choices for representing neural implicit surfaces, ranging from monolithic MLP models over single-grid to multi-resolution grid representations. We observe that geometric monocular priors improve performance both for small-scale single-object as well as large-scale multi-object scenes, independent of the choice of representation.\n\n8. Title: Mip-nerf: A multiscale representation for anti-aliasing neural radiance fields\n   Abstract: Mip-NeRF: A Multiscale Representation for Anti-Aliasing Neural Radiance Fields: The rendering procedure used by neural radiance fields (NeRF) samples a scene with a single ray per pixel and may therefore produce renderings that are excessively blurred or aliased when training or testing images observe scene content at different resolutions. The straightforward solution of supersampling by rendering with multiple rays per pixel is impractical for NeRF, because rendering each ray requires querying a multilayer perceptron hundreds of times. Our solution, which we call \"mip-NeRF\" (a la \"mipmap\"), extends NeRF to represent the scene at a continuously-valued scale. By efficiently rendering anti-aliased conical frustums instead of rays, mip-NeRF reduces objectionable aliasing artifacts and significantly improves NeRF's ability to represent fine details, while also being 7% faster than NeRF and half the size. Compared to NeRF, mip-NeRF reduces average error rates by 17% on the dataset presented with NeRF and by 60% on a challenging multiscale variant of that dataset that we present. Mip-NeRF is also able to match the accuracy of a brute-force supersampled NeRF on our multiscale dataset while being 22x faster.\n\n9. Title: Spacetime gaussian feature splatting for real-time dynamic view synthesis\n   Abstract: Spacetime Gaussian Feature Splatting for Real-Time Dynamic View Synthesis: Novel view synthesis of dynamic scenes has been an intriguing yet challenging problem. Despite recent advancements, simultaneously achieving high-resolution photorealistic results, real-time rendering, and compact storage remains a formidable task. To address these challenges, we propose Spacetime Gaussian Feature Splatting as a novel dynamic scene representation, composed of three pivotal components. First, we formulate expressive Spacetime Gaussians by enhancing 3D Gaussians with temporal opacity and parametric motion/rotation. This enables Spacetime Gaussians to capture static, dynamic, as well as transient content within a scene. Second, we introduce splatted feature rendering, which replaces spherical harmonics with neural features. These features facilitate the modeling of view- and time-dependent appearance while maintaining small size. Third, we leverage the guidance of training error and coarse depth to sample new Gaussians in areas that are challenging to converge with existing pipelines. Experiments on several established real-world datasets demonstrate that our method achieves state-of-the-art rendering quality and speed, while retaining compact storage. At 8K resolution, our lite-version model can render at 60 FPS on an Nvidia RTX 4090 GPU. Our code is available at https://github.com/oppo-us-research/SpacetimeGaussians.\n\n10. Title: Pixel-GS: Density Control with Pixel-aware Gradient for 3D Gaussian Splatting\n   Abstract: Pixel-GS: Density Control with Pixel-aware Gradient for 3D Gaussian Splatting: 3D Gaussian Splatting (3DGS) has demonstrated impressive novel view synthesis results while advancing real-time rendering performance. However, it relies heavily on the quality of the initial point cloud, resulting in blurring and needle-like artifacts in areas with insufficient initializing points. This is mainly attributed to the point cloud growth condition in 3DGS that only considers the average gradient magnitude of points from observable views, thereby failing to grow for large Gaussians that are observable for many viewpoints while many of them are only covered in the boundaries. To this end, we propose a novel method, named Pixel-GS, to take into account the number of pixels covered by the Gaussian in each view during the computation of the growth condition. We regard the covered pixel numbers as the weights to dynamically average the gradients from different views, such that the growth of large Gaussians can be prompted. As a result, points within the areas with insufficient initializing points can be grown more effectively, leading to a more accurate and detailed reconstruction. In addition, we propose a simple yet effective strategy to scale the gradient field according to the distance to the camera, to suppress the growth of floaters near the camera. Extensive experiments both qualitatively and quantitatively demonstrate that our method achieves state-of-the-art rendering quality while maintaining real-time rendering speed, on the challenging Mip-NeRF 360 and Tanks & Temples datasets.\n\nThink step-by-step about which candidate best matches the context. Summarize your reasoning, then provide the exact title that should fill the citation. Respond in JSON with fields:\n{\n  \"reasoning\": \"<your step-by-step analysis>\",\n  \"selected_title\": \"<exact candidate title>\"\n}", "response": {"reasoning": "The citation context discusses the application of 3D Gaussian Splatting (3DGS) in various domains, highlighting its real-time high-quality rendering capabilities and mentioning some drawbacks related to Gaussian storage and multi-resolution handling. Among the candidate papers, the one titled 'Pixel-GS: Density Control with Pixel-aware Gradient for 3D Gaussian Splatting' directly addresses 3DGS and its challenges, specifically focusing on improving the quality of the initial point cloud and rendering performance. This aligns closely with the context's mention of drawbacks and enhancements in 3DGS. Other candidates, while related to rendering and neural graphics, do not specifically focus on 3DGS or its improvements as directly as the selected paper does.", "selected_title": "Pixel-GS: Density Control with Pixel-aware Gradient for 3D Gaussian Splatting"}, "selected_title": "Pixel-GS: Density Control with Pixel-aware Gradient for 3D Gaussian Splatting", "positive_title": "Spacetime gaussian feature splatting for real-time dynamic view synthesis", "is_correct": false, "reasoning": "The citation context discusses the application of 3D Gaussian Splatting (3DGS) in various domains, highlighting its real-time high-quality rendering capabilities and mentioning some drawbacks related to Gaussian storage and multi-resolution handling. Among the candidate papers, the one titled 'Pixel-GS: Density Control with Pixel-aware Gradient for 3D Gaussian Splatting' directly addresses 3DGS and its challenges, specifically focusing on improving the quality of the initial point cloud and rendering performance. This aligns closely with the context's mention of drawbacks and enhancements in 3DGS. Other candidates, while related to rendering and neural graphics, do not specifically focus on 3DGS or its improvements as directly as the selected paper does."}
{"example_id": "blind_295", "prompt": "You are an expert citation retrieval system as described in the paper \"Multi-Agent System for Reliable Citation Retrieval\".\nYour goal is to autonomously retrieve, verify, and recommend academic references given a query or document excerpt.\n\nTask:\nGiven a citation context from a scientific paper (where a citation is missing), identify the correct paper from a list of candidates.\nAnalyze the context to understand the specific claim, method, or result being cited.\nThen, evaluate each candidate paper to see if it matches the context.\nFinally, select the best matching paper.\n\nCitation Context:\nults well demonstrate that both proposed optimizations bring substantial improvements to unsup-SimCSE. \\end{itemize} Related Work Unsupervised sentence representation learning has been widely studied.[CITATION] propose to learn sentence representation according to the internal structure of each sentence. predict the surrounding sentences of a given sentence based on the distribution hypothesis.<|c\n\nCandidate Papers:\n1. Title: Simcse: Simple contrastive learning of sentence embeddings\n   Abstract: SimCSE: Simple Contrastive Learning of Sentence Embeddings: This paper presents SimCSE, a simple contrastive learning framework that greatly advances state-of-the-art sentence embeddings. We first describe an unsupervised approach, which takes an input sentence and predicts itself in a contrastive objective, with only standard dropout used as noise. This simple method works surprisingly well, performing on par with previous supervised counterparts. We find that dropout acts as minimal data augmentation, and removing it leads to a representation collapse. Then, we propose a supervised approach, which incorporates annotated pairs from natural language inference datasets into our contrastive learning framework by using \"entailment\" pairs as positives and \"contradiction\" pairs as hard negatives. We evaluate SimCSE on standard semantic textual similarity (STS) tasks, and our unsupervised and supervised models using BERT base achieve an average of 76.3% and 81.6% Spearman's correlation respectively, a 4.2% and 2.2% improvement compared to the previous best results. We also show -- both theoretically and empirically -- that the contrastive learning objective regularizes pre-trained embeddings' anisotropic space to be more uniform, and it better aligns positive pairs when supervised signals are available.\n\n2. Title: Consert: A contrastive framework for self-supervised sentence representation transfer\n   Abstract: ConSERT: A Contrastive Framework for Self-Supervised Sentence Representation Transfer: Learning high-quality sentence representations benefits a wide range of natural language processing tasks. Though BERT-based pre-trained language models achieve high performance on many downstream tasks, the native derived sentence representations are proved to be collapsed and thus produce a poor performance on the semantic textual similarity (STS) tasks. In this paper, we present ConSERT, a Contrastive Framework for Self-Supervised Sentence Representation Transfer, that adopts contrastive learning to fine-tune BERT in an unsupervised and effective way. By making use of unlabeled texts, ConSERT solves the collapse issue of BERT-derived sentence representations and make them more applicable for downstream tasks. Experiments on STS datasets demonstrate that ConSERT achieves an 8\\% relative improvement over the previous state-of-the-art, even comparable to the supervised SBERT-NLI. And when further incorporating NLI supervision, we achieve new state-of-the-art performance on STS tasks. Moreover, ConSERT obtains comparable results with only 1000 samples available, showing its robustness in data scarcity scenarios.\n\n3. Title: An unsupervised sentence embedding method by mutual information maximization\n   Abstract: An Unsupervised Sentence Embedding Method by Mutual Information Maximization: BERT is inefficient for sentence-pair tasks such as clustering or semantic search as it needs to evaluate combinatorially many sentence pairs which is very time-consuming. Sentence BERT (SBERT) attempted to solve this challenge by learning semantically meaningful representations of single sentences, such that similarity comparison can be easily accessed. However, SBERT is trained on corpus with high-quality labeled sentence pairs, which limits its application to tasks where labeled data is extremely scarce. In this paper, we propose a lightweight extension on top of BERT and a novel self-supervised learning objective based on mutual information maximization strategies to derive meaningful sentence embeddings in an unsupervised manner. Unlike SBERT, our method is not restricted by the availability of labeled data, such that it can be applied on different domain-specific corpus. Experimental results show that the proposed method significantly outperforms other unsupervised sentence embedding baselines on common semantic textual similarity (STS) tasks and downstream supervised tasks. It also outperforms SBERT in a setting where in-domain labeled data is not available, and achieves performance competitive with supervised methods on various tasks.\n\n4. Title: Learning distributed representations of sentences from unlabelled data\n   Abstract: Learning Distributed Representations of Sentences from Unlabelled Data: Unsupervised methods for learning distributed representations of words are ubiquitous in today's NLP research, but far less is known about the best ways to learn distributed phrase or sentence representations from unlabelled data. This paper is a systematic comparison of models that learn such representations. We find that the optimal approach depends critically on the intended application. Deeper, more complex models are preferable for representations to be used in supervised systems, but shallow log-linear models work best for building representation spaces that can be decoded with simple spatial distance metrics. We also propose two new unsupervised representation-learning objectives designed to optimise the trade-off between training time, domain portability and performance.\n\n5. Title: Clear: Contrastive learning for sentence representation\n   Abstract: CLEAR: Contrastive Learning for Sentence Representation: Pre-trained language models have proven their unique powers in capturing implicit language features. However, most pre-training approaches focus on the word-level training objective, while sentence-level objectives are rarely studied. In this paper, we propose Contrastive LEArning for sentence Representation (CLEAR), which employs multiple sentence-level augmentation strategies in order to learn a noise-invariant sentence representation. These augmentations include word and span deletion, reordering, and substitution. Furthermore, we investigate the key reasons that make contrastive learning effective through numerous experiments. We observe that different sentence augmentations during pre-training lead to different performance improvements on various downstream tasks. Our approach is shown to outperform multiple existing methods on both SentEval and GLUE benchmarks.\n\n6. Title: A simple framework for contrastive learning of visual representations\n   Abstract: A Simple Framework for Contrastive Learning of Visual Representations: This paper presents SimCLR: a simple framework for contrastive learning of visual representations. We simplify recently proposed contrastive self-supervised learning algorithms without requiring specialized architectures or a memory bank. In order to understand what enables the contrastive prediction tasks to learn useful representations, we systematically study the major components of our framework. We show that (1) composition of data augmentations plays a critical role in defining effective predictive tasks, (2) introducing a learnable nonlinear transformation between the representation and the contrastive loss substantially improves the quality of the learned representations, and (3) contrastive learning benefits from larger batch sizes and more training steps compared to supervised learning. By combining these findings, we are able to considerably outperform previous methods for self-supervised and semi-supervised learning on ImageNet. A linear classifier trained on self-supervised representations learned by SimCLR achieves 76.5% top-1 accuracy, which is a 7% relative improvement over previous state-of-the-art, matching the performance of a supervised ResNet-50. When fine-tuned on only 1% of the labels, we achieve 85.8% top-5 accuracy, outperforming AlexNet with 100X fewer labels.\n\n7. Title: Bert: Pre-training of deep bidirectional transformers for language understanding\n   Abstract: BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding: We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models, BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications.   BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5% (7.7% point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).\n\n8. Title: An efficient framework for learning sentence representations\n   Abstract: An efficient framework for learning sentence representations: In this work we propose a simple and efficient framework for learning sentence representations from unlabelled data. Drawing inspiration from the distributional hypothesis and recent work on learning sentence representations, we reformulate the problem of predicting the context in which a sentence appears as a classification problem. Given a sentence and its context, a classifier distinguishes context sentences from other contrastive sentences based on their vector representations. This allows us to efficiently learn different types of encoding functions, and we show that the model learns high-quality sentence representations. We demonstrate that our sentence representations outperform state-of-the-art unsupervised and supervised representation learning methods on several downstream NLP tasks that involve understanding sentence semantics while achieving an order of magnitude speedup in training time.\n\n9. Title: Momentum contrast for unsupervised visual representation learning\n   Abstract: Momentum Contrast for Unsupervised Visual Representation Learning: We present Momentum Contrast (MoCo) for unsupervised visual representation learning. From a perspective on contrastive learning as dictionary look-up, we build a dynamic dictionary with a queue and a moving-averaged encoder. This enables building a large and consistent dictionary on-the-fly that facilitates contrastive unsupervised learning. MoCo provides competitive results under the common linear protocol on ImageNet classification. More importantly, the representations learned by MoCo transfer well to downstream tasks. MoCo can outperform its supervised pre-training counterpart in 7 detection/segmentation tasks on PASCAL VOC, COCO, and other datasets, sometimes surpassing it by large margins. This suggests that the gap between unsupervised and supervised representation learning has been largely closed in many vision tasks.\n\nThink step-by-step about which candidate best matches the context. Summarize your reasoning, then provide the exact title that should fill the citation. Respond in JSON with fields:\n{\n  \"reasoning\": \"<your step-by-step analysis>\",\n  \"selected_title\": \"<exact candidate title>\"\n}", "response": {"reasoning": "The citation context discusses optimizations that improve unsupervised SimCSE, specifically mentioning learning sentence representations based on the internal structure of sentences and predicting surrounding sentences. Among the candidate papers, the first one, 'Simcse: Simple contrastive learning of sentence embeddings,' directly addresses the SimCSE framework and its unsupervised approach to learning sentence embeddings. It also discusses improvements in performance metrics, which aligns with the context's mention of substantial improvements. The second candidate, 'Consert: A contrastive framework for self-supervised sentence representation transfer,' while relevant, focuses more on the transfer aspect and does not specifically mention the internal structure of sentences or the surrounding sentence prediction. The third candidate discusses a different method based on mutual information maximization, which does not align with the context. The fourth and fifth candidates also do not directly relate to the specific claims made in the context. Therefore, the first candidate is the best match as it directly relates to the SimCSE framework and the improvements mentioned in the citation context.", "selected_title": "Simcse: Simple contrastive learning of sentence embeddings"}, "selected_title": "Simcse: Simple contrastive learning of sentence embeddings", "positive_title": "Learning distributed representations of sentences from unlabelled data", "is_correct": false, "reasoning": "The citation context discusses optimizations that improve unsupervised SimCSE, specifically mentioning learning sentence representations based on the internal structure of sentences and predicting surrounding sentences. Among the candidate papers, the first one, 'Simcse: Simple contrastive learning of sentence embeddings,' directly addresses the SimCSE framework and its unsupervised approach to learning sentence embeddings. It also discusses improvements in performance metrics, which aligns with the context's mention of substantial improvements. The second candidate, 'Consert: A contrastive framework for self-supervised sentence representation transfer,' while relevant, focuses more on the transfer aspect and does not specifically mention the internal structure of sentences or the surrounding sentence prediction. The third candidate discusses a different method based on mutual information maximization, which does not align with the context. The fourth and fifth candidates also do not directly relate to the specific claims made in the context. Therefore, the first candidate is the best match as it directly relates to the SimCSE framework and the improvements mentioned in the citation context."}
m unlabelled data but does not specifically mention sentence representation learning in the context of SimCSE. The fifth candidate, 'Clear: Contrastive learning for sentence representation,' discusses contrastive learning but does not mention SimCSE or the specific improvements related to it. Therefore, the first candidate is the best match as it directly relates to the context of unsupervised SimCSE and the improvements mentioned."}
