id,excerpt,target_paper_title,target_paper_url,source_paper_title,source_paper_url,year,split
301,"Cascaded diffusion models with noise conditioning augmentation [CITATION] have been extremely effective in progressively generating high-fidelity images",Cascaded diffusion models for high fidelity image generation,https://arxiv.org/pdf/2106.15282,"Photorealistic Text-to-Image Diffusion Models with Deep Language Understanding",https://arxiv.org/pdf/2205.11487.pdf,2022,test
302,"Hence, [CITATION] trained a network in a sliding-window setup to predict the class label of each pixel by providing a local region (patch) around that pixel as input.",Deep neural networks segment neuronal membranes in electron microscopy images,https://papers.nips.cc/paper_files/paper/2012/file/459a4ddcb586f24efd9395aa7662bc7c-Paper.pdf,"U-Net: Convolutional Networks for Biomedical Image Segmentation",https://arxiv.org/pdf/1505.04597,2015,train
303,"[CITATION] applied the self-attention only in local neighborhoods for each query pixel instead of globally",Image Transformer,https://arxiv.org/pdf/1802.05751,"AN IMAGE IS WORTH 16X16 WORDS:TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE",https://arxiv.org/pdf/2010.11929,2021,test
304,"Empirically, previous work has found that LSTM language models use 200 context words on average [CITATION], indicating room for further improvement.","Sharp nearby, fuzzy far away: How neural language models use context.",https://arxiv.org/pdf/1805.04623,Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context,https://arxiv.org/pdf/1901.02860,2019,test
305,"For knowledge-intensive generation, we experiment with MS-MARCO [CITATION] and Jeopardy question generation, and we find that our models generate responses that are more factual, specific, and diverse than a BART baseline","MS MARCO: A Human Generated MAchine Reading COmprehension Dataset",https://arxiv.org/pdf/1611.09268,Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks,https://arxiv.org/pdf/2005.11401,2021,train
306,"In order to find more informative negatives, we adopt the hard negative mining strategy by [CITATION], where negatives pairs with higher contrastive similarity in a batch are more likely to be selected to compute the loss","Align before Fuse: Vision and Language Representation Learning with Momentum Distillation",https://arxiv.org/pdf/2107.07651,"BLIP: Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation",https://arxiv.org/pdf/2201.12086.pdf,2022,test
307,Our experiments employed the Pythia-160M [CITATION] codebase and followed its training recipe.,"Pythia: A Suite for Analyzing Large Language Models Across Training and Scaling",https://arxiv.org/pdf/2304.01373,"EFFICIENT STREAMING LANGUAGE MODELS WITH ATTENTION SINKS",https://arxiv.org/pdf/2309.17453.pdf,2024,test
308,"Similarly to [CITATION], we find that our SFT models overfit on validation loss after 1 epoch; however, we find that training for more epochs helps both the RM score and human preference ratings, despite this overfitting.",Recursively summarizing books with human feedback,https://arxiv.org/pdf/2109.10862,Training Language Models to Follow Instructions with Human Feedback,https://arxiv.org/pdf/2203.02155.pdf,2022,train
309,"Moreover, as a promising signal, our bigger self-supervised ViT can achieve better accuracy, unlike the ImageNet-supervised ViT in [CITATION] whose accuracy degrades if getting bigger",An image is worth 16x16 words: Transformers for image recognition at scale.,https://arxiv.org/pdf/2010.11929,An Empirical Study of Training Self-Supervised Vision Transformers,https://arxiv.org/pdf/2104.02057.pdf,2021,test
310,"[CITATION] task is a large-scale reading comprehension dataset with more than 28,000 passages and nearly 100,000 questions",RACE: Large-scale ReAding Comprehension Dataset From Examinations,https://arxiv.org/pdf/1704.04683,RoBERTa: A Robustly Optimized BERT Pretraining Approach,https://arxiv.org/pdf/1907.11692.pdf,2019,test
311,It is a popular choice in recent works [CITATION] to use a cosine learning rate decay,"SGDR: STOCHASTIC GRADIENT DESCENT WITH WARM RESTARTS",https://arxiv.org/pdf/1608.03983,FixMatch: Simplifying Semi-Supervised Learning with Consistency and Confidence,https://arxiv.org/pdf/2001.07685.pdf,2020,train
312,"While a small blocksize is required for precise 4-bit quantization [CITATION], it also has a considerable memory overhead",The case for 4-bit precision: k-bit inference scaling laws,https://arxiv.org/pdf/2212.09720,QLoRA: Efficient Finetuning of Quantized LLMs,https://arxiv.org/pdf/2305.14314.pdf,2023,test
313,"[CITATION] show that strong data augmentation can improve out-of-distribution generalization while not impacting in-distribution generalization","When Unseen Domain Generalization is Unnecessary? Rethinking Data Augmentation",https://arxiv.org/pdf/1906.03347,DomainBed: A Testbed for Domain Generalization,https://arxiv.org/pdf/2007.01434.pdf,2020,train
314,"We implement the fixed pattern for sparse attention as in [CITATION] with multiple heads attending to distinct subblocks",Generating long sequences with sparse transformers,https://arxiv.org/pdf/1904.10509,LongNet: Scaling Transformers to 1 Billion Tokens,https://arxiv.org/pdf/2307.02486.pdf,2023,test
315,"The losses that are commonly used to train classification models, such as cross-entropy and squared error, are proper scoring rules [CITATION] and are therefore guaranteed to yield perfectly calibrated models at their minimumâ€”in the infinite-data limit","Probabilistic forecasts, calibration and sharpness",https://sites.stat.washington.edu/raftery/Research/PDF/Gneiting2007jrssb.pdf,Revisiting the Calibration of Modern Neural Networks,https://proceedings.neurips.cc/paper/2021/file/8420d359404024567b5aefda1231af24-Paper.pdf,2021,test
