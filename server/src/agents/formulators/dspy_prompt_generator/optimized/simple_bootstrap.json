{
  "retrieve.predict": {
    "traces": [],
    "train": [],
    "demos": [
      {
        "augmented": true,
        "citation_context": "the least important ones out. There are a lot more structured pruning algorithms, we refer to for a more comprehensive survey. \\subsection{Sparsity Combination Search} Experiments in AMC[CITATION] showed that the reinforcement learning based channel pruning beat other handcrafted policy by a large margin. Following this line of development, lots of new approaches surfaced. The genetic algorith",
        "candidate_papers": "1. Title: Discrimination-aware channel pruning for deep neural networks\n   Abstract: Discrimination-aware Channel Pruning for Deep Neural Networks: Channel pruning is one of the predominant approaches for deep model compression. Existing pruning methods either train from scratch with sparsity constraints on channels, or minimize the reconstruction error between the pre-trained featu...\n\n2. Title: DARTS: Differentiable Architecture Search\n   Abstract: DARTS: Differentiable Architecture Search: This paper addresses the scalability challenge of architecture search by formulating the task in a differentiable manner. Unlike conventional approaches of applying evolution or reinforcement learning over a discrete and non-differentiable search space, our...\n\n3. Title: Fbnet: Hardware-aware efficient convnet design via differentiable neural architecture search\n   Abstract: FBNet: Hardware-Aware Efficient ConvNet Design via Differentiable Neural Architecture Search: Designing accurate and efficient ConvNets for mobile devices is challenging because the design space is combinatorially large. Due to this, previous neural architecture search (NAS) methods are computationa...\n\n4. Title: Network Pruning via Transformable Architecture Search\n   Abstract: Network Pruning via Transformable Architecture Search: Network pruning reduces the computation costs of an over-parameterized network without performance damage. Prevailing pruning algorithms pre-define the width and depth of the pruned networks, and then transfer parameters from the unpruned networ...\n\n5. Title: Data-driven sparse structure selection for deep neural networks\n   Abstract: Data-Driven Sparse Structure Selection for Deep Neural Networks: Deep convolutional neural networks have liberated its extraordinary power on various tasks. However, it is still very challenging to deploy state-of-the-art models into real-world applications due to their high computational complexity...\n\n6. Title: AMC: Automl for model compression and acceleration on mobile devices\n   Abstract: AMC: AutoML for Model Compression and Acceleration on Mobile Devices: Model compression is a critical technique to efficiently deploy neural network models on mobile devices which have limited computation resources and tight power budgets. Conventional model compression techniques rely on hand-craft...\n\n7. Title: Dynamic channel pruning: Feature boosting and suppression\n   Abstract: Dynamic Channel Pruning: Feature Boosting and Suppression: Making deep convolutional neural networks more accurate typically comes at the cost of increased computational and memory resources. In this paper, we reduce this cost by exploiting the fact that the importance of features computed by convol...\n\n8. Title: Filter pruning via geometric median for deep convolutional neural networks acceleration\n   Abstract: Filter Pruning via Geometric Median for Deep Convolutional Neural Networks Acceleration: Previous works utilized ''smaller-norm-less-important'' criterion to prune filters with smaller norm values in a convolutional neural network. In this paper, we analyze this norm-based criterion and point out th...\n",
        "reasoning": "The citation context mentions \"reinforcement learning based channel pruning\" and states that it \"beat other handcrafted policy by a large margin.\" Among the candidate papers, the one titled \"AMC: AutoML for Model Compression and Acceleration on Mobile Devices\" (paper 6) specifically discusses model compression techniques, which include channel pruning methods. The abstract indicates that it focuses on automating model compression, which aligns with the context's mention of reinforcement learning approaches. The other candidates either focus on different aspects of pruning or do not mention reinforcement learning, making paper 6 the most relevant choice.",
        "selected_title": "AMC: AutoML for Model Compression and Acceleration on Mobile Devices"
      },
      {
        "augmented": true,
        "citation_context": "ching in a few-shot setting. Recently, several researchers have proposed continuous prompts training to overcome the challenges in discrete prompt searching. Prefix tuning and P-tuningv2[CITATION] prepend a sequence of trainable embeddings at each transformer layer and optimizes them. Two contemporaneous works -- prompt tuning and P-tuning, interleave the training paramet",
        "candidate_papers": "1. Title: P-Tuning v2: Prompt Tuning Can Be Comparable to Fine-tuning Universally Across Scales and Tasks\n   Abstract: P-Tuning v2: Prompt Tuning Can Be Comparable to Fine-tuning Universally Across Scales and Tasks: Prompt tuning, which only tunes continuous prompts with a frozen language model, substantially reduces per-task storage and memory usage at training. However, in the context of NLU, prior work reveals th...\n\n2. Title: The power of scale for parameter-efficient prompt tuning\n   Abstract: The Power of Scale for Parameter-Efficient Prompt Tuning: In this work, we explore \"prompt tuning\", a simple yet effective mechanism for learning \"soft prompts\" to condition frozen language models to perform specific downstream tasks. Unlike the discrete text prompts used by GPT-3, soft prompts are ...\n\n3. Title: GPT Understands, Too\n   Abstract: GPT Understands, Too: Prompting a pretrained language model with natural language patterns has been proved effective for natural language understanding (NLU). However, our preliminary study reveals that manual discrete prompts often lead to unstable performance -- e.g., changing a single word in the...\n\n4. Title: Entailment as Few-Shot Learner\n   Abstract: Entailment as Few-Shot Learner: Large pre-trained language models (LMs) have demonstrated remarkable ability as few-shot learners. However, their success hinges largely on scaling model parameters to a degree that makes it challenging to train and serve. In this paper, we propose a new approach, nam...\n\n5. Title: Making pre-trained language models better few-shot learners\n   Abstract: Making Pre-trained Language Models Better Few-shot Learners: The recent GPT-3 model (Brown et al., 2020) achieves remarkable few-shot performance solely by leveraging a natural-language prompt and a few task demonstrations as input context. Inspired by their findings, we study few-shot learning in a...\n\n6. Title: Language models are few-shot learners\n   Abstract: Language Models are Few-Shot Learners: Recent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training on a large corpus of text followed by fine-tuning on a specific task. While typically task-agnostic in architecture, this method still requires task-specific fine-tu...\n\n7. Title: Prefix-tuning: Optimizing continuous prompts for generation\n   Abstract: Prefix-Tuning: Optimizing Continuous Prompts for Generation: Fine-tuning is the de facto way to leverage large pretrained language models to perform downstream tasks. However, it modifies all the language model parameters and therefore necessitates storing a full copy for each task. In this paper, w...\n",
        "reasoning": "The citation context discusses continuous prompt training methods, specifically mentioning \"Prefix tuning\" and \"P-tuningv2\" as techniques that prepend trainable embeddings at each transformer layer. Among the candidate papers, the first paper titled \"P-Tuning v2: Prompt Tuning Can Be Comparable to Fine-tuning Universally Across Scales and Tasks\" directly addresses P-Tuning v2, which is explicitly mentioned in the context. Additionally, the seventh paper titled \"Prefix-tuning: Optimizing continuous prompts for generation\" discusses prefix tuning, which is also referenced in the context. However, since the context specifically mentions P-tuningv2, the first paper is the most relevant as it focuses on that method and its implications in prompt tuning.",
        "selected_title": "P-Tuning v2: Prompt Tuning Can Be Comparable to Fine-tuning Universally Across Scales and Tasks"
      },
      {
        "augmented": true,
        "citation_context": "ormer and adapters to connect pretrained image, audio and video encoders with LLMs. ChatBridge and AnyMAL follow a similar architecture with X-LLM but adopts Perceiver[CITATION] and linear layers respectively to align modality encoders with LLMs. Meanwhile, PandaGPT and ImageBind-LLM utilize ImageBind as the modality encoder and therefore nat",
        "candidate_papers": "1. Title: VideoChat: Chat-Centric Video Understanding\n   Abstract: VideoChat: Chat-Centric Video Understanding: In this paper, we initiate an attempt of developing an end-to-end chat-centric video understanding system, coined as VideoChat. It integrates video foundation models and large language models via a learnable neural interface, excelling in spatiotemporal r...\n\n2. Title: Listen, Think, and Understand\n   Abstract: Listen, Think, and Understand: The ability of artificial intelligence (AI) systems to perceive and comprehend audio signals is crucial for many applications. Although significant progress has been made in this area since the development of AudioSet, most existing models are designed to map audio inp...\n\n3. Title: Point-bind \\& point-llm: Aligning point cloud with multi-modality for 3d understanding, generation, and instruction following\n   Abstract: Point-Bind & Point-LLM: Aligning Point Cloud with Multi-modality for 3D Understanding, Generation, and Instruction Following: We introduce Point-Bind, a 3D multi-modality model aligning point clouds with 2D image, language, audio, and video. Guided by ImageBind, we construct a joint embedding space ...\n\n4. Title: Improved Baselines with Visual Instruction Tuning\n   Abstract: Improved Baselines with Visual Instruction Tuning: Large multimodal models (LMM) have recently shown encouraging progress with visual instruction tuning. In this note, we show that the fully-connected vision-language cross-modal connector in LLaVA is surprisingly powerful and data-efficient. With si...\n\n5. Title: Chatbridge: Bridging modalities with large language model as a language catalyst\n   Abstract: ChatBridge: Bridging Modalities with Large Language Model as a Language Catalyst: Building general-purpose models that can perceive diverse real-world modalities and solve various tasks is an appealing target in artificial intelligence. In this paper, we present ChatBridge, a novel multimodal langua...\n\n6. Title: X-llm: Bootstrapping advanced large language models by treating multi-modalities as foreign languages\n   Abstract: X-LLM: Bootstrapping Advanced Large Language Models by Treating Multi-Modalities as Foreign Languages: Large language models (LLMs) have demonstrated remarkable language abilities. GPT-4, based on advanced LLMs, exhibits extraordinary multimodal capabilities beyond previous visual language models. W...\n\n7. Title: Perceiver: General perception with iterative attention\n   Abstract: Perceiver: General Perception with Iterative Attention: Biological systems perceive the world by simultaneously processing high-dimensional inputs from modalities as diverse as vision, audition, touch, proprioception, etc. The perception models used in deep learning on the other hand are designed fo...\n\n8. Title: VALOR: Vision-Audio-Language Omni-Perception Pretraining Model and Dataset\n   Abstract: VALOR: Vision-Audio-Language Omni-Perception Pretraining Model and Dataset: In this paper, we propose a Vision-Audio-Language Omni-peRception pretraining model (VALOR) for multi-modal understanding and generation. Different from widely-studied vision-language pretraining models, VALOR jointly models...\n\n9. Title: AnyMAL: An Efficient and Scalable Any-Modality Augmented Language Model\n   Abstract: AnyMAL: An Efficient and Scalable Any-Modality Augmented Language Model: We present Any-Modality Augmented Language Model (AnyMAL), a unified model that reasons over diverse input modality signals (i.e. text, image, video, audio, IMU motion sensor), and generates textual responses. AnyMAL inherits t...\n",
        "reasoning": "The citation context mentions \"Perceiver\" in relation to architectures that connect modality encoders with large language models (LLMs). Among the candidate papers, the one titled \"Perceiver: General perception with iterative attention\" directly discusses the Perceiver model, which is designed for processing high-dimensional inputs from various modalities. This aligns with the context's focus on connecting different modality encoders with LLMs. The other candidates either focus on different models or do not mention Perceiver, making this paper the most relevant choice.",
        "selected_title": "Perceiver: General perception with iterative attention"
      },
      {
        "augmented": true,
        "citation_context": "network architectures. Except for the downstream classification performance of contrastive learning, some works focus on other properties of representations learned by contrastive learning.[CITATION] discuss the feature diversity by analyzing the dimensional collapse in contrastive learning. prove that the contrastive models are identifiable up to linear transformations under certain a",
        "candidate_papers": "1. Title: Understanding contrastive representation learning through alignment and uniformity on the hypersphere\n   Abstract: Understanding Contrastive Representation Learning through Alignment and Uniformity on the Hypersphere: Contrastive representation learning has been outstandingly successful in practice. In this work, we identify two key properties related to the contrastive loss: (1) alignment (closeness) of feature...\n\n2. Title: Momentum contrast for unsupervised visual representation learning\n   Abstract: Momentum Contrast for Unsupervised Visual Representation Learning: We present Momentum Contrast (MoCo) for unsupervised visual representation learning. From a perspective on contrastive learning as dictionary look-up, we build a dynamic dictionary with a queue and a moving-averaged encoder. This ena...\n\n3. Title: Provable Guarantees for Self-Supervised Deep Learning with Spectral Contrastive Loss\n   Abstract: Provable Guarantees for Self-Supervised Deep Learning with Spectral Contrastive Loss: Recent works in self-supervised learning have advanced the state-of-the-art by relying on the contrastive learning paradigm, which learns representations by pushing positive pairs, or similar examples from the same...\n\n4. Title: ContraNorm: A Contrastive Learning Perspective on Oversmoothing and Beyond\n   Abstract: ContraNorm: A Contrastive Learning Perspective on Oversmoothing and Beyond: Oversmoothing is a common phenomenon in a wide range of Graph Neural Networks (GNNs) and Transformers, where performance worsens as the number of layers increases. Instead of characterizing oversmoothing from the view of com...\n\n5. Title: On the transfer of disentangled representations in realistic settings\n   Abstract: On the Transfer of Disentangled Representations in Realistic Settings: Learning meaningful representations that disentangle the underlying structure of the data generating process is considered to be of key importance in machine learning. While disentangled representations were found to be useful fo...\n\n6. Title: A theoretical analysis of contrastive unsupervised representation learning\n   Abstract: A Theoretical Analysis of Contrastive Unsupervised Representation Learning: Recent empirical works have successfully used unlabeled data to learn feature representations that are broadly useful in downstream classification tasks. Several of these methods are reminiscent of the well-known word2vec em...\n\n7. Title: Understanding dimensional collapse in contrastive self-supervised learning\n   Abstract: Understanding Dimensional Collapse in Contrastive Self-supervised Learning: Self-supervised visual representation learning aims to learn useful representations without relying on human annotations. Joint embedding approach bases on maximizing the agreement between embedding vectors from different vi...\n\n8. Title: A Message Passing Perspective on Learning Dynamics of Contrastive Learning\n   Abstract: A Message Passing Perspective on Learning Dynamics of Contrastive Learning: In recent years, contrastive learning achieves impressive results on self-supervised visual representation learning, but there still lacks a rigorous understanding of its learning dynamics. In this paper, we show that if we ...\n",
        "reasoning": "The citation context discusses the properties of representations learned by contrastive learning, specifically mentioning \"feature diversity\" and \"dimensional collapse.\" Among the candidate papers, the title \"Understanding dimensional collapse in contrastive self-supervised learning\" directly addresses the concept of dimensional collapse, which is a key aspect mentioned in the citation context. This paper likely analyzes the feature diversity in the context of contrastive learning, making it a strong candidate for the citation. Other papers, while related to contrastive learning, do not specifically focus on dimensional collapse or feature diversity as highlighted in the citation context.",
        "selected_title": "Understanding dimensional collapse in contrastive self-supervised learning"
      }
    ],
    "signature": {
      "instructions": "You are an expert citation retrieval system as described in the paper \"Multi-Agent System for Reliable Citation Retrieval\".\nYour goal is to autonomously retrieve, verify, and recommend academic references given a query or document excerpt.\n\nTask:\nGiven a citation context from a scientific paper (where a citation is missing), identify the correct paper from a list of candidates.\nAnalyze the context to understand the specific claim, method, or result being cited.\nThen, evaluate each candidate paper to see if it matches the context.\nFinally, select the best matching paper.",
      "fields": [
        {
          "prefix": "Citation Context:",
          "description": "Text excerpt from a paper containing [CITATION] marker where a reference was removed"
        },
        {
          "prefix": "Candidate Papers:",
          "description": "List of candidate papers with titles and abstracts to choose from"
        },
        {
          "prefix": "Reasoning:",
          "description": "Step-by-step analysis of why a particular paper is the cited reference"
        },
        {
          "prefix": "Selected Title:",
          "description": "The exact title of the paper that is being cited"
        }
      ]
    },
    "lm": null
  },
  "metadata": {
    "dependency_versions": {
      "python": "3.12",
      "dspy": "3.0.4",
      "cloudpickle": "3.0"
    }
  }
}
