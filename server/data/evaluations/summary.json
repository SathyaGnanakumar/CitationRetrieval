[
  {
    "query": "Introduction Transformer<|cite_0|> is widely used in natural language processing due to its high training efficiency and superior capability in capturing long-distance dependencies. Building on top of them, modern state-of-the-art models, such as BERT<|cite_1|>, are able to learn powerful language representations from unlabeled text and even surpass the human performance on",
    "paper_id": "2004.11886",
    "retrieved_ids": [
      "llmsurvey2",
      "kalyan2021ammus",
      "tay2020efficient",
      "ganesh2020compressing",
      "TRL",
      "wolf2019huggingface",
      "lan2019albert",
      "minaee2024large",
      "jiao2019tinybert",
      "li2020sentence",
      "jiao20tinybert",
      "devlin2019bert",
      "Devlin:2019uk",
      "dai2019transformer",
      "funnel",
      "jin2020bert",
      "wang2020cluster",
      "bondarenko2021understanding",
      "wang-etal-2019-learning",
      "han2020survey",
      "du2022glamefficientscalinglanguage",
      "distillbert",
      "lite_transformer",
      "pappagari2019hierarchical",
      "conformer",
      "mehta_gss_iclr_2023",
      "sun2019fine",
      "pixelbert",
      "tsai2019multimodal",
      "yang2017breaking",
      "kobayashi-etal-2020-attention",
      "he2020realformer",
      "ao2021speecht5",
      "geng2022multimodal",
      "yan2021consert",
      "guo2019star",
      "zhang2019ernie",
      "fang2020cert",
      "zafrir2019q8bert",
      "yang2019xlnet",
      "liu2019text",
      "Mees2022WhatMI",
      "phang2018sentence",
      "qiu2020blockwise",
      "uniformer",
      "zhen2022cosformer",
      "mrkvsic2017neural"
    ],
    "relevant_ids": [
      "Ahmed:2017tm",
      "Tan:2018vw",
      "Devlin:2019uk",
      "Liu:2019tx",
      "Wu:2016wt",
      "liu2019point",
      "Han:2016uf",
      "Sukhbaatar:2019augmenting",
      "Yang:2018tp",
      "Luong:2015wx",
      "Kaiser:2018wo",
      "Paulus:2018to",
      "Han:2015vn",
      "Cai:2019ui",
      "Liu:2017wj",
      "Gehring:2017tva",
      "Zoph:2017uo",
      "Pham:2018tl",
      "Wu:2019tk",
      "Wang:2019tj",
      "Zoph:2018ta",
      "Strubell:2019uv",
      "Wu:2019wt",
      "li2020gan",
      "Courbariaux:2016tm",
      "Bahdanau:2015vz",
      "Sukhbaatar:2019adaptive",
      "Child:2019sparsetransformer",
      "Sutskever:2014tya",
      "Chen:2018vf",
      "Zhu:2017wy",
      "Kalchbrenner:2016vf",
      "He:2017vq",
      "Shaw:2018wh",
      "So:2019wo",
      "Krishnamoorthi:2018wr",
      "Vaswani:2017ul",
      "Ott:2018ui",
      "He:2018vj"
    ],
    "metrics": {
      "R@5": 0.0,
      "R@10": 0.0,
      "R@20": 0.02564102564102564,
      "MRR": 0.07692307692307693,
      "hits": 1,
      "total_relevant": 39
    },
    "score": 0.023076923076923078,
    "timestamp": "2025-12-18T10:46:43.613856"
  },
  {
    "query": "Introduction Graph neural network (GNN) has attracted much attention recently, and have been applied to various tasks such as bio-medicine<|cite_0|>, computational chemistry<|cite_1|>, social networks<|cite_2|>, computer vision, and natural language understanding<|cite_3|>. GNN assumes structured graphical inputs, for example, molecule graphs, protein-protein interaction networks, or language syntax trees, which can be represented",
    "paper_id": "2004.02001",
    "retrieved_ids": [
      "Zhou2018",
      "you2020graph",
      "wu2021graph",
      "velivckovic2017graph",
      "Zonghan2019",
      "benchmarking-gnn",
      "mavromatis2023train",
      "random-features",
      "dwivedi2020benchmarking",
      "fan2019graph",
      "Sato2020ASO",
      "10.1145/3394486.3403237",
      "zhu2021survey",
      "ref:gnn-rag",
      "morris2019weisfeiler",
      "gnn",
      "Jaume2019",
      "higher-order",
      "grohewl",
      "frasca2020sign",
      "gin",
      "li2016gated",
      "liu2019hyperbolic",
      "shao2022distributed",
      "zhao2021data",
      "xu2018powerful",
      "hu2020strategies",
      "wang2022lifelong",
      "wang2019heterogeneous",
      "Zhang2017NetworkRL",
      "pei2020geom",
      "26_Tang_Haoteng",
      "zhang2019_bgcn",
      "atwood2016diffusion",
      "chen2020can",
      "rong2020self",
      "zhang2020motif",
      "bronstein2017geometric",
      "zhou2021overcoming",
      "monti2017geometric",
      "flam2020neural"
    ],
    "relevant_ids": [
      "tu2019hdegraph",
      "zitnik2018modeling",
      "seo2016bidirectional",
      "kipf2016semi",
      "xiong2016dynamic",
      "gilmer2017neural",
      "zhang2019aspect",
      "de2018question",
      "zhong2019coarse",
      "marcheggiani2018exploiting",
      "zhang2018graph",
      "tai2015improved",
      "bastings2017graph",
      "xiao2019dfgn",
      "velivckovic2017graph",
      "vashishth2019incorporating",
      "yao2018graph",
      "zhang2018sentence",
      "hamilton2017inductive",
      "thorne2018fever",
      "tu2019select",
      "yang2018hotpotqa",
      "fan2019graph",
      "xu2018powerful",
      "fang2019hierarchical",
      "zhou2019gear"
    ],
    "metrics": {
      "R@5": 0.038461538461538464,
      "R@10": 0.07692307692307693,
      "R@20": 0.07692307692307693,
      "MRR": 0.25,
      "hits": 3,
      "total_relevant": 26
    },
    "score": 0.11346153846153846,
    "timestamp": "2025-12-18T10:46:46.419907"
  },
  {
    "query": "Introduction Semantic segmentation aims to assign a semantic label for each pixel in an image. It is broadly applied in a lot of areas such as automatic driving and robot sensing. Machine learning based methods treat semantic segmentation as a pattern recognition problem, and tackle it by classifying each pixel",
    "paper_id": "2004.12679",
    "retrieved_ids": [
      "cheng2021maskformer",
      "minaee2020image",
      "long2015fully",
      "ahn2018learning",
      "acfnet",
      "mask2former",
      "shaban2017oneshot",
      "strudel2021segmenter",
      "xu2022groupvit",
      "sgone",
      "ziegler2022self",
      "SEPL",
      "SAS",
      "masklab",
      "Noh_2015",
      "bbam",
      "Li2021SemanticSW",
      "sec",
      "dai2018dark",
      "wang2021explore",
      "fcn",
      "depth-layering",
      "cermelli2020modeling",
      "SegFix",
      "li2019bidirectional",
      "wang2021survey",
      "chan2021segmentmeifyoucan",
      "SOLO",
      "tsai2018learning",
      "efficientps",
      "li2022deep",
      "Liu2019",
      "wen2022self",
      "metric",
      "lazarow2020learninginstance",
      "kirillov2019panoptic",
      "Li2018",
      "ding2022decoupling",
      "Li2018b",
      "van2021unsupervised",
      "wang2021domain",
      "discriminative",
      "DCM",
      "zheng2015crf_rnn"
    ],
    "relevant_ids": [
      "deeplabv1",
      "acfnet",
      "pspnet",
      "fcn",
      "senet",
      "danet",
      "deeplabv2",
      "dfn",
      "ccnet",
      "resnet",
      "bisenet",
      "deeplabv3",
      "unet",
      "ann",
      "vgg",
      "ocnet",
      "deeplabv3+",
      "segnet",
      "refinenet"
    ],
    "metrics": {
      "R@5": 0.05263157894736842,
      "R@10": 0.05263157894736842,
      "R@20": 0.05263157894736842,
      "MRR": 0.2,
      "hits": 2,
      "total_relevant": 19
    },
    "score": 0.0968421052631579,
    "timestamp": "2025-12-18T10:46:48.038029"
  },
  {
    "query": "Introduction \\label{sec:intro} A standard approach to learning tasks on graph-structured data, such as vertex classification, edge prediction, and graph classification, consists of the construction of a representation of vertices and graphs that captures their structural information. Graph Neural Networks (GNNs) are currently considered as the state-of-the art approach for learning",
    "paper_id": "2004.02593",
    "retrieved_ids": [
      "you2020graph",
      "hamilton2017representation",
      "Zhou2018",
      "zhu2021survey",
      "Zonghan2019",
      "hamilton2017inductive",
      "Sato2020ASO",
      "10.1145/3394486.3403237",
      "chen2019powerful",
      "Zhang2017NetworkRL",
      "zhao2021data",
      "gin",
      "higher-order",
      "wu2021graph",
      "nikolentzos2020k",
      "grohewl",
      "zhu2019aligraph",
      "mavromatis2023train",
      "hu2020strategies",
      "morris2019weisfeiler",
      "wang2021certified",
      "xu2018powerful",
      "huang2020combining",
      "ying2018hierarchical",
      "hoang2019revisiting",
      "article38",
      "yu2023empower",
      "grover2016node2vec",
      "liu2019hyperbolic",
      "Jaume2019",
      "pei2020geom",
      "shao2022distributed",
      "zhang2019_bgcn",
      "wang2022lifelong",
      "kipf2016semi",
      "gcn",
      "pprgo",
      "lee2019attention",
      "zhang2021backdoor",
      "atwood2016diffusion",
      "chenWHDL2020gcnii",
      "26_Tang_Haoteng",
      "gcmc_vdberg2018",
      "hu2020heterogeneous",
      "article36"
    ],
    "relevant_ids": [
      "kipf-loose",
      "ArvindKRV17",
      "KieferSS15",
      "Zonghan2019",
      "Zhou2018",
      "Jaume2019",
      "GilmerSRVD17",
      "Sato2020ASO",
      "grohewl",
      "xhlj19"
    ],
    "metrics": {
      "R@5": 0.2,
      "R@10": 0.3,
      "R@20": 0.4,
      "MRR": 0.3333333333333333,
      "hits": 5,
      "total_relevant": 10
    },
    "score": 0.27,
    "timestamp": "2025-12-18T10:46:50.056009"
  },
  {
    "query": "Introduction The bias-variance trade-off in classical learning theory suggests that models with large capacity to minimize the empirical risk to almost zero usually yield poor generalization performance. However, this is not the case of modern deep neural networks (DNNs): Zhang \\emph{et al.}~\\shortcite{zhang2017} showed that over-parameterized networks have powerful expressivity to",
    "paper_id": "2004.13954",
    "retrieved_ids": [
      "Yang2020",
      "Neyshabur2019TowardsUT",
      "zhang2018mixup",
      "zou2019improved",
      "zhang2017",
      "zhang2019type",
      "Geiger2020",
      "nagarajan2019uniform",
      "huang2020self",
      "allen2018convergence",
      "arora2019fine",
      "rosenfeld2020risks",
      "ortiz2021tradeoffs",
      "Cao2019",
      "Arpit2017",
      "Dinh2017",
      "liu2021orthogonal",
      "maddox2020rethinking",
      "du2018gradientA",
      "oymak2020towards",
      "liu2022loss",
      "jin2022pruning",
      "simsekli2020hausdorff",
      "liu2021learning",
      "song2019understanding",
      "dynamicrepara",
      "lafon2024understanding",
      "zou2018stochastic",
      "xu2021representation",
      "lee2019wide",
      "haochen2022theoretical",
      "uhlich2019mixed",
      "bounds",
      "zhou2019nonvacuous",
      "GCE",
      "mukhoti2020calibrating",
      "mvsgcn",
      "Xu2019a",
      "10.1145/3394486.3403192",
      "schaeffer2023double",
      "you2020shiftaddnet",
      "chen2021iterative",
      "liang2018understanding",
      "zhu2017prune",
      "andriushchenko2022towards",
      "anil_conv",
      "yu2019playing",
      "singla2021curvature"
    ],
    "relevant_ids": [
      "Arora2018",
      "Montufar2014",
      "Zhang2020",
      "Rahaman2019",
      "Geiger2020",
      "Cao2019",
      "Arpit2017",
      "Nakkiran2020",
      "Kalimeris2019",
      "Poole2016",
      "Ronen2019",
      "Pascanu2014",
      "Yang2020",
      "zhang2017",
      "Xu2019",
      "Xu2019a"
    ],
    "metrics": {
      "R@5": 0.125,
      "R@10": 0.1875,
      "R@20": 0.3125,
      "MRR": 1.0,
      "hits": 6,
      "total_relevant": 16
    },
    "score": 0.40625,
    "timestamp": "2025-12-18T10:46:52.335300"
  },
  {
    "query": "Introduction Instance segmentation is a more complex task comparing with object detection and semantic segmentation. It requires predicting each instance not only an approximate location but also a pixel-level segmentation. The recent instance segmentation networks tend to be lighter and try to keep the State-of-the-Art performance. Despite the anchor-free and",
    "paper_id": "2004.00123",
    "retrieved_ids": [
      "masklab",
      "cheng2021maskformer",
      "BlendMask",
      "DeGeus2018",
      "metric",
      "solov2",
      "cascades",
      "depth-layering",
      "fang2019instaboost",
      "wang2018sgpn",
      "proposal-free",
      "SOLO",
      "FCIS",
      "discriminative",
      "BAIS",
      "bridging",
      "jiang2020pointgroup",
      "ssap",
      "SEPL",
      "Zhang2015",
      "novotny2018semi",
      "kirillov2019panoptic",
      "Liu2019",
      "Li2018",
      "Sofiiuk2019",
      "chen2020banet",
      "chan2021segmentmeifyoucan",
      "xu2019camel",
      "fcos",
      "Yang2019a",
      "FCOS",
      "Li2018b",
      "wang2022freesolo",
      "DCM",
      "instancecut",
      "narita2019panopticfusion",
      "kirillov2019pafpn",
      "DeGeus2018a",
      "wang2021dense"
    ],
    "relevant_ids": [
      "ssap",
      "PolarMask",
      "CenterNet",
      "FCIS",
      "FCOS",
      "DenseBox",
      "Keypoint",
      "Bottom-up",
      "YOLO",
      "FastRCNN",
      "FCN",
      "R-FCN",
      "ssd",
      "FoveaBox",
      "SOLO",
      "RepPoints",
      "ISFCN",
      "RCNN",
      "FasterRCNN",
      "RetinaNet",
      "TensorMask",
      "yolact",
      "yolov3",
      "MaskR-CNN",
      "yolov2"
    ],
    "metrics": {
      "R@5": 0.0,
      "R@10": 0.0,
      "R@20": 0.12,
      "MRR": 0.08333333333333333,
      "hits": 4,
      "total_relevant": 25
    },
    "score": 0.024999999999999998,
    "timestamp": "2025-12-18T10:46:54.005913"
  },
  {
    "query": "Introduction \\label{Introduction} Neural networks based systems have been shown to be vulnerable to adversarial examples<|cite_0|>, i.e. maliciously modified inputs that fool a model at inference time. Many directions have been explored to explain and characterize this phenomenon<|cite_1|> that became a growing concern and a major brake on the deployment of",
    "paper_id": "2004.04919",
    "retrieved_ids": [
      "goodfellow2015laceyella",
      "lu2017adversarial",
      "overview",
      "Goodfellow2014",
      "shafahi2018are",
      "grosse2017statistical",
      "sur2",
      "wu2018understanding",
      "tanay2016boundary",
      "yuan2019adversarial",
      "Madry2017",
      "kurakin2016adversarial",
      "chakraborty2018adversarial",
      "Carlini_Dill_2018",
      "meng2017magnet",
      "Mad+18",
      "hendrycks2019natural",
      "carlini2017adversarial",
      "ross2018improving",
      "fawaz2019adversarial",
      "bai2021recent",
      "akhtar2018threat",
      "arnab2018robustness",
      "athalye2018robustness",
      "Goodfellow2016",
      "wong2019wasserstein",
      "ma2021understanding",
      "xiao2018characterizing",
      "biggio_2018",
      "bulusu2020anomalous",
      "lecuyer2018certified",
      "2_Dai",
      "jakubovitz2018improving",
      "xu2019adversarial",
      "ford2019adversarial",
      "yu2021lafeat",
      "LID",
      "papernot2016distillation",
      "narodytska2016simple",
      "moosavi2016deepfool",
      "rossolini2022increasing",
      "Li2020BackdoorLA",
      "boucher2021bad",
      "li2023sok",
      "chou2022backdoor",
      "PapernotM17",
      "geiping2021witches"
    ],
    "relevant_ids": [
      "hendrycks2019",
      "shafahi2018are",
      "Dong_2018",
      "cohen2019certified",
      "meng2017magnet",
      "carlini2017towards",
      "zhang2019theoretically",
      "carmon2019unlabeled",
      "chen2019boundary",
      "ilyas2019adversarial",
      "su2019one",
      "chen2018ead",
      "Madry2017",
      "szegedy2013intriguing",
      "guo2019simple",
      "uesato2018adversarial",
      "schmidt2018adversarially",
      "chen2019stateful",
      "papernot2017practical",
      "ilyas2018black",
      "wang2019enhancing",
      "brendel2017decision",
      "ford2019adversarial",
      "hwang2019puvae",
      "goodfellow2015laceyella"
    ],
    "metrics": {
      "R@5": 0.08,
      "R@10": 0.08,
      "R@20": 0.16,
      "MRR": 1.0,
      "hits": 5,
      "total_relevant": 25
    },
    "score": 0.356,
    "timestamp": "2025-12-18T10:46:56.203160"
  },
  {
    "query": "Introduction Convolutional Neural Networks (CNNs) have demonstrated superior performances in computer vision tasks. However, CNNs are computational and storage intensive, which poses significant challenges on the NN deployments under resource constrained scenarios. Model compression techniques<|cite_0|> are proposed to reduce the computational cost of CNNs. Moreover, there are situations (e.g., deploying",
    "paper_id": "2004.02164",
    "retrieved_ids": [
      "yang2023introduction",
      "bucilu2006model",
      "cheng2017survey",
      "polino2018distillation",
      "kim2015compression",
      "Han:2016uf",
      "amc",
      "guo2022cmt",
      "Zhang_2019",
      "He:2018vj",
      "li2022nextvit",
      "zhang2017shufflenet",
      "zhang2020dynet",
      "liu2017learning",
      "pruning",
      "wang2019cop",
      "ashok2017n2n",
      "amer2021high",
      "Liu:2017wj",
      "agrawal2014analyzing",
      "cspnet",
      "li2020gan",
      "denton2014exploiting",
      "Jaderberg_2014",
      "frantar2023optimal",
      "he2020group",
      "mao2022towards",
      "gong2014compressing",
      "lin2017towards",
      "ye2018rethinking",
      "binary_survey",
      "cai2019WNQ",
      "elhoushi2019deepshift",
      "yang2019multi",
      "DUAL-VIT",
      "Srinivas2017TrainingSN",
      "li2017pruning",
      "baskin2021nice",
      "iandola2014densenet",
      "li2021neural",
      "schaefer2023mixed",
      "ghostnet",
      "hooker2019compressed"
    ],
    "relevant_ids": [
      "morphnet",
      "fpgm",
      "liu2019metapruning",
      "luo2017thinet",
      "ecc",
      "legr",
      "autocompress",
      "amc",
      "grouplasso",
      "liu2017learning",
      "netadapt",
      "rethinking"
    ],
    "metrics": {
      "R@5": 0.0,
      "R@10": 0.08333333333333333,
      "R@20": 0.16666666666666666,
      "MRR": 0.14285714285714285,
      "hits": 2,
      "total_relevant": 12
    },
    "score": 0.06785714285714285,
    "timestamp": "2025-12-18T10:46:58.407457"
  },
  {
    "query": "Introduction Image synthesis refers to the task of generating diverse and photo-realistic images, where a prevalent sub-category known as conditional image synthesis outputs images that are conditioned on some input data. Recently, deep neural networks have been successful at conditional image synthesis<|cite_0|> where one of the conditional inputs is a",
    "paper_id": "2004.10289",
    "retrieved_ids": [
      "anokhin2021image",
      "pixelcnn",
      "composer",
      "zhang2017stackgan",
      "Odena2017",
      "AuxOdena",
      "liu2023more",
      "zhang2017stackgan++",
      "zhu2020sean",
      "pigan",
      "chen2017photographic",
      "inade",
      "liao2020towards",
      "nguyen2016synthesizing",
      "nguyen2017plug",
      "shi2022semanticstylegan",
      "scgan",
      "frido",
      "granot2022drop",
      "razzhigaev2023kandinsky",
      "huang2022multimodal",
      "meng2022sdedit",
      "ccfpse",
      "zhang2018photographic",
      "qi2018semi",
      "wang2018high",
      "reed2016generative",
      "blattmann2022semi",
      "gan_tian2021good",
      "frolov2021adversarial",
      "perarnau2016invertible",
      "xu20223d",
      "giraffe",
      "pixelnerf",
      "wang2022semantic",
      "niemeyer2021giraffe",
      "Dong2017",
      "karacan2016learning",
      "yu2023long",
      "watson2022novel",
      "oasis"
    ],
    "relevant_ids": [
      "huang2018multimodal",
      "zhu2017toward",
      "wang2019carafe",
      "lorenz2019unsupervised",
      "yang2016stacked",
      "qi2018semi",
      "wang2018high",
      "liu2018image",
      "chen2017photographic",
      "karacan2016learning",
      "shi_2016",
      "su2019pixel",
      "zhang2017stackgan",
      "brock2018large",
      "harley2017segmentation",
      "caesar2018coco",
      "zhu2017unpaired",
      "mazzini2018guided",
      "Noh_2015",
      "shih2019video",
      "liu2018partial",
      "hong2018inferring",
      "Zhao2018layout",
      "reed2016generative",
      "jakab2018unsupervised",
      "isola2017image",
      "liu2017unsupervised",
      "zhang2018self",
      "park2019semantic",
      "zhang2017stackgan++",
      "uhrig2017sparsity",
      "karacan2018manipulating",
      "xu2017attngan",
      "dundar2020unsupervised",
      "wang2018non",
      "cordts2016cityscapes"
    ],
    "metrics": {
      "R@5": 0.027777777777777776,
      "R@10": 0.05555555555555555,
      "R@20": 0.08333333333333333,
      "MRR": 0.25,
      "hits": 7,
      "total_relevant": 36
    },
    "score": 0.10277777777777777,
    "timestamp": "2025-12-18T10:47:00.240368"
  },
  {
    "query": "Introduction Image landmark localization aims to detect a set of semantic keypoints on the given objects from images, such as the eyes, nose, and ears of human faces. It has been an essential process to assist many high-level computer vision tasks<|cite_0|>. Traditional fully supervised approach relies on a set of",
    "paper_id": "2004.07936",
    "retrieved_ids": [
      "krantz2023navigating",
      "sanchez2019object",
      "DenseBox",
      "Gidaris_2016",
      "zhang2018unsupervised",
      "kulkarni2019unsupervised",
      "zhao2019object",
      "jakab2018unsupervised",
      "SOLO",
      "deeplabv1",
      "thewlis2017unsupervised",
      "bulat2018super",
      "thewlis2019unsupervised",
      "Keypoint",
      "ahn2018learning",
      "zhou2016learning",
      "RepPoints",
      "feng2018wing",
      "minaee2020image",
      "van2021unsupervised",
      "cinbis2016weakly",
      "bency2016weakly",
      "rasheed2022bridging",
      "choe2019attention",
      "jin2020whole",
      "hossain2019comprehensive",
      "oza2021unsupervised",
      "2020arXiv200109518D",
      "LinMBHPRDZ14",
      "lin2014microsoft",
      "mscoco",
      "imagenet",
      "wu2018look",
      "Zhu2015Visual7WGQ",
      "maaz2022class",
      "COCO",
      "dundar2020unsupervised",
      "object_tracking_in_autunomous_driving",
      "lu2023open",
      "wang2018every",
      "scenegraph",
      "roddick2018orthographic",
      "datasetgan",
      "fei2023self",
      "durall2019unmasking",
      "cao2021openpose"
    ],
    "relevant_ids": [
      "zhang2018unsupervised",
      "thewlis2017unsupervised",
      "lenc2016learning",
      "wiles2018self",
      "feng2018wing",
      "wu2018look",
      "kulkarni2019unsupervised",
      "bulat2018super",
      "thewlis2019unsupervised",
      "shu2018deforming",
      "jakab2018unsupervised",
      "wang2019adaptive",
      "siarohin2019animating",
      "suwajanakorn2018discovery",
      "sanchez2019object"
    ],
    "metrics": {
      "R@5": 0.13333333333333333,
      "R@10": 0.26666666666666666,
      "R@20": 0.5333333333333333,
      "MRR": 0.5,
      "hits": 9,
      "total_relevant": 15
    },
    "score": 0.2833333333333333,
    "timestamp": "2025-12-18T10:47:02.252114"
  },
  {
    "query": "Introduction As a part of the interdisciplinary research that combines natural language processing with computer vision, a wide variety of vision--language tasks ({\\em{e.g.,}} visual question answering (VQA), image captioning, referring expressions, and etc.) have been introduced in recent years. Considerable efforts in this field have advanced the capabilities of artificial",
    "paper_id": "2004.14025",
    "retrieved_ids": [
      "antol2015vqa",
      "wuKb",
      "survey3",
      "agrawal2016analyzing",
      "lu2022unified",
      "conimgcap2",
      "lu2019vilbert",
      "balanced_vqa_v2",
      "lu202012",
      "Cho2021UnifyingVT",
      "liu2019clevr",
      "goyal2016towards",
      "nguyen2018improved",
      "okvqa",
      "chen2023palix",
      "sariyildiz2020learning",
      "schwenk2022okvqa",
      "survey7",
      "shih2016att",
      "DBLP:journals/corr/IlievskiYF16",
      "FVQA",
      "showaskattend",
      "irlc",
      "jang2017tgif",
      "seenivasan2023surgicalgpt",
      "Changpinyo2022AllYM",
      "wang2020general",
      "gan2020large",
      "li2022mplug",
      "Zhu2015Visual7WGQ",
      "Malinowski2015AskYN",
      "lu2016hierarchical",
      "ujain2018two",
      "coin",
      "DBLP:journals/corr/abs-1806-07243",
      "veagle_paper",
      "ganz2023towards",
      "gurari2018vizwiz",
      "dou2022coarse",
      "gan2017vqs",
      "tallyqa",
      "Wang_Yu_Yu_Dai_Tsvetkov_Cao_2021",
      "Wu_2016_CVPR",
      "depalm",
      "andreas2016neural"
    ],
    "relevant_ids": [
      "murahari2019large",
      "hudson2019gqa",
      "schwartz2019factor",
      "guo2020iterative",
      "kim2020modality",
      "kottur2018visual",
      "wu2018you",
      "das2017visual",
      "vaswani2017attention",
      "zellers2019recognition",
      "gan2019multi",
      "devlin2019bert",
      "agarwal2020history",
      "qi2019two",
      "kang2019dual",
      "lu2019vilbert",
      "guo2019image",
      "wang2020vd",
      "niu2019recursive",
      "zheng2019reasoning",
      "andreas2016neural",
      "seo2017visual"
    ],
    "metrics": {
      "R@5": 0.0,
      "R@10": 0.045454545454545456,
      "R@20": 0.045454545454545456,
      "MRR": 0.14285714285714285,
      "hits": 2,
      "total_relevant": 22
    },
    "score": 0.056493506493506485,
    "timestamp": "2025-12-18T10:47:04.372436"
  },
  {
    "query": "Introduction As one of the representative of deep generative models, variational autoencoders~(VAEs)<|cite_0|> have been widely applied in natural language generation<|cite_1|>. Given input text $x$, VAE learns the variational posterior $q(z|x)$ through the encoder, and reconstructs the output from latent variables $z$ via the decoder $p(x|z)$. To generate diverse sentences, the",
    "paper_id": "2004.09764",
    "retrieved_ids": [
      "kingma2019introduction",
      "maaloe2019biva",
      "graphvae",
      "vahdat2020nvae",
      "xiao2018dirichlet",
      "aneja2021contrastive",
      "2013arXiv1312.6114K",
      "duan2020pre",
      "Semeniuta2017AHC",
      "lossy_vae",
      "liang2018variational",
      "gulrajani2016pixelvae",
      "Yang2017ImprovedVA",
      "Wang2019NeuralGC",
      "Tolstikhin2018",
      "makhzani_adversarial_2016",
      "kim2018semi",
      "Roy2018TheoryAE",
      "Li2019ASE",
      "IWAE",
      "mescheder_adversarial_2018",
      "he2023diffusionbert",
      "vqvae2",
      "VQ_VAE_2",
      "vnmt",
      "pmlr-v119-xu20a",
      "Xu2018SphericalLS",
      "kuzina2022alleviating",
      "He2019LaggingIN",
      "bahuleyan2017variational",
      "Grover2018",
      "pang2020learning",
      "nijkamp2020learning",
      "guo2018long",
      "pixelcnn",
      "Oord2017NeuralDR",
      "zhao2018unsupervised",
      "mahabadi2023tess",
      "yu2017seqgan",
      "petrovich2021action",
      "CycleDiffusion",
      "Fu2019CyclicalAS"
    ],
    "relevant_ids": [
      "Wang2019NeuralGC",
      "Bowman2015GeneratingSF",
      "Yang2017ImprovedVA",
      "Li2019ASE",
      "bahuleyan2017variational",
      "Semeniuta2017AHC",
      "Oord2016PixelRN",
      "Xu2018SphericalLS",
      "Kingma2013AutoEncodingVB",
      "deng2018latent",
      "Bahdanau2014NeuralMT",
      "Roy2018TheoryAE",
      "He2019LaggingIN",
      "Fu2019CyclicalAS",
      "Oord2017NeuralDR"
    ],
    "metrics": {
      "R@5": 0.0,
      "R@10": 0.06666666666666667,
      "R@20": 0.3333333333333333,
      "MRR": 0.1111111111111111,
      "hits": 10,
      "total_relevant": 15
    },
    "score": 0.05333333333333333,
    "timestamp": "2025-12-18T10:47:06.691177"
  },
  {
    "query": "Introduction The demand for on-device training is recently increasing, as evinced by the surge of interest in federated learning<|cite_0|>. Federated learning leverages on-device training at multiple distributed devices to obtain a knowledge-abundant global model without centralizing private on-device data<|cite_1|>. Classical federated learning algorithms, represented by FedAvg<|cite_2|>, require on-device training with",
    "paper_id": "2109.03775",
    "retrieved_ids": [
      "ruan2021towards",
      "liang2020think",
      "hanzely2020federated",
      "Konecn2016FederatedOD",
      "jeong2018communication",
      "zhao2018federated",
      "kairouz2021advances",
      "bonawitz2019towards",
      "li2019convergence",
      "ramaswamy2020training",
      "qiu2022zerofl",
      "wang2019federated",
      "jiang2022model",
      "itahara2020distillation",
      "kulkarni2020survey",
      "Smith2017MOCHA",
      "reisizadeh2020fedpaq",
      "gu2021fast",
      "he2020group",
      "nguyen2020fast",
      "konevcny2016federated",
      "hsu2020federated",
      "thapa2020splitfed",
      "diao2020heterofl",
      "zhu2021data",
      "li2019fedmd",
      "acar2020federated",
      "hard2018federated",
      "Fedlearning_edge_1",
      "reddi2020adaptive",
      "li2019fair",
      "chen2020wireless",
      "lin2020ensemble",
      "li2020federated",
      "chen2018federated",
      "abad2020hierarchical",
      "sattler2020communication"
    ],
    "relevant_ids": [
      "diao2020heterofl",
      "ba2013deep",
      "micaelli2019zero",
      "sun2020federated",
      "chen2021age",
      "he2020group",
      "guha2019one",
      "hinton2015distilling",
      "choi2020data",
      "mcmahan2017communication",
      "yang2019federated",
      "liu2020accelerating",
      "nishio2019client",
      "fang2019data",
      "seo2020federated",
      "truong2021data",
      "he2016deep",
      "cho2019efficacy",
      "itahara2020distillation",
      "vepakomma2018split",
      "bucilu2006model",
      "zhu2021data",
      "chang2019cronus",
      "lin2020ensemble",
      "jeong2018communication",
      "li2020federated",
      "sattler2020communication",
      "sattler2021fedaux",
      "zhao2018federated",
      "li2019convergence",
      "li2021fedh2l",
      "lin2020mcunet",
      "li2019fedmd",
      "sandler2018mobilenetv2",
      "ma2018shufflenet",
      "cai2019once"
    ],
    "metrics": {
      "R@5": 0.027777777777777776,
      "R@10": 0.08333333333333333,
      "R@20": 0.1388888888888889,
      "MRR": 0.2,
      "hits": 11,
      "total_relevant": 36
    },
    "score": 0.0961111111111111,
    "timestamp": "2025-12-18T10:47:08.705096"
  },
  {
    "query": "Introduction \\begin{figure}[h] \\begin{center} \\includegraphics[width=1.0\\linewidth]{figures/compare.pdf} \\end{center} \\caption{Comparison of current temporal action detection (TAD) pipelines for the two-stream inputs. \\textbf{(a)} Offline feature extractor. \\textbf{(b)} Two separate networks. \\textbf{(c)} Ours. SP-TAD is a unified framework where the two-stream features are fused at the early stage and then adopts only one subsequent network for",
    "paper_id": "2109.08847",
    "retrieved_ids": [
      "zhao2017ssn",
      "lin2017ssad",
      "xu2020gtad",
      "tdn",
      "liu2021tadtr",
      "xu2017rc3d",
      "wang2021taotad",
      "simonyan2014twostream",
      "lin2020dbg",
      "faure2023holistic",
      "nag2022zero",
      "huang2023interaction",
      "Feichtenhofer-CVPR-2016",
      "weng2020temporal",
      "tang2020asynchronous",
      "jiang2019stm",
      "wu2023stmixer",
      "wang2017spatiotemporal",
      "christoph2016spatiotemporal",
      "two_stage",
      "lin2019tsm",
      "lin2021afsd",
      "singh2017online",
      "neimark2021video",
      "chen2021you",
      "li2021frequency",
      "hou2017tube",
      "shiftct",
      "ding2023unireplknet",
      "liu2021sg",
      "shenimproving",
      "Mo_2021_ICCV",
      "feichtenhofer2019slowfast",
      "vidtr",
      "atm",
      "egeunet",
      "wang2020scale",
      "ann",
      "deeplabv3+",
      "motionformer",
      "wang2021survey",
      "solov2",
      "chen2019hybrid",
      "gberta_2021_ICML",
      "sharir2021image"
    ],
    "relevant_ids": [
      "feichtenhofer2019slowfast",
      "lin2018bsn",
      "tran2018r3d",
      "lin2021afsd",
      "dosovitskiy2020vit",
      "lin2019bmn",
      "lin2017fpn",
      "wang2021pvt",
      "liu2021tadtr",
      "zeng2019pgch",
      "carreira2017i3d",
      "zhao2017ssn",
      "chang2021atag",
      "qing2021tcanet",
      "xie2018s3d",
      "lin2017ssad",
      "yang2020a2net",
      "chao2018talnet",
      "vaswani2017transformer",
      "wang2016tsn",
      "lin2020dbg",
      "wang2021tapgtr",
      "gao2020rapnet",
      "tan2021rtdnet",
      "carion2020detr",
      "ren2015fastercnn",
      "tran2015c3d",
      "simonyan2014twostream",
      "xu2017rc3d",
      "xu2020gtad",
      "donahue2015rnn",
      "wang2021taotad"
    ],
    "metrics": {
      "R@5": 0.125,
      "R@10": 0.25,
      "R@20": 0.25,
      "MRR": 1.0,
      "hits": 10,
      "total_relevant": 32
    },
    "score": 0.425,
    "timestamp": "2025-12-18T10:47:11.806286"
  },
  {
    "query": "Introduction \\begin{figure*} \\centering \\includegraphics[width=14cm]{ESimCSE.jpg} \\caption{The schematic diagram of the ESimCSE method. Unlike the unsup-SimCSE, ESimCSE performs word repetition operations on the batch so that the lengths of positive pairs vary without changing the semantics of sentences. This mechanism weakens the same-length hint for the model when predicting positive pairs. In",
    "paper_id": "2109.04380",
    "retrieved_ids": [
      "wu2021esimcse",
      "riley-chiang-2022-continuum",
      "dai2019transformer",
      "le2014distributed",
      "bai2023qwen",
      "goel2022cyclip",
      "zhou2023instruction",
      "fogel2020scrabblegan",
      "gao2021simcse",
      "ref:ragsurvey2",
      "naveed2024comprehensivellms",
      "lee2021deduplicating",
      "wu2022infocse",
      "ref:ragsurvey3",
      "jiang2023llmlingua",
      "zhao2023clip",
      "wu2021smoothed",
      "survey1",
      "pang2024anchor",
      "gpt_summary",
      "luddecke2022image",
      "he2023rest",
      "chen2023extending",
      "xiao2022retromae",
      "chen2020exploring",
      "zhang2024instruct",
      "voynov2023p+",
      "zhu2021improving",
      "Jiang2023LongLLMLinguaAA",
      "Guo2022FromIT",
      "rennie2017self",
      "jiang2022improved",
      "awasthi2023improving",
      "liu2018show",
      "nitzan2023domain",
      "chi2023dissecting",
      "ermolov2020whitening",
      "Yang:CVPR19",
      "zhang2022reducing",
      "honda2021removing",
      "press2021train",
      "yao2017novel",
      "Zhao2024RetrievalAugmentedGF",
      "xia2023speculative",
      "ke2020rethinking",
      "Hendricks:CVPR16",
      "laina2019towards",
      "sun2021long",
      "bulatov2023scaling",
      "li2023functional",
      "min2022rethinking",
      "zhou2024transformers",
      "ruoss2023randomized",
      "kazemnejad2023impact",
      "li2018delete"
    ],
    "relevant_ids": [
      "wu2020clear",
      "yan2021consert",
      "he2020momentum",
      "liu2019roberta",
      "hill2016learning",
      "meng2021coco",
      "fang2020cert",
      "kiros2015skip",
      "le2014distributed",
      "devlin2018bert",
      "pagliardini2017unsupervised",
      "zhang2020unsupervised",
      "gao2021simcse",
      "logeswaran2018efficient",
      "chen2020simple"
    ],
    "metrics": {
      "R@5": 0.06666666666666667,
      "R@10": 0.13333333333333333,
      "R@20": 0.13333333333333333,
      "MRR": 0.25,
      "hits": 2,
      "total_relevant": 15
    },
    "score": 0.14166666666666666,
    "timestamp": "2025-12-18T10:47:14.976602"
  },
  {
    "query": "Introduction Designing low-latency neural networks is critical to the application on mobile devices, e.g., mobile phones, security cameras, augmented reality glasses, self-driving cars, and many others. Neural architecture search(NAS) is expected to automatically search a neural network where the performance surpasses the human-designed one under the most common constraints, such",
    "paper_id": "2109.03508",
    "retrieved_ids": [
      "wistuba2019survey",
      "Yang:2018tp",
      "elsken2019neural",
      "yang2019cars",
      "netadapt",
      "Tan:2018vw",
      "white2023neural",
      "tan2019mnasnet",
      "He:2018vj",
      "neuralpredictor",
      "bayesnas",
      "liu2022neural",
      "yu2019evaluating",
      "amc",
      "Cai:2019ui",
      "akhauri2021rhnas",
      "tang2023elasticvit",
      "npenas",
      "chen2021bn",
      "Wu:2019tk",
      "cai2018proxylessnas",
      "lin2020mcunet",
      "hu2020dsnas",
      "elsken2017simple",
      "lanas",
      "iccv2019hmnas",
      "dong2019bench",
      "wu2019fbnet",
      "chen2021neural",
      "han2021dynamic",
      "zhang2018graph",
      "alam2020survey",
      "lim2020federated",
      "ref14",
      "howard2019searching",
      "cai2019once",
      "pang2021security",
      "chen2019progressive",
      "guo2020single",
      "li2022efficientformer",
      "zhang2021repnas",
      "ghiasi2019fpn",
      "zhong2018practical",
      "liu2019auto",
      "menghani2021efficient",
      "zhang2020dna",
      "choi2021dance",
      "scardapane2020should"
    ],
    "relevant_ids": [
      "ding2021repmlp",
      "hu2020dsnas",
      "ioffe2015batch",
      "zoph2018learning",
      "liu2018darts",
      "szegedy2017inception",
      "yu2020bignas",
      "ding2021repvgg",
      "ding2019acnet",
      "ding2021diverse",
      "xie2018snas",
      "tan2019efficientnet",
      "zoph2016neural",
      "dong2019searching",
      "real2019regularized",
      "guo2020single",
      "iandola2014densenet"
    ],
    "metrics": {
      "R@5": 0.0,
      "R@10": 0.0,
      "R@20": 0.0,
      "MRR": 0.043478260869565216,
      "hits": 2,
      "total_relevant": 17
    },
    "score": 0.013043478260869565,
    "timestamp": "2025-12-18T10:47:17.140045"
  },
  {
    "query": "Introduction Accurately predicting traffic participants\u2019 future trajectories is critical for autonomous vehicles to make safe, informed, and human-like decisions<|cite_0|>, especially in complex traffic scenarios. However, motion prediction is a remarkably challenging task due to the complicated dependencies of agents' behaviors on the road structure and interactions among agents in addition",
    "paper_id": "2109.06446",
    "retrieved_ids": [
      "song2021learning",
      "pini2023safe",
      "TrafficGen2022",
      "chen2023end",
      "JLB",
      "caesar2021nuplan",
      "dong2021multi",
      "hong2019rules",
      "wang2023drivedreamer",
      "pdm",
      "gameformer",
      "cui2019multimodal",
      "rella2021decoder",
      "Waymo2021",
      "zhao2020tnt",
      "waymax",
      "fang2020tpnet",
      "zhang2020map",
      "salzmann2020trajectron++",
      "gilles2021home",
      "chai2020multipath",
      "hu2021fiery",
      "liang2020learning",
      "gu2022vip3d",
      "thiede2019analyzing",
      "gu2021densetnt",
      "hu2020probabilistic",
      "casas2021mp3",
      "djuric2021multixnet",
      "prakash2021multi",
      "casas2018intentnet",
      "gupta2018social",
      "wang2018reinforcement",
      "gao2020vectornet",
      "hu2023imitation",
      "zhu2024sora"
    ],
    "relevant_ids": [
      "fang2020tpnet",
      "phan2020covernet",
      "dong2021multi",
      "thiede2019analyzing",
      "zhang2020map",
      "huang2021driving",
      "lee2017desire",
      "gupta2018social",
      "ye2021tpcn",
      "song2021learning",
      "ivanovic2020multimodal",
      "liu2021multimodal",
      "liang2020learning",
      "zhao2020tnt",
      "salzmann2020trajectron++",
      "cui2019multimodal",
      "chai2020multipath",
      "gao2020vectornet"
    ],
    "metrics": {
      "R@5": 0.05555555555555555,
      "R@10": 0.1111111111111111,
      "R@20": 0.3888888888888889,
      "MRR": 1.0,
      "hits": 12,
      "total_relevant": 18
    },
    "score": 0.3555555555555555,
    "timestamp": "2025-12-18T10:47:18.836532"
  },
  {
    "query": "Introduction In many real scenarios where interpretability is important<|cite_0|> or robustness is critical<|cite_1|> we may have strict requirements on the type of model that is desired. For example, domain experts in multiple industries such as chip manufacturing or oil production might have a strong preference for decision trees<|cite_2|>, since they",
    "paper_id": "2109.06961",
    "retrieved_ids": [
      "zhang2019interpreting",
      "imagenet-r",
      "gilpin2018explaining",
      "lundberg2017unified",
      "etmann2019connection",
      "bubeck2019adversarial",
      "survey0",
      "arnab2018robustness",
      "kushwhyInt",
      "fu23cotkd",
      "wang2021learning",
      "stolfo-etal-2023-causal",
      "huang2024",
      "khodabandeh2019robust",
      "wang2023surveyfactualitylargelanguage",
      "huang2023survey",
      "bastani2017interpreting",
      "tran2022plex",
      "yamada2022does",
      "teney2022id",
      "Ghorbani2019",
      "ross2018improving",
      "madsen2021posthoc",
      "overview",
      "liu2023trustworthy",
      "frosst2017distilling",
      "li2023sok",
      "zhuang2024toolchain",
      "siddiqui2019tsviz",
      "singh2023benchmarking",
      "landeghem2023document",
      "bo",
      "chen2024tree",
      "fujisawa2022logical",
      "de2023reliability",
      "dehghani2021efficiency",
      "yang2023unisim",
      "balaguer2024rag",
      "jiang2023rtmpose",
      "malinin2018predictive",
      "cho2014exponentially",
      "mehrabi2019survey",
      "Peng2017MultiagentBN",
      "zhou2023don",
      "NIPS2017_6995",
      "wu2024",
      "chakraborty2018adversarial",
      "ballet2019imperceptible",
      "safety"
    ],
    "relevant_ids": [
      "ren2018learning",
      "distill",
      "twostageMC",
      "priv16",
      "bastani2017interpreting",
      "fitnet",
      "modelcompr2",
      "dehghani2017fidelity",
      "sratio",
      "profweight",
      "calib",
      "crd",
      "furlanello2018born",
      "frosst2017distilling",
      "kushwhyInt",
      "modelcompr",
      "takd"
    ],
    "metrics": {
      "R@5": 0.0,
      "R@10": 0.058823529411764705,
      "R@20": 0.11764705882352941,
      "MRR": 0.1111111111111111,
      "hits": 3,
      "total_relevant": 17
    },
    "score": 0.050980392156862744,
    "timestamp": "2025-12-18T10:47:20.956023"
  },
  {
    "query": "Introduction \\label{intro} Graph Neural Networks (GNNs) are effective methods for analyzing graph data, and various downstream graph learning tasks such as node classification, similarity search, graph classification, and link prediction have benefited from its recent developments<|cite_0|>. However, most of the existing GNNs frameworks are supervised learning methods that have many",
    "paper_id": "2109.14159",
    "retrieved_ids": [
      "Zhou2018",
      "Zonghan2019",
      "gin",
      "jin2023survey",
      "zhu2021survey",
      "h2",
      "10.1145/3394486.3403237",
      "chen2019powerful",
      "xu2018powerful",
      "gnn",
      "cpf",
      "random-features",
      "higher-order",
      "not_all_neighbor",
      "zhao2021data",
      "Sato2020ASO",
      "mavromatis2023train",
      "rni",
      "grohewl",
      "chi2022enhancing",
      "morris2019weisfeiler",
      "wu2021graph",
      "gcn",
      "bai2021ripple",
      "article38",
      "hu2020strategies",
      "kipf2016semi",
      "choi2022finding",
      "shao2022distributed",
      "nikolentzos2020k",
      "Jaume2019",
      "liu2019hyperbolic",
      "sankar2018dynamic",
      "26_Tang_Haoteng",
      "Zhang2017NetworkRL",
      "wang2021certified",
      "Dai_2023",
      "pprgo",
      "ying2018hierarchical",
      "zhang2021backdoor",
      "zhou2021overcoming",
      "wang2022lifelong",
      "atwood2016diffusion",
      "zhou2021egnn"
    ],
    "relevant_ids": [
      "article24",
      "article20",
      "article18",
      "article17",
      "article34",
      "article21",
      "article22",
      "article4",
      "article10",
      "article25",
      "article38",
      "article12",
      "article2",
      "article15",
      "article27",
      "article32",
      "article36",
      "article37",
      "article31",
      "article1",
      "article13"
    ],
    "metrics": {
      "R@5": 0.0,
      "R@10": 0.0,
      "R@20": 0.0,
      "MRR": 0.04,
      "hits": 1,
      "total_relevant": 21
    },
    "score": 0.012,
    "timestamp": "2025-12-18T10:47:23.162759"
  },
  {
    "query": "Introduction \\label{sec:intro} The Graph Convolution Network (GCN) model is an effective graph representation learning technique. Its ability to exploit network topology offers superior performance in several applications such as node classification<|cite_0|>, recommendation systems<|cite_1|> and program repair<|cite_2|>. However, training multi-layer GCNs on large and dense graphs remains challenging due to the",
    "paper_id": "2109.13995",
    "retrieved_ids": [
      "hamilton2017representation",
      "clustergcn",
      "sgc",
      "zhu2021survey",
      "recsys",
      "ladies",
      "10.1145/3292500.3330925",
      "fastgcn",
      "ying2018graph",
      "chen2018fastgcn",
      "li2018deeper",
      "pei2020geom",
      "rong2019dropedge",
      "zou2019layerdependent",
      "vrgcn",
      "luan2019break",
      "article34",
      "sgl",
      "pareja2020evolvegcn",
      "dehmamy2019understanding",
      "chen2018stochastic",
      "gnn",
      "yidingNeurIPS20",
      "chenWHDL2020gcnii",
      "chen2019multi",
      "Zhang2017NetworkRL",
      "Zonghan2019",
      "DBLP:conf/acl/BaldwinCR18",
      "asgcn",
      "zhang2019_bgcn",
      "huang2018adaptive",
      "bastings2017graph",
      "velivckovic2017graph",
      "oono2019graph",
      "ying2018hierarchical",
      "graphsaint",
      "liu2020towards",
      "chen2019powerful",
      "zeng2020graphsaint",
      "frasca2020sign",
      "li2020deepergcn",
      "gin",
      "zhu2019aligraph",
      "article38",
      "gcnlpa",
      "xu2018powerful",
      "wang2022lifelong",
      "jin2021graph",
      "wang2020multi",
      "wang2019heterogeneous"
    ],
    "relevant_ids": [
      "recsys",
      "asgcn",
      "vrgcn",
      "defferrard2016convolutional",
      "clustergcn",
      "fastgcn",
      "mvsgcn",
      "ladies",
      "graphsage",
      "orig",
      "frasca2020sign",
      "appnp",
      "pprgo",
      "gin",
      "drrepair",
      "graphsaint",
      "gcn",
      "fey2021gnnautoscale"
    ],
    "metrics": {
      "R@5": 0.1111111111111111,
      "R@10": 0.2222222222222222,
      "R@20": 0.2777777777777778,
      "MRR": 0.5,
      "hits": 9,
      "total_relevant": 18
    },
    "score": 0.26111111111111107,
    "timestamp": "2025-12-18T10:47:25.676251"
  },
  {
    "query": "Introduction Transformer<|cite_0|> is widely used in natural language processing due to its high training efficiency and superior capability in capturing long-distance dependencies. Building on top of them, modern state-of-the-art models, such as BERT<|cite_1|>, are able to learn powerful language representations from unlabeled text and even surpass the human performance on",
    "paper_id": "2004.11886",
    "retrieved_ids": [
      "llmsurvey2",
      "kalyan2021ammus",
      "tay2020efficient",
      "ganesh2020compressing",
      "TRL",
      "wolf2019huggingface",
      "lan2019albert",
      "minaee2024large",
      "jiao2019tinybert",
      "li2020sentence",
      "jiao20tinybert",
      "devlin2019bert",
      "Devlin:2019uk",
      "dai2019transformer",
      "funnel",
      "jin2020bert",
      "wang2020cluster",
      "bondarenko2021understanding",
      "wang-etal-2019-learning",
      "han2020survey",
      "du2022glamefficientscalinglanguage",
      "distillbert",
      "lite_transformer",
      "pappagari2019hierarchical",
      "conformer",
      "mehta_gss_iclr_2023",
      "sun2019fine",
      "pixelbert",
      "tsai2019multimodal",
      "yang2017breaking",
      "kobayashi-etal-2020-attention",
      "he2020realformer",
      "ao2021speecht5",
      "geng2022multimodal",
      "yan2021consert",
      "guo2019star",
      "zhang2019ernie",
      "fang2020cert",
      "zafrir2019q8bert",
      "yang2019xlnet",
      "liu2019text",
      "Mees2022WhatMI",
      "phang2018sentence",
      "qiu2020blockwise",
      "uniformer",
      "zhen2022cosformer",
      "mrkvsic2017neural"
    ],
    "relevant_ids": [
      "Chen:2018vf",
      "Luong:2015wx",
      "Courbariaux:2016tm",
      "Sutskever:2014tya",
      "Krishnamoorthi:2018wr",
      "Han:2015vn",
      "Wu:2016wt",
      "Ahmed:2017tm",
      "Wang:2019tj",
      "Sukhbaatar:2019adaptive",
      "Wu:2019tk",
      "He:2018vj",
      "Strubell:2019uv",
      "He:2017vq",
      "Kaiser:2018wo",
      "Bahdanau:2015vz",
      "Child:2019sparsetransformer",
      "Liu:2019tx",
      "liu2019point",
      "Pham:2018tl",
      "Vaswani:2017ul",
      "li2020gan",
      "Liu:2017wj",
      "Ott:2018ui",
      "Paulus:2018to",
      "Zoph:2018ta",
      "Kalchbrenner:2016vf",
      "So:2019wo",
      "Han:2016uf",
      "Shaw:2018wh",
      "Cai:2019ui",
      "Wu:2019wt",
      "Zhu:2017wy",
      "Devlin:2019uk",
      "Zoph:2017uo",
      "Sukhbaatar:2019augmenting",
      "Yang:2018tp",
      "Gehring:2017tva",
      "Tan:2018vw"
    ],
    "metrics": {
      "R@5": 0.0,
      "R@10": 0.0,
      "R@20": 0.02564102564102564,
      "MRR": 0.07692307692307693,
      "hits": 1,
      "total_relevant": 39
    },
    "score": 0.023076923076923078,
    "timestamp": "2025-12-18T10:48:01.846744"
  },
  {
    "query": "Introduction Graph neural network (GNN) has attracted much attention recently, and have been applied to various tasks such as bio-medicine<|cite_0|>, computational chemistry<|cite_1|>, social networks<|cite_2|>, computer vision, and natural language understanding<|cite_3|>. GNN assumes structured graphical inputs, for example, molecule graphs, protein-protein interaction networks, or language syntax trees, which can be represented",
    "paper_id": "2004.02001",
    "retrieved_ids": [
      "Zhou2018",
      "you2020graph",
      "wu2021graph",
      "velivckovic2017graph",
      "Zonghan2019",
      "benchmarking-gnn",
      "mavromatis2023train",
      "random-features",
      "dwivedi2020benchmarking",
      "fan2019graph",
      "Sato2020ASO",
      "10.1145/3394486.3403237",
      "zhu2021survey",
      "ref:gnn-rag",
      "morris2019weisfeiler",
      "gnn",
      "Jaume2019",
      "higher-order",
      "grohewl",
      "frasca2020sign",
      "gin",
      "li2016gated",
      "liu2019hyperbolic",
      "shao2022distributed",
      "zhao2021data",
      "xu2018powerful",
      "hu2020strategies",
      "wang2022lifelong",
      "wang2019heterogeneous",
      "Zhang2017NetworkRL",
      "pei2020geom",
      "26_Tang_Haoteng",
      "zhang2019_bgcn",
      "atwood2016diffusion",
      "chen2020can",
      "rong2020self",
      "zhang2020motif",
      "bronstein2017geometric",
      "zhou2021overcoming",
      "monti2017geometric",
      "flam2020neural"
    ],
    "relevant_ids": [
      "tu2019select",
      "zhang2019aspect",
      "thorne2018fever",
      "hamilton2017inductive",
      "zhang2018sentence",
      "seo2016bidirectional",
      "tu2019hdegraph",
      "de2018question",
      "kipf2016semi",
      "tai2015improved",
      "bastings2017graph",
      "zhou2019gear",
      "fang2019hierarchical",
      "zhong2019coarse",
      "vashishth2019incorporating",
      "xu2018powerful",
      "fan2019graph",
      "gilmer2017neural",
      "velivckovic2017graph",
      "zhang2018graph",
      "marcheggiani2018exploiting",
      "yao2018graph",
      "xiao2019dfgn",
      "yang2018hotpotqa",
      "xiong2016dynamic",
      "zitnik2018modeling"
    ],
    "metrics": {
      "R@5": 0.038461538461538464,
      "R@10": 0.07692307692307693,
      "R@20": 0.07692307692307693,
      "MRR": 0.25,
      "hits": 3,
      "total_relevant": 26
    },
    "score": 0.11346153846153846,
    "timestamp": "2025-12-18T10:48:04.784400"
  },
  {
    "query": "Introduction Semantic segmentation aims to assign a semantic label for each pixel in an image. It is broadly applied in a lot of areas such as automatic driving and robot sensing. Machine learning based methods treat semantic segmentation as a pattern recognition problem, and tackle it by classifying each pixel",
    "paper_id": "2004.12679",
    "retrieved_ids": [
      "cheng2021maskformer",
      "minaee2020image",
      "long2015fully",
      "ahn2018learning",
      "acfnet",
      "mask2former",
      "shaban2017oneshot",
      "strudel2021segmenter",
      "xu2022groupvit",
      "sgone",
      "ziegler2022self",
      "SEPL",
      "SAS",
      "masklab",
      "Noh_2015",
      "bbam",
      "Li2021SemanticSW",
      "sec",
      "dai2018dark",
      "wang2021explore",
      "fcn",
      "depth-layering",
      "cermelli2020modeling",
      "SegFix",
      "li2019bidirectional",
      "wang2021survey",
      "chan2021segmentmeifyoucan",
      "SOLO",
      "tsai2018learning",
      "efficientps",
      "li2022deep",
      "Liu2019",
      "wen2022self",
      "metric",
      "lazarow2020learninginstance",
      "kirillov2019panoptic",
      "Li2018",
      "ding2022decoupling",
      "Li2018b",
      "van2021unsupervised",
      "wang2021domain",
      "discriminative",
      "DCM",
      "zheng2015crf_rnn"
    ],
    "relevant_ids": [
      "ocnet",
      "fcn",
      "deeplabv2",
      "deeplabv3+",
      "vgg",
      "acfnet",
      "resnet",
      "deeplabv3",
      "danet",
      "segnet",
      "bisenet",
      "refinenet",
      "ccnet",
      "unet",
      "deeplabv1",
      "pspnet",
      "senet",
      "ann",
      "dfn"
    ],
    "metrics": {
      "R@5": 0.05263157894736842,
      "R@10": 0.05263157894736842,
      "R@20": 0.05263157894736842,
      "MRR": 0.2,
      "hits": 2,
      "total_relevant": 19
    },
    "score": 0.0968421052631579,
    "timestamp": "2025-12-18T10:48:06.496613"
  },
  {
    "query": "Introduction \\label{sec:intro} A standard approach to learning tasks on graph-structured data, such as vertex classification, edge prediction, and graph classification, consists of the construction of a representation of vertices and graphs that captures their structural information. Graph Neural Networks (GNNs) are currently considered as the state-of-the art approach for learning",
    "paper_id": "2004.02593",
    "retrieved_ids": [
      "you2020graph",
      "hamilton2017representation",
      "Zhou2018",
      "zhu2021survey",
      "Zonghan2019",
      "hamilton2017inductive",
      "Sato2020ASO",
      "10.1145/3394486.3403237",
      "chen2019powerful",
      "Zhang2017NetworkRL",
      "zhao2021data",
      "gin",
      "higher-order",
      "wu2021graph",
      "nikolentzos2020k",
      "grohewl",
      "zhu2019aligraph",
      "mavromatis2023train",
      "hu2020strategies",
      "morris2019weisfeiler",
      "wang2021certified",
      "xu2018powerful",
      "huang2020combining",
      "ying2018hierarchical",
      "hoang2019revisiting",
      "article38",
      "yu2023empower",
      "grover2016node2vec",
      "liu2019hyperbolic",
      "Jaume2019",
      "pei2020geom",
      "shao2022distributed",
      "zhang2019_bgcn",
      "wang2022lifelong",
      "kipf2016semi",
      "gcn",
      "pprgo",
      "lee2019attention",
      "zhang2021backdoor",
      "atwood2016diffusion",
      "chenWHDL2020gcnii",
      "26_Tang_Haoteng",
      "gcmc_vdberg2018",
      "hu2020heterogeneous",
      "article36"
    ],
    "relevant_ids": [
      "Jaume2019",
      "Sato2020ASO",
      "xhlj19",
      "KieferSS15",
      "Zhou2018",
      "ArvindKRV17",
      "kipf-loose",
      "grohewl",
      "GilmerSRVD17",
      "Zonghan2019"
    ],
    "metrics": {
      "R@5": 0.2,
      "R@10": 0.3,
      "R@20": 0.4,
      "MRR": 0.3333333333333333,
      "hits": 5,
      "total_relevant": 10
    },
    "score": 0.27,
    "timestamp": "2025-12-18T10:48:08.605993"
  },
  {
    "query": "Introduction The bias-variance trade-off in classical learning theory suggests that models with large capacity to minimize the empirical risk to almost zero usually yield poor generalization performance. However, this is not the case of modern deep neural networks (DNNs): Zhang \\emph{et al.}~\\shortcite{zhang2017} showed that over-parameterized networks have powerful expressivity to",
    "paper_id": "2004.13954",
    "retrieved_ids": [
      "Yang2020",
      "Neyshabur2019TowardsUT",
      "zhang2018mixup",
      "zou2019improved",
      "zhang2017",
      "zhang2019type",
      "Geiger2020",
      "nagarajan2019uniform",
      "huang2020self",
      "allen2018convergence",
      "arora2019fine",
      "rosenfeld2020risks",
      "ortiz2021tradeoffs",
      "Cao2019",
      "Arpit2017",
      "Dinh2017",
      "liu2021orthogonal",
      "maddox2020rethinking",
      "du2018gradientA",
      "oymak2020towards",
      "liu2022loss",
      "jin2022pruning",
      "simsekli2020hausdorff",
      "liu2021learning",
      "song2019understanding",
      "dynamicrepara",
      "lafon2024understanding",
      "zou2018stochastic",
      "xu2021representation",
      "lee2019wide",
      "haochen2022theoretical",
      "uhlich2019mixed",
      "bounds",
      "zhou2019nonvacuous",
      "GCE",
      "mukhoti2020calibrating",
      "mvsgcn",
      "Xu2019a",
      "10.1145/3394486.3403192",
      "schaeffer2023double",
      "you2020shiftaddnet",
      "chen2021iterative",
      "liang2018understanding",
      "zhu2017prune",
      "andriushchenko2022towards",
      "anil_conv",
      "yu2019playing",
      "singla2021curvature"
    ],
    "relevant_ids": [
      "Arpit2017",
      "Rahaman2019",
      "Geiger2020",
      "Xu2019a",
      "Yang2020",
      "Nakkiran2020",
      "Zhang2020",
      "Pascanu2014",
      "Poole2016",
      "Ronen2019",
      "Montufar2014",
      "zhang2017",
      "Kalimeris2019",
      "Arora2018",
      "Cao2019",
      "Xu2019"
    ],
    "metrics": {
      "R@5": 0.125,
      "R@10": 0.1875,
      "R@20": 0.3125,
      "MRR": 1.0,
      "hits": 6,
      "total_relevant": 16
    },
    "score": 0.40625,
    "timestamp": "2025-12-18T10:48:10.981163"
  },
  {
    "query": "Introduction Instance segmentation is a more complex task comparing with object detection and semantic segmentation. It requires predicting each instance not only an approximate location but also a pixel-level segmentation. The recent instance segmentation networks tend to be lighter and try to keep the State-of-the-Art performance. Despite the anchor-free and",
    "paper_id": "2004.00123",
    "retrieved_ids": [
      "masklab",
      "cheng2021maskformer",
      "BlendMask",
      "DeGeus2018",
      "metric",
      "solov2",
      "cascades",
      "depth-layering",
      "fang2019instaboost",
      "wang2018sgpn",
      "proposal-free",
      "SOLO",
      "FCIS",
      "discriminative",
      "BAIS",
      "bridging",
      "jiang2020pointgroup",
      "ssap",
      "SEPL",
      "Zhang2015",
      "novotny2018semi",
      "kirillov2019panoptic",
      "Liu2019",
      "Li2018",
      "Sofiiuk2019",
      "chen2020banet",
      "chan2021segmentmeifyoucan",
      "xu2019camel",
      "fcos",
      "Yang2019a",
      "FCOS",
      "Li2018b",
      "wang2022freesolo",
      "DCM",
      "instancecut",
      "narita2019panopticfusion",
      "kirillov2019pafpn",
      "DeGeus2018a",
      "wang2021dense"
    ],
    "relevant_ids": [
      "FCOS",
      "FoveaBox",
      "ISFCN",
      "ssd",
      "yolact",
      "FasterRCNN",
      "DenseBox",
      "PolarMask",
      "R-FCN",
      "RepPoints",
      "RCNN",
      "CenterNet",
      "Bottom-up",
      "FCN",
      "FastRCNN",
      "yolov2",
      "RetinaNet",
      "TensorMask",
      "YOLO",
      "MaskR-CNN",
      "ssap",
      "Keypoint",
      "SOLO",
      "yolov3",
      "FCIS"
    ],
    "metrics": {
      "R@5": 0.0,
      "R@10": 0.0,
      "R@20": 0.12,
      "MRR": 0.08333333333333333,
      "hits": 4,
      "total_relevant": 25
    },
    "score": 0.024999999999999998,
    "timestamp": "2025-12-18T10:48:12.758017"
  },
  {
    "query": "Introduction \\label{Introduction} Neural networks based systems have been shown to be vulnerable to adversarial examples<|cite_0|>, i.e. maliciously modified inputs that fool a model at inference time. Many directions have been explored to explain and characterize this phenomenon<|cite_1|> that became a growing concern and a major brake on the deployment of",
    "paper_id": "2004.04919",
    "retrieved_ids": [
      "goodfellow2015laceyella",
      "lu2017adversarial",
      "overview",
      "Goodfellow2014",
      "shafahi2018are",
      "grosse2017statistical",
      "sur2",
      "wu2018understanding",
      "tanay2016boundary",
      "yuan2019adversarial",
      "Madry2017",
      "kurakin2016adversarial",
      "chakraborty2018adversarial",
      "Carlini_Dill_2018",
      "meng2017magnet",
      "Mad+18",
      "hendrycks2019natural",
      "carlini2017adversarial",
      "ross2018improving",
      "fawaz2019adversarial",
      "bai2021recent",
      "akhtar2018threat",
      "arnab2018robustness",
      "athalye2018robustness",
      "Goodfellow2016",
      "wong2019wasserstein",
      "ma2021understanding",
      "xiao2018characterizing",
      "biggio_2018",
      "bulusu2020anomalous",
      "lecuyer2018certified",
      "2_Dai",
      "jakubovitz2018improving",
      "xu2019adversarial",
      "ford2019adversarial",
      "yu2021lafeat",
      "LID",
      "papernot2016distillation",
      "narodytska2016simple",
      "moosavi2016deepfool",
      "rossolini2022increasing",
      "Li2020BackdoorLA",
      "boucher2021bad",
      "li2023sok",
      "chou2022backdoor",
      "PapernotM17",
      "geiping2021witches"
    ],
    "relevant_ids": [
      "hwang2019puvae",
      "hendrycks2019",
      "uesato2018adversarial",
      "szegedy2013intriguing",
      "ford2019adversarial",
      "chen2019boundary",
      "chen2018ead",
      "Madry2017",
      "chen2019stateful",
      "wang2019enhancing",
      "meng2017magnet",
      "ilyas2018black",
      "carmon2019unlabeled",
      "zhang2019theoretically",
      "papernot2017practical",
      "cohen2019certified",
      "su2019one",
      "schmidt2018adversarially",
      "guo2019simple",
      "shafahi2018are",
      "Dong_2018",
      "brendel2017decision",
      "carlini2017towards",
      "goodfellow2015laceyella",
      "ilyas2019adversarial"
    ],
    "metrics": {
      "R@5": 0.08,
      "R@10": 0.08,
      "R@20": 0.16,
      "MRR": 1.0,
      "hits": 5,
      "total_relevant": 25
    },
    "score": 0.356,
    "timestamp": "2025-12-18T10:48:14.937415"
  },
  {
    "query": "Introduction Convolutional Neural Networks (CNNs) have demonstrated superior performances in computer vision tasks. However, CNNs are computational and storage intensive, which poses significant challenges on the NN deployments under resource constrained scenarios. Model compression techniques<|cite_0|> are proposed to reduce the computational cost of CNNs. Moreover, there are situations (e.g., deploying",
    "paper_id": "2004.02164",
    "retrieved_ids": [
      "yang2023introduction",
      "bucilu2006model",
      "cheng2017survey",
      "polino2018distillation",
      "kim2015compression",
      "Han:2016uf",
      "amc",
      "guo2022cmt",
      "Zhang_2019",
      "He:2018vj",
      "li2022nextvit",
      "zhang2017shufflenet",
      "zhang2020dynet",
      "liu2017learning",
      "pruning",
      "wang2019cop",
      "ashok2017n2n",
      "amer2021high",
      "Liu:2017wj",
      "agrawal2014analyzing",
      "cspnet",
      "li2020gan",
      "denton2014exploiting",
      "Jaderberg_2014",
      "frantar2023optimal",
      "he2020group",
      "mao2022towards",
      "gong2014compressing",
      "lin2017towards",
      "ye2018rethinking",
      "binary_survey",
      "cai2019WNQ",
      "elhoushi2019deepshift",
      "yang2019multi",
      "DUAL-VIT",
      "Srinivas2017TrainingSN",
      "li2017pruning",
      "baskin2021nice",
      "iandola2014densenet",
      "li2021neural",
      "schaefer2023mixed",
      "ghostnet",
      "hooker2019compressed"
    ],
    "relevant_ids": [
      "amc",
      "autocompress",
      "luo2017thinet",
      "grouplasso",
      "fpgm",
      "liu2019metapruning",
      "morphnet",
      "liu2017learning",
      "netadapt",
      "rethinking",
      "ecc",
      "legr"
    ],
    "metrics": {
      "R@5": 0.0,
      "R@10": 0.08333333333333333,
      "R@20": 0.16666666666666666,
      "MRR": 0.14285714285714285,
      "hits": 2,
      "total_relevant": 12
    },
    "score": 0.06785714285714285,
    "timestamp": "2025-12-18T10:48:17.093023"
  },
  {
    "query": "Introduction Image synthesis refers to the task of generating diverse and photo-realistic images, where a prevalent sub-category known as conditional image synthesis outputs images that are conditioned on some input data. Recently, deep neural networks have been successful at conditional image synthesis<|cite_0|> where one of the conditional inputs is a",
    "paper_id": "2004.10289",
    "retrieved_ids": [
      "anokhin2021image",
      "pixelcnn",
      "composer",
      "zhang2017stackgan",
      "Odena2017",
      "AuxOdena",
      "liu2023more",
      "zhang2017stackgan++",
      "zhu2020sean",
      "pigan",
      "chen2017photographic",
      "inade",
      "liao2020towards",
      "nguyen2016synthesizing",
      "nguyen2017plug",
      "shi2022semanticstylegan",
      "scgan",
      "frido",
      "granot2022drop",
      "razzhigaev2023kandinsky",
      "huang2022multimodal",
      "meng2022sdedit",
      "ccfpse",
      "zhang2018photographic",
      "qi2018semi",
      "wang2018high",
      "reed2016generative",
      "blattmann2022semi",
      "gan_tian2021good",
      "frolov2021adversarial",
      "perarnau2016invertible",
      "xu20223d",
      "giraffe",
      "pixelnerf",
      "wang2022semantic",
      "niemeyer2021giraffe",
      "Dong2017",
      "karacan2016learning",
      "yu2023long",
      "watson2022novel",
      "oasis"
    ],
    "relevant_ids": [
      "chen2017photographic",
      "zhang2018self",
      "liu2018partial",
      "zhu2017toward",
      "wang2018non",
      "zhang2017stackgan++",
      "park2019semantic",
      "shi_2016",
      "zhang2017stackgan",
      "Zhao2018layout",
      "uhrig2017sparsity",
      "caesar2018coco",
      "cordts2016cityscapes",
      "brock2018large",
      "lorenz2019unsupervised",
      "yang2016stacked",
      "wang2019carafe",
      "harley2017segmentation",
      "Noh_2015",
      "shih2019video",
      "dundar2020unsupervised",
      "zhu2017unpaired",
      "xu2017attngan",
      "hong2018inferring",
      "karacan2018manipulating",
      "mazzini2018guided",
      "liu2017unsupervised",
      "jakab2018unsupervised",
      "qi2018semi",
      "liu2018image",
      "karacan2016learning",
      "su2019pixel",
      "isola2017image",
      "wang2018high",
      "reed2016generative",
      "huang2018multimodal"
    ],
    "metrics": {
      "R@5": 0.027777777777777776,
      "R@10": 0.05555555555555555,
      "R@20": 0.08333333333333333,
      "MRR": 0.25,
      "hits": 7,
      "total_relevant": 36
    },
    "score": 0.10277777777777777,
    "timestamp": "2025-12-18T10:48:18.977561"
  },
  {
    "query": "Introduction Image landmark localization aims to detect a set of semantic keypoints on the given objects from images, such as the eyes, nose, and ears of human faces. It has been an essential process to assist many high-level computer vision tasks<|cite_0|>. Traditional fully supervised approach relies on a set of",
    "paper_id": "2004.07936",
    "retrieved_ids": [
      "krantz2023navigating",
      "sanchez2019object",
      "DenseBox",
      "Gidaris_2016",
      "zhang2018unsupervised",
      "kulkarni2019unsupervised",
      "zhao2019object",
      "jakab2018unsupervised",
      "SOLO",
      "deeplabv1",
      "thewlis2017unsupervised",
      "bulat2018super",
      "thewlis2019unsupervised",
      "Keypoint",
      "ahn2018learning",
      "zhou2016learning",
      "RepPoints",
      "feng2018wing",
      "minaee2020image",
      "van2021unsupervised",
      "cinbis2016weakly",
      "bency2016weakly",
      "rasheed2022bridging",
      "choe2019attention",
      "jin2020whole",
      "hossain2019comprehensive",
      "oza2021unsupervised",
      "2020arXiv200109518D",
      "LinMBHPRDZ14",
      "lin2014microsoft",
      "mscoco",
      "imagenet",
      "wu2018look",
      "Zhu2015Visual7WGQ",
      "maaz2022class",
      "COCO",
      "dundar2020unsupervised",
      "object_tracking_in_autunomous_driving",
      "lu2023open",
      "wang2018every",
      "scenegraph",
      "roddick2018orthographic",
      "datasetgan",
      "fei2023self",
      "durall2019unmasking",
      "cao2021openpose"
    ],
    "relevant_ids": [
      "suwajanakorn2018discovery",
      "siarohin2019animating",
      "wang2019adaptive",
      "wiles2018self",
      "jakab2018unsupervised",
      "thewlis2019unsupervised",
      "kulkarni2019unsupervised",
      "lenc2016learning",
      "sanchez2019object",
      "thewlis2017unsupervised",
      "zhang2018unsupervised",
      "wu2018look",
      "shu2018deforming",
      "feng2018wing",
      "bulat2018super"
    ],
    "metrics": {
      "R@5": 0.13333333333333333,
      "R@10": 0.26666666666666666,
      "R@20": 0.5333333333333333,
      "MRR": 0.5,
      "hits": 9,
      "total_relevant": 15
    },
    "score": 0.2833333333333333,
    "timestamp": "2025-12-18T10:48:20.944853"
  },
  {
    "query": "Introduction As a part of the interdisciplinary research that combines natural language processing with computer vision, a wide variety of vision--language tasks ({\\em{e.g.,}} visual question answering (VQA), image captioning, referring expressions, and etc.) have been introduced in recent years. Considerable efforts in this field have advanced the capabilities of artificial",
    "paper_id": "2004.14025",
    "retrieved_ids": [
      "antol2015vqa",
      "wuKb",
      "survey3",
      "agrawal2016analyzing",
      "lu2022unified",
      "conimgcap2",
      "lu2019vilbert",
      "balanced_vqa_v2",
      "lu202012",
      "Cho2021UnifyingVT",
      "liu2019clevr",
      "goyal2016towards",
      "nguyen2018improved",
      "okvqa",
      "chen2023palix",
      "sariyildiz2020learning",
      "schwenk2022okvqa",
      "survey7",
      "shih2016att",
      "DBLP:journals/corr/IlievskiYF16",
      "FVQA",
      "showaskattend",
      "irlc",
      "jang2017tgif",
      "seenivasan2023surgicalgpt",
      "Changpinyo2022AllYM",
      "wang2020general",
      "gan2020large",
      "li2022mplug",
      "Zhu2015Visual7WGQ",
      "Malinowski2015AskYN",
      "lu2016hierarchical",
      "ujain2018two",
      "coin",
      "DBLP:journals/corr/abs-1806-07243",
      "veagle_paper",
      "ganz2023towards",
      "gurari2018vizwiz",
      "dou2022coarse",
      "gan2017vqs",
      "tallyqa",
      "Wang_Yu_Yu_Dai_Tsvetkov_Cao_2021",
      "Wu_2016_CVPR",
      "depalm",
      "andreas2016neural"
    ],
    "relevant_ids": [
      "schwartz2019factor",
      "das2017visual",
      "kang2019dual",
      "kim2020modality",
      "guo2020iterative",
      "andreas2016neural",
      "niu2019recursive",
      "agarwal2020history",
      "qi2019two",
      "guo2019image",
      "hudson2019gqa",
      "devlin2019bert",
      "kottur2018visual",
      "zellers2019recognition",
      "seo2017visual",
      "vaswani2017attention",
      "gan2019multi",
      "lu2019vilbert",
      "wang2020vd",
      "murahari2019large",
      "zheng2019reasoning",
      "wu2018you"
    ],
    "metrics": {
      "R@5": 0.0,
      "R@10": 0.045454545454545456,
      "R@20": 0.045454545454545456,
      "MRR": 0.14285714285714285,
      "hits": 2,
      "total_relevant": 22
    },
    "score": 0.056493506493506485,
    "timestamp": "2025-12-18T10:48:22.968360"
  },
  {
    "query": "Introduction As one of the representative of deep generative models, variational autoencoders~(VAEs)<|cite_0|> have been widely applied in natural language generation<|cite_1|>. Given input text $x$, VAE learns the variational posterior $q(z|x)$ through the encoder, and reconstructs the output from latent variables $z$ via the decoder $p(x|z)$. To generate diverse sentences, the",
    "paper_id": "2004.09764",
    "retrieved_ids": [
      "kingma2019introduction",
      "maaloe2019biva",
      "graphvae",
      "vahdat2020nvae",
      "xiao2018dirichlet",
      "aneja2021contrastive",
      "2013arXiv1312.6114K",
      "duan2020pre",
      "Semeniuta2017AHC",
      "lossy_vae",
      "liang2018variational",
      "gulrajani2016pixelvae",
      "Yang2017ImprovedVA",
      "Wang2019NeuralGC",
      "Tolstikhin2018",
      "makhzani_adversarial_2016",
      "kim2018semi",
      "Roy2018TheoryAE",
      "Li2019ASE",
      "IWAE",
      "mescheder_adversarial_2018",
      "he2023diffusionbert",
      "vqvae2",
      "VQ_VAE_2",
      "vnmt",
      "pmlr-v119-xu20a",
      "Xu2018SphericalLS",
      "kuzina2022alleviating",
      "He2019LaggingIN",
      "bahuleyan2017variational",
      "Grover2018",
      "pang2020learning",
      "nijkamp2020learning",
      "guo2018long",
      "pixelcnn",
      "Oord2017NeuralDR",
      "zhao2018unsupervised",
      "mahabadi2023tess",
      "yu2017seqgan",
      "petrovich2021action",
      "CycleDiffusion",
      "Fu2019CyclicalAS"
    ],
    "relevant_ids": [
      "Li2019ASE",
      "bahuleyan2017variational",
      "Kingma2013AutoEncodingVB",
      "Bowman2015GeneratingSF",
      "Semeniuta2017AHC",
      "Fu2019CyclicalAS",
      "Roy2018TheoryAE",
      "He2019LaggingIN",
      "Wang2019NeuralGC",
      "Oord2017NeuralDR",
      "deng2018latent",
      "Bahdanau2014NeuralMT",
      "Yang2017ImprovedVA",
      "Xu2018SphericalLS",
      "Oord2016PixelRN"
    ],
    "metrics": {
      "R@5": 0.0,
      "R@10": 0.06666666666666667,
      "R@20": 0.3333333333333333,
      "MRR": 0.1111111111111111,
      "hits": 10,
      "total_relevant": 15
    },
    "score": 0.05333333333333333,
    "timestamp": "2025-12-18T10:48:25.258476"
  },
  {
    "query": "Introduction The demand for on-device training is recently increasing, as evinced by the surge of interest in federated learning<|cite_0|>. Federated learning leverages on-device training at multiple distributed devices to obtain a knowledge-abundant global model without centralizing private on-device data<|cite_1|>. Classical federated learning algorithms, represented by FedAvg<|cite_2|>, require on-device training with",
    "paper_id": "2109.03775",
    "retrieved_ids": [
      "ruan2021towards",
      "liang2020think",
      "hanzely2020federated",
      "Konecn2016FederatedOD",
      "jeong2018communication",
      "zhao2018federated",
      "kairouz2021advances",
      "bonawitz2019towards",
      "li2019convergence",
      "ramaswamy2020training",
      "qiu2022zerofl",
      "wang2019federated",
      "jiang2022model",
      "itahara2020distillation",
      "kulkarni2020survey",
      "Smith2017MOCHA",
      "reisizadeh2020fedpaq",
      "gu2021fast",
      "he2020group",
      "nguyen2020fast",
      "konevcny2016federated",
      "hsu2020federated",
      "thapa2020splitfed",
      "diao2020heterofl",
      "zhu2021data",
      "li2019fedmd",
      "acar2020federated",
      "hard2018federated",
      "Fedlearning_edge_1",
      "reddi2020adaptive",
      "li2019fair",
      "chen2020wireless",
      "lin2020ensemble",
      "li2020federated",
      "chen2018federated",
      "abad2020hierarchical",
      "sattler2020communication"
    ],
    "relevant_ids": [
      "cho2019efficacy",
      "lin2020ensemble",
      "liu2020accelerating",
      "he2016deep",
      "nishio2019client",
      "sun2020federated",
      "he2020group",
      "vepakomma2018split",
      "ba2013deep",
      "chen2021age",
      "truong2021data",
      "yang2019federated",
      "zhu2021data",
      "itahara2020distillation",
      "choi2020data",
      "hinton2015distilling",
      "mcmahan2017communication",
      "guha2019one",
      "bucilu2006model",
      "li2021fedh2l",
      "sattler2020communication",
      "zhao2018federated",
      "ma2018shufflenet",
      "li2020federated",
      "lin2020mcunet",
      "li2019convergence",
      "cai2019once",
      "seo2020federated",
      "sattler2021fedaux",
      "fang2019data",
      "micaelli2019zero",
      "sandler2018mobilenetv2",
      "chang2019cronus",
      "jeong2018communication",
      "li2019fedmd",
      "diao2020heterofl"
    ],
    "metrics": {
      "R@5": 0.027777777777777776,
      "R@10": 0.08333333333333333,
      "R@20": 0.1388888888888889,
      "MRR": 0.2,
      "hits": 11,
      "total_relevant": 36
    },
    "score": 0.0961111111111111,
    "timestamp": "2025-12-18T10:48:27.255679"
  },
  {
    "query": "Introduction \\begin{figure}[h] \\begin{center} \\includegraphics[width=1.0\\linewidth]{figures/compare.pdf} \\end{center} \\caption{Comparison of current temporal action detection (TAD) pipelines for the two-stream inputs. \\textbf{(a)} Offline feature extractor. \\textbf{(b)} Two separate networks. \\textbf{(c)} Ours. SP-TAD is a unified framework where the two-stream features are fused at the early stage and then adopts only one subsequent network for",
    "paper_id": "2109.08847",
    "retrieved_ids": [
      "zhao2017ssn",
      "lin2017ssad",
      "xu2020gtad",
      "tdn",
      "liu2021tadtr",
      "xu2017rc3d",
      "wang2021taotad",
      "simonyan2014twostream",
      "lin2020dbg",
      "faure2023holistic",
      "nag2022zero",
      "huang2023interaction",
      "Feichtenhofer-CVPR-2016",
      "weng2020temporal",
      "tang2020asynchronous",
      "jiang2019stm",
      "wu2023stmixer",
      "wang2017spatiotemporal",
      "christoph2016spatiotemporal",
      "two_stage",
      "lin2019tsm",
      "lin2021afsd",
      "singh2017online",
      "neimark2021video",
      "chen2021you",
      "li2021frequency",
      "hou2017tube",
      "shiftct",
      "ding2023unireplknet",
      "liu2021sg",
      "shenimproving",
      "Mo_2021_ICCV",
      "feichtenhofer2019slowfast",
      "vidtr",
      "atm",
      "egeunet",
      "wang2020scale",
      "ann",
      "deeplabv3+",
      "motionformer",
      "wang2021survey",
      "solov2",
      "chen2019hybrid",
      "gberta_2021_ICML",
      "sharir2021image"
    ],
    "relevant_ids": [
      "donahue2015rnn",
      "xie2018s3d",
      "zhao2017ssn",
      "lin2019bmn",
      "feichtenhofer2019slowfast",
      "dosovitskiy2020vit",
      "tran2015c3d",
      "xu2017rc3d",
      "lin2018bsn",
      "wang2021pvt",
      "qing2021tcanet",
      "wang2021taotad",
      "chang2021atag",
      "carion2020detr",
      "lin2021afsd",
      "gao2020rapnet",
      "tan2021rtdnet",
      "lin2020dbg",
      "vaswani2017transformer",
      "lin2017ssad",
      "lin2017fpn",
      "simonyan2014twostream",
      "wang2016tsn",
      "carreira2017i3d",
      "liu2021tadtr",
      "wang2021tapgtr",
      "zeng2019pgch",
      "chao2018talnet",
      "ren2015fastercnn",
      "yang2020a2net",
      "tran2018r3d",
      "xu2020gtad"
    ],
    "metrics": {
      "R@5": 0.125,
      "R@10": 0.25,
      "R@20": 0.25,
      "MRR": 1.0,
      "hits": 10,
      "total_relevant": 32
    },
    "score": 0.425,
    "timestamp": "2025-12-18T10:48:30.354366"
  },
  {
    "query": "Introduction \\begin{figure*} \\centering \\includegraphics[width=14cm]{ESimCSE.jpg} \\caption{The schematic diagram of the ESimCSE method. Unlike the unsup-SimCSE, ESimCSE performs word repetition operations on the batch so that the lengths of positive pairs vary without changing the semantics of sentences. This mechanism weakens the same-length hint for the model when predicting positive pairs. In",
    "paper_id": "2109.04380",
    "retrieved_ids": [
      "wu2021esimcse",
      "riley-chiang-2022-continuum",
      "dai2019transformer",
      "le2014distributed",
      "bai2023qwen",
      "goel2022cyclip",
      "zhou2023instruction",
      "fogel2020scrabblegan",
      "gao2021simcse",
      "ref:ragsurvey2",
      "naveed2024comprehensivellms",
      "lee2021deduplicating",
      "wu2022infocse",
      "ref:ragsurvey3",
      "jiang2023llmlingua",
      "zhao2023clip",
      "wu2021smoothed",
      "survey1",
      "pang2024anchor",
      "gpt_summary",
      "luddecke2022image",
      "he2023rest",
      "chen2023extending",
      "xiao2022retromae",
      "chen2020exploring",
      "zhang2024instruct",
      "voynov2023p+",
      "zhu2021improving",
      "Jiang2023LongLLMLinguaAA",
      "Guo2022FromIT",
      "rennie2017self",
      "jiang2022improved",
      "awasthi2023improving",
      "liu2018show",
      "nitzan2023domain",
      "chi2023dissecting",
      "ermolov2020whitening",
      "Yang:CVPR19",
      "zhang2022reducing",
      "honda2021removing",
      "press2021train",
      "yao2017novel",
      "Zhao2024RetrievalAugmentedGF",
      "xia2023speculative",
      "ke2020rethinking",
      "Hendricks:CVPR16",
      "laina2019towards",
      "sun2021long",
      "bulatov2023scaling",
      "li2023functional",
      "min2022rethinking",
      "zhou2024transformers",
      "ruoss2023randomized",
      "kazemnejad2023impact",
      "li2018delete"
    ],
    "relevant_ids": [
      "chen2020simple",
      "fang2020cert",
      "pagliardini2017unsupervised",
      "yan2021consert",
      "he2020momentum",
      "wu2020clear",
      "logeswaran2018efficient",
      "le2014distributed",
      "gao2021simcse",
      "kiros2015skip",
      "devlin2018bert",
      "meng2021coco",
      "zhang2020unsupervised",
      "liu2019roberta",
      "hill2016learning"
    ],
    "metrics": {
      "R@5": 0.06666666666666667,
      "R@10": 0.13333333333333333,
      "R@20": 0.13333333333333333,
      "MRR": 0.25,
      "hits": 2,
      "total_relevant": 15
    },
    "score": 0.14166666666666666,
    "timestamp": "2025-12-18T10:48:33.505627"
  },
  {
    "query": "Introduction Designing low-latency neural networks is critical to the application on mobile devices, e.g., mobile phones, security cameras, augmented reality glasses, self-driving cars, and many others. Neural architecture search(NAS) is expected to automatically search a neural network where the performance surpasses the human-designed one under the most common constraints, such",
    "paper_id": "2109.03508",
    "retrieved_ids": [
      "wistuba2019survey",
      "Yang:2018tp",
      "elsken2019neural",
      "yang2019cars",
      "netadapt",
      "Tan:2018vw",
      "white2023neural",
      "tan2019mnasnet",
      "He:2018vj",
      "neuralpredictor",
      "bayesnas",
      "liu2022neural",
      "yu2019evaluating",
      "amc",
      "Cai:2019ui",
      "akhauri2021rhnas",
      "tang2023elasticvit",
      "npenas",
      "chen2021bn",
      "Wu:2019tk",
      "cai2018proxylessnas",
      "lin2020mcunet",
      "hu2020dsnas",
      "elsken2017simple",
      "lanas",
      "iccv2019hmnas",
      "dong2019bench",
      "wu2019fbnet",
      "chen2021neural",
      "han2021dynamic",
      "zhang2018graph",
      "alam2020survey",
      "lim2020federated",
      "ref14",
      "howard2019searching",
      "cai2019once",
      "pang2021security",
      "chen2019progressive",
      "guo2020single",
      "li2022efficientformer",
      "zhang2021repnas",
      "ghiasi2019fpn",
      "zhong2018practical",
      "liu2019auto",
      "menghani2021efficient",
      "zhang2020dna",
      "choi2021dance",
      "scardapane2020should"
    ],
    "relevant_ids": [
      "real2019regularized",
      "guo2020single",
      "ioffe2015batch",
      "szegedy2017inception",
      "zoph2016neural",
      "iandola2014densenet",
      "xie2018snas",
      "ding2021repmlp",
      "zoph2018learning",
      "hu2020dsnas",
      "dong2019searching",
      "liu2018darts",
      "ding2019acnet",
      "yu2020bignas",
      "ding2021repvgg",
      "tan2019efficientnet",
      "ding2021diverse"
    ],
    "metrics": {
      "R@5": 0.0,
      "R@10": 0.0,
      "R@20": 0.0,
      "MRR": 0.043478260869565216,
      "hits": 2,
      "total_relevant": 17
    },
    "score": 0.013043478260869565,
    "timestamp": "2025-12-18T10:48:35.679269"
  },
  {
    "query": "Introduction Accurately predicting traffic participants\u2019 future trajectories is critical for autonomous vehicles to make safe, informed, and human-like decisions<|cite_0|>, especially in complex traffic scenarios. However, motion prediction is a remarkably challenging task due to the complicated dependencies of agents' behaviors on the road structure and interactions among agents in addition",
    "paper_id": "2109.06446",
    "retrieved_ids": [
      "song2021learning",
      "pini2023safe",
      "TrafficGen2022",
      "chen2023end",
      "JLB",
      "caesar2021nuplan",
      "dong2021multi",
      "hong2019rules",
      "wang2023drivedreamer",
      "pdm",
      "gameformer",
      "cui2019multimodal",
      "rella2021decoder",
      "Waymo2021",
      "zhao2020tnt",
      "waymax",
      "fang2020tpnet",
      "zhang2020map",
      "salzmann2020trajectron++",
      "gilles2021home",
      "chai2020multipath",
      "hu2021fiery",
      "liang2020learning",
      "gu2022vip3d",
      "thiede2019analyzing",
      "gu2021densetnt",
      "hu2020probabilistic",
      "casas2021mp3",
      "djuric2021multixnet",
      "prakash2021multi",
      "casas2018intentnet",
      "gupta2018social",
      "wang2018reinforcement",
      "gao2020vectornet",
      "hu2023imitation",
      "zhu2024sora"
    ],
    "relevant_ids": [
      "fang2020tpnet",
      "chai2020multipath",
      "phan2020covernet",
      "cui2019multimodal",
      "lee2017desire",
      "song2021learning",
      "salzmann2020trajectron++",
      "huang2021driving",
      "ivanovic2020multimodal",
      "zhang2020map",
      "dong2021multi",
      "ye2021tpcn",
      "zhao2020tnt",
      "liu2021multimodal",
      "gupta2018social",
      "thiede2019analyzing",
      "liang2020learning",
      "gao2020vectornet"
    ],
    "metrics": {
      "R@5": 0.05555555555555555,
      "R@10": 0.1111111111111111,
      "R@20": 0.3888888888888889,
      "MRR": 1.0,
      "hits": 12,
      "total_relevant": 18
    },
    "score": 0.3555555555555555,
    "timestamp": "2025-12-18T10:48:37.362949"
  },
  {
    "query": "Introduction In many real scenarios where interpretability is important<|cite_0|> or robustness is critical<|cite_1|> we may have strict requirements on the type of model that is desired. For example, domain experts in multiple industries such as chip manufacturing or oil production might have a strong preference for decision trees<|cite_2|>, since they",
    "paper_id": "2109.06961",
    "retrieved_ids": [
      "zhang2019interpreting",
      "imagenet-r",
      "gilpin2018explaining",
      "lundberg2017unified",
      "etmann2019connection",
      "bubeck2019adversarial",
      "survey0",
      "arnab2018robustness",
      "kushwhyInt",
      "fu23cotkd",
      "wang2021learning",
      "stolfo-etal-2023-causal",
      "huang2024",
      "khodabandeh2019robust",
      "wang2023surveyfactualitylargelanguage",
      "huang2023survey",
      "bastani2017interpreting",
      "tran2022plex",
      "yamada2022does",
      "teney2022id",
      "Ghorbani2019",
      "ross2018improving",
      "madsen2021posthoc",
      "overview",
      "liu2023trustworthy",
      "frosst2017distilling",
      "li2023sok",
      "zhuang2024toolchain",
      "siddiqui2019tsviz",
      "singh2023benchmarking",
      "landeghem2023document",
      "bo",
      "chen2024tree",
      "fujisawa2022logical",
      "de2023reliability",
      "dehghani2021efficiency",
      "yang2023unisim",
      "balaguer2024rag",
      "jiang2023rtmpose",
      "malinin2018predictive",
      "cho2014exponentially",
      "mehrabi2019survey",
      "Peng2017MultiagentBN",
      "zhou2023don",
      "NIPS2017_6995",
      "wu2024",
      "chakraborty2018adversarial",
      "ballet2019imperceptible",
      "safety"
    ],
    "relevant_ids": [
      "bastani2017interpreting",
      "crd",
      "furlanello2018born",
      "frosst2017distilling",
      "fitnet",
      "modelcompr",
      "ren2018learning",
      "twostageMC",
      "priv16",
      "takd",
      "modelcompr2",
      "sratio",
      "kushwhyInt",
      "dehghani2017fidelity",
      "distill",
      "calib",
      "profweight"
    ],
    "metrics": {
      "R@5": 0.0,
      "R@10": 0.058823529411764705,
      "R@20": 0.11764705882352941,
      "MRR": 0.1111111111111111,
      "hits": 3,
      "total_relevant": 17
    },
    "score": 0.050980392156862744,
    "timestamp": "2025-12-18T10:48:39.484968"
  },
  {
    "query": "Introduction \\label{intro} Graph Neural Networks (GNNs) are effective methods for analyzing graph data, and various downstream graph learning tasks such as node classification, similarity search, graph classification, and link prediction have benefited from its recent developments<|cite_0|>. However, most of the existing GNNs frameworks are supervised learning methods that have many",
    "paper_id": "2109.14159",
    "retrieved_ids": [
      "Zhou2018",
      "Zonghan2019",
      "gin",
      "jin2023survey",
      "zhu2021survey",
      "h2",
      "10.1145/3394486.3403237",
      "chen2019powerful",
      "xu2018powerful",
      "gnn",
      "cpf",
      "random-features",
      "higher-order",
      "not_all_neighbor",
      "zhao2021data",
      "Sato2020ASO",
      "mavromatis2023train",
      "rni",
      "grohewl",
      "chi2022enhancing",
      "morris2019weisfeiler",
      "wu2021graph",
      "gcn",
      "bai2021ripple",
      "article38",
      "hu2020strategies",
      "kipf2016semi",
      "choi2022finding",
      "shao2022distributed",
      "nikolentzos2020k",
      "Jaume2019",
      "liu2019hyperbolic",
      "sankar2018dynamic",
      "26_Tang_Haoteng",
      "Zhang2017NetworkRL",
      "wang2021certified",
      "Dai_2023",
      "pprgo",
      "ying2018hierarchical",
      "zhang2021backdoor",
      "zhou2021overcoming",
      "wang2022lifelong",
      "atwood2016diffusion",
      "zhou2021egnn"
    ],
    "relevant_ids": [
      "article37",
      "article21",
      "article25",
      "article12",
      "article17",
      "article22",
      "article24",
      "article27",
      "article36",
      "article20",
      "article2",
      "article38",
      "article32",
      "article10",
      "article31",
      "article1",
      "article18",
      "article15",
      "article4",
      "article34",
      "article13"
    ],
    "metrics": {
      "R@5": 0.0,
      "R@10": 0.0,
      "R@20": 0.0,
      "MRR": 0.04,
      "hits": 1,
      "total_relevant": 21
    },
    "score": 0.012,
    "timestamp": "2025-12-18T10:48:41.730738"
  },
  {
    "query": "Introduction \\label{sec:intro} The Graph Convolution Network (GCN) model is an effective graph representation learning technique. Its ability to exploit network topology offers superior performance in several applications such as node classification<|cite_0|>, recommendation systems<|cite_1|> and program repair<|cite_2|>. However, training multi-layer GCNs on large and dense graphs remains challenging due to the",
    "paper_id": "2109.13995",
    "retrieved_ids": [
      "hamilton2017representation",
      "clustergcn",
      "sgc",
      "zhu2021survey",
      "recsys",
      "ladies",
      "10.1145/3292500.3330925",
      "fastgcn",
      "ying2018graph",
      "chen2018fastgcn",
      "li2018deeper",
      "pei2020geom",
      "rong2019dropedge",
      "zou2019layerdependent",
      "vrgcn",
      "luan2019break",
      "article34",
      "sgl",
      "pareja2020evolvegcn",
      "dehmamy2019understanding",
      "chen2018stochastic",
      "gnn",
      "yidingNeurIPS20",
      "chenWHDL2020gcnii",
      "chen2019multi",
      "Zhang2017NetworkRL",
      "Zonghan2019",
      "DBLP:conf/acl/BaldwinCR18",
      "asgcn",
      "zhang2019_bgcn",
      "huang2018adaptive",
      "bastings2017graph",
      "velivckovic2017graph",
      "oono2019graph",
      "ying2018hierarchical",
      "graphsaint",
      "liu2020towards",
      "chen2019powerful",
      "zeng2020graphsaint",
      "frasca2020sign",
      "li2020deepergcn",
      "gin",
      "zhu2019aligraph",
      "article38",
      "gcnlpa",
      "xu2018powerful",
      "wang2022lifelong",
      "jin2021graph",
      "wang2020multi",
      "wang2019heterogeneous"
    ],
    "relevant_ids": [
      "recsys",
      "vrgcn",
      "mvsgcn",
      "defferrard2016convolutional",
      "ladies",
      "clustergcn",
      "frasca2020sign",
      "fey2021gnnautoscale",
      "pprgo",
      "asgcn",
      "graphsaint",
      "gin",
      "fastgcn",
      "gcn",
      "appnp",
      "orig",
      "drrepair",
      "graphsage"
    ],
    "metrics": {
      "R@5": 0.1111111111111111,
      "R@10": 0.2222222222222222,
      "R@20": 0.2777777777777778,
      "MRR": 0.5,
      "hits": 9,
      "total_relevant": 18
    },
    "score": 0.26111111111111107,
    "timestamp": "2025-12-18T10:48:44.260533"
  },
  {
    "query": "Introduction Transformer<|cite_0|> is widely used in natural language processing due to its high training efficiency and superior capability in capturing long-distance dependencies. Building on top of them, modern state-of-the-art models, such as BERT<|cite_1|>, are able to learn powerful language representations from unlabeled text and even surpass the human performance on",
    "paper_id": "2004.11886",
    "retrieved_ids": [
      "llmsurvey2",
      "kalyan2021ammus",
      "tay2020efficient",
      "ganesh2020compressing",
      "TRL",
      "wolf2019huggingface",
      "lan2019albert",
      "minaee2024large",
      "jiao2019tinybert",
      "li2020sentence",
      "jiao20tinybert",
      "devlin2019bert",
      "Devlin:2019uk",
      "dai2019transformer",
      "funnel",
      "jin2020bert",
      "wang2020cluster",
      "bondarenko2021understanding",
      "wang-etal-2019-learning",
      "han2020survey",
      "du2022glamefficientscalinglanguage",
      "distillbert",
      "lite_transformer",
      "pappagari2019hierarchical",
      "conformer",
      "mehta_gss_iclr_2023",
      "sun2019fine",
      "pixelbert",
      "tsai2019multimodal",
      "yang2017breaking",
      "kobayashi-etal-2020-attention",
      "he2020realformer",
      "ao2021speecht5",
      "geng2022multimodal",
      "yan2021consert",
      "guo2019star",
      "zhang2019ernie",
      "fang2020cert",
      "zafrir2019q8bert",
      "yang2019xlnet",
      "liu2019text",
      "Mees2022WhatMI",
      "phang2018sentence",
      "qiu2020blockwise",
      "uniformer",
      "zhen2022cosformer",
      "mrkvsic2017neural"
    ],
    "relevant_ids": [
      "Krishnamoorthi:2018wr",
      "Wu:2016wt",
      "Liu:2017wj",
      "Vaswani:2017ul",
      "So:2019wo",
      "Han:2015vn",
      "Ott:2018ui",
      "He:2018vj",
      "Devlin:2019uk",
      "Sukhbaatar:2019augmenting",
      "liu2019point",
      "Luong:2015wx",
      "Cai:2019ui",
      "Zhu:2017wy",
      "Wang:2019tj",
      "Shaw:2018wh",
      "Gehring:2017tva",
      "Kaiser:2018wo",
      "Han:2016uf",
      "Wu:2019tk",
      "Zoph:2017uo",
      "Wu:2019wt",
      "He:2017vq",
      "Sutskever:2014tya",
      "Child:2019sparsetransformer",
      "Strubell:2019uv",
      "Pham:2018tl",
      "Ahmed:2017tm",
      "Yang:2018tp",
      "Paulus:2018to",
      "Bahdanau:2015vz",
      "Zoph:2018ta",
      "Tan:2018vw",
      "li2020gan",
      "Courbariaux:2016tm",
      "Chen:2018vf",
      "Sukhbaatar:2019adaptive",
      "Kalchbrenner:2016vf",
      "Liu:2019tx"
    ],
    "metrics": {
      "R@5": 0.0,
      "R@10": 0.0,
      "R@20": 0.02564102564102564,
      "MRR": 0.07692307692307693,
      "hits": 1,
      "total_relevant": 39
    },
    "score": 0.023076923076923078,
    "timestamp": "2025-12-18T10:50:47.637936"
  },
  {
    "query": "Introduction Graph neural network (GNN) has attracted much attention recently, and have been applied to various tasks such as bio-medicine<|cite_0|>, computational chemistry<|cite_1|>, social networks<|cite_2|>, computer vision, and natural language understanding<|cite_3|>. GNN assumes structured graphical inputs, for example, molecule graphs, protein-protein interaction networks, or language syntax trees, which can be represented",
    "paper_id": "2004.02001",
    "retrieved_ids": [
      "Zhou2018",
      "you2020graph",
      "wu2021graph",
      "velivckovic2017graph",
      "Zonghan2019",
      "benchmarking-gnn",
      "mavromatis2023train",
      "random-features",
      "dwivedi2020benchmarking",
      "fan2019graph",
      "Sato2020ASO",
      "10.1145/3394486.3403237",
      "zhu2021survey",
      "ref:gnn-rag",
      "morris2019weisfeiler",
      "gnn",
      "Jaume2019",
      "higher-order",
      "grohewl",
      "frasca2020sign",
      "gin",
      "li2016gated",
      "liu2019hyperbolic",
      "shao2022distributed",
      "zhao2021data",
      "xu2018powerful",
      "hu2020strategies",
      "wang2022lifelong",
      "wang2019heterogeneous",
      "Zhang2017NetworkRL",
      "pei2020geom",
      "26_Tang_Haoteng",
      "zhang2019_bgcn",
      "atwood2016diffusion",
      "chen2020can",
      "rong2020self",
      "zhang2020motif",
      "bronstein2017geometric",
      "zhou2021overcoming",
      "monti2017geometric",
      "flam2020neural"
    ],
    "relevant_ids": [
      "de2018question",
      "xiong2016dynamic",
      "marcheggiani2018exploiting",
      "tu2019select",
      "zitnik2018modeling",
      "tu2019hdegraph",
      "zhang2019aspect",
      "zhang2018sentence",
      "tai2015improved",
      "kipf2016semi",
      "bastings2017graph",
      "xiao2019dfgn",
      "vashishth2019incorporating",
      "fang2019hierarchical",
      "gilmer2017neural",
      "seo2016bidirectional",
      "zhang2018graph",
      "zhou2019gear",
      "xu2018powerful",
      "thorne2018fever",
      "yao2018graph",
      "hamilton2017inductive",
      "fan2019graph",
      "yang2018hotpotqa",
      "velivckovic2017graph",
      "zhong2019coarse"
    ],
    "metrics": {
      "R@5": 0.038461538461538464,
      "R@10": 0.07692307692307693,
      "R@20": 0.07692307692307693,
      "MRR": 0.25,
      "hits": 3,
      "total_relevant": 26
    },
    "score": 0.11346153846153846,
    "timestamp": "2025-12-18T10:50:50.407777"
  },
  {
    "query": "Introduction Semantic segmentation aims to assign a semantic label for each pixel in an image. It is broadly applied in a lot of areas such as automatic driving and robot sensing. Machine learning based methods treat semantic segmentation as a pattern recognition problem, and tackle it by classifying each pixel",
    "paper_id": "2004.12679",
    "retrieved_ids": [
      "cheng2021maskformer",
      "minaee2020image",
      "long2015fully",
      "ahn2018learning",
      "acfnet",
      "mask2former",
      "shaban2017oneshot",
      "strudel2021segmenter",
      "xu2022groupvit",
      "sgone",
      "ziegler2022self",
      "SEPL",
      "SAS",
      "masklab",
      "Noh_2015",
      "bbam",
      "Li2021SemanticSW",
      "sec",
      "dai2018dark",
      "wang2021explore",
      "fcn",
      "depth-layering",
      "cermelli2020modeling",
      "SegFix",
      "li2019bidirectional",
      "wang2021survey",
      "chan2021segmentmeifyoucan",
      "SOLO",
      "tsai2018learning",
      "efficientps",
      "li2022deep",
      "Liu2019",
      "wen2022self",
      "metric",
      "lazarow2020learninginstance",
      "kirillov2019panoptic",
      "Li2018",
      "ding2022decoupling",
      "Li2018b",
      "van2021unsupervised",
      "wang2021domain",
      "discriminative",
      "DCM",
      "zheng2015crf_rnn"
    ],
    "relevant_ids": [
      "acfnet",
      "pspnet",
      "danet",
      "senet",
      "segnet",
      "deeplabv1",
      "bisenet",
      "deeplabv2",
      "ann",
      "unet",
      "ccnet",
      "dfn",
      "fcn",
      "resnet",
      "refinenet",
      "deeplabv3+",
      "vgg",
      "ocnet",
      "deeplabv3"
    ],
    "metrics": {
      "R@5": 0.05263157894736842,
      "R@10": 0.05263157894736842,
      "R@20": 0.05263157894736842,
      "MRR": 0.2,
      "hits": 2,
      "total_relevant": 19
    },
    "score": 0.0968421052631579,
    "timestamp": "2025-12-18T10:50:52.131762"
  },
  {
    "query": "Introduction \\label{sec:intro} A standard approach to learning tasks on graph-structured data, such as vertex classification, edge prediction, and graph classification, consists of the construction of a representation of vertices and graphs that captures their structural information. Graph Neural Networks (GNNs) are currently considered as the state-of-the art approach for learning",
    "paper_id": "2004.02593",
    "retrieved_ids": [
      "you2020graph",
      "hamilton2017representation",
      "Zhou2018",
      "zhu2021survey",
      "Zonghan2019",
      "hamilton2017inductive",
      "Sato2020ASO",
      "10.1145/3394486.3403237",
      "chen2019powerful",
      "Zhang2017NetworkRL",
      "zhao2021data",
      "gin",
      "higher-order",
      "wu2021graph",
      "nikolentzos2020k",
      "grohewl",
      "zhu2019aligraph",
      "mavromatis2023train",
      "hu2020strategies",
      "morris2019weisfeiler",
      "wang2021certified",
      "xu2018powerful",
      "huang2020combining",
      "ying2018hierarchical",
      "hoang2019revisiting",
      "article38",
      "yu2023empower",
      "grover2016node2vec",
      "liu2019hyperbolic",
      "Jaume2019",
      "pei2020geom",
      "shao2022distributed",
      "zhang2019_bgcn",
      "wang2022lifelong",
      "kipf2016semi",
      "gcn",
      "pprgo",
      "lee2019attention",
      "zhang2021backdoor",
      "atwood2016diffusion",
      "chenWHDL2020gcnii",
      "26_Tang_Haoteng",
      "gcmc_vdberg2018",
      "hu2020heterogeneous",
      "article36"
    ],
    "relevant_ids": [
      "GilmerSRVD17",
      "kipf-loose",
      "KieferSS15",
      "Zhou2018",
      "Sato2020ASO",
      "Jaume2019",
      "Zonghan2019",
      "xhlj19",
      "ArvindKRV17",
      "grohewl"
    ],
    "metrics": {
      "R@5": 0.2,
      "R@10": 0.3,
      "R@20": 0.4,
      "MRR": 0.3333333333333333,
      "hits": 5,
      "total_relevant": 10
    },
    "score": 0.27,
    "timestamp": "2025-12-18T10:50:54.246146"
  },
  {
    "query": "Introduction The bias-variance trade-off in classical learning theory suggests that models with large capacity to minimize the empirical risk to almost zero usually yield poor generalization performance. However, this is not the case of modern deep neural networks (DNNs): Zhang \\emph{et al.}~\\shortcite{zhang2017} showed that over-parameterized networks have powerful expressivity to",
    "paper_id": "2004.13954",
    "retrieved_ids": [
      "Yang2020",
      "Neyshabur2019TowardsUT",
      "zhang2018mixup",
      "zou2019improved",
      "zhang2017",
      "zhang2019type",
      "Geiger2020",
      "nagarajan2019uniform",
      "huang2020self",
      "allen2018convergence",
      "arora2019fine",
      "rosenfeld2020risks",
      "ortiz2021tradeoffs",
      "Cao2019",
      "Arpit2017",
      "Dinh2017",
      "liu2021orthogonal",
      "maddox2020rethinking",
      "du2018gradientA",
      "oymak2020towards",
      "liu2022loss",
      "jin2022pruning",
      "simsekli2020hausdorff",
      "liu2021learning",
      "song2019understanding",
      "dynamicrepara",
      "lafon2024understanding",
      "zou2018stochastic",
      "xu2021representation",
      "lee2019wide",
      "haochen2022theoretical",
      "uhlich2019mixed",
      "bounds",
      "zhou2019nonvacuous",
      "GCE",
      "mukhoti2020calibrating",
      "mvsgcn",
      "Xu2019a",
      "10.1145/3394486.3403192",
      "schaeffer2023double",
      "you2020shiftaddnet",
      "chen2021iterative",
      "liang2018understanding",
      "zhu2017prune",
      "andriushchenko2022towards",
      "anil_conv",
      "yu2019playing",
      "singla2021curvature"
    ],
    "relevant_ids": [
      "Montufar2014",
      "zhang2017",
      "Kalimeris2019",
      "Pascanu2014",
      "Cao2019",
      "Arpit2017",
      "Xu2019",
      "Yang2020",
      "Nakkiran2020",
      "Poole2016",
      "Ronen2019",
      "Rahaman2019",
      "Geiger2020",
      "Zhang2020",
      "Arora2018",
      "Xu2019a"
    ],
    "metrics": {
      "R@5": 0.125,
      "R@10": 0.1875,
      "R@20": 0.3125,
      "MRR": 1.0,
      "hits": 6,
      "total_relevant": 16
    },
    "score": 0.40625,
    "timestamp": "2025-12-18T10:50:56.661888"
  },
  {
    "query": "Introduction Instance segmentation is a more complex task comparing with object detection and semantic segmentation. It requires predicting each instance not only an approximate location but also a pixel-level segmentation. The recent instance segmentation networks tend to be lighter and try to keep the State-of-the-Art performance. Despite the anchor-free and",
    "paper_id": "2004.00123",
    "retrieved_ids": [
      "masklab",
      "cheng2021maskformer",
      "BlendMask",
      "DeGeus2018",
      "metric",
      "solov2",
      "cascades",
      "depth-layering",
      "fang2019instaboost",
      "wang2018sgpn",
      "proposal-free",
      "SOLO",
      "FCIS",
      "discriminative",
      "BAIS",
      "bridging",
      "jiang2020pointgroup",
      "ssap",
      "SEPL",
      "Zhang2015",
      "novotny2018semi",
      "kirillov2019panoptic",
      "Liu2019",
      "Li2018",
      "Sofiiuk2019",
      "chen2020banet",
      "chan2021segmentmeifyoucan",
      "xu2019camel",
      "fcos",
      "Yang2019a",
      "FCOS",
      "Li2018b",
      "wang2022freesolo",
      "DCM",
      "instancecut",
      "narita2019panopticfusion",
      "kirillov2019pafpn",
      "DeGeus2018a",
      "wang2021dense"
    ],
    "relevant_ids": [
      "CenterNet",
      "FoveaBox",
      "RepPoints",
      "ISFCN",
      "RetinaNet",
      "yolov3",
      "ssap",
      "FCOS",
      "YOLO",
      "DenseBox",
      "TensorMask",
      "yolact",
      "RCNN",
      "Bottom-up",
      "yolov2",
      "Keypoint",
      "FastRCNN",
      "R-FCN",
      "MaskR-CNN",
      "FCIS",
      "SOLO",
      "ssd",
      "PolarMask",
      "FCN",
      "FasterRCNN"
    ],
    "metrics": {
      "R@5": 0.0,
      "R@10": 0.0,
      "R@20": 0.12,
      "MRR": 0.08333333333333333,
      "hits": 4,
      "total_relevant": 25
    },
    "score": 0.024999999999999998,
    "timestamp": "2025-12-18T10:50:58.381667"
  },
  {
    "query": "Introduction \\label{Introduction} Neural networks based systems have been shown to be vulnerable to adversarial examples<|cite_0|>, i.e. maliciously modified inputs that fool a model at inference time. Many directions have been explored to explain and characterize this phenomenon<|cite_1|> that became a growing concern and a major brake on the deployment of",
    "paper_id": "2004.04919",
    "retrieved_ids": [
      "goodfellow2015laceyella",
      "lu2017adversarial",
      "overview",
      "Goodfellow2014",
      "shafahi2018are",
      "grosse2017statistical",
      "sur2",
      "wu2018understanding",
      "tanay2016boundary",
      "yuan2019adversarial",
      "Madry2017",
      "kurakin2016adversarial",
      "chakraborty2018adversarial",
      "Carlini_Dill_2018",
      "meng2017magnet",
      "Mad+18",
      "hendrycks2019natural",
      "carlini2017adversarial",
      "ross2018improving",
      "fawaz2019adversarial",
      "bai2021recent",
      "akhtar2018threat",
      "arnab2018robustness",
      "athalye2018robustness",
      "Goodfellow2016",
      "wong2019wasserstein",
      "ma2021understanding",
      "xiao2018characterizing",
      "biggio_2018",
      "bulusu2020anomalous",
      "lecuyer2018certified",
      "2_Dai",
      "jakubovitz2018improving",
      "xu2019adversarial",
      "ford2019adversarial",
      "yu2021lafeat",
      "LID",
      "papernot2016distillation",
      "narodytska2016simple",
      "moosavi2016deepfool",
      "rossolini2022increasing",
      "Li2020BackdoorLA",
      "boucher2021bad",
      "li2023sok",
      "chou2022backdoor",
      "PapernotM17",
      "geiping2021witches"
    ],
    "relevant_ids": [
      "Dong_2018",
      "papernot2017practical",
      "brendel2017decision",
      "cohen2019certified",
      "szegedy2013intriguing",
      "carlini2017towards",
      "su2019one",
      "hwang2019puvae",
      "Madry2017",
      "ilyas2019adversarial",
      "chen2018ead",
      "uesato2018adversarial",
      "hendrycks2019",
      "ford2019adversarial",
      "schmidt2018adversarially",
      "chen2019stateful",
      "guo2019simple",
      "meng2017magnet",
      "ilyas2018black",
      "wang2019enhancing",
      "zhang2019theoretically",
      "shafahi2018are",
      "carmon2019unlabeled",
      "goodfellow2015laceyella",
      "chen2019boundary"
    ],
    "metrics": {
      "R@5": 0.08,
      "R@10": 0.08,
      "R@20": 0.16,
      "MRR": 1.0,
      "hits": 5,
      "total_relevant": 25
    },
    "score": 0.356,
    "timestamp": "2025-12-18T10:51:00.661484"
  },
  {
    "query": "Introduction Convolutional Neural Networks (CNNs) have demonstrated superior performances in computer vision tasks. However, CNNs are computational and storage intensive, which poses significant challenges on the NN deployments under resource constrained scenarios. Model compression techniques<|cite_0|> are proposed to reduce the computational cost of CNNs. Moreover, there are situations (e.g., deploying",
    "paper_id": "2004.02164",
    "retrieved_ids": [
      "yang2023introduction",
      "bucilu2006model",
      "cheng2017survey",
      "polino2018distillation",
      "kim2015compression",
      "Han:2016uf",
      "amc",
      "guo2022cmt",
      "Zhang_2019",
      "He:2018vj",
      "li2022nextvit",
      "zhang2017shufflenet",
      "zhang2020dynet",
      "liu2017learning",
      "pruning",
      "wang2019cop",
      "ashok2017n2n",
      "amer2021high",
      "Liu:2017wj",
      "agrawal2014analyzing",
      "cspnet",
      "li2020gan",
      "denton2014exploiting",
      "Jaderberg_2014",
      "frantar2023optimal",
      "he2020group",
      "mao2022towards",
      "gong2014compressing",
      "lin2017towards",
      "ye2018rethinking",
      "binary_survey",
      "cai2019WNQ",
      "elhoushi2019deepshift",
      "yang2019multi",
      "DUAL-VIT",
      "Srinivas2017TrainingSN",
      "li2017pruning",
      "baskin2021nice",
      "iandola2014densenet",
      "li2021neural",
      "schaefer2023mixed",
      "ghostnet",
      "hooker2019compressed"
    ],
    "relevant_ids": [
      "ecc",
      "autocompress",
      "legr",
      "amc",
      "liu2019metapruning",
      "liu2017learning",
      "morphnet",
      "grouplasso",
      "rethinking",
      "luo2017thinet",
      "netadapt",
      "fpgm"
    ],
    "metrics": {
      "R@5": 0.0,
      "R@10": 0.08333333333333333,
      "R@20": 0.16666666666666666,
      "MRR": 0.14285714285714285,
      "hits": 2,
      "total_relevant": 12
    },
    "score": 0.06785714285714285,
    "timestamp": "2025-12-18T10:51:02.966496"
  },
  {
    "query": "Introduction Image synthesis refers to the task of generating diverse and photo-realistic images, where a prevalent sub-category known as conditional image synthesis outputs images that are conditioned on some input data. Recently, deep neural networks have been successful at conditional image synthesis<|cite_0|> where one of the conditional inputs is a",
    "paper_id": "2004.10289",
    "retrieved_ids": [
      "anokhin2021image",
      "pixelcnn",
      "composer",
      "zhang2017stackgan",
      "Odena2017",
      "AuxOdena",
      "liu2023more",
      "zhang2017stackgan++",
      "zhu2020sean",
      "pigan",
      "chen2017photographic",
      "inade",
      "liao2020towards",
      "nguyen2016synthesizing",
      "nguyen2017plug",
      "shi2022semanticstylegan",
      "scgan",
      "frido",
      "granot2022drop",
      "razzhigaev2023kandinsky",
      "huang2022multimodal",
      "meng2022sdedit",
      "ccfpse",
      "zhang2018photographic",
      "qi2018semi",
      "wang2018high",
      "reed2016generative",
      "blattmann2022semi",
      "gan_tian2021good",
      "frolov2021adversarial",
      "perarnau2016invertible",
      "xu20223d",
      "giraffe",
      "pixelnerf",
      "wang2022semantic",
      "niemeyer2021giraffe",
      "Dong2017",
      "karacan2016learning",
      "yu2023long",
      "watson2022novel",
      "oasis"
    ],
    "relevant_ids": [
      "huang2018multimodal",
      "yang2016stacked",
      "caesar2018coco",
      "liu2017unsupervised",
      "isola2017image",
      "qi2018semi",
      "mazzini2018guided",
      "harley2017segmentation",
      "shi_2016",
      "cordts2016cityscapes",
      "Noh_2015",
      "park2019semantic",
      "shih2019video",
      "uhrig2017sparsity",
      "su2019pixel",
      "wang2019carafe",
      "zhu2017unpaired",
      "karacan2016learning",
      "liu2018image",
      "karacan2018manipulating",
      "zhang2017stackgan++",
      "wang2018non",
      "zhang2018self",
      "Zhao2018layout",
      "zhang2017stackgan",
      "jakab2018unsupervised",
      "liu2018partial",
      "chen2017photographic",
      "dundar2020unsupervised",
      "xu2017attngan",
      "wang2018high",
      "zhu2017toward",
      "reed2016generative",
      "hong2018inferring",
      "brock2018large",
      "lorenz2019unsupervised"
    ],
    "metrics": {
      "R@5": 0.027777777777777776,
      "R@10": 0.05555555555555555,
      "R@20": 0.08333333333333333,
      "MRR": 0.25,
      "hits": 7,
      "total_relevant": 36
    },
    "score": 0.10277777777777777,
    "timestamp": "2025-12-18T10:51:04.908170"
  },
  {
    "query": "Introduction Image landmark localization aims to detect a set of semantic keypoints on the given objects from images, such as the eyes, nose, and ears of human faces. It has been an essential process to assist many high-level computer vision tasks<|cite_0|>. Traditional fully supervised approach relies on a set of",
    "paper_id": "2004.07936",
    "retrieved_ids": [
      "krantz2023navigating",
      "sanchez2019object",
      "DenseBox",
      "Gidaris_2016",
      "zhang2018unsupervised",
      "kulkarni2019unsupervised",
      "zhao2019object",
      "jakab2018unsupervised",
      "SOLO",
      "deeplabv1",
      "thewlis2017unsupervised",
      "bulat2018super",
      "thewlis2019unsupervised",
      "Keypoint",
      "ahn2018learning",
      "zhou2016learning",
      "RepPoints",
      "feng2018wing",
      "minaee2020image",
      "van2021unsupervised",
      "cinbis2016weakly",
      "bency2016weakly",
      "rasheed2022bridging",
      "choe2019attention",
      "jin2020whole",
      "hossain2019comprehensive",
      "oza2021unsupervised",
      "2020arXiv200109518D",
      "LinMBHPRDZ14",
      "lin2014microsoft",
      "mscoco",
      "imagenet",
      "wu2018look",
      "Zhu2015Visual7WGQ",
      "maaz2022class",
      "COCO",
      "dundar2020unsupervised",
      "object_tracking_in_autunomous_driving",
      "lu2023open",
      "wang2018every",
      "scenegraph",
      "roddick2018orthographic",
      "datasetgan",
      "fei2023self",
      "durall2019unmasking",
      "cao2021openpose"
    ],
    "relevant_ids": [
      "wu2018look",
      "siarohin2019animating",
      "zhang2018unsupervised",
      "lenc2016learning",
      "thewlis2019unsupervised",
      "sanchez2019object",
      "wang2019adaptive",
      "jakab2018unsupervised",
      "shu2018deforming",
      "bulat2018super",
      "feng2018wing",
      "suwajanakorn2018discovery",
      "kulkarni2019unsupervised",
      "wiles2018self",
      "thewlis2017unsupervised"
    ],
    "metrics": {
      "R@5": 0.13333333333333333,
      "R@10": 0.26666666666666666,
      "R@20": 0.5333333333333333,
      "MRR": 0.5,
      "hits": 9,
      "total_relevant": 15
    },
    "score": 0.2833333333333333,
    "timestamp": "2025-12-18T10:51:06.984718"
  },
  {
    "query": "Introduction As a part of the interdisciplinary research that combines natural language processing with computer vision, a wide variety of vision--language tasks ({\\em{e.g.,}} visual question answering (VQA), image captioning, referring expressions, and etc.) have been introduced in recent years. Considerable efforts in this field have advanced the capabilities of artificial",
    "paper_id": "2004.14025",
    "retrieved_ids": [
      "antol2015vqa",
      "wuKb",
      "survey3",
      "agrawal2016analyzing",
      "lu2022unified",
      "conimgcap2",
      "lu2019vilbert",
      "balanced_vqa_v2",
      "lu202012",
      "Cho2021UnifyingVT",
      "liu2019clevr",
      "goyal2016towards",
      "nguyen2018improved",
      "okvqa",
      "chen2023palix",
      "sariyildiz2020learning",
      "schwenk2022okvqa",
      "survey7",
      "shih2016att",
      "DBLP:journals/corr/IlievskiYF16",
      "FVQA",
      "showaskattend",
      "irlc",
      "jang2017tgif",
      "seenivasan2023surgicalgpt",
      "Changpinyo2022AllYM",
      "wang2020general",
      "gan2020large",
      "li2022mplug",
      "Zhu2015Visual7WGQ",
      "Malinowski2015AskYN",
      "lu2016hierarchical",
      "ujain2018two",
      "coin",
      "DBLP:journals/corr/abs-1806-07243",
      "veagle_paper",
      "ganz2023towards",
      "gurari2018vizwiz",
      "dou2022coarse",
      "gan2017vqs",
      "tallyqa",
      "Wang_Yu_Yu_Dai_Tsvetkov_Cao_2021",
      "Wu_2016_CVPR",
      "depalm",
      "andreas2016neural"
    ],
    "relevant_ids": [
      "wang2020vd",
      "kim2020modality",
      "lu2019vilbert",
      "vaswani2017attention",
      "devlin2019bert",
      "das2017visual",
      "zheng2019reasoning",
      "kang2019dual",
      "wu2018you",
      "agarwal2020history",
      "hudson2019gqa",
      "schwartz2019factor",
      "guo2019image",
      "qi2019two",
      "niu2019recursive",
      "guo2020iterative",
      "zellers2019recognition",
      "murahari2019large",
      "seo2017visual",
      "kottur2018visual",
      "andreas2016neural",
      "gan2019multi"
    ],
    "metrics": {
      "R@5": 0.0,
      "R@10": 0.045454545454545456,
      "R@20": 0.045454545454545456,
      "MRR": 0.14285714285714285,
      "hits": 2,
      "total_relevant": 22
    },
    "score": 0.056493506493506485,
    "timestamp": "2025-12-18T10:51:09.135296"
  },
  {
    "query": "Introduction As one of the representative of deep generative models, variational autoencoders~(VAEs)<|cite_0|> have been widely applied in natural language generation<|cite_1|>. Given input text $x$, VAE learns the variational posterior $q(z|x)$ through the encoder, and reconstructs the output from latent variables $z$ via the decoder $p(x|z)$. To generate diverse sentences, the",
    "paper_id": "2004.09764",
    "retrieved_ids": [
      "kingma2019introduction",
      "maaloe2019biva",
      "graphvae",
      "vahdat2020nvae",
      "xiao2018dirichlet",
      "aneja2021contrastive",
      "2013arXiv1312.6114K",
      "duan2020pre",
      "Semeniuta2017AHC",
      "lossy_vae",
      "liang2018variational",
      "gulrajani2016pixelvae",
      "Yang2017ImprovedVA",
      "Wang2019NeuralGC",
      "Tolstikhin2018",
      "makhzani_adversarial_2016",
      "kim2018semi",
      "Roy2018TheoryAE",
      "Li2019ASE",
      "IWAE",
      "mescheder_adversarial_2018",
      "he2023diffusionbert",
      "vqvae2",
      "VQ_VAE_2",
      "vnmt",
      "pmlr-v119-xu20a",
      "Xu2018SphericalLS",
      "kuzina2022alleviating",
      "He2019LaggingIN",
      "bahuleyan2017variational",
      "Grover2018",
      "pang2020learning",
      "nijkamp2020learning",
      "guo2018long",
      "pixelcnn",
      "Oord2017NeuralDR",
      "zhao2018unsupervised",
      "mahabadi2023tess",
      "yu2017seqgan",
      "petrovich2021action",
      "CycleDiffusion",
      "Fu2019CyclicalAS"
    ],
    "relevant_ids": [
      "Bahdanau2014NeuralMT",
      "Wang2019NeuralGC",
      "Yang2017ImprovedVA",
      "Fu2019CyclicalAS",
      "Kingma2013AutoEncodingVB",
      "He2019LaggingIN",
      "Li2019ASE",
      "Semeniuta2017AHC",
      "Oord2017NeuralDR",
      "Oord2016PixelRN",
      "Bowman2015GeneratingSF",
      "deng2018latent",
      "Roy2018TheoryAE",
      "Xu2018SphericalLS",
      "bahuleyan2017variational"
    ],
    "metrics": {
      "R@5": 0.0,
      "R@10": 0.06666666666666667,
      "R@20": 0.3333333333333333,
      "MRR": 0.1111111111111111,
      "hits": 10,
      "total_relevant": 15
    },
    "score": 0.05333333333333333,
    "timestamp": "2025-12-18T10:51:11.671994"
  },
  {
    "query": "Introduction The demand for on-device training is recently increasing, as evinced by the surge of interest in federated learning<|cite_0|>. Federated learning leverages on-device training at multiple distributed devices to obtain a knowledge-abundant global model without centralizing private on-device data<|cite_1|>. Classical federated learning algorithms, represented by FedAvg<|cite_2|>, require on-device training with",
    "paper_id": "2109.03775",
    "retrieved_ids": [
      "ruan2021towards",
      "liang2020think",
      "hanzely2020federated",
      "Konecn2016FederatedOD",
      "jeong2018communication",
      "zhao2018federated",
      "kairouz2021advances",
      "bonawitz2019towards",
      "li2019convergence",
      "ramaswamy2020training",
      "qiu2022zerofl",
      "wang2019federated",
      "jiang2022model",
      "itahara2020distillation",
      "kulkarni2020survey",
      "Smith2017MOCHA",
      "reisizadeh2020fedpaq",
      "gu2021fast",
      "he2020group",
      "nguyen2020fast",
      "konevcny2016federated",
      "hsu2020federated",
      "thapa2020splitfed",
      "diao2020heterofl",
      "zhu2021data",
      "li2019fedmd",
      "acar2020federated",
      "hard2018federated",
      "Fedlearning_edge_1",
      "reddi2020adaptive",
      "li2019fair",
      "chen2020wireless",
      "lin2020ensemble",
      "li2020federated",
      "chen2018federated",
      "abad2020hierarchical",
      "sattler2020communication"
    ],
    "relevant_ids": [
      "ba2013deep",
      "li2020federated",
      "itahara2020distillation",
      "mcmahan2017communication",
      "sun2020federated",
      "he2020group",
      "hinton2015distilling",
      "seo2020federated",
      "cai2019once",
      "ma2018shufflenet",
      "zhu2021data",
      "bucilu2006model",
      "yang2019federated",
      "chen2021age",
      "sandler2018mobilenetv2",
      "fang2019data",
      "choi2020data",
      "nishio2019client",
      "he2016deep",
      "lin2020ensemble",
      "li2019convergence",
      "micaelli2019zero",
      "liu2020accelerating",
      "lin2020mcunet",
      "sattler2021fedaux",
      "cho2019efficacy",
      "jeong2018communication",
      "diao2020heterofl",
      "guha2019one",
      "chang2019cronus",
      "vepakomma2018split",
      "sattler2020communication",
      "truong2021data",
      "li2019fedmd",
      "li2021fedh2l",
      "zhao2018federated"
    ],
    "metrics": {
      "R@5": 0.027777777777777776,
      "R@10": 0.08333333333333333,
      "R@20": 0.1388888888888889,
      "MRR": 0.2,
      "hits": 11,
      "total_relevant": 36
    },
    "score": 0.0961111111111111,
    "timestamp": "2025-12-18T10:51:13.889219"
  },
  {
    "query": "Introduction \\begin{figure}[h] \\begin{center} \\includegraphics[width=1.0\\linewidth]{figures/compare.pdf} \\end{center} \\caption{Comparison of current temporal action detection (TAD) pipelines for the two-stream inputs. \\textbf{(a)} Offline feature extractor. \\textbf{(b)} Two separate networks. \\textbf{(c)} Ours. SP-TAD is a unified framework where the two-stream features are fused at the early stage and then adopts only one subsequent network for",
    "paper_id": "2109.08847",
    "retrieved_ids": [
      "zhao2017ssn",
      "lin2017ssad",
      "xu2020gtad",
      "tdn",
      "liu2021tadtr",
      "xu2017rc3d",
      "wang2021taotad",
      "simonyan2014twostream",
      "lin2020dbg",
      "faure2023holistic",
      "nag2022zero",
      "huang2023interaction",
      "Feichtenhofer-CVPR-2016",
      "weng2020temporal",
      "tang2020asynchronous",
      "jiang2019stm",
      "wu2023stmixer",
      "wang2017spatiotemporal",
      "christoph2016spatiotemporal",
      "two_stage",
      "lin2019tsm",
      "lin2021afsd",
      "singh2017online",
      "neimark2021video",
      "chen2021you",
      "li2021frequency",
      "hou2017tube",
      "shiftct",
      "ding2023unireplknet",
      "liu2021sg",
      "shenimproving",
      "Mo_2021_ICCV",
      "feichtenhofer2019slowfast",
      "vidtr",
      "atm",
      "egeunet",
      "wang2020scale",
      "ann",
      "deeplabv3+",
      "motionformer",
      "wang2021survey",
      "solov2",
      "chen2019hybrid",
      "gberta_2021_ICML",
      "sharir2021image"
    ],
    "relevant_ids": [
      "wang2021taotad",
      "liu2021tadtr",
      "yang2020a2net",
      "carreira2017i3d",
      "zhao2017ssn",
      "lin2020dbg",
      "wang2016tsn",
      "xu2017rc3d",
      "tran2015c3d",
      "chang2021atag",
      "chao2018talnet",
      "simonyan2014twostream",
      "tan2021rtdnet",
      "lin2018bsn",
      "ren2015fastercnn",
      "dosovitskiy2020vit",
      "donahue2015rnn",
      "lin2019bmn",
      "lin2017ssad",
      "lin2021afsd",
      "tran2018r3d",
      "xie2018s3d",
      "wang2021tapgtr",
      "carion2020detr",
      "feichtenhofer2019slowfast",
      "wang2021pvt",
      "qing2021tcanet",
      "lin2017fpn",
      "vaswani2017transformer",
      "xu2020gtad",
      "gao2020rapnet",
      "zeng2019pgch"
    ],
    "metrics": {
      "R@5": 0.125,
      "R@10": 0.25,
      "R@20": 0.25,
      "MRR": 1.0,
      "hits": 10,
      "total_relevant": 32
    },
    "score": 0.425,
    "timestamp": "2025-12-18T10:51:17.049208"
  },
  {
    "query": "Introduction \\begin{figure*} \\centering \\includegraphics[width=14cm]{ESimCSE.jpg} \\caption{The schematic diagram of the ESimCSE method. Unlike the unsup-SimCSE, ESimCSE performs word repetition operations on the batch so that the lengths of positive pairs vary without changing the semantics of sentences. This mechanism weakens the same-length hint for the model when predicting positive pairs. In",
    "paper_id": "2109.04380",
    "retrieved_ids": [
      "wu2021esimcse",
      "riley-chiang-2022-continuum",
      "dai2019transformer",
      "le2014distributed",
      "bai2023qwen",
      "goel2022cyclip",
      "zhou2023instruction",
      "fogel2020scrabblegan",
      "gao2021simcse",
      "ref:ragsurvey2",
      "naveed2024comprehensivellms",
      "lee2021deduplicating",
      "wu2022infocse",
      "ref:ragsurvey3",
      "jiang2023llmlingua",
      "zhao2023clip",
      "wu2021smoothed",
      "survey1",
      "pang2024anchor",
      "gpt_summary",
      "luddecke2022image",
      "he2023rest",
      "chen2023extending",
      "xiao2022retromae",
      "chen2020exploring",
      "zhang2024instruct",
      "voynov2023p+",
      "zhu2021improving",
      "Jiang2023LongLLMLinguaAA",
      "Guo2022FromIT",
      "rennie2017self",
      "jiang2022improved",
      "awasthi2023improving",
      "liu2018show",
      "nitzan2023domain",
      "chi2023dissecting",
      "ermolov2020whitening",
      "Yang:CVPR19",
      "zhang2022reducing",
      "honda2021removing",
      "press2021train",
      "yao2017novel",
      "Zhao2024RetrievalAugmentedGF",
      "xia2023speculative",
      "ke2020rethinking",
      "Hendricks:CVPR16",
      "laina2019towards",
      "sun2021long",
      "bulatov2023scaling",
      "li2023functional",
      "min2022rethinking",
      "zhou2024transformers",
      "ruoss2023randomized",
      "kazemnejad2023impact",
      "li2018delete"
    ],
    "relevant_ids": [
      "kiros2015skip",
      "meng2021coco",
      "devlin2018bert",
      "zhang2020unsupervised",
      "he2020momentum",
      "fang2020cert",
      "yan2021consert",
      "chen2020simple",
      "gao2021simcse",
      "hill2016learning",
      "wu2020clear",
      "logeswaran2018efficient",
      "liu2019roberta",
      "pagliardini2017unsupervised",
      "le2014distributed"
    ],
    "metrics": {
      "R@5": 0.06666666666666667,
      "R@10": 0.13333333333333333,
      "R@20": 0.13333333333333333,
      "MRR": 0.25,
      "hits": 2,
      "total_relevant": 15
    },
    "score": 0.14166666666666666,
    "timestamp": "2025-12-18T10:51:20.246719"
  },
  {
    "query": "Introduction Designing low-latency neural networks is critical to the application on mobile devices, e.g., mobile phones, security cameras, augmented reality glasses, self-driving cars, and many others. Neural architecture search(NAS) is expected to automatically search a neural network where the performance surpasses the human-designed one under the most common constraints, such",
    "paper_id": "2109.03508",
    "retrieved_ids": [
      "wistuba2019survey",
      "Yang:2018tp",
      "elsken2019neural",
      "yang2019cars",
      "netadapt",
      "Tan:2018vw",
      "white2023neural",
      "tan2019mnasnet",
      "He:2018vj",
      "neuralpredictor",
      "bayesnas",
      "liu2022neural",
      "yu2019evaluating",
      "amc",
      "Cai:2019ui",
      "akhauri2021rhnas",
      "tang2023elasticvit",
      "npenas",
      "chen2021bn",
      "Wu:2019tk",
      "cai2018proxylessnas",
      "lin2020mcunet",
      "hu2020dsnas",
      "elsken2017simple",
      "lanas",
      "iccv2019hmnas",
      "dong2019bench",
      "wu2019fbnet",
      "chen2021neural",
      "han2021dynamic",
      "zhang2018graph",
      "alam2020survey",
      "lim2020federated",
      "ref14",
      "howard2019searching",
      "cai2019once",
      "pang2021security",
      "chen2019progressive",
      "guo2020single",
      "li2022efficientformer",
      "zhang2021repnas",
      "ghiasi2019fpn",
      "zhong2018practical",
      "liu2019auto",
      "menghani2021efficient",
      "zhang2020dna",
      "choi2021dance",
      "scardapane2020should"
    ],
    "relevant_ids": [
      "xie2018snas",
      "ding2021repmlp",
      "ding2021diverse",
      "ioffe2015batch",
      "hu2020dsnas",
      "szegedy2017inception",
      "real2019regularized",
      "yu2020bignas",
      "iandola2014densenet",
      "dong2019searching",
      "tan2019efficientnet",
      "ding2021repvgg",
      "liu2018darts",
      "guo2020single",
      "zoph2018learning",
      "ding2019acnet",
      "zoph2016neural"
    ],
    "metrics": {
      "R@5": 0.0,
      "R@10": 0.0,
      "R@20": 0.0,
      "MRR": 0.043478260869565216,
      "hits": 2,
      "total_relevant": 17
    },
    "score": 0.013043478260869565,
    "timestamp": "2025-12-18T10:51:22.717550"
  },
  {
    "query": "Introduction Accurately predicting traffic participants\u2019 future trajectories is critical for autonomous vehicles to make safe, informed, and human-like decisions<|cite_0|>, especially in complex traffic scenarios. However, motion prediction is a remarkably challenging task due to the complicated dependencies of agents' behaviors on the road structure and interactions among agents in addition",
    "paper_id": "2109.06446",
    "retrieved_ids": [
      "song2021learning",
      "pini2023safe",
      "TrafficGen2022",
      "chen2023end",
      "JLB",
      "caesar2021nuplan",
      "dong2021multi",
      "hong2019rules",
      "wang2023drivedreamer",
      "pdm",
      "gameformer",
      "cui2019multimodal",
      "rella2021decoder",
      "Waymo2021",
      "zhao2020tnt",
      "waymax",
      "fang2020tpnet",
      "zhang2020map",
      "salzmann2020trajectron++",
      "gilles2021home",
      "chai2020multipath",
      "hu2021fiery",
      "liang2020learning",
      "gu2022vip3d",
      "thiede2019analyzing",
      "gu2021densetnt",
      "hu2020probabilistic",
      "casas2021mp3",
      "djuric2021multixnet",
      "prakash2021multi",
      "casas2018intentnet",
      "gupta2018social",
      "wang2018reinforcement",
      "gao2020vectornet",
      "hu2023imitation",
      "zhu2024sora"
    ],
    "relevant_ids": [
      "gao2020vectornet",
      "fang2020tpnet",
      "cui2019multimodal",
      "dong2021multi",
      "zhao2020tnt",
      "thiede2019analyzing",
      "gupta2018social",
      "zhang2020map",
      "chai2020multipath",
      "phan2020covernet",
      "liu2021multimodal",
      "lee2017desire",
      "ivanovic2020multimodal",
      "ye2021tpcn",
      "liang2020learning",
      "huang2021driving",
      "song2021learning",
      "salzmann2020trajectron++"
    ],
    "metrics": {
      "R@5": 0.05555555555555555,
      "R@10": 0.1111111111111111,
      "R@20": 0.3888888888888889,
      "MRR": 1.0,
      "hits": 12,
      "total_relevant": 18
    },
    "score": 0.3555555555555555,
    "timestamp": "2025-12-18T10:51:24.486548"
  },
  {
    "query": "Introduction In many real scenarios where interpretability is important<|cite_0|> or robustness is critical<|cite_1|> we may have strict requirements on the type of model that is desired. For example, domain experts in multiple industries such as chip manufacturing or oil production might have a strong preference for decision trees<|cite_2|>, since they",
    "paper_id": "2109.06961",
    "retrieved_ids": [
      "zhang2019interpreting",
      "imagenet-r",
      "gilpin2018explaining",
      "lundberg2017unified",
      "etmann2019connection",
      "bubeck2019adversarial",
      "survey0",
      "arnab2018robustness",
      "kushwhyInt",
      "fu23cotkd",
      "wang2021learning",
      "stolfo-etal-2023-causal",
      "huang2024",
      "khodabandeh2019robust",
      "wang2023surveyfactualitylargelanguage",
      "huang2023survey",
      "bastani2017interpreting",
      "tran2022plex",
      "yamada2022does",
      "teney2022id",
      "Ghorbani2019",
      "ross2018improving",
      "madsen2021posthoc",
      "overview",
      "liu2023trustworthy",
      "frosst2017distilling",
      "li2023sok",
      "zhuang2024toolchain",
      "siddiqui2019tsviz",
      "singh2023benchmarking",
      "landeghem2023document",
      "bo",
      "chen2024tree",
      "fujisawa2022logical",
      "de2023reliability",
      "dehghani2021efficiency",
      "yang2023unisim",
      "balaguer2024rag",
      "jiang2023rtmpose",
      "malinin2018predictive",
      "cho2014exponentially",
      "mehrabi2019survey",
      "Peng2017MultiagentBN",
      "zhou2023don",
      "NIPS2017_6995",
      "wu2024",
      "chakraborty2018adversarial",
      "ballet2019imperceptible",
      "safety"
    ],
    "relevant_ids": [
      "bastani2017interpreting",
      "dehghani2017fidelity",
      "frosst2017distilling",
      "ren2018learning",
      "twostageMC",
      "takd",
      "calib",
      "modelcompr2",
      "distill",
      "profweight",
      "fitnet",
      "crd",
      "furlanello2018born",
      "priv16",
      "kushwhyInt",
      "modelcompr",
      "sratio"
    ],
    "metrics": {
      "R@5": 0.0,
      "R@10": 0.058823529411764705,
      "R@20": 0.11764705882352941,
      "MRR": 0.1111111111111111,
      "hits": 3,
      "total_relevant": 17
    },
    "score": 0.050980392156862744,
    "timestamp": "2025-12-18T10:51:26.795283"
  },
  {
    "query": "Introduction \\label{intro} Graph Neural Networks (GNNs) are effective methods for analyzing graph data, and various downstream graph learning tasks such as node classification, similarity search, graph classification, and link prediction have benefited from its recent developments<|cite_0|>. However, most of the existing GNNs frameworks are supervised learning methods that have many",
    "paper_id": "2109.14159",
    "retrieved_ids": [
      "Zhou2018",
      "Zonghan2019",
      "gin",
      "jin2023survey",
      "zhu2021survey",
      "h2",
      "10.1145/3394486.3403237",
      "chen2019powerful",
      "xu2018powerful",
      "gnn",
      "cpf",
      "random-features",
      "higher-order",
      "not_all_neighbor",
      "zhao2021data",
      "Sato2020ASO",
      "mavromatis2023train",
      "rni",
      "grohewl",
      "chi2022enhancing",
      "morris2019weisfeiler",
      "wu2021graph",
      "gcn",
      "bai2021ripple",
      "article38",
      "hu2020strategies",
      "kipf2016semi",
      "choi2022finding",
      "shao2022distributed",
      "nikolentzos2020k",
      "Jaume2019",
      "liu2019hyperbolic",
      "sankar2018dynamic",
      "26_Tang_Haoteng",
      "Zhang2017NetworkRL",
      "wang2021certified",
      "Dai_2023",
      "pprgo",
      "ying2018hierarchical",
      "zhang2021backdoor",
      "zhou2021overcoming",
      "wang2022lifelong",
      "atwood2016diffusion",
      "zhou2021egnn"
    ],
    "relevant_ids": [
      "article36",
      "article31",
      "article22",
      "article24",
      "article38",
      "article27",
      "article12",
      "article34",
      "article13",
      "article18",
      "article1",
      "article2",
      "article25",
      "article10",
      "article17",
      "article32",
      "article20",
      "article37",
      "article4",
      "article15",
      "article21"
    ],
    "metrics": {
      "R@5": 0.0,
      "R@10": 0.0,
      "R@20": 0.0,
      "MRR": 0.04,
      "hits": 1,
      "total_relevant": 21
    },
    "score": 0.012,
    "timestamp": "2025-12-18T10:51:29.617784"
  },
  {
    "query": "Introduction \\label{sec:intro} The Graph Convolution Network (GCN) model is an effective graph representation learning technique. Its ability to exploit network topology offers superior performance in several applications such as node classification<|cite_0|>, recommendation systems<|cite_1|> and program repair<|cite_2|>. However, training multi-layer GCNs on large and dense graphs remains challenging due to the",
    "paper_id": "2109.13995",
    "retrieved_ids": [
      "hamilton2017representation",
      "clustergcn",
      "sgc",
      "zhu2021survey",
      "recsys",
      "ladies",
      "10.1145/3292500.3330925",
      "fastgcn",
      "ying2018graph",
      "chen2018fastgcn",
      "li2018deeper",
      "pei2020geom",
      "rong2019dropedge",
      "zou2019layerdependent",
      "vrgcn",
      "luan2019break",
      "article34",
      "sgl",
      "pareja2020evolvegcn",
      "dehmamy2019understanding",
      "chen2018stochastic",
      "gnn",
      "yidingNeurIPS20",
      "chenWHDL2020gcnii",
      "chen2019multi",
      "Zhang2017NetworkRL",
      "Zonghan2019",
      "DBLP:conf/acl/BaldwinCR18",
      "asgcn",
      "zhang2019_bgcn",
      "huang2018adaptive",
      "bastings2017graph",
      "velivckovic2017graph",
      "oono2019graph",
      "ying2018hierarchical",
      "graphsaint",
      "liu2020towards",
      "chen2019powerful",
      "zeng2020graphsaint",
      "frasca2020sign",
      "li2020deepergcn",
      "gin",
      "zhu2019aligraph",
      "article38",
      "gcnlpa",
      "xu2018powerful",
      "wang2022lifelong",
      "jin2021graph",
      "wang2020multi",
      "wang2019heterogeneous"
    ],
    "relevant_ids": [
      "asgcn",
      "gin",
      "ladies",
      "pprgo",
      "vrgcn",
      "drrepair",
      "clustergcn",
      "gcn",
      "defferrard2016convolutional",
      "appnp",
      "recsys",
      "fastgcn",
      "orig",
      "graphsage",
      "fey2021gnnautoscale",
      "mvsgcn",
      "graphsaint",
      "frasca2020sign"
    ],
    "metrics": {
      "R@5": 0.1111111111111111,
      "R@10": 0.2222222222222222,
      "R@20": 0.2777777777777778,
      "MRR": 0.5,
      "hits": 9,
      "total_relevant": 18
    },
    "score": 0.26111111111111107,
    "timestamp": "2025-12-18T10:51:32.352535"
  },
  {
    "query": "Introduction Inspired by the great success of transformer<|cite_0|> in natural language processing (NLP)<|cite_1|>, there is an increasing effort on applying it to computer vision. Following vision transformer (ViT)<|cite_2|>, which is the first attempt of applyting transformer to vision, plenty of studies<|cite_3|> have adopted transformer to dense image prediction tasks, such",
    "paper_id": "2109.08963",
    "retrieved_ids": [
      "chen2022vision",
      "khan2021transformers",
      "survey3",
      "mao2022towards",
      "han2020survey",
      "zhai2022scaling",
      "survey7",
      "tnt",
      "lee2021vision",
      "zhou2021deepvit",
      "Chen2021VisformerTV",
      "dosovitskiy2020vit",
      "amir2021deep",
      "heo2021rethinking",
      "mslongformer",
      "wang2021pvt",
      "shamshad2022transformers",
      "vit",
      "arnab2021vivit",
      "yuan2021incorporating",
      "paul2022vision",
      "chen2022adaptformer",
      "lin2022cat",
      "sun_vvt_pami_2023",
      "lin2022super",
      "li2021can",
      "wu2021rethinking",
      "song2023comprehensive",
      "survey1",
      "li2021localvit",
      "wang2021knowledge",
      "duan_visionRWKV_2024",
      "tay2020long",
      "pan202iared2",
      "shao2022adversarial",
      "dehghani2023scalingvit",
      "darcet2023vision",
      "do-VTs-see-like-CNNs",
      "yu2022metaformer",
      "zhang2022topformer",
      "liu2022convnet",
      "yu2023turning"
    ],
    "relevant_ids": [
      "yang2019reppoints",
      "vaswani2017attention",
      "chen2019hybrid",
      "kirillov2020pointrend",
      "ronneberger2015u",
      "ren2015faster",
      "cai2018cascade",
      "liu2019auto",
      "chen2020feature",
      "he2017mask",
      "chen2021you",
      "lin2017focal",
      "lin2017feature",
      "devlin2018bert",
      "chen2017deeplab",
      "wu2021cvt",
      "zhu2020deformable",
      "wang2021pyramid",
      "liu2021swin",
      "long2015fully",
      "dosovitskiy2020image",
      "carion2020end",
      "liu2018path"
    ],
    "metrics": {
      "R@5": 0.0,
      "R@10": 0.0,
      "R@20": 0.0,
      "MRR": 0.0,
      "hits": 0,
      "total_relevant": 23
    },
    "score": 0.0,
    "timestamp": "2025-12-18T10:51:35.153968"
  },
  {
    "query": "Introduction \\label{sec:intro} The advent of Generative Adversarial Networks (GANs) had a huge influence on the progress of image synthesis research and applications. Starting from low-resolution, gray-scale face images, current methods can generate high-resolution face images which are very difficult to distinguish from real photographs<|cite_0|>. While unconditional image synthesis is interesting,",
    "paper_id": "2103.13722",
    "retrieved_ids": [
      "GAN",
      "Marchesi2017",
      "wang2018high",
      "Deb19",
      "robb2020few",
      "reed2016generative",
      "brock2018biggan",
      "sauer2023stylegan",
      "kang2023scaling",
      "chen2017photographic",
      "aldausari2020video",
      "DMGAN",
      "pigan",
      "DonahueMcAuleyPuckette2019",
      "zhang2017stackgan",
      "frolov2021adversarial",
      "brock2018large",
      "bulat2018super",
      "Karnewar2020",
      "zhang2017stackgan++",
      "zhou2021cips",
      "AuxOdena",
      "Odena2017",
      "xia2022gan",
      "fang2020identity",
      "pinkney2020resolution",
      "chan2021glean",
      "casanova2021instance",
      "jin2017towards",
      "sun2022ide",
      "persion_dm",
      "granot2022drop",
      "gu2019mask",
      "graf",
      "perarnau2016invertible",
      "song2019joint",
      "hou2021guidedstyle",
      "oasis",
      "durall2019unmasking"
    ],
    "relevant_ids": [
      "BigGAN",
      "cGAN",
      "Isola_2017_CVPR",
      "Attribute2Image",
      "Wang2017HighResolutionIS",
      "VQ_VAE_2",
      "StyleGAN",
      "Zhu_2017_ICCV",
      "DMGAN",
      "Dong2017",
      "StarGAN",
      "Reed2016",
      "OPGAN",
      "LostGAN",
      "canonicalSg2Im",
      "TAGAN",
      "VAE",
      "GCN",
      "karacan2016learning",
      "Hinz2019GeneratingMO",
      "ControlGAN",
      "Sg2Im",
      "InferGAN",
      "Choi2020FromIT",
      "pavllo2020controlling",
      "PasteGAN",
      "Layout2Im",
      "frolov2021adversarial",
      "StackGAN",
      "ObjGAN",
      "wang2020domain",
      "Zhou2019",
      "AttrLayout2Im",
      "LostGANv2",
      "AttnGAN",
      "AuxOdena",
      "Grid2Im",
      "OCGAN",
      "ManiGAN"
    ],
    "metrics": {
      "R@5": 0.0,
      "R@10": 0.0,
      "R@20": 0.05128205128205128,
      "MRR": 0.08333333333333333,
      "hits": 3,
      "total_relevant": 39
    },
    "score": 0.024999999999999998,
    "timestamp": "2025-12-18T10:51:38.089201"
  },
  {
    "query": "Introduction Deep learning on point cloud analysis has been attracting more and more attention recently. Among the tasks of point cloud analysis, efficient semantic segmentation of large-scale 3D point cloud is a challenging task with huge applications<|cite_0|>. A key challenge is that 3D point cloud semantic segmentation relies on unstructured",
    "paper_id": "2103.10339",
    "retrieved_ids": [
      "Hu_2020",
      "landrieu2018large",
      "jiang2018pointsift",
      "xiao20233d",
      "liu2022less",
      "jiang2021guided",
      "Charles_2017",
      "xiao2023unsupervised",
      "fei2023self",
      "Qiu2021Att3Ddetection",
      "zhang-shellnet-iccv19",
      "wang2018sgpn",
      "cheng2021s3cnet",
      "8805456",
      "graham20183d",
      "scanobjectnn",
      "wu2019pointconv",
      "pointacc",
      "yan2021sparse",
      "xie2020pointcontrast",
      "aoki2019pointnetlk",
      "Komarichev_2019",
      "wang2018dynamic",
      "guo2021pct",
      "Sun2022PointDPDP",
      "jiang2020pointgroup",
      "behley2019semantickitti",
      "liu2019extending",
      "jaritz2019multi"
    ],
    "relevant_ids": [
      "Hu_2020",
      "Han_2020",
      "cao2019gcnet",
      "yan2020pointasnl",
      "li2017not",
      "yang2019modeling",
      "Wang_2018",
      "zhang-shellnet-iccv19",
      "8805456",
      "thomas2019kpconv",
      "Xu_2020",
      "Komarichev_2019",
      "wu2019pointconv",
      "chen2017multi",
      "qi2017pointnet++",
      "jiang2018pointsift",
      "Charles_2017"
    ],
    "metrics": {
      "R@5": 0.11764705882352941,
      "R@10": 0.17647058823529413,
      "R@20": 0.35294117647058826,
      "MRR": 1.0,
      "hits": 7,
      "total_relevant": 17
    },
    "score": 0.4,
    "timestamp": "2025-12-18T10:51:39.875719"
  },
  {
    "query": "Introduction The ability for an intelligent agent to navigate through an environment to carry out human provided instructions is one of the primary objectives of robotics and artificial intelligence. Visual navigation has emerged to enable such agents to \\emph{learn} (\\eg using deep learning) to move towards a given target object<|cite_0|>.",
    "paper_id": "2103.00446",
    "retrieved_ids": [
      "gervet2022navigating",
      "objectnav",
      "r2r",
      "thor_target_driven",
      "krantz2023navigating",
      "Morad2021EmbodiedVN",
      "envdrop",
      "ORG",
      "hill2020human",
      "chaplot2020object",
      "zhang2022lovis",
      "jacob2020vlnce",
      "cmp",
      "citi_nav",
      "qi2020oaam",
      "hong2023learning",
      "wang2023lana",
      "zhang2021towards",
      "dialog_nav",
      "hao2020towards",
      "zhu2021soon",
      "anderson2018evaluation",
      "scene_priors",
      "zhu2020vision",
      "lei2024gaussnav",
      "zhang2023vln",
      "r4r",
      "wang2020SERL",
      "an2023etpnav",
      "trlvln",
      "BRM",
      "padmakumar2021teach",
      "deng2020EGP",
      "chen2022weakly",
      "majumdar2022zson",
      "regret",
      "selfMonitor",
      "landi2019dynamic",
      "reverie",
      "francis2022core"
    ],
    "relevant_ids": [
      "savn",
      "gibson",
      "scene_priors",
      "citi_nav",
      "r2r",
      "reverie",
      "touchdown",
      "look_before_leap",
      "imagine_explore",
      "rig",
      "dreamer",
      "her_goalrl",
      "a3c",
      "leap",
      "world_models",
      "vae",
      "i2a",
      "cmp",
      "habitat",
      "dialog_nav",
      "thor_target_driven",
      "ifig",
      "thor_env",
      "graph_topo",
      "her",
      "hanna",
      "eqa",
      "dqn"
    ],
    "metrics": {
      "R@5": 0.07142857142857142,
      "R@10": 0.07142857142857142,
      "R@20": 0.17857142857142858,
      "MRR": 0.3333333333333333,
      "hits": 7,
      "total_relevant": 28
    },
    "score": 0.15,
    "timestamp": "2025-12-18T10:51:42.654468"
  },
  {
    "query": "Introduction \\begin{figure}[!t] \\centering \\includegraphics[width=\\linewidth]{figures/flops-acc.pdf} \\caption{Performance comparisons on ImageNet. With comparable GFLOPs (1.25 vs. 1.39), our proposed Scale HVT-Ti-4 surpasses DeiT-Ti by 3.03\\% in Top-1 accuracy. } \\label{fig:flops-acc} \\vspace{-10pt} \\end{figure} Equipped with the self-attention mechanism that has strong capability of capturing long-range dependencies, Transformer<|cite_0|> based models have achieved significant breakthroughs in",
    "paper_id": "2103.10619",
    "retrieved_ids": [
      "sasa",
      "chen2020pre",
      "T2T",
      "t2tformer",
      "uniformer",
      "yang2021lite",
      "wang2021notallimage16",
      "touvron2021going",
      "wang2023closer",
      "dat",
      "suchengCVPR22",
      "zhai2022scaling",
      "meng2022adavit",
      "wu2021pale",
      "ref17",
      "resformer",
      "yao2023dual",
      "mao2022towards",
      "transformertts",
      "tu2022maxvit",
      "zhou2021deepvit",
      "zhang2022vitaev2",
      "zhou2022understanding",
      "tnt",
      "wu2021rethinking",
      "nat",
      "guo2022cmt",
      "li2022efficientformer",
      "lin2022super",
      "Xie:2021:SSE",
      "shaker2022unetr++",
      "wu2021cvt",
      "yao2022wave",
      "chen2023pixartalpha",
      "liu2021swin",
      "simmim",
      "dong2021cswin",
      "pan202iared2",
      "xie2021fignerf",
      "zhang2022parcnet",
      "li2016ternary",
      "inceptionformer",
      "tang2023elasticvit",
      "localmamba",
      "fang2020densely",
      "efficientvmamba",
      "liu2021video",
      "Yan2021ConTNetWN",
      "tang2022patchslim",
      "yu2022metaformer",
      "li2024llavamed"
    ],
    "relevant_ids": [
      "sparse_attn",
      "peg",
      "tan2019efficientnet",
      "dedetr",
      "Devlin2019BERTPO",
      "t2tformer",
      "distillbert",
      "pvt",
      "simonyan2014very",
      "cbert",
      "vit",
      "jiao2019tinybert",
      "lin2017feature",
      "funnel",
      "peng2021random",
      "lrnet",
      "vistr",
      "performer",
      "li2017pruning",
      "powerbert",
      "maxdlab",
      "axial_attn",
      "rae2020compressive",
      "katharopoulos2020transformers",
      "deit",
      "transformer",
      "resnet",
      "selfconv",
      "ternarybert",
      "sasa",
      "fqt",
      "dai2019transformer",
      "hat",
      "axiallab",
      "reformer",
      "botnet",
      "linformer",
      "detr",
      "sixteen_heads"
    ],
    "metrics": {
      "R@5": 0.05128205128205128,
      "R@10": 0.05128205128205128,
      "R@20": 0.05128205128205128,
      "MRR": 1.0,
      "hits": 2,
      "total_relevant": 39
    },
    "score": 0.33589743589743587,
    "timestamp": "2025-12-18T10:51:47.492242"
  },
  {
    "query": "Introduction \\begin{figure}[t] \\centering \\includegraphics[width=0.95\\columnwidth]{fps.pdf} \\caption{Inference speed and mIoU performance on the Cityscapes test set. Our method is marked as red points, while grey dots represent other methods. $\\ast$ indicates that the model uses TensorRT for acceleration.} \\label{fig1} \\end{figure} Scene parsing, also known as semantic segmentation, predicts dense labels for all",
    "paper_id": "2103.05930",
    "retrieved_ids": [
      "cordts2016cityscapes",
      "wu2023diffumask",
      "ccnet",
      "jegou2017one",
      "SAS",
      "edanet",
      "dai2018scancomplete",
      "luddecke2022image",
      "lin2016scribblesup",
      "fanet",
      "liu2020efficient_per_frame",
      "jain2021semask",
      "espnet",
      "swiftnet",
      "ddrnet",
      "li2019dfanet",
      "NekrasovS018",
      "zheng2021rethinking_seg",
      "xiao20233d",
      "He_2019_CVPR",
      "CenterNet",
      "wang2021dense",
      "hou2020realtime",
      "scannet200",
      "Xie:2021:SSE",
      "kirillov2020pointrend",
      "li2020semantic",
      "scene_priors",
      "niemeyer2024radsplat",
      "solov2",
      "wang2020centermask",
      "bisenet",
      "FastPSNet1",
      "DVSN",
      "hurtik2022poly",
      "dcama",
      "jain2023oneformer",
      "SegFix",
      "panopticdeeplab",
      "lahoud20193d",
      "fastscnn",
      "riaz2021fouriernet",
      "chen2021hierarchical",
      "ghiasi2022scaling",
      "depth-layering",
      "ocnet",
      "pfenet",
      "peng2023openscene",
      "Yang2019",
      "behley2019semantickitti",
      "fang2019instaboost",
      "Sofiiuk2019",
      "hqsam"
    ],
    "relevant_ids": [
      "li2020semantic",
      "lin2017refinenet",
      "he2016identity",
      "vaswani2017attention",
      "zhang2019acfnet",
      "huang2019ccnet",
      "ronneberger2015u",
      "fu2019dual",
      "cordts2016cityscapes",
      "badrinarayanan2017segnet",
      "cheng2016long",
      "Li2019GFFGF",
      "zhao2017pyramid",
      "huang2020alignseg",
      "wang2018non",
      "fu2019adaptive",
      "mou2019relation",
      "hou2020strip",
      "yu2018bisenet",
      "yu2020bisenet",
      "emara2019liteseg",
      "long2015fully",
      "li2019dfanet",
      "zhao2018icnet",
      "Yuan2018OCNetOC"
    ],
    "metrics": {
      "R@5": 0.04,
      "R@10": 0.04,
      "R@20": 0.08,
      "MRR": 1.0,
      "hits": 3,
      "total_relevant": 25
    },
    "score": 0.328,
    "timestamp": "2025-12-18T10:51:51.736346"
  },
  {
    "query": "Introduction Action recognition has been an essential building block for video understanding, whereby the key is to extract powerful spatial-temporal features that contain rich motion information. 3D CNN-based methods<|cite_0|> establish spatial and temporal correlation among video frames by employing 3D convolution kernels. Due to the large number of parameters, these",
    "paper_id": "2103.12278",
    "retrieved_ids": [
      "tran2015c3d",
      "tran2018r3d",
      "gberta_2021_ICML",
      "meng2019interpretable",
      "jiang2019stm",
      "kwon2020motionsqueeze",
      "christoph2016spatiotemporal",
      "wang2017spatiotemporal",
      "lea2017temporal",
      "xie2018s3d",
      "diba2018spatio",
      "liu2020teinet",
      "lin2019tsm",
      "weng2020temporal",
      "hou2017tube",
      "SpatialTG",
      "wang2016hierarchical",
      "corrnet",
      "gst",
      "Feichtenhofer-CVPR-2016",
      "wang2016tsn",
      "tdn",
      "simonyan2014twostream",
      "li2021spatial",
      "li2020tea",
      "qiu2017learning",
      "liu2020tam",
      "grinciunaite16",
      "xu2017rc3d",
      "long2022dynamic",
      "choy20194d",
      "lee2021diverse",
      "feichtenhofer2019slowfast",
      "long2022sifa"
    ],
    "relevant_ids": [
      "meng2019interpretable",
      "liu2020teinet",
      "lin2019tsm",
      "woo2018cbam",
      "qiu2017learning",
      "hu2018squeeze",
      "tran2018closer",
      "li2020tea",
      "tran2015learning",
      "hara2018can",
      "wang2016hierarchical",
      "diba2018spatio",
      "kay2017kinetics",
      "jiang2019stm",
      "carreira2017quo",
      "wang2017residual",
      "weng2020temporal",
      "wang2016temporal"
    ],
    "metrics": {
      "R@5": 0.1111111111111111,
      "R@10": 0.1111111111111111,
      "R@20": 0.3888888888888889,
      "MRR": 0.25,
      "hits": 9,
      "total_relevant": 18
    },
    "score": 0.1527777777777778,
    "timestamp": "2025-12-18T10:51:54.065311"
  },
  {
    "query": "Introduction \\label{sec:intro} The vulnerability of neural networks to adversarial attack has been plaguing machine learning researchers ever since the discovery by Szegedy et al.<|cite_0|>. In the years since, many research efforts have been geared towards making neural networks robust to adversarial perturbations, but many defense strategies have failed to stand",
    "paper_id": "2103.11589",
    "retrieved_ids": [
      "metzen2017detecting",
      "bai2021recent",
      "sur2",
      "chakraborty2018adversarial",
      "Madry2017",
      "overview",
      "Schott2018a",
      "Mad+18",
      "akhtar2018threat",
      "NRP",
      "yuan2019adversarial",
      "grosse2016adversarial",
      "huang2021exploring",
      "raghunathan2018certified",
      "potdevin19",
      "strauss2017ensemble",
      "shafahi2018are",
      "xiao2018generating",
      "biggio_2018",
      "samangouei2018defense",
      "meng2017magnet",
      "jakubovitz2018improving",
      "simon2019first",
      "xu2019adversarial",
      "carlini2017adversarial",
      "Raghunathan2018",
      "papernot2016distillation",
      "26_Tang_Haoteng",
      "fawzi2018adversarial",
      "dong2019evading",
      "isit2018",
      "2_Dai",
      "yu2021lafeat",
      "papernot2016crafting",
      "lecuyer2018certified",
      "liu2019extending",
      "blau2022threat",
      "shamir2019simple",
      "hwang2019puvae",
      "li2020towards",
      "PapernotM17",
      "Carlini_Dill_2018",
      "xiao2018spatially"
    ],
    "relevant_ids": [
      "goodfellow2015explaining",
      "xie2020adversarial",
      "verma2019manifold",
      "hendrycks2019natural",
      "tramer2020adaptive",
      "zhang2019theoretically",
      "lee2020adversarial",
      "ilyas2019adversarial",
      "zhang2018mixup",
      "pang2020mixup",
      "archambault2019mixup",
      "guo2019mixup",
      "wong2020overfitting",
      "madry2018towards",
      "athalye2018obfuscated",
      "szegedy2014intriguing"
    ],
    "metrics": {
      "R@5": 0.0,
      "R@10": 0.0,
      "R@20": 0.0,
      "MRR": 0.0,
      "hits": 0,
      "total_relevant": 16
    },
    "score": 0.0,
    "timestamp": "2025-12-18T10:51:56.205963"
  },
  {
    "query": "Introduction Transformers<|cite_0|>, characterized by stacked self-attention modules and feed-forward transformations, have become a staple in modern deep learning, natural language processing<|cite_1|> and even computer vision<|cite_2|>. One key defining characteristics in the self-attention mechanism is the global receptive field in which each token is accessible to every other token in the",
    "paper_id": "2103.01075",
    "retrieved_ids": [
      "vig-belinkov-2019-analyzing",
      "khan2021transformers",
      "SEED",
      "zhou2022understanding",
      "Sukhbaatar:2019adaptive",
      "abnar2020quantifying_attentionrollout",
      "chu2021twins",
      "tay2020efficient",
      "sukhbaatar-etal-2019-adaptive",
      "melas2021resmlp",
      "yang2021focal",
      "chen2022regionvit",
      "dat",
      "lin2022cat",
      "ding2022davit",
      "katharopoulos2020transformers",
      "dosovitskiy2020vit",
      "ICLRsub2022",
      "han2020survey",
      "vit",
      "chefer2021transformerbeyond",
      "zhu2019empirical",
      "paul2022vision",
      "lee2021vision",
      "sukhbaatar2019augmentingmemory",
      "wu2021pale",
      "ergen2022convexifying",
      "ali2023centered",
      "Sukhbaatar:2019augmenting",
      "edelman2022inductive",
      "yang2022scalablevit",
      "xu2022evo_token",
      "xu2021vitae",
      "dehghani2018universal",
      "lu2021soft",
      "DUAL-VIT",
      "darcet2023vision",
      "selfconv",
      "zhang2022vitaev2",
      "li2021localvit",
      "suchengCVPR22",
      "xcit",
      "li2022CoT",
      "shen2018reinforced",
      "yu2021rethinking",
      "yu2022metaformer",
      "Lambdanetworks",
      "lou2022crosstoken",
      "mamba"
    ],
    "relevant_ids": [
      "tay2020efficient",
      "vaswani2017attention",
      "srivastava2015highway",
      "wang2020linformer",
      "bahdanau2014neural",
      "liu2020very",
      "bapna2018training",
      "raffel2019exploring",
      "brown2020language",
      "chelba2013one",
      "parikh2016decomposable",
      "devlin2018bert",
      "he2016deep",
      "choromanski2020rethinking",
      "tay2018densely",
      "dai2019transformer",
      "zaheer2020big",
      "dehghani2018universal",
      "he2020realformer",
      "huang2017densely",
      "tay2020long",
      "dosovitskiy2020image"
    ],
    "metrics": {
      "R@5": 0.0,
      "R@10": 0.045454545454545456,
      "R@20": 0.045454545454545456,
      "MRR": 0.125,
      "hits": 2,
      "total_relevant": 22
    },
    "score": 0.05113636363636363,
    "timestamp": "2025-12-18T10:51:59.015006"
  },
  {
    "query": "Introduction \\begin{figure*}[ht] \\centering \\includegraphics[scale=0.65]{figures/shuffle-spl.pdf} \\caption{Our framework. {\\bf [Bottom]} Patches are generated and permuted on the fly. The network is pre-trained on the associated permutation. Dotted layers indicate weight sharing across input patches. Embeddings used for downstream tasks are extracted by removing the network's last few layers. {\\bf [Top]} Downstream training",
    "paper_id": "2103.09879",
    "retrieved_ids": [
      "frankle2020early",
      "santa2017deeppermnet",
      "bao2021beit",
      "selfie",
      "zhang2019hibert",
      "brea2019weight",
      "simmim",
      "ScorebasedGraph",
      "ma2020weightnet",
      "zagoruyko2017diracnets",
      "resmlp",
      "cui2022democratizing",
      "pmlr-v139-lewis21a",
      "pointgpt",
      "mallya2018piggyback",
      "kim2018paraphrasing",
      "yu2018slimmable",
      "qi2018semi",
      "yu2021lite",
      "ham2023modulating",
      "skorokhodov2021adversarial",
      "wang2023patch",
      "pfenet",
      "jabri2022scalable",
      "park2019semantic",
      "lin2016scribblesup",
      "Khaki_2023_CVPR",
      "zhang2022minivit",
      "graikos2023learned",
      "Jordan2022REPAIRRP",
      "patchesallyouneed",
      "ding2021diverse",
      "wei2023elite",
      "melas2021resmlp",
      "karimi2021convolution",
      "jang2024model_Stock",
      "sun2019meta",
      "jelassi2022vision",
      "vedaldi2016instance",
      "t2tformer",
      "liu2023cones",
      "jin2021teachers",
      "pmlr-v119-singla20a",
      "cai-et-al:scheme",
      "nitzan2023domain",
      "visprompt",
      "abdal2020image2stylegan++",
      "li2024merge",
      "jiang2023masked",
      "he2016identity",
      "li2021localvit",
      "Yu2022CoCaCC",
      "dani2023devil",
      "si2024freeu",
      "tang2022patchslim"
    ],
    "relevant_ids": [
      "noroozi2016unsupervised",
      "devlin2018bert",
      "conneau2020unsupervised",
      "ravanelli2020multi",
      "doersch2015unsupervised",
      "rajpurkar2017chexnet",
      "caron2018deep",
      "chen2020simple",
      "berthet2020learning",
      "riviere2020unsupervised",
      "blondel2020fast",
      "inpainting",
      "rotation",
      "santa2017deeppermnet",
      "liu2020mockingjay",
      "colorization",
      "contrastivAudio2020"
    ],
    "metrics": {
      "R@5": 0.058823529411764705,
      "R@10": 0.058823529411764705,
      "R@20": 0.058823529411764705,
      "MRR": 0.5,
      "hits": 1,
      "total_relevant": 17
    },
    "score": 0.19117647058823528,
    "timestamp": "2025-12-18T10:52:03.634181"
  },
  {
    "query": "Introduction \\raggedbottom Video semantic segmentation is a compute-intensive vision task. It aims at classifying pixels in video frames into semantic classes. This task often has real-time or even faster than real-time requirements in data-center applications, in order to process hours-long videos in a much shorter time. The ever increasing video",
    "paper_id": "2103.08834",
    "retrieved_ids": [
      "huang2018efficient",
      "Clockwork",
      "hu2020temporally",
      "NekrasovS018",
      "Accel",
      "long2015fully",
      "LLVSS",
      "masktrackrcnn",
      "zhao2018icnet",
      "NilssonS18",
      "jain2018fast",
      "wang2021tmanet",
      "shaban2017oneshot",
      "fcn",
      "kurzman2019class",
      "ifc",
      "strudel2021segmenter",
      "wang2021survey",
      "liu2020efficient_per_frame",
      "hwang2021video",
      "lin2021video",
      "du2023video",
      "ovis",
      "metric",
      "xu2018youtube",
      "cheng2021rethinking",
      "kim2020vps",
      "de2023reliability",
      "bisenet",
      "han2022visolo",
      "chen2016semantic",
      "fastscnn",
      "vistr",
      "qi2022occluded",
      "DVSN",
      "kirillov2019panoptic",
      "Li2018",
      "weber2021step",
      "woo2021learning",
      "hou2020realtime",
      "FastPSNet1",
      "brooks2022generating",
      "wang2023genlvideo",
      "huang2022real",
      "flow_guided",
      "liu2018mobile",
      "nivlff",
      "sharir2021image",
      "seichter2022efficient"
    ],
    "relevant_ids": [
      "deeplabv3+",
      "pspnet",
      "swiftnet",
      "jain2018fast",
      "espnet",
      "fastscnn",
      "DVSN",
      "bisenet",
      "Clockwork",
      "DFF",
      "cityscapes",
      "LLVSS",
      "Accel"
    ],
    "metrics": {
      "R@5": 0.15384615384615385,
      "R@10": 0.23076923076923078,
      "R@20": 0.3076923076923077,
      "MRR": 0.5,
      "hits": 7,
      "total_relevant": 13
    },
    "score": 0.28076923076923077,
    "timestamp": "2025-12-18T10:52:06.823127"
  },
  {
    "query": "Introduction Recent interest in neural network architectures that operate on sets<|cite_0|> has garnered momentum given that many problems in machine learning can be reformulated as learning functions on sets. Problems such as point cloud classification<|cite_1|>, image reconstruction<|cite_2|>, classification, set prediction<|cite_3|>, and set extension can all be cast in this framework",
    "paper_id": "2103.01615",
    "retrieved_ids": [
      "rezatofighi2021learn",
      "wistuba2019survey",
      "scanobjectnn",
      "abs-1809-04184",
      "elsken2019neural",
      "evolvingNN",
      "white2023neural",
      "fan2017point",
      "xiao2023unsupervised",
      "zhang2019deep",
      "koestler2022intrinsic",
      "neuralpredictor",
      "kovachki2021neural",
      "Charles_2017",
      "point_set",
      "fei2023self",
      "Zoph:2017uo",
      "wu2019pointconv",
      "qi2017pointnet++",
      "wang2018dynamic",
      "zhang-shellnet-iccv19",
      "sener2017active",
      "xu2022image2point",
      "settransformer",
      "mescheder2019occupancy",
      "atzmon2019controlling",
      "such2020generative",
      "sun2017revisiting",
      "xie2021neural",
      "deepsets",
      "agrawal2014analyzing",
      "cai2018efficient",
      "soelch2019deep",
      "aoki2019pointnetlk",
      "bronstein2017geometric",
      "Ren2018",
      "polytopes",
      "gilmer2017neural",
      "liu2019extending",
      "yang2019adversarial",
      "lin2020regularizing",
      "xiang2019generating",
      "monti2017geometric",
      "pvrcnn",
      "gropp2020implicit",
      "li2019sgas",
      "grosse2016adversarial",
      "zhou2014object",
      "scardapane2020should"
    ],
    "relevant_ids": [
      "cnp",
      "anp",
      "gru",
      "deepsets",
      "settransformer",
      "janossy",
      "transformer",
      "fspool",
      "slotattention",
      "modelnet",
      "celeba",
      "causalsignals"
    ],
    "metrics": {
      "R@5": 0.0,
      "R@10": 0.0,
      "R@20": 0.0,
      "MRR": 0.041666666666666664,
      "hits": 2,
      "total_relevant": 12
    },
    "score": 0.012499999999999999,
    "timestamp": "2025-12-18T10:52:09.216365"
  },
  {
    "query": "Introduction \\label{sec:intro} Self-Supervised learning (SSL) has emerged as powerful representation learning paradigm bridging the gap between unsupervised and supervised learning methods. Recent developments across different fields, such as Computer Vision (CV) and Natural Language Processing (NLP), achieve promising results using SSL techniques in some cases performing better than supervised methods<|cite_0|>.",
    "paper_id": "2103.14958",
    "retrieved_ids": [
      "van2021revisiting",
      "lee2024computer",
      "compress",
      "li2022data",
      "zhang2022leverage",
      "survey3",
      "chen23l_interspeech",
      "tian2021divide",
      "Azabou2021",
      "Schiappa-ACM-2023",
      "garrido2022duality",
      "chen2022semi",
      "liu2020self",
      "bendidi2023free",
      "li2023mert",
      "Tian2021UnderstandingSL",
      "kalyan2021ammus",
      "selvaraju2021casting",
      "ssl",
      "yang2022survey",
      "ericsson2021well",
      "odin",
      "le2020contrastive",
      "henaff2022object",
      "ermolov2020whitening",
      "zheng2021weakly",
      "cao2022synergistic",
      "wang2022does",
      "wavelablm",
      "Tosh2021ContrastiveLM",
      "iscen2019label",
      "cao2021open",
      "gyawali2019semi",
      "deacl",
      "wang2021knowledge",
      "dangovski2021equivariant",
      "chang2022speechprompt",
      "chaitanya2020contrastive",
      "haochen2022theoretical",
      "liu2021contrastive",
      "fini2021unified"
    ],
    "relevant_ids": [
      "robinson2021contrastive",
      "10.5555/3294771.3294869",
      "zou2019layerdependent",
      "kipf2017semisupervised",
      "10.1145/3292500.3330925",
      "devlin2019bert",
      "richemond2020byol",
      "hu2020strategies",
      "grill2020bootstrap",
      "10.1145/2623330.2623732",
      "velickovic2018graph",
      "brown2020language",
      "pmlr-v119-chen20j",
      "10.1145/2736277.2741093",
      "srinivas2020curl",
      "10.1145/3394486.3403237",
      "chen2020exploring",
      "10.1145/3394486.3403168",
      "10.1145/3394486.3403192",
      "kefato2020gossip",
      "Velickovic2019DeepGI",
      "hassani2020contrastive",
      "he2020momentum",
      "chen2018fastgcn",
      "zeng2020graphsaint"
    ],
    "metrics": {
      "R@5": 0.0,
      "R@10": 0.0,
      "R@20": 0.0,
      "MRR": 0.0,
      "hits": 0,
      "total_relevant": 25
    },
    "score": 0.0,
    "timestamp": "2025-12-18T10:52:11.403389"
  },
  {
    "query": "Introduction Despite achieving exceptional performance in benign settings, modern machine learning models have been shown to be highly vulnerable to inputs, known as \\emph{adversarial examples}, crafted with targeted but imperceptible perturbations<|cite_0|>. This discovery has prompted a wave of research studies to propose defense mechanisms, including heuristic approaches<|cite_1|> and certifiable methods<|cite_2|>.",
    "paper_id": "2103.12913",
    "retrieved_ids": [
      "overview",
      "PMJ+16",
      "yuan2019adversarial",
      "raghunathan2018certified",
      "qin2019imperceptible",
      "metzen2017detecting",
      "goodfellow2015laceyella",
      "Goodfellow2014",
      "Raghunathan2018",
      "bai2021recent",
      "grosse2017statistical",
      "kurakin2016adversarial",
      "meng2017magnet",
      "potdevin19",
      "carlini2017adversarial",
      "Goodfellow2016",
      "brown2018unrestricted",
      "xiao2020one",
      "biggio_2018",
      "strauss2017ensemble",
      "chakraborty2018adversarial",
      "hendrycks2019natural",
      "lecuyer2018certified",
      "Carlini_Dill_2018",
      "sur2",
      "papernot2016transferability",
      "tramer2020fundamental",
      "raghunathan2018semidefinite",
      "tramer2017space",
      "papernot2016distillation",
      "xie2017mitigating",
      "yu2021lafeat",
      "tramer2017ensemble",
      "croce2020reliable",
      "isit2018",
      "li2020towards",
      "samangouei2018defense",
      "blau2022threat",
      "cohen2019certified",
      "DBLP:conf/iclr/SinhaND18",
      "li2019certified",
      "papernot2017practical",
      "Cohen2019",
      "boucher2021bad",
      "ford2019adversarial",
      "stutz2020confidence",
      "xue2024pixelbarrierdiffusionmodels",
      "SI-NI-FGSM",
      "Li2020BackdoorLA",
      "wang2020backdoor",
      "PapernotM17"
    ],
    "relevant_ids": [
      "fawzi2018adversarial",
      "mahloujifar2019curse",
      "papernot2016distillation",
      "dohmatob2018generalized",
      "cohen2019certified",
      "zhang2020understanding",
      "gilmer2018adversarial",
      "bhagoji2019lower",
      "mahloujifar2019empirically",
      "zhang2019theoretically",
      "madry2017towards",
      "shafahi2018adversarial",
      "wong2018provable",
      "goodfellow2015explaining",
      "szegedy2014intriguing"
    ],
    "metrics": {
      "R@5": 0.0,
      "R@10": 0.0,
      "R@20": 0.0,
      "MRR": 0.03333333333333333,
      "hits": 2,
      "total_relevant": 15
    },
    "score": 0.01,
    "timestamp": "2025-12-18T10:52:14.242653"
  },
  {
    "query": "Introduction \\label{intro_ewn} Deep neural networks are susceptible to {\\em adversarial attacks} -- the presentation of almost imperceptibly manipulated ``adversarial'' inputs that cause the system to misclassify them<|cite_0|>. A variety of defense techniques have been proposed in the literature to address this problem. The most successful line of work is {\\em",
    "paper_id": "2103.08265",
    "retrieved_ids": [
      "fawaz2019adversarial",
      "yuan2019adversarial",
      "Madry2017",
      "chakraborty2018adversarial",
      "PMJ+16",
      "Mad+18",
      "sur2",
      "akhtar2018threat",
      "ma2021understanding",
      "liang2017deep",
      "potdevin19",
      "xu2017feature",
      "he2019parametric",
      "yan2018deep",
      "strauss2017ensemble",
      "samangouei2018defense",
      "fineprune2018",
      "papernot2016distillation",
      "xu2019adversarial",
      "metzen2017detecting",
      "vijaykeerthy2018hardening",
      "overview",
      "liu2019extending",
      "meng2017magnet",
      "carlini2017adversarial",
      "2_Dai",
      "ghosh2019resisting",
      "shafahi2018are",
      "su2019one",
      "prakash2018deflecting",
      "NRP",
      "chou2020senti",
      "yu2021lafeat",
      "xiao2020one",
      "cheng2018seq2sick",
      "biggio_2018",
      "IJADD17",
      "Carlini_Dill_2018",
      "ganeshan2019fda",
      "das2017keeping",
      "PapernotM17"
    ],
    "relevant_ids": [
      "papernot2016distillation",
      "moosavi2016deepfool",
      "kurakin2016adversarial",
      "yang2019me",
      "apernot2017practical",
      "gilmer2019adversarial",
      "cisse2017parseval",
      "zhang2019theoretically",
      "carmon2019unlabeled",
      "xie2019feature",
      "szegedy2013properties",
      "goodfellow2014explaining",
      "hendrycks2019using",
      "madry2018towards",
      "xu2012robustnessvi",
      "tsipras2018robustness"
    ],
    "metrics": {
      "R@5": 0.0,
      "R@10": 0.0,
      "R@20": 0.0625,
      "MRR": 0.05555555555555555,
      "hits": 1,
      "total_relevant": 16
    },
    "score": 0.016666666666666666,
    "timestamp": "2025-12-18T10:52:16.263461"
  },
  {
    "query": "Introduction The investigation on image editing is growing as it reduces significant manual efforts during image content generation. Benefiting from the realistic image representations brought by convolutional neural networks (CNNs), image editing is able to create meaningful while visually pleasant content. As shown in Fig.~\\ref{fig:teaser}, users can draw arbitrary holes",
    "paper_id": "2103.12723",
    "retrieved_ids": [
      "liu2018image",
      "sheynin2024emu",
      "zhu2016generative",
      "instructpix2pix",
      "edit",
      "hui2024hq",
      "brock2017neural",
      "wang2023imagen",
      "ling2021editgan",
      "zhang2024hive",
      "basu2023editval",
      "yu2018generative",
      "zhuang2021enjoy",
      "elharrouss2020image",
      "simsar2023lime",
      "paint",
      "zhang2022sine",
      "ceylan2023pix2video",
      "portenier2018faceshop",
      "matsunaga2022fine",
      "bar2022text2live",
      "kim2023reference",
      "meng2022sdedit",
      "li2023layerdiffusion",
      "yu2019free",
      "thies2019deferred",
      "mao2023guided",
      "li2024tuning",
      "wu2019editing",
      "ren2024grounded",
      "perarnau2016invertible",
      "Avrahami_2022_CVPR",
      "sine",
      "ccedit",
      "aksan2018deepwriting",
      "mikaeili2023sked",
      "kandala2024pix2gif",
      "hong2018learning",
      "repaintnerf",
      "song2022clipvg",
      "huang2024creativesynth",
      "lin2023regeneration",
      "dong2019fashion",
      "esser2023structure",
      "blending",
      "dragon",
      "parmar2023zero",
      "xie2021fignerf",
      "ghosh2019interactive",
      "liu2022nerf"
    ],
    "relevant_ids": [
      "jo2019sc",
      "pathak2016context",
      "song2019joint",
      "isola2017image",
      "nam2018text",
      "perarnau2016invertible",
      "shen2019interpreting",
      "portenier2018faceshop",
      "liu2019coherent",
      "wang2018pix2pixHD",
      "yan2018shift",
      "gu2019mask",
      "PConv",
      "chen2018language",
      "hong2018learning",
      "dong2019fashion",
      "song2018vital",
      "wang2020rethinking",
      "Bau:Ganpaint:2019",
      "song2017crest",
      "yu2019free"
    ],
    "metrics": {
      "R@5": 0.0,
      "R@10": 0.0,
      "R@20": 0.047619047619047616,
      "MRR": 0.05263157894736842,
      "hits": 5,
      "total_relevant": 21
    },
    "score": 0.015789473684210523,
    "timestamp": "2025-12-18T10:52:18.525277"
  },
  {
    "query": "Introduction \\label{submission} Sparse expert models enable sparse computation by spreading model capacity across a set of experts, while ensuring that only a small subset of the experts are used for each input<|cite_0|>. Sparse models can often realize the strong performance gains that come with training very large models, while also",
    "paper_id": "2103.16716",
    "retrieved_ids": [
      "riquelme2021scaling",
      "puigcerver2023sparse",
      "stmoe",
      "pmlr-v139-lewis21a",
      "sparse-upcycling",
      "jiang2024mixtral",
      "artetxe2022efficient",
      "lu2024",
      "pan2024",
      "fevry2020entities",
      "chen2022taskspecific",
      "Srinivas2017TrainingSN",
      "sparsemomentum",
      "shen2023mixtureofexperts",
      "krajewski2024scaling",
      "mirzadeh2023relustrikesbackexploiting",
      "moe",
      "du2022glamefficientscalinglanguage",
      "roller2021hash",
      "dai2024deepseekmoe",
      "zhou2022mixtureofexperts",
      "kudugunta2021distillation",
      "yu2022efficient",
      "huang2024",
      "martins2016softmax",
      "szatkowski2024exploitingactivationsparsitydense",
      "hazimeh2021dselectk",
      "li2023",
      "ahmad2019can",
      "mustafa2022multimodal",
      "model_compression",
      "zeng2024",
      "zuo2022taming",
      "hinton2015distilling",
      "locatello2019challenging",
      "hooker2020characterising",
      "iofinova2021well",
      "DBLP:conf/icml/KongSSKO20",
      "pham2022revisiting",
      "namburi2023cost",
      "zhang2022",
      "wu2024",
      "qiu2023exploring",
      "li2024merge",
      "stanton2021does"
    ],
    "relevant_ids": [
      "riggingthelottery",
      "switch",
      "gshard",
      "knnmt",
      "roy2020efficient",
      "dynamicrepara",
      "pkn",
      "child2019generating",
      "moe",
      "angela_mmmt",
      "sparsemomentum",
      "strubell2019energy",
      "knnlm",
      "bapna2019simple",
      "adapter",
      "correia2019adaptively"
    ],
    "metrics": {
      "R@5": 0.0,
      "R@10": 0.0,
      "R@20": 0.125,
      "MRR": 0.07692307692307693,
      "hits": 2,
      "total_relevant": 16
    },
    "score": 0.023076923076923078,
    "timestamp": "2025-12-18T10:52:20.495461"
  },
  {
    "query": "Introduction Recent years have witnessed significant progress in object detection<|cite_0|> in still images. However, directly applying these detectors to videos faces new challenges. First, applying the deep networks on all video frames introduces unaffordable computational cost. Second, recognition accuracy suffers from deteriorated appearances in videos that are seldom observed in",
    "paper_id": "1711.11577",
    "retrieved_ids": [
      "zhao2019object",
      "mot_survey",
      "DFF",
      "han2016seqnms",
      "liu2020teinet",
      "flow_guided",
      "kang2016object",
      "object_tracking_in_autunomous_driving",
      "Schiappa-ACM-2023",
      "oza2021unsupervised",
      "kang2017tpn",
      "michaelis2019benchmarking",
      "STLattice2018CVPR",
      "zhou2014object",
      "oh2019video",
      "kang2016tcnn",
      "zhu2017flow",
      "deformabledetr",
      "wang2021survey",
      "zhu2020deformable",
      "memory_guided",
      "liu2018mobile",
      "FRTM",
      "eykholt2018physical",
      "Zareian_2021_CVPR",
      "simonyan2014twostream",
      "Feichtenhofer17DetectTrack",
      "lu2022video",
      "lin2019tsm",
      "gberta_2021_ICML",
      "tran2018r3d",
      "RetinaNet",
      "liang2023movideo",
      "Clockwork",
      "weng2020temporal",
      "LLVSS",
      "ControlVideo",
      "wang2021taotad",
      "cogvideo",
      "renderavideo"
    ],
    "relevant_ids": [
      "Feichtenhofer17DetectTrack",
      "zhu2017flow",
      "kang2016object",
      "redmon2016yolo9000",
      "zhang2017shufflenet",
      "szegedy2016inception",
      "huang2016densely",
      "girshick2015fast",
      "redmon2016you",
      "kang2016tcnn",
      "kim2016pvanet",
      "huang2016speed",
      "ren2015faster",
      "han2016seqnms",
      "he2017mask",
      "zhu2016dff",
      "girshick2014rich",
      "he2014spatial",
      "lin2017focal",
      "kang2017tpn",
      "chollet2016xception",
      "he2016deep",
      "dai2016rfcn",
      "lin2016fpn",
      "lee2016multi",
      "liu2016ssd",
      "howard2017mobilenets",
      "xie2017resnext",
      "dai2017deformable",
      "simonyan2015very",
      "szegedy2015going"
    ],
    "metrics": {
      "R@5": 0.03225806451612903,
      "R@10": 0.06451612903225806,
      "R@20": 0.16129032258064516,
      "MRR": 0.25,
      "hits": 6,
      "total_relevant": 31
    },
    "score": 0.10725806451612903,
    "timestamp": "2025-12-18T10:52:22.290778"
  },
  {
    "query": "Introduction \\label{sec:intro} In the adversarial-perturbation problem for neural networks, an adversary starts with a neural network model $F$ and a point $\\bfx$ that $F$ classifies correctly (we assume that $F$ ends with a softmax layer, which is common in the literature), and crafts a small perturbation to produce another point",
    "paper_id": "1711.08001",
    "retrieved_ids": [
      "metzen2017detecting",
      "goodfellow2015laceyella",
      "overview",
      "huang2015learning",
      "wu2018understanding",
      "narodytska2016simple",
      "grosse2016adversarial",
      "xiang2019generating",
      "Hendrycks2019BenchmarkingNN",
      "poursaeed2018generative",
      "Goodfellow2014",
      "kurakin2016adversarial",
      "tramer2019adversarial",
      "moosavi2017universal",
      "Schott2018a",
      "Goodfellow2016",
      "carlini2017towards",
      "iclrw2018",
      "yuan2019adversarial",
      "chakraborty2018adversarial",
      "gilmer2018adversarial",
      "sur2",
      "meng2017magnet",
      "PMJ+16",
      "charles2018geometric",
      "strauss2017ensemble",
      "alzantot2018generating",
      "raghunathan2018certified",
      "yan2018deep",
      "Raghunathan2018",
      "Carlini2016",
      "huang2019enhancing",
      "tramer2020fundamental",
      "papernot2016distillation",
      "mok2021advrush",
      "yao2019trust",
      "li2019nattack",
      "tsp2019",
      "liu2019extending",
      "ding2019mma",
      "samangouei2018defense",
      "akhtar2018threat",
      "li2020practical",
      "Croce2020MinimallyDA",
      "kang2021stable",
      "su2019one",
      "brendel2017decision",
      "dhillon2018stochastic",
      "chen2018shapeshifter",
      "ganeshan2019fda"
    ],
    "relevant_ids": [
      "CW17",
      "MMSTV17",
      "IJADD17",
      "SZSBEGF13",
      "MMKI17",
      "CW17-2",
      "MC17",
      "GSS14",
      "PMJ+16",
      "PMWJS16",
      "MDFF16"
    ],
    "metrics": {
      "R@5": 0.0,
      "R@10": 0.0,
      "R@20": 0.0,
      "MRR": 0.041666666666666664,
      "hits": 1,
      "total_relevant": 11
    },
    "score": 0.012499999999999999,
    "timestamp": "2025-12-18T10:52:24.755414"
  },
  {
    "query": "Introduction Deep convolutional neural networks are powerful and popular algorithms that achieve state-of-the-art performance in various computer vision tasks, such as object recognition. Despite the advances made by the recent architectures<|cite_0|>, they are discovered to be fragile to small but carefully directed perturbations of images<|cite_1|>, such that the targeted images",
    "paper_id": "1711.01791",
    "retrieved_ids": [
      "vgg",
      "Ghorbani2019",
      "zhao2019object",
      "geirhos2017comparing",
      "agrawal2014analyzing",
      "kang2016tcnn",
      "simonyan2013deep",
      "zhou2020universality",
      "dodge2017study",
      "nguyen2015deep",
      "zhou2014object",
      "narodytska2016simple",
      "dosovitskiy2020vit",
      "luo2016understanding",
      "elsken2019neural",
      "khan2021transformers",
      "zheng2016improving",
      "akhtar2018threat",
      "bhojanapalli2021understanding",
      "huang2017densely",
      "jung2023neural",
      "vit",
      "guo2020meets",
      "sen2020empir",
      "qusecnet",
      "deeplabv1",
      "radford2015unsupervised",
      "moosavi2016deepfool",
      "ding2023unireplknet",
      "cspnet",
      "devries2017improved",
      "gao2020patch",
      "he2014spatial",
      "he2019parametric",
      "li2021neural",
      "shao2022adversarial",
      "jakubovitz2018improving",
      "ben2019demystifying",
      "ma2021understanding",
      "xu2023resilient",
      "monti2017geometric",
      "xu2019adversarial",
      "MAXIM",
      "iandola2014densenet",
      "arnab2018robustness",
      "object_tracking_in_autunomous_driving",
      "limitations",
      "das2017keeping",
      "szegedy2017inception"
    ],
    "relevant_ids": [
      "papernot2016limitations",
      "papernot2017practical",
      "he2016deep",
      "kurakin2016adversarial",
      "moosavi2016deepfool",
      "ha2016hypernetworks",
      "papernot2016distillation",
      "metzen2017detecting",
      "szegedy2013intriguing",
      "de2016dynamic",
      "goodfellow2014explaining",
      "tabacof2016exploring",
      "narodytska2016simple",
      "simonyan2014very",
      "jin2015robust",
      "zheng2016improving",
      "szegedy2015going",
      "tramer2017ensemble"
    ],
    "metrics": {
      "R@5": 0.0,
      "R@10": 0.0,
      "R@20": 0.1111111111111111,
      "MRR": 0.08333333333333333,
      "hits": 3,
      "total_relevant": 18
    },
    "score": 0.024999999999999998,
    "timestamp": "2025-12-18T10:52:27.088525"
  },
  {
    "query": "Introduction A Laplacian $\\mlap \\in \\R^{n \\times n}$ is a symmetric matrix, with non-positive off-diagonal entries, such that its diagonal entries are equal to the sum of its off-diagonal entries, i.e. $\\mlap_{ij} = \\mlap_{ji} \\leq 0$ for all $i \\neq j$ and $\\mlap \\vones = \\vzeros$. An $\\epsilon$-spectral sparsifier is",
    "paper_id": "1711.00571",
    "retrieved_ids": [
      "ChristianoKMST11",
      "spielman2010spectral",
      "LeeS17",
      "LeeS15a",
      "ST",
      "KoutisX16",
      "SS",
      "ZhuLO15",
      "lpe",
      "KelnerLOS14",
      "simsek2021geometry",
      "Zhang2015ASO",
      "BSS",
      "gu2022parameterization",
      "DurfeeKPRS17",
      "signnet",
      "quadforms",
      "loukas2018graph",
      "hammond2011wavelets",
      "Martens2015",
      "bartlett2017spectrallynormalized",
      "martins2016softmax",
      "miyato2018spectral",
      "Geiger2020",
      "YL1",
      "orig",
      "cai2020note",
      "bao2022estimating",
      "CohenKPPR14",
      "PengS14",
      "brea2019weight",
      "tishby2000information",
      "nguyen2020dataset",
      "zhang2019theoretically",
      "montanari2022interpolation",
      "mate2023learning",
      "Pascanu2014",
      "diagonalgan",
      "chen2020multispeech",
      "draxler2022whitening",
      "nguyen2021tight",
      "bora2017compressed",
      "bartlett2018gradient",
      "meng2020pruning",
      "safran2018spurious"
    ],
    "relevant_ids": [
      "quadforms",
      "LeeS17",
      "KelnerM09",
      "KelnerLOS14",
      "LeGall14",
      "PengS14",
      "LeeS15a",
      "DingLP11",
      "CohenKPPR14",
      "DurfeeKPRS17",
      "SS",
      "ChristianoKMST11",
      "KoutisX16",
      "BSS",
      "ZhuLO15",
      "Sherman13",
      "ST",
      "AnariG15"
    ],
    "metrics": {
      "R@5": 0.2222222222222222,
      "R@10": 0.4444444444444444,
      "R@20": 0.6111111111111112,
      "MRR": 1.0,
      "hits": 13,
      "total_relevant": 18
    },
    "score": 0.5222222222222221,
    "timestamp": "2025-12-18T10:52:30.100604"
  },
  {
    "query": "Introduction \\label{sec1} Adversarial attacks, typically viewed as a subset of the trustworthy AI domain, seek to manipulate neural network models by inducing minor perturbations in the input image<|cite_0|>. These attacks are predicated on three conditions: white-box, black-box, and no-box<|cite_1|>. The no-box scenario assumes an attacker devoid of access to the",
    "paper_id": "2307.06608",
    "retrieved_ids": [
      "tabacof2016exploring",
      "chakraborty2018adversarial",
      "guo2019simple",
      "metzen2017detecting",
      "ilyas2018black",
      "yuan2019adversarial",
      "li2020practical",
      "narodytska2016simple",
      "xu2019adversarial",
      "li2019nattack",
      "meunier2019yet",
      "overview",
      "Madry2017",
      "huang2019black",
      "Mad+18",
      "xiao2018generating",
      "andriushchenko2020square",
      "sun2022towards",
      "akhtar2018threat",
      "brendel2017decision",
      "mustafa2019adversarial",
      "tu2019autozoom",
      "Raghunathan2018",
      "poursaeed2018generative",
      "chen2017zoo",
      "byunEffectivenessSmallInput2022",
      "kurakin2016adversarial",
      "Schott2018a",
      "Goodfellow2016",
      "cartella2021adversarial",
      "song2017pixeldefend",
      "deng2020analysis",
      "cheng2019improving",
      "gao2018black",
      "chen2020rays",
      "rony_2020",
      "wu2018understanding",
      "Ebrahimi:18b",
      "finlay2019logbarrier",
      "xiao2020one",
      "NRP",
      "raghunathan2018certified",
      "tramer2017ensemble",
      "nasr2019comprehensive",
      "yu2021lafeat",
      "ballet2019imperceptible",
      "naseer2018task",
      "ford2019adversarial",
      "xie2019feature",
      "ganeshan2019fda"
    ],
    "relevant_ids": [
      "he2022masked",
      "bao2021beit",
      "szegedy2013intriguing",
      "esmaeilpour2022zero",
      "wang2022cris",
      "goodfellow2014explaining",
      "aich2022gama",
      "radford2021learning",
      "brown2020language",
      "ramesh2021zero",
      "ilyas2019adversarial",
      "touvron2023llama",
      "poursaeed2018generative",
      "madry2017towards",
      "zhuang2020comprehensive",
      "chen2020simple",
      "li2020practical",
      "gpt",
      "he2020momentum",
      "bommasani2021opportunities",
      "chowdhery2022palm",
      "sun2022towards",
      "russakovsky2015imagenet",
      "zhang2022beyond"
    ],
    "metrics": {
      "R@5": 0.0,
      "R@10": 0.041666666666666664,
      "R@20": 0.08333333333333333,
      "MRR": 0.14285714285714285,
      "hits": 3,
      "total_relevant": 24
    },
    "score": 0.05535714285714285,
    "timestamp": "2025-12-18T10:52:32.800660"
  },
  {
    "query": "Introduction \\label{sec:intro} \\begin{figure}[t] \\centering \\includegraphics[width=.85\\columnwidth]{figs/new_teaser.pdf} \\vspace{-1mm} \\caption{ Comparison of inconsistent and consistent training (Ours). (a) Previous methods typically build contrastive items (CIs) and supervise the instance embeddings between key and reference frames. We call this paradigm inconsistent training, where the interaction with the long-term memory bank during training and the",
    "paper_id": "2307.12616",
    "retrieved_ids": [
      "van2021revisiting",
      "khosla2020supervised",
      "wu2020clear",
      "tian2020makes",
      "cui2022democratizing",
      "goel2022cyclip",
      "chen2020simple",
      "kgcl",
      "zhu2020eqco",
      "purushwalkam2020demystifying",
      "selfie",
      "dangovski2021equivariant",
      "recon",
      "simclr",
      "huynh2022fnc",
      "cui2022contrastive",
      "xu2020adversarial",
      "MindGap",
      "ye2021improving",
      "ge2021robust",
      "wu2021esimcse",
      "wang2023consistent",
      "article17",
      "minvis",
      "kalantidis2020hard",
      "caron2020unsupervised",
      "SoCo",
      "dai2022cluster",
      "athiwaratkun2019there",
      "wang2020double",
      "jang2024model_Stock",
      "yu2023long",
      "genvis",
      "fang2019instaboost",
      "wang2023critic",
      "tewel2024trainingfree",
      "zhang2023dvis",
      "xie2021fignerf",
      "lin2021video",
      "he2018rethinking",
      "lisweetdreamer",
      "lee2023holistic",
      "vedaldi2016instance",
      "mocov3",
      "li2022recurrent",
      "liu2020efficient_per_frame",
      "ControlVideo",
      "Zhao2024RetrievalAugmentedGF",
      "2208.05516",
      "azad2023foundational",
      "wu2021rethinking",
      "luo2023closer",
      "chen2022transmix",
      "cao2019gcnet",
      "park2023self",
      "pureclipnerf"
    ],
    "relevant_ids": [
      "crossvis",
      "simclr",
      "deformabledetr",
      "fcos",
      "nguyen",
      "ifc",
      "mask2former",
      "copypaste",
      "qdtrack",
      "ovis",
      "idol",
      "masktrackrcnn",
      "minvis",
      "genvis",
      "mask2formervis",
      "vistr",
      "mscoco",
      "transformer",
      "seqformer",
      "vita",
      "maskrcnn",
      "stc",
      "isda",
      "detr"
    ],
    "metrics": {
      "R@5": 0.0,
      "R@10": 0.0,
      "R@20": 0.041666666666666664,
      "MRR": 0.07142857142857142,
      "hits": 3,
      "total_relevant": 24
    },
    "score": 0.021428571428571425,
    "timestamp": "2025-12-18T10:52:36.953081"
  },
  {
    "query": "Introduction \\label{s:intro} Semantic segmentation of organs, anatomical structures, or anomalies in medical images (e.g. CT or MRI scans) remains one of the fundamental tasks in medical image analysis. Volumetric medical image segmentation (MIS) helps healthcare professionals to diagnose conditions more accurately, plan medical treatments, and perform image-guided procedures. Although deep",
    "paper_id": "2307.07269",
    "retrieved_ids": [
      "minaee2020image",
      "KARIMI2020101759",
      "daza2021towards",
      "li2019volumetric",
      "zhou2021review",
      "dong2023adversarial",
      "cheplygina2019not",
      "Uncertainty5",
      "ma2024segment",
      "nnUNet",
      "yang2023directional",
      "chaitanya2020contrastive",
      "zhou2018unet++",
      "azad2023foundational",
      "Li2021SemanticSW",
      "transfuse",
      "hatamizadeh2022swin",
      "cao2022swin",
      "ma2024u",
      "chen2021transunet",
      "shamshad2022transformers",
      "xu2019camel",
      "chen2023versatile",
      "dice",
      "wu2021federated",
      "hu2023advancing",
      "hatamizadeh2022unetr",
      "chen2020unsupervised",
      "schlemper2019attention",
      "zhang2021dodnet",
      "bortsova2019semi",
      "peng2020deep",
      "zhou2017fixed",
      "karimi2021convolution",
      "liu2023clip",
      "automaticmrg",
      "limitations",
      "Uncertainty3",
      "tang2022self",
      "cheng2023sam",
      "wolleb2021diffusion",
      "li2023zeroshot",
      "ouyang2019data",
      "tomar2021self",
      "ding2021local"
    ],
    "relevant_ids": [
      "karimi2021convolution",
      "ronneberger2015u",
      "kurakin2016adversarial",
      "hatamizadeh2022unetr",
      "croce2020reliable",
      "madry2017towards",
      "szegedy2013intriguing",
      "kurakin2018adversarial",
      "carlini2017towards",
      "akhtar2018threat",
      "duan2021advdrop",
      "shaker2022unetr++",
      "ma2021understanding",
      "goodfellow2014explaining",
      "daza2021towards",
      "carlini2017adversarial",
      "li2019volumetric"
    ],
    "metrics": {
      "R@5": 0.11764705882352941,
      "R@10": 0.11764705882352941,
      "R@20": 0.11764705882352941,
      "MRR": 0.3333333333333333,
      "hits": 4,
      "total_relevant": 17
    },
    "score": 0.18235294117647058,
    "timestamp": "2025-12-18T10:52:39.188350"
  },
  {
    "query": "Introduction Score-based generative models (SGMs)<|cite_0|> and Diffusion probabilistic model (DPMs)<|cite_1|> have gained great attention for its training stability, scalability, and impressive image synthesis quality. These models have been shown to achieve impressive performance on diverse domains spanning computer vision<|cite_2|>, natural language processing<|cite_3|>, multi-model modeling<|cite_4|>. Nevertheless, the diffusion model has not",
    "paper_id": "2307.05899",
    "retrieved_ids": [
      "theis2016note",
      "wang2022semantic",
      "croitoru2023diffusion",
      "yang2022diffusion",
      "ma2022accelerating",
      "lim2023scorebased",
      "rombach2022high",
      "he2023diffusionbert",
      "ddpm",
      "chen2022sampling",
      "latentdiffusion",
      "zhang2022unsupervised",
      "improved_ddpm",
      "kawar2022enhancing",
      "po2023state",
      "DiffuSeq2022",
      "moser2024diffusion",
      "kim2022stasy",
      "liu2023more",
      "nichol2021improved",
      "ho2020denoising",
      "lu2023dpmsolver",
      "Zhang_2023_CVPR",
      "jing2022subspace",
      "yang2023diffusion",
      "Bao2022AnalyticDPMAA",
      "gong2022diffuseq",
      "vahdat2021score",
      "lam2022bddm",
      "muller2022diffusion",
      "li2023divide",
      "chen2023pixartalpha",
      "b2",
      "luo2023videofusion",
      "bao2022estimating",
      "frolov2021adversarial",
      "b25",
      "holodiff",
      "ddim",
      "song2020denoising",
      "song2023consistency",
      "loudiscrete",
      "luo2023latent",
      "austin2021structured",
      "jain2022vectorfusion",
      "chang2023muse",
      "avrahami2023blended",
      "pandey2022diffusevae",
      "ghosh2023geneval",
      "liu2024swin"
    ],
    "relevant_ids": [
      "bao2023unidiffuser",
      "yu2022latent",
      "song2020denoising",
      "vahdat2021score",
      "luo2021score",
      "li2022srdiff",
      "saharia2022photorealistic",
      "preechakul2022diffusion",
      "ho2020denoising",
      "song2020score",
      "li2022diffusion",
      "amit2021segdiff",
      "zhang2022unsupervised",
      "pandey2022diffusevae",
      "GeAXI21"
    ],
    "metrics": {
      "R@5": 0.0,
      "R@10": 0.0,
      "R@20": 0.06666666666666667,
      "MRR": 0.08333333333333333,
      "hits": 5,
      "total_relevant": 15
    },
    "score": 0.024999999999999998,
    "timestamp": "2025-12-18T10:52:42.141811"
  },
  {
    "query": "Introduction With the continued growth of infrastructure and environmental systems, a vast amount of urban data is being measured and collected. This data includes information on traffic flow, meteorological records, and energy consumption. Analyzing urban data can be challenging due to its partially observable dynamics, which include human interactions, information",
    "paper_id": "2307.01482",
    "retrieved_ids": [
      "kanagavalli2016study",
      "liu2023largest",
      "jang2023learning",
      "han2023groundlink",
      "zheng2020gman",
      "li2018diffusion",
      "Strubell:2019uv",
      "piroli2023energy",
      "hong2019rules",
      "strubell2019energy",
      "salzmann2020trajectron++",
      "siddiqui2022metadata",
      "cordts2016cityscapes",
      "dosovitskiy2017carla",
      "yan2024urbanclip",
      "yu2017spatio",
      "hu2023avis",
      "chen2023periodic",
      "scheel2022urban",
      "Waymo2021",
      "Chen2021InterpretableLearning",
      "zhang2023mlpst",
      "caesar2021nuplan",
      "han2021dynamic",
      "kushwhyInt",
      "chen2023end",
      "nie2023correlating",
      "roach",
      "dong2021multi",
      "overview3",
      "yao2018deep",
      "pdm",
      "toromanoff2020end",
      "sun2020scalability",
      "baumgartner2020pushshift",
      "dreissig2023survey",
      "zhu2024sora",
      "object_tracking_in_autunomous_driving",
      "kurup2021dsor",
      "ma2019exploiting",
      "lu2023imitation",
      "menghani2021efficient",
      "asai2024reliable",
      "jang2022bc",
      "piroli2022detection",
      "retail_analytics",
      "djuric2021multixnet",
      "cui2019multimodal",
      "levine2020offline",
      "chen2020learning",
      "garg2021iq",
      "treviso2023efficient",
      "nmp"
    ],
    "relevant_ids": [
      "li2018diffusion",
      "shao2022spatial",
      "liu2023largest",
      "elsayed2021we",
      "cini2023graph",
      "zhang2023mlpst",
      "bai2020adaptive",
      "jin2023survey",
      "yi2023frequency",
      "yu2017spatio",
      "nie2023correlating",
      "tolstikhin2021mlp",
      "fusco2022pnlp",
      "wu2019graph",
      "wu2021autoformer",
      "chen2023tsmixer",
      "zhou2022fedformer",
      "he2023generalization",
      "zhong2023multi"
    ],
    "metrics": {
      "R@5": 0.05263157894736842,
      "R@10": 0.10526315789473684,
      "R@20": 0.15789473684210525,
      "MRR": 0.5,
      "hits": 5,
      "total_relevant": 19
    },
    "score": 0.2026315789473684,
    "timestamp": "2025-12-18T10:52:44.357184"
  },
  {
    "query": "Introduction StyleGAN and its family<|cite_0|> emerge victorious in Generative Adversarial Networks (GANs) not only for high-quality generated images but also an intermediate latent space with disentangled attributes, making latent-based image manipulation possible<|cite_1|>. However, only randomly generated images can enjoy this editability. The lack of making inferences on a target image",
    "paper_id": "2307.16151",
    "retrieved_ids": [
      "GAN",
      "wu2021stylespace",
      "kim2021exploiting",
      "zhuang2021enjoy",
      "brock2017neural",
      "shen2019interpreting",
      "patashnik2021styleclip",
      "tov2021designing",
      "pehlivan2023styleres",
      "alaluf2022hyperstyle",
      "shi2022semanticstylegan",
      "wang2022high",
      "hu2022style",
      "collins2020editing",
      "pinnimty2020transforming",
      "shen2020interfacegan",
      "zhang2018photographic",
      "wang2018high",
      "fox2021stylevideogan",
      "abdal2021styleflow",
      "chong2021stylegan",
      "sauer2023stylegan",
      "lipton2017precise",
      "bai2022high",
      "zhu2020domain",
      "perarnau2016invertible",
      "vqvae2",
      "lineargan",
      "chaiusing",
      "xia2022gan",
      "li2023reganie",
      "gal2021swagan",
      "ling2021editgan",
      "pidhorskyi2020adversarial",
      "AuxOdena",
      "frolov2021adversarial",
      "kang2023scaling",
      "wang2021hijack",
      "hou2021guidedstyle",
      "zhao2022synthesizing",
      "Alaluf2021only",
      "He2019",
      "hou2022feat",
      "oasis"
    ],
    "relevant_ids": [
      "mao2022cycle",
      "goetschalckx2019ganalyze",
      "patashnik2021styleclip",
      "wei2022e2style",
      "vaswani2017attention",
      "alaluf2022hyperstyle",
      "zhu2020improved",
      "shen2020interpreting",
      "liu2022swin",
      "voynov2020unsupervised",
      "shen2021closed",
      "abdal2019image2stylegan",
      "tov2021designing",
      "radford2021learning",
      "liu2022delving",
      "karras2020analyzing",
      "karras2019style",
      "kang2021gan",
      "abdal2020image2stylegan++",
      "yao2022feature",
      "fang2021you",
      "karras2020training",
      "richardson2021encoding",
      "dinh2022hyperinverter",
      "abdal2021styleflow",
      "plumerault2020controlling",
      "roich2022pivotal",
      "alaluf2021restyle",
      "harkonen2020ganspace",
      "liu2021swin",
      "karras2021alias",
      "abdal2022clip2stylegan",
      "hu2022style",
      "wang2022high",
      "dosovitskiy2020image",
      "bai2022high"
    ],
    "metrics": {
      "R@5": 0.0,
      "R@10": 0.08333333333333333,
      "R@20": 0.16666666666666666,
      "MRR": 0.14285714285714285,
      "hits": 7,
      "total_relevant": 36
    },
    "score": 0.06785714285714285,
    "timestamp": "2025-12-18T10:52:46.542167"
  },
  {
    "query": "Introduction \\vspace{-1em} Recently, significant advancements have been made in the field of Artificial Intelligence Generative Content (AIGC), specifically in the development of diffusion models<|cite_0|> that have greatly improved the quality of generative content. As a result, AIGC platforms like DALLE, Stability AI, Runway, and Midjourney have gained widespread popularity, enabling",
    "paper_id": "2307.00716",
    "retrieved_ids": [
      "cao2022survey",
      "yang2022diffusion",
      "b8",
      "b3",
      "po2023state",
      "DiffuSeq2022",
      "zhang2023text",
      "gong2022diffuseq",
      "b1",
      "liang2023mistimprovedadversarialexamples",
      "zhu2024diffusion",
      "xu2023ufogen",
      "Zhao2024RetrievalAugmentedGF",
      "xi2023rise",
      "he2022lvdm",
      "podell2023sdxl",
      "witteveen2022investigating",
      "miao2023towards",
      "vonr\u00fctte2023fabric",
      "avrahami2023chosen",
      "park2023understanding",
      "b2",
      "friedrich2023fair",
      "gpt_summary",
      "razzhigaev2023kandinsky",
      "ref:ragsurvey3",
      "chen2023pixartalpha",
      "wu2023speechgen",
      "Gao2023RetrievalAugmentedGF",
      "li2023your",
      "zhou2023recurrentgpt",
      "wang2023vigc",
      "laion5b",
      "wu2023easyphoto",
      "hu2023radar",
      "sauer2023adversarial",
      "ref:ragsurvey1",
      "kang2023scaling",
      "du2023demofusion",
      "survey3",
      "basu2023inspecting"
    ],
    "relevant_ids": [
      "lu2022unified",
      "suncontrollable",
      "imagen",
      "zhang2023llama",
      "zhu2022unip",
      "piao2021inverting",
      "rombach2022high",
      "radford2021learning",
      "liu2023visual",
      "chen2015microsoft",
      "touvron2023llama",
      "crowson2022vqgan",
      "alayrac2022flamingo",
      "Gregor2015DRAWAR",
      "zhu2023minigpt",
      "OpenAI2023GPT4TR",
      "li2023uni",
      "jia2021scaling",
      "sun2022cgof++",
      "gao2023llama",
      "zhu2022uni",
      "ding2021cogview",
      "goodfellow2020generative",
      "stable_diffusion",
      "wang2022ofa",
      "wu2023human",
      "li2023blip",
      "Mansimov2015GeneratingIF",
      "dalle2"
    ],
    "metrics": {
      "R@5": 0.0,
      "R@10": 0.0,
      "R@20": 0.0,
      "MRR": 0.0,
      "hits": 0,
      "total_relevant": 29
    },
    "score": 0.0,
    "timestamp": "2025-12-18T10:52:48.616063"
  },
  {
    "query": "Introduction Photorealistic image synthesis aims to generate highly realistic images, enabling broad applications in virtual reality, augmented reality, video games, and filmmaking. The field has seen significant advancements in recent years, driven by the rapid development of deep learning techniques such as diffusion-based generative models<|cite_0|>. One particularly successful domain is",
    "paper_id": "2307.01097",
    "retrieved_ids": [
      "yang2022diffusion",
      "nichol2021glide",
      "tewari2022advances",
      "latentdiffusion",
      "hodavn2019photorealistic",
      "zhang2017stackgan++",
      "liao2020towards",
      "b8",
      "gan_tian2021good",
      "po2023state",
      "get3d",
      "chen2017photographic",
      "persion_dm",
      "du2023demofusion",
      "xiang20233d",
      "movshovitz-attias2016how",
      "zhou:stereomagni:siggraph2018",
      "zhou2018stereo",
      "pigan",
      "corona2024vlogger",
      "chan2023generative",
      "cao2023texfusion",
      "minaee2020image",
      "qi2018semi",
      "reed2016generative",
      "tseng2023consistent",
      "Marchesi2017",
      "meng2022sdedit",
      "Odena2017",
      "pinkney2020resolution",
      "ravi2020accelerating",
      "li2024image",
      "AuxOdena",
      "frolov2021adversarial",
      "chen2024it3d",
      "kim2023neuralfield",
      "dai2023animateanything",
      "tsalicoglou2023textmesh",
      "kang2023scaling",
      "tewari2020state",
      "cao2022fwd",
      "deng2022fov",
      "morelli2023ladivton",
      "bokhovkin2023mesh2tex",
      "niemeyer2021giraffe",
      "giraffe",
      "campari",
      "pei2024deepfake",
      "chen2023gaussianeditor",
      "wang2023breathing",
      "benaim2022volumetric"
    ],
    "relevant_ids": [
      "zhang2023diffcollage",
      "chen2023text2tex",
      "karras2022elucidating",
      "chen2022text2light",
      "poole2022dreamfusion",
      "nichol2021glide",
      "dhariwal2021guided-diffusion",
      "song2019NCSN",
      "ramesh2022dalle2",
      "hollein2023text2room",
      "saharia2022imagen",
      "sohl2015DMfirst",
      "song2020ddim",
      "song2020improved",
      "ho2022classifier",
      "karras2022edm",
      "mildenhall2020nerf",
      "song2019generative",
      "bar2023multidiffusion",
      "lu2022dpm-solver",
      "razavi2019generating",
      "ho2020DDPM",
      "fridman2023scenescape",
      "Song2020ScoreBasedGM",
      "goodfellow2020generative",
      "karras2021alias",
      "van2017neural",
      "rombach2022StableDiffusion",
      "blattmann2023align",
      "lin2022magic3d",
      "brock2018large",
      "esser2021taming"
    ],
    "metrics": {
      "R@5": 0.03125,
      "R@10": 0.03125,
      "R@20": 0.03125,
      "MRR": 0.5,
      "hits": 1,
      "total_relevant": 32
    },
    "score": 0.171875,
    "timestamp": "2025-12-18T10:52:50.743435"
  },
  {
    "query": "Introduction A standard approach to knowledge-intensive language tasks such as question answering (QA), entity disambigution, and fact verification is retrieval-based. Given an query, a retriever is used to efficiently search a large knowledge base (KB) to retrieve relevant ``contexts'', typically in the form of short paragraphs. How these contexts are",
    "paper_id": "2307.00342",
    "retrieved_ids": [
      "maillard2021multi",
      "petroni2021kilt",
      "khattab2023demonstratesearchpredict",
      "talmor2018complex",
      "watanabe2017question",
      "cohen2023crawling",
      "Liu2023LostIT",
      "heinzerling2021language",
      "sui2024fidelis",
      "wang2023knowledge",
      "trivedi2022interleaving",
      "karpukhin-etal-2020-dense",
      "srinivasan2022quill",
      "qu2020open",
      "izacard2020distilling",
      "dhingra2017quasar",
      "FVQA",
      "petroni-contextaffectslanguagemodels-2020",
      "baek2023knowledge",
      "asai2019learning",
      "seo2016query",
      "Kandpal2022LargeLM",
      "cao2021autoregressive",
      "yu2022generate",
      "ma2023query",
      "zayats2021representations",
      "sun2018open",
      "guu2020realm",
      "jin2022survey",
      "ref:gnn-rag",
      "christmann2019look",
      "memory",
      "li2021dual",
      "chen2020open",
      "wang2023learning",
      "chen2020hybridqa",
      "roberts2020much",
      "chen2019multi",
      "ref:graphrag",
      "dasigi2021dataset",
      "gao2022precise",
      "Yan2024CorrectiveRA",
      "bajaj2018ms",
      "miller2016key"
    ],
    "relevant_ids": [
      "piratla2021focus",
      "xiong2020approximate",
      "chen2022corpusbrain",
      "devlin2019bert",
      "raffel2019t5",
      "maillard2021multi",
      "liang2022no",
      "michel2019sixteen",
      "chen2018gradnorm",
      "asai2022task",
      "molchanov2019importance",
      "logeswaran2019zero",
      "wang2020gradient",
      "molchanov2016pruning",
      "liang2021super",
      "karpukhin-etal-2020-dense",
      "petroni2021kilt",
      "yu2020gradient",
      "cao2021autoregressive"
    ],
    "metrics": {
      "R@5": 0.10526315789473684,
      "R@10": 0.10526315789473684,
      "R@20": 0.15789473684210525,
      "MRR": 1.0,
      "hits": 4,
      "total_relevant": 19
    },
    "score": 0.3736842105263158,
    "timestamp": "2025-12-18T10:52:52.727306"
  },
  {
    "query": "Introduction Reducing the storage and computational requirements of the state-of-the-art deep neural networks (DNNs) is of great practical importance. An effective way to compress DNNs is through model quantization<|cite_0|>. A quantization strategy that starts with a pre-trained model is known as post-training quantization (PTQ). Typically, PTQ requires only a small",
    "paper_id": "2307.05657",
    "retrieved_ids": [
      "bucilu2006model",
      "cheng2017survey",
      "pruning",
      "Han:2016uf",
      "polino2018distillation",
      "Alvarez_2017",
      "gong2014compressing",
      "Nagel2021AWP",
      "frantar2023optimal",
      "cai2019WNQ",
      "zhou2023dataset",
      "li2021brecq",
      "elthakeb2018releq",
      "zhang2018lq",
      "choukroun2019lowbit",
      "zhu2016ttq",
      "banner2018posttraining",
      "yuan2022ptq4vit",
      "choi2018bridging",
      "frantar2023gptq",
      "Wei2022QDropRD",
      "schaefer2023mixed",
      "dong2017learning",
      "llm_quant",
      "Dong2019HAWQHA",
      "zhou2017incremental",
      "li2023repq",
      "liu2023qllm",
      "zhao2019ocs",
      "pan2023smoothquant+",
      "shao2023omniquant",
      "rakin2018defend",
      "gong2019dsq",
      "hooker2019compressed",
      "choi2018pact",
      "Wang:2019tj",
      "Cai2020ZeroQAN",
      "pbllm",
      "Wang2019HAQHA",
      "liu2023llm"
    ],
    "relevant_ids": [
      "Nagel2020UpOD",
      "Cai2020ZeroQAN",
      "Nagel2021AWP",
      "Lou2020AutoQAK",
      "Courbariaux2015BinaryConnectTD",
      "Wei2022QDropRD",
      "Dong2020HAWQV2HA",
      "Chen2021TowardsMQ",
      "Dong2019HAWQHA",
      "Yang2020FracBitsMP",
      "Tang2022MixedPrecisionNN",
      "Wang2019HAQHA",
      "Yao2020HAWQV3DN",
      "Guo2020SinglePO",
      "Kim2021IBERTIB",
      "Wu2018MixedPQ",
      "Deng2022VariabilityAwareTA"
    ],
    "metrics": {
      "R@5": 0.0,
      "R@10": 0.058823529411764705,
      "R@20": 0.058823529411764705,
      "MRR": 0.125,
      "hits": 5,
      "total_relevant": 17
    },
    "score": 0.05514705882352941,
    "timestamp": "2025-12-18T10:52:54.782567"
  },
  {
    "query": "Introduction Whole-body pose estimation plays a crucial role in numerous human-centric perception, understanding, and generation tasks, including 3D whole-body mesh recovery<|cite_0|>, human-object interaction<|cite_1|>, and pose-conditioned human image and motion generation<|cite_2|>. Furthermore, capturing human poses for virtual content creation and VR/AR has gained significant popularity, relying on user-friendly algorithms like OpenPose<|cite_3|>",
    "paper_id": "2307.15880",
    "retrieved_ids": [
      "chen17",
      "kocabas2020",
      "kanazawa18",
      "toshev2014deeppose",
      "belagiannis2017recurrent",
      "pavllo19",
      "martinez17",
      "GyeongsikMoon2020hand4whole",
      "jin2020whole",
      "SimpleBaseline",
      "Zheng20213DHP",
      "rong2021frankmocap",
      "HDO",
      "yi2023generating",
      "lin2023motion",
      "zhou16",
      "CanonPoseSM",
      "rogez16",
      "ju2023human",
      "EndtoEndHP",
      "taheri2020grab",
      "guler2018densepose",
      "huang2023humannorm",
      "cao2021openpose",
      "djrn",
      "lin2023one",
      "baldrati2023multimodal",
      "Monocular3H",
      "pavlakos2019expressive",
      "SingleShotM3",
      "zhang2023avatarverse",
      "zero12345plus",
      "han2023groundlink",
      "animatabledreamer",
      "sinha2023sparsepose",
      "rong2020frankmocap",
      "zhang2023getavatar",
      "zheng2021pamir",
      "zhou2021monocular",
      "he2021arch++",
      "liu2019liquid",
      "zhao2022humannerf",
      "choutas2020monocular",
      "feng2021collaborative",
      "huang2020arch",
      "zheng2023pointavatar",
      "bergman2022generative",
      "jiang2022selfrecon",
      "chen2023gaussianeditor",
      "kolotouros2023dreamhuman",
      "v3d",
      "xiu2022icon",
      "remelli2022drivable"
    ],
    "relevant_ids": [
      "black2023bedlam",
      "GyeongsikMoon2020hand4whole",
      "hinton2015distilling",
      "fang2023depgraph",
      "zong2022better",
      "lin2014microsoft",
      "taheri2020grab",
      "yu2023boost",
      "jin2020whole",
      "liu2022beat",
      "yi2023generating",
      "lugaresi2019mediapipe",
      "lin2023motion",
      "jiang2023rtmpose",
      "zhang2020mediapipe",
      "fan2023arctic",
      "forte2023reconstructing",
      "lin2023one",
      "liu2022transkd",
      "zheng2023avatarrex",
      "cao2021openpose"
    ],
    "metrics": {
      "R@5": 0.0,
      "R@10": 0.09523809523809523,
      "R@20": 0.19047619047619047,
      "MRR": 0.125,
      "hits": 7,
      "total_relevant": 21
    },
    "score": 0.06607142857142856,
    "timestamp": "2025-12-18T10:52:57.908573"
  },
  {
    "query": "Introduction The recent development of deep learning has achieved remarkable success in a broad range of visual recognition tasks<|cite_0|>. However, most traditional methods focus on the closed-world setting, in which all the visual classes are pre-defined. As a result, it is usually difficult to deploy the learned models in realistic",
    "paper_id": "2307.09158",
    "retrieved_ids": [
      "young2018recent",
      "liu2019large",
      "wu2019wider",
      "wang2018deep",
      "survey7",
      "zhao2019object",
      "zhou2021review",
      "dataaugmentation",
      "oquab2023dinov2",
      "Donahue:2014:DDC",
      "deepsets",
      "minaee2020image",
      "chen2022adaptformer",
      "survey3",
      "mot_survey",
      "wang2021knowledge",
      "GFNet",
      "cao2021open",
      "yuan2021florence",
      "akhtar2018threat",
      "openMax",
      "ju2022prompting",
      "wang2016tsn",
      "chen2022semi",
      "bai2021recent",
      "beal2021billion",
      "zhang2021distribution",
      "hu2018learning",
      "SZHBFB19",
      "zhuang2019local",
      "ye2021disentangling",
      "cen2021deep",
      "ding2023unireplknet",
      "salem2018ml",
      "zhu2023ghost",
      "shen2021much",
      "abadi2016deep",
      "oza2019c2ae",
      "khan2021transformers",
      "besnier2020dataset",
      "elsken2019neural",
      "li2019leveraging",
      "parisi2022unsurprising",
      "srinidhi2021deep",
      "Schiappa-ACM-2023",
      "li2021benchmarking",
      "xie2023darkmim",
      "zhao2022synthesizing",
      "borrego2018applying",
      "object_tracking_in_autunomous_driving",
      "oza2021unsupervised",
      "nakashima2021can",
      "elnouby2021largescale",
      "liu2022swin",
      "chakraborty2018adversarial"
    ],
    "relevant_ids": [
      "park2019relational",
      "han2021autonovel",
      "wang2021knowledge",
      "hinton2015distilling",
      "chi2021meta",
      "tian2019contrastive",
      "vaze2022generalized",
      "hsu2018learning",
      "ahn2019variational",
      "ren2015faster",
      "zhong2021neighborhood",
      "he2017mask",
      "zhao2021novel",
      "zhao2022decoupled",
      "fini2021unified",
      "he2016deep",
      "han2019learning",
      "gou2021knowledge",
      "zhong2021openmix",
      "hsu2018multi",
      "cao2021open"
    ],
    "metrics": {
      "R@5": 0.0,
      "R@10": 0.0,
      "R@20": 0.09523809523809523,
      "MRR": 0.0625,
      "hits": 2,
      "total_relevant": 21
    },
    "score": 0.01875,
    "timestamp": "2025-12-18T10:53:00.209104"
  },
  {
    "query": "Introduction \\label{sec:introduction} Machine Learning (ML) models are vital in many security-sensitive applications<|cite_0|>. In the financial services industry they are used to classify, for example, credit card transactions as legitimate or fraudulent<|cite_1|>. In this naturally adversarial setting, fraudsters continuously adapt their techniques to bypass the system, while the system maintainers try",
    "paper_id": "2307.15677",
    "retrieved_ids": [
      "overview",
      "grosse2017statistical",
      "meng2017magnet",
      "kurakin2016adversarial",
      "brown2018unrestricted",
      "Papernot2018",
      "biggio_2018",
      "GDG17",
      "Goodfellow2016",
      "SZHBFB19",
      "chakraborty2018adversarial",
      "papernot2016transferability",
      "SSSS17",
      "yang2019federated",
      "salem2018ml",
      "permuteattack",
      "cartella2021adversarial",
      "Big+13",
      "b19",
      "b2",
      "pang2021security",
      "mehrabi2019survey",
      "yeom2017privacy",
      "bulusu2020anomalous",
      "song2017machine",
      "grosse2016adversarial",
      "li2018textbugger",
      "rnns_feedzai_10.1145/3394486.3403361",
      "fawaz2019adversarial",
      "papernot2017practical",
      "papernot2016crafting",
      "b27",
      "chen2019stateful",
      "ballet2019imperceptible",
      "yang2023introduction",
      "xiang2021post",
      "papernot2016distillation"
    ],
    "relevant_ids": [
      "carlini2017evaluating",
      "papernot2016distillation",
      "cartella2021adversarial",
      "ballet2019imperceptible",
      "cheng2018queryefficient",
      "zhang2019theoretically",
      "madry2017towards",
      "rnns_feedzai_10.1145/3394486.3403361",
      "tsipras2019robustness",
      "permuteattack",
      "goodfellow2015explaining",
      "szegedy2014intriguing",
      "hsja"
    ],
    "metrics": {
      "R@5": 0.0,
      "R@10": 0.0,
      "R@20": 0.15384615384615385,
      "MRR": 0.0625,
      "hits": 5,
      "total_relevant": 13
    },
    "score": 0.01875,
    "timestamp": "2025-12-18T10:53:02.028707"
  },
  {
    "query": "Introduction \\begin{figure}[!t] \\centerline{\\includegraphics[width=\\linewidth, height=5cm]{figure/method.png}} \\caption{The summary of our method when batch size $B$ is 3 and sorted by each row for simplicity. Total number is $B \\times (2B-2)$. Each row means sequence. Our method adapts the threshold for each row to reflect the composition of the batch.The threshold can be",
    "paper_id": "2307.05469",
    "retrieved_ids": [
      "noguchi2019image",
      "lim2023ttn",
      "guide_paper",
      "luddecke2022image",
      "luo2018towards",
      "nado2020evaluating",
      "zhou2018open3d",
      "richemond2020byol",
      "ashkboos2024slicegptcompresslargelanguage",
      "orthDNN",
      "you2021test",
      "dong2019bench",
      "KARIMI2020101759",
      "yuan2024llminferenceunveiledsurvey",
      "mondal2021mini",
      "keskar2016large",
      "daras2022multiresolution",
      "li2021contrastive",
      "lee2023holistic",
      "jayaram2020span",
      "era3d",
      "liang2022nuwa",
      "huang2024",
      "Lin2020GAS",
      "hu2021mixnorm",
      "wu2018group",
      "dialog-gen",
      "liu2023cones",
      "Mao:ICCV15",
      "wu2023easyphoto",
      "nguyen2024bellman",
      "zheng2023layoutdiffusion",
      "egeunet",
      "ba2016layer",
      "voynov2023p+",
      "two_stage",
      "yolov3",
      "wu2021smoothed",
      "bhat2023loosecontrol",
      "yu2022scaling",
      "you2023few",
      "park2019effect",
      "liu2021condlanenet",
      "b3",
      "eisenschlos2021mate",
      "CascadeRCNN",
      "Zhao2024RetrievalAugmentedGF",
      "cai2018cascade",
      "post_ocr",
      "cascade_rcnn",
      "bolya2023token",
      "Gidaris_2016",
      "xing2018walk",
      "t2tformer",
      "zhao2023clip",
      "nitzan2023domain",
      "meng2020pruning",
      "sharir2021image",
      "smilkov2017smoothgrad"
    ],
    "relevant_ids": [
      "SASRec2018",
      "sun2019bert4rec",
      "qiu2022contrastive",
      "xie2022contrastive",
      "jiang2022improved",
      "oord2018representation",
      "wang2020understanding",
      "gao2021simcse",
      "chen2020simple",
      "le2020contrastive",
      "dangovski2021equivariant",
      "wu2022infocse",
      "chuang2022diffcse"
    ],
    "metrics": {
      "R@5": 0.0,
      "R@10": 0.0,
      "R@20": 0.0,
      "MRR": 0.0,
      "hits": 0,
      "total_relevant": 13
    },
    "score": 0.0,
    "timestamp": "2025-12-18T10:53:05.464878"
  },
  {
    "query": "Introduction Referring image segmentation (RIS) aims to predict a mask for the target object described by a given natural language sentence based on the input image and text. This task is distinct from semantic segmentation, which assigns each pixel in an image with a label from a fixed word set.",
    "paper_id": "2307.11545",
    "retrieved_ids": [
      "luddecke2022image",
      "wang2022cris",
      "ye2019cross",
      "li2022languagedriven",
      "minaee2020image",
      "cheng2021maskformer",
      "strudel2021segmenter",
      "zou2022generalized",
      "van2021unsupervised",
      "SAS",
      "ghiasi2022scaling",
      "shaban2017oneshot",
      "mask2former",
      "chen2016semantic",
      "wu2023diffumask",
      "SOLO",
      "Noh_2015",
      "sec",
      "Uncertainty3",
      "sgone",
      "cheng2022masked",
      "peng2020deep",
      "mao2016generation",
      "wang2023seggpt",
      "kirillov2019panoptic",
      "bbam",
      "Li2018",
      "liu2018show",
      "avrahami2023break",
      "semantic_structure",
      "pixelbert",
      "ding2022decoupling",
      "dcama",
      "ref6",
      "lazarow2020learninginstance",
      "ma2024segment",
      "basic_model",
      "michaelis2018one",
      "li2024univs",
      "huynh2022open",
      "honda2021removing",
      "DCM",
      "nivlff",
      "zou2024segment"
    ],
    "relevant_ids": [
      "gao2021clip",
      "houlsby2019parameter",
      "he2016deep",
      "li2021grounded",
      "chen2023advancing",
      "wang2021cris",
      "ding2022vlt",
      "chen2022pali",
      "chen2022vision",
      "radford2021learning",
      "chen2022adaptformer",
      "zhou2022learning",
      "dosovitskiy2020image",
      "guo2020parameter"
    ],
    "metrics": {
      "R@5": 0.0,
      "R@10": 0.0,
      "R@20": 0.0,
      "MRR": 0.0,
      "hits": 0,
      "total_relevant": 14
    },
    "score": 0.0,
    "timestamp": "2025-12-18T10:53:07.246747"
  },
  {
    "query": "Introduction 3D detection technology is rapidly developing in the field of autonomous driving. The detection task based on the nuScenes dataset has also become one of the most popular competitions in the detection challenge task in recent years. Multiple sensors are used to collect different types of data in most",
    "paper_id": "2307.11323",
    "retrieved_ids": [
      "caesar2020nuscenes",
      "chen2017multi",
      "object_tracking_in_autunomous_driving",
      "chen2023end",
      "ravi2018real",
      "Choi_2019_ICCV",
      "yu2022dair",
      "sun2020scalability",
      "pang2023transcar",
      "chen2019f",
      "yan2023cross",
      "voxelnext",
      "wang2021fcos3d",
      "kim2023crn",
      "nabati2021centerfusion",
      "Wu2023LanguagePF",
      "RSN",
      "chen2022autoalignv2",
      "zhang2022beverse",
      "huang2021bevdet",
      "bai2022transfusion",
      "wang2022detr3d",
      "xie2023sparsefusion",
      "liu2022pai3d",
      "prakash2021multi",
      "yang2018pixor",
      "chen2022bevdistill",
      "FSD++",
      "yang2022deepinteraction",
      "zhao2019object",
      "cui2022coopernaut",
      "Qiu2021Att3Ddetection",
      "reading2021categorical",
      "xu2022opv2v",
      "wang2022probabilistic",
      "pei2024deepfake",
      "li2022unifying",
      "behley2019semantickitti",
      "park2021pseudo",
      "BEV-survey",
      "zhang2022survey"
    ],
    "relevant_ids": [
      "huang2022bevdet4d",
      "kim2022craft",
      "li2022unifying",
      "huang2021bevdet",
      "wang2022probabilistic",
      "bai2022transfusion",
      "long2021radar",
      "pang2023transcar",
      "park2021pseudo",
      "yang2022deepinteraction",
      "li2022bevdepth",
      "liu2022bevfusion",
      "liu2022pai3d",
      "kim2023crn",
      "lin2020depth",
      "nabati2021centerfusion",
      "yoo20203d",
      "wang2021fcos3d",
      "yan2023cross",
      "liu2022petrv2",
      "xie2023sparsefusion"
    ],
    "metrics": {
      "R@5": 0.0,
      "R@10": 0.047619047619047616,
      "R@20": 0.2857142857142857,
      "MRR": 0.1111111111111111,
      "hits": 13,
      "total_relevant": 21
    },
    "score": 0.047619047619047616,
    "timestamp": "2025-12-18T10:53:08.997496"
  },
  {
    "query": "Introduction \\label{sec:intro} Generative AI has garnered significant interests in computer vision community. Recent advancements in text-driven image and video synthesis (T2I/T2V)<|cite_0|> bolstered by the advent of diffusion models<|cite_1|> exhibited remarkable ingenuity and generative quality, demonstrating considerable potential in image and video synthesis, editing, and animation. However, the synthesized images/videos are",
    "paper_id": "2307.00040",
    "retrieved_ids": [
      "zhang2023text",
      "b8",
      "yang2022diffusion",
      "ho2022video",
      "esser2023structure",
      "po2023state",
      "liao2023text",
      "ge2023preserve",
      "mao2023guided",
      "b53",
      "chen2023videocrafter1",
      "Lumiere",
      "xing2023dynamicrafter",
      "he2022synthetic",
      "wu2022tune",
      "razzhigaev2023kandinsky",
      "karras2023dreampose",
      "molad2023dreamix",
      "guo2023animatediff",
      "wang2023lavie",
      "UdiffText",
      "neuralvideofieldsediting",
      "khachatryan2023text2video",
      "customizeavideo",
      "zhang2024moonshot",
      "hu2024animate",
      "b2",
      "he2022lvdm",
      "wu2023cvpr",
      "svd",
      "b1",
      "singer2022make",
      "chen2023pixartalpha",
      "reed2016generative",
      "frolov2021adversarial",
      "yu2023quality",
      "tokenflow2023",
      "ren2024consisti2v",
      "voynov2023p+",
      "dai2023animateanything",
      "wang2023genlvideo",
      "ruta2023diffnst",
      "du2023demofusion",
      "liang2023flowvid",
      "aldausari2020video",
      "hertz2023style"
    ],
    "relevant_ids": [
      "sohl2015deep",
      "ma2023follow",
      "siarohin2019animating",
      "ge2022metadance",
      "wang2018video",
      "zhao2022thin",
      "karras2023dreampose",
      "gafni2019vid2game",
      "nichol2021glide",
      "liu2019liquid",
      "siarohin2021motion",
      "singer2022make",
      "ramesh2022hierarchical",
      "saharia2022photorealistic",
      "lee2019metapix",
      "ho2022imagen",
      "rombach2022high",
      "karras2020analyzing",
      "wu2022tune",
      "zhang2023adding",
      "jiang2023text2performer",
      "ni2023conditional",
      "song2020denoising",
      "wang2019few",
      "esser2023structure",
      "ho2022classifier",
      "song2020score",
      "siarohin2019first",
      "blattmann2021ipoke",
      "dhariwal2021diffusion",
      "chan2019everybody",
      "zhou2019dance",
      "kumari2022multi",
      "suo2022jointly",
      "xu2022versatile",
      "mahapatra2022controllable",
      "goodfellow2020generative",
      "ho2020denoising",
      "wang2022latent",
      "khachatryan2023text2video",
      "holynski2021animating",
      "brock2018large",
      "li2021ai",
      "weng2019photo"
    ],
    "metrics": {
      "R@5": 0.022727272727272728,
      "R@10": 0.022727272727272728,
      "R@20": 0.06818181818181818,
      "MRR": 0.2,
      "hits": 5,
      "total_relevant": 44
    },
    "score": 0.07590909090909091,
    "timestamp": "2025-12-18T10:53:11.711910"
  },
  {
    "query": "Introduction \\label{sec:intro} The perception of the 3D scene plays a vital role in autonomous driving systems. It's essential that the perception of the surrounding 3D scene is both quick and accurate, which places high demands on both performance and latency for perception methods. Voxel-based 3D deep learning methods convert the",
    "paper_id": "2307.08209",
    "retrieved_ids": [
      "sun2020scalability",
      "jiang2023vad",
      "Argoverse2021",
      "chen2023end",
      "wu2023mars",
      "chen2017multi",
      "cao2021invisible",
      "hu2022st",
      "wang2019deep",
      "chitta2021neat",
      "ravi2018real",
      "chitta2022transfuser",
      "chen2019f",
      "Zhang2016",
      "lei2022latency",
      "kim2023crn",
      "prakash2021multi",
      "liu2022petrv2",
      "object_tracking_in_autunomous_driving",
      "10160674",
      "BEV-survey",
      "shan2018pixel",
      "Bewley2018LearningLabels",
      "hong2019rules",
      "spvnas",
      "zuo2024fmgs",
      "li2023amongus",
      "richter2017playing",
      "calib-uncertainty",
      "can2021structured",
      "dreissig2023survey",
      "fei2023self",
      "li2022bevformer",
      "wang2021fcos3d",
      "chen2022gkt",
      "huang2023tri",
      "wang2022probabilistic",
      "gu2022vip3d",
      "Qiu2021Att3Ddetection",
      "behley2019semantickitti",
      "piroli2023towards",
      "varma2022transformers",
      "reading2021categorical",
      "roddick2018orthographic",
      "yang2018pixor",
      "oza2021unsupervised",
      "FSD",
      "FSD++",
      "liu2022bevfusion",
      "xu2022cobevt"
    ],
    "relevant_ids": [
      "not_all_neighbor",
      "GFNet",
      "sps-conv",
      "FSD++",
      "spatial-aware-dynamic",
      "pvrcnn",
      "spvnas",
      "figurnov2017spatially",
      "voxelnext",
      "RSN",
      "spatial_adaptive",
      "centerpoint",
      "pointacc",
      "channel_gating",
      "BEV-survey",
      "FSD",
      "pointdistiller"
    ],
    "metrics": {
      "R@5": 0.0,
      "R@10": 0.0,
      "R@20": 0.0,
      "MRR": 0.047619047619047616,
      "hits": 4,
      "total_relevant": 17
    },
    "score": 0.014285714285714284,
    "timestamp": "2025-12-18T10:53:13.799793"
  },
  {
    "query": "Introduction Recently, phenomenal advancements have been made in the field of text-to-image generation<|cite_0|>, mainly due to the significant achievements in large aligned image-text datasets<|cite_1|>, vision-language pre-training models<|cite_2|>, and diffusion models<|cite_3|>. Inspired by these text-to-image generation results, many works have explored text-conditional diffusion models in other modalities, \\eg, text-to-video<|cite_4|> and text-to-3D<|cite_5|>.",
    "paper_id": "2307.13908",
    "retrieved_ids": [
      "zhang2023text",
      "zhong2023adapter",
      "ho2022video",
      "DiffuSeq2022",
      "gong2022diffuseq",
      "ho2022imagen",
      "balaji2022ediff",
      "xu2022versatile",
      "laion5b",
      "imagen",
      "zhang2022sine",
      "dream3d",
      "xue2024raphael",
      "saharia2022photorealistic",
      "poole2022dreamfusion",
      "show1",
      "wu2022tune",
      "li20223ddesigner",
      "lakhanpal2024refining",
      "yang2024mastering",
      "huang2023humannorm",
      "chen2023vast",
      "li2023instant3d",
      "khachatryan2023text2video",
      "chen2023pixartalpha",
      "ling2023align",
      "li2022upainting",
      "lovelace2023latent",
      "chen2024it3d",
      "azadi2023make",
      "tumanyan2023plug",
      "chang2023muse",
      "singer2022make",
      "UdiffText",
      "b52",
      "couairon2023zerolayout",
      "gligen",
      "liu2023improved",
      "tiue",
      "wu2024hd",
      "liao2023text",
      "zhao2023michelangelo",
      "avrahami2023spatext",
      "frolov2021adversarial",
      "wang2023genlvideo",
      "phung2023grounded",
      "li2023your",
      "lee2024conditional",
      "han2024headsculpt",
      "b1",
      "wang2023breathing"
    ],
    "relevant_ids": [
      "sohl2015deep",
      "ddpm",
      "voynov2022sketch",
      "nerf",
      "pix2vox",
      "latentnerf",
      "stylegan",
      "sinnerf",
      "achlioptas2018learning",
      "dreamfields",
      "CLIP",
      "dreambooth",
      "pixel2mesh++",
      "hedman2021baking",
      "neus",
      "stablediffusion",
      "mo2019structurenet",
      "ho2022video",
      "instructpix2pix",
      "zhou20213d",
      "tensorf",
      "shapenet",
      "Imagen",
      "composer",
      "fan2017point",
      "autorf",
      "fastnerf",
      "luo2021diffusion",
      "makeavideo",
      "magic3d",
      "pointE",
      "cogvideo",
      "laion5b",
      "dreamfusion",
      "jia2021scaling",
      "dhariwal2021diffusion",
      "clipmesh",
      "clipforge",
      "t2i-adapter",
      "3d-r2n2",
      "instantnpg",
      "kilonerf",
      "pureclipnerf",
      "dvgo",
      "controlnet",
      "BLIP",
      "SJC",
      "ramesh2021zero",
      "get3d",
      "dalle2",
      "gan"
    ],
    "metrics": {
      "R@5": 0.0196078431372549,
      "R@10": 0.0392156862745098,
      "R@20": 0.0392156862745098,
      "MRR": 0.3333333333333333,
      "hits": 2,
      "total_relevant": 51
    },
    "score": 0.1196078431372549,
    "timestamp": "2025-12-18T10:53:17.307764"
  },
  {
    "query": "Introduction \\label{sec:intro} With the increasing demand for diverse and high-quality 3D content in various industries, such as gaming, architecture, and social platforms, the ability to efficiently generate 3D models is becoming increasingly important. However, the manual creation of 3D assets is time-consuming and requires specific expertise and artistic modeling skills.",
    "paper_id": "2307.14918",
    "retrieved_ids": [
      "zhang2024clay",
      "tang2023dreamgaussian",
      "chen2023fantasia3d",
      "tang2024lgm",
      "lin2022magic3d",
      "v3d",
      "im3d",
      "xu2024instantmesh",
      "get3d",
      "zhao2023efficientdreamer",
      "huang2023dreamtime",
      "po2023compositional",
      "tang2023make",
      "wang2023breathing",
      "zhou2024dreamscene360",
      "zhang2023avatarverse",
      "earle2024dreamcraft",
      "song2023efficient",
      "sun20233dgpt",
      "zhang2023text2nerf",
      "li2024dreamscene",
      "yi2023gaussiandreamer",
      "cheng2023sdfusion",
      "liu2023unidream",
      "zero12345plus",
      "mo2023dit",
      "po2023state",
      "yang2022diffusion",
      "chen2024meshanything",
      "han2021image",
      "bd23",
      "downs2022google",
      "zhang2023text",
      "b8",
      "lin2023consistent123",
      "garbin2022voltemorph",
      "shi2023dragdiffusion",
      "liu2023meshdiffusion",
      "kim2023reference",
      "chen2023gaussianeditor",
      "zhang2023getavatar",
      "chen2024textto3dgsgen",
      "fang2023gaussianeditor"
    ],
    "relevant_ids": [
      "yang2019pointflow",
      "sitzmann2020implicit",
      "chen2019learning",
      "selfcnerf",
      "nerf",
      "pigan",
      "achlioptas2018learning",
      "adm",
      "pixelnerf",
      "chen2021learning",
      "ldm",
      "zhou20213d",
      "nerf--",
      "mescheder2019occupancy",
      "eg3d",
      "graf",
      "sg1",
      "pavllo2020convolutional",
      "sg3",
      "pof3d",
      "giraffe",
      "campari",
      "plenoxels",
      "sg2",
      "DMTET",
      "mipnerf",
      "voxgraf",
      "huang2022multimodal",
      "controlnet",
      "nvdiffrast",
      "pavllo2021learning",
      "get3d",
      "esser2021taming"
    ],
    "metrics": {
      "R@5": 0.0,
      "R@10": 0.030303030303030304,
      "R@20": 0.030303030303030304,
      "MRR": 0.1111111111111111,
      "hits": 1,
      "total_relevant": 33
    },
    "score": 0.04242424242424242,
    "timestamp": "2025-12-18T10:53:19.306502"
  },
  {
    "query": "Introduction \\label{sec:intro} Divergent thinking is the capability to generate diverse ideas with free-flowing thought. It is crucial for human creativity and for solving complex problems that require multiple steps of reasoning. As shown in Figure~\\ref{fig:tasks}, such problems might involve planning a sequence of block manipulation actions in embodied reasoning, composing",
    "paper_id": "2406.05673",
    "retrieved_ids": [
      "huang2022inner",
      "eysenbach2018diversity",
      "hao2023reasoning",
      "Wang2022DivergenceRA",
      "kobyzev2020normalizing",
      "cao2022survey",
      "lanchantin2023selfnote",
      "wei2022chain",
      "lin2023swiftsage",
      "shlens2014notes",
      "yu2023towards",
      "francis2022core",
      "besta2024graph",
      "feng2024towards",
      "zhou2022least",
      "wang-etal-2023-towards",
      "xiao2023conditions",
      "ding2023everything",
      "du2020improved",
      "ALFWorld20",
      "wang2022selfConsistency",
      "yao2023react",
      "prasad2023adapt",
      "zhang2022automatic",
      "wang2023boosting",
      "fujisawa2022logical",
      "liu2023cones",
      "lyu-etal-2023-faithful",
      "lee2019drit",
      "gupta2021reset",
      "qin2023tool",
      "reid2022diffuser",
      "wang2024rat",
      "sun2023pearl",
      "zhang2020dive",
      "zhuang2024toolchain",
      "yasunaga2023analogical",
      "saparov2022language",
      "wang-etal-2022-scienceworld",
      "yao2024tree",
      "schwarzschild2021can",
      "mitchell2021abstraction",
      "deng2020EGP",
      "gupta2023visprog",
      "huang2023voxposer",
      "levine2020offline",
      "li2024self",
      "avdeyev2023dirichlet",
      "lin2023text2motion",
      "gabbay2019demystifying",
      "Albrecht_2018",
      "ravi2024small"
    ],
    "relevant_ids": [
      "chowdhery2023palm",
      "deleu2024joint",
      "achiam2023gpt",
      "zhang2022generative",
      "wei2022chain",
      "chen2023order",
      "chen2024tree",
      "fan2018hierarchical",
      "hu2023amortizing",
      "cobbe2021training",
      "bai2022constitutional",
      "zhang2023let",
      "meister2023locally",
      "li2023dag",
      "malkin2022gflownets",
      "besta2024graph",
      "mishra2022lila",
      "pan2023pre",
      "zimmermann2022variational",
      "hu2023gflownet",
      "singh2023beyond",
      "jain2023gflownets",
      "yu2023metamath",
      "bengio2023gflownet",
      "hendrycks2021measuring",
      "liu2023generative",
      "deleu2022bayesian",
      "pan2023better",
      "zhu2023generalized",
      "jang2023learning",
      "touvron2023llama",
      "deleu2024discrete",
      "hernandez2023multi",
      "kim2024ant",
      "havrilla2024teaching",
      "jain2023multi",
      "ouyang2022training",
      "holtzman2019curious",
      "deleu2023generative",
      "schulman2017proximal",
      "chen2022program",
      "pan2022generative",
      "hosseini2024v",
      "ikram2024evolution",
      "rein2023gpqa",
      "zelikman2022star",
      "mialon2023gaia",
      "malkin2022trajectory",
      "yue2023mammoth",
      "roy2023goal",
      "hao2023reasoning",
      "bengio2021flow",
      "yao2024tree",
      "jain2022biological"
    ],
    "metrics": {
      "R@5": 0.018518518518518517,
      "R@10": 0.037037037037037035,
      "R@20": 0.05555555555555555,
      "MRR": 0.3333333333333333,
      "hits": 4,
      "total_relevant": 54
    },
    "score": 0.11851851851851851,
    "timestamp": "2025-12-18T10:53:21.708601"
  },
  {
    "query": "Introduction \\label{sec:intro} \\begin{wrapfigure}[14]{r}{0.35\\textwidth} \\centering \\vspace{-1.5em} \\includegraphics[width=\\linewidth]{plots/cascade_pattern_visualize_intro.pdf} \\vspace{-2.0em} \\caption{\\small Attention matrices from Streaming LLM<|cite_0|> and Cascading KV Cache (Ours), \\textbf{both with the same total cache size}.} \\label{fig:intro_visualization} \\end{wrapfigure} A key problem in deploying large language models (LLMs) is the compute cost necessary to perform the attention<|cite_1|> operation during inference. While there",
    "paper_id": "2406.17808",
    "retrieved_ids": [
      "generalpatternmachines2023",
      "sink",
      "naveed2024comprehensivellms",
      "chen2023frugalgpt",
      "survey2",
      "ge2023model",
      "jiang2023llmlingua",
      "llm_quant",
      "hsieh2023distilling",
      "SEED",
      "pang2024anchor",
      "model_compression",
      "mehta2024openelm",
      "qin2023transnormerllm",
      "yuan2024llminferenceunveiledsurvey",
      "lian2023llmgrounded",
      "qin2024lightning",
      "liu2023llm",
      "onebit",
      "kim2023memoryefficient",
      "qin2024various",
      "jiang2023scaling",
      "jaiswal2024compressing",
      "ashkboos2024slicegptcompresslargelanguage",
      "miao2023specinfer",
      "qlora",
      "namburi2023cost",
      "hassid2022does",
      "llmsurvey3",
      "flashattention2",
      "phung2023grounded",
      "wimbauer2023cache",
      "fu2024break",
      "karmanov2024efficient",
      "dong2024promptpromptedadaptivestructuredpruning",
      "aminabadi2022deepspeed",
      "sqzllm",
      "lin2024awq",
      "jamba",
      "li2022efficient",
      "cherti2023reproducible",
      "infiniattention",
      "switch",
      "dao2022flashattention",
      "Pan2021OnTI",
      "han2023imagebindllm",
      "MaddahAliCentralized",
      "dettmers2023case",
      "ShirinWiggerYenerCacheAssingment",
      "dynabert"
    ],
    "relevant_ids": [
      "flashattention2",
      "landmarkattention",
      "longformer",
      "reformer",
      "compressedcontext",
      "mistral",
      "performer",
      "sink",
      "infiniattention",
      "pg19",
      "wikitext",
      "attention"
    ],
    "metrics": {
      "R@5": 0.08333333333333333,
      "R@10": 0.08333333333333333,
      "R@20": 0.08333333333333333,
      "MRR": 0.5,
      "hits": 3,
      "total_relevant": 12
    },
    "score": 0.20833333333333331,
    "timestamp": "2025-12-18T10:53:26.179340"
  },
  {
    "query": "Introduction Chain-of-Thought (CoT) has been widely proven to effectively improve the accuracy of Large Language Models (LLMs) in reasoning tasks. However, recent research<|cite_0|> found the issue of Early Answering in LLMs, where LLMs have already predicted an answer before generating the CoT. This implies that in many cases, the contribution",
    "paper_id": "2406.16144",
    "retrieved_ids": [
      "wei2022chain",
      "zheng2023progressivehint",
      "yu2023towards",
      "wang2023boosting",
      "zhang2023multimodal",
      "yasunaga2023analogical",
      "zhang2022automatic",
      "hu2024rankprompt",
      "creswell2022faithful",
      "paul2024making",
      "feng2024towards",
      "wang2022selfConsistency",
      "wang-etal-2023-towards",
      "radhakrishnan2023question",
      "wu2023analyzing",
      "wang-etal-2022-iteratively",
      "li2024faithful",
      "wang2023knowledge",
      "turpin2023language",
      "Pan2023AutomaticallyCL",
      "ranaldi2023empowering",
      "lanham2023measuring",
      "felm",
      "lyu-etal-2023-faithful",
      "qin2023cross",
      "jiang2020can",
      "bentham2024chainofthought",
      "tai2023exploring",
      "trivedi2022interleaving",
      "fu2023complexitybased",
      "wang2024chain",
      "saparov2022language",
      "he2022rethinking",
      "zhong2023mquake",
      "chen2023universal",
      "kojima2022large",
      "deyoung2020eraser",
      "chen-etal-2023-many",
      "groeneveld2020simple"
    ],
    "relevant_ids": [
      "turpin2023language",
      "wei2023chainofthought",
      "lyu-etal-2023-faithful",
      "yeo2024interpretable",
      "parcalabescu2024measuring",
      "sui2024fidelis",
      "zheng2023progressivehint",
      "radhakrishnan2023question",
      "lanham2023measuring",
      "bentham2024chainofthought",
      "lightman2023lets",
      "paul2024making",
      "li2024faithful",
      "creswell2022faithful",
      "zhang2023language"
    ],
    "metrics": {
      "R@5": 0.06666666666666667,
      "R@10": 0.2,
      "R@20": 0.4,
      "MRR": 0.5,
      "hits": 9,
      "total_relevant": 15
    },
    "score": 0.23666666666666666,
    "timestamp": "2025-12-18T10:53:28.209229"
  },
  {
    "query": "Introduction \\label{sec:intro} \\footnotetext{Authors contributed equally. Contact information for Amandeep is available on his \\href{https://virobo-15.github.io/}{page}, and for Awais, on his \\href{https://awaisrauf-15.github.io/}{page}.} StyleGAN<|cite_0|> has demonstrated exceptional capabilities in generating unconditional, photorealistic 2D images. The disentangled properties present in the learned latent space of StyleGAN have facilitated its utilization for attribute-specific realistic image<|cite_1|>.",
    "paper_id": "2406.04413",
    "retrieved_ids": [
      "ControlGAN",
      "wu2021stylespace",
      "AuxOdena",
      "Odena2017",
      "engel2018latent",
      "deng2020disentangled",
      "li2023photomaker",
      "zhu2020improved",
      "granot2022drop",
      "karras2020stylegan2",
      "karras2020analyzing",
      "or2022stylesdf",
      "shi2022semanticstylegan",
      "abdal2021styleflow",
      "abdal2019image2stylegan",
      "kim2021exploiting",
      "kwon2022diffusion",
      "liu2022delving",
      "diagonalgan",
      "zhao2022generative",
      "zhou2021cips",
      "tov2021designing",
      "sohn2023styledrop",
      "pehlivan2023styleres",
      "chen2023photoverse",
      "patashnik2021styleclip",
      "wu2023easyphoto",
      "voynov2023p+",
      "RCG2023",
      "jin2021teachers",
      "zero12345plus",
      "chong2021stylegan",
      "kang2023scaling",
      "chaiusing",
      "hou2021guidedstyle",
      "khwanmuang2023stylegan",
      "wu2023human",
      "athar2022rignerf",
      "wu2019transgaga",
      "pinnimty2020transforming",
      "sevastopolsky2022boost",
      "wang2024instantid",
      "saha2021loho",
      "vedaldi2016instance",
      "wang2023breathing",
      "wang2021hijack",
      "yu2023text",
      "abdal2022clip2stylegan",
      "collins2020editing",
      "armandpour2023re"
    ],
    "relevant_ids": [
      "patashnik2021styleclip",
      "xu20223d",
      "gadelha20173d",
      "shen2020interpreting",
      "liao2020towards",
      "szabo2019unsupervised",
      "singh2022flava",
      "chan2022efficient",
      "karras2017progressive",
      "wu2021stylespace",
      "radford2021learning",
      "karras2020analyzing",
      "karras2019style",
      "miyato2018spectral",
      "henzler2019escaping",
      "gu2021stylenerf",
      "sun2022ide",
      "nguyen2020blockgan",
      "lin20223d",
      "alayrac2022flamingo",
      "karras2020training",
      "shen2020interfacegan",
      "yuan2021florence",
      "abdal2021styleflow",
      "tewari2020stylerig",
      "jia2021scaling",
      "radford2015unsupervised",
      "li2023preim3d",
      "roich2022pivotal",
      "zhou2018stereo",
      "harkonen2020ganspace",
      "goodfellow2020generative",
      "schwarz2020graf",
      "yao2021filip",
      "wu2016learning",
      "chan2021pi",
      "xie2023high",
      "brock2018large",
      "Gal2021StyleGANNADACD",
      "collins2020editing",
      "zhao2022generative",
      "li2022blip"
    ],
    "metrics": {
      "R@5": 0.023809523809523808,
      "R@10": 0.023809523809523808,
      "R@20": 0.09523809523809523,
      "MRR": 0.5,
      "hits": 6,
      "total_relevant": 42
    },
    "score": 0.16666666666666666,
    "timestamp": "2025-12-18T10:53:31.668873"
  },
  {
    "query": "Introduction Robustness is a highly desired property of any machine learning system. Since the discovery of adversarial examples in deep neural networks<|cite_0|>, adversarial robustness - the ability of a model to withstand small, adversarial, perturbations of the input at test time - has received significant attention. A canonical way to",
    "paper_id": "2406.04981",
    "retrieved_ids": [
      "Gu2015",
      "overview",
      "metzen2017detecting",
      "carlini2017towards",
      "Madry2017",
      "huang2021exploring",
      "dong2020adversarially",
      "huang2015learning",
      "goodfellow2015laceyella",
      "tramer2019adversarial",
      "engstrom2019adversarial",
      "Goodfellow2014",
      "PMJ+16",
      "sur2",
      "xu2012robustnessvi",
      "Carlini2016",
      "grosse2016adversarial",
      "bai2021recent",
      "wang2019convergence",
      "xu2020exploring",
      "wu2018understanding",
      "awasthi2021adversarially",
      "NRP",
      "schmidt2018adversarially",
      "narodytska2016simple",
      "biggio_2018",
      "Eykholt2018Robust",
      "zhao2017generating",
      "cheng2020cat",
      "pang2022robustness",
      "tjeng2017evaluating",
      "strauss2017ensemble",
      "AFM20",
      "vijaykeerthy2018hardening",
      "testtimefrequency23",
      "Carlini_Dill_2018",
      "xiao2018training",
      "discretization_based",
      "fawzi2018adversarial",
      "evtimov2017robust",
      "papernot2016crafting",
      "jakubovitz2018improving",
      "DBLP:conf/iclr/SinhaND18",
      "bhagoji2019lower",
      "ford2019adversarial",
      "papernot2016distillation",
      "feinman2017detecting",
      "yu2021lafeat",
      "li2019certified",
      "adversarialrobustness23",
      "li2020towards",
      "balaji2019",
      "isit2018",
      "shi2021online"
    ],
    "relevant_ids": [
      "Gun+18b",
      "Sou+18",
      "AFM20",
      "NTS15",
      "RWK20",
      "Woo+20",
      "Deb+23",
      "YRB19",
      "Sze+14",
      "Wan+23",
      "Mad+18",
      "Bel21",
      "Zha+17",
      "Cro+21",
      "Big+13",
      "Gun+18a"
    ],
    "metrics": {
      "R@5": 0.0,
      "R@10": 0.0,
      "R@20": 0.0,
      "MRR": 0.030303030303030304,
      "hits": 1,
      "total_relevant": 16
    },
    "score": 0.00909090909090909,
    "timestamp": "2025-12-18T10:53:33.923913"
  },
  {
    "query": "Introduction \\label{sec:intro} \\begin{figure} \\centering \\includegraphics[width=1\\linewidth]{img/Fig.1_challenges.drawio.pdf} \\caption{A comparison between RAG and R$^2$AG. R$^2$AG employs a trainable R$^2$-Former to bridge the semantic gap between retrievers and LLMs. Optionally, LLMs can be fine-tuned to understand the retrieval information further.} \\label{fig:intro} \\end{figure} Retrieval augmented generation (RAG)<|cite_0|> significantly enhances the capabilities of large language models",
    "paper_id": "2406.13249",
    "retrieved_ids": [
      "Ke2024BridgingTP",
      "ref:ragsurvey1",
      "Gao2023RetrievalAugmentedGF",
      "ref:ragsurvey3",
      "feng2023synergistic",
      "ram2023context",
      "mmllms2",
      "fu2023mme",
      "liu2023reta",
      "tang2023struc",
      "lian2023llmgrounded",
      "ref:ragsurvey2",
      "ref:gnn-rag",
      "ref:ragsurvey7",
      "woodpecker",
      "ref:ragsurvey6",
      "zhang2024raft",
      "Lewis2020RetrievalAugmentedGF",
      "Zhao2024RetrievalAugmentedGF",
      "Barnett2024SevenFP",
      "He2024GRetrieverRG",
      "ref:grag",
      "Yan2024CorrectiveRA",
      "balaguer2024rag",
      "Asai2023SelfRAGLT",
      "cuconasu2024power",
      "glass2022re2g",
      "Ivison2023CamelsIA",
      "ref:graphrag",
      "b2",
      "voynov2023p+",
      "bevilacqua2022autoregressive"
    ],
    "relevant_ids": [
      "Zhu2023MiniGPT4EV",
      "Zhu2023LargeLM",
      "Asai2023SelfRAGLT",
      "Jiang2023LongLLMLinguaAA",
      "He2024GRetrieverRG",
      "Xu2024ListawareRJ",
      "Yasunaga2023RetrievalAugmentedML",
      "Zhao2024RetrievalAugmentedGF",
      "Wu2024HowED",
      "Gao2023RetrievalAugmentedGF",
      "Kandpal2022LargeLM",
      "Li2023BLIP2BL",
      "Barnett2024SevenFP",
      "Shi2023REPLUGRB",
      "Yan2024CorrectiveRA",
      "Mallen2022WhenNT",
      "Koizumi2020AudioCU",
      "Zhao2022DenseTR",
      "BehnamGhader2022CanRL",
      "Xu2023RECOMPIR",
      "Ke2024BridgingTP",
      "Lewis2020RetrievalAugmentedGF",
      "Liu2023LostIT",
      "Izacard2022AtlasFL",
      "Zhao2023ASO"
    ],
    "metrics": {
      "R@5": 0.08,
      "R@10": 0.08,
      "R@20": 0.2,
      "MRR": 1.0,
      "hits": 8,
      "total_relevant": 25
    },
    "score": 0.356,
    "timestamp": "2025-12-18T10:53:36.642864"
  },
  {
    "query": "Introduction A series of Large Language Models (LLMs)<|cite_0|> like GPT-4<|cite_1|>, PaLM<|cite_2|> and LLaMA<|cite_3|> have showcased the impressive performance in various reasoning tasks. In addition to scaling up the model size to improve the reasoning performance, there are more effective prompting methods that further enhance the functionality and performance of LLMs.",
    "paper_id": "2406.04271",
    "retrieved_ids": [
      "Zhao2023ASO",
      "huang2022large",
      "survey0",
      "zheng2023progressivehint",
      "minaee2024large",
      "naveed2024comprehensivellms",
      "wei2022chain",
      "zhang2023cumulative",
      "sahoo2024systematic",
      "hu2024rankprompt",
      "zheng2023take",
      "wang2024large",
      "sun2023pearl",
      "jiang2023scaling",
      "cd-reasoning",
      "mmllms1",
      "lu2023chameleon",
      "WizardMath",
      "xie2024me",
      "bubeck2023sparks",
      "gao2023pal",
      "stechly2023gpt",
      "deepseekllm",
      "chia2024instructeval",
      "huang2023languages",
      "tang2023struc",
      "llmsurvey3",
      "Liu2023LLMRecBL",
      "hao2023reasoning",
      "qin2024large",
      "makatura2023large",
      "liu2023reta",
      "yu2023metamath",
      "fu2023complexitybased",
      "wang-etal-2023-towards",
      "petroni2021kilt",
      "wan-etal-2023-better",
      "openfunction",
      "ref:ragsurvey2",
      "he2022rethinking",
      "zhou2022teaching",
      "zhou2023solving",
      "ahuja-etal-2023-mega",
      "singhal2023large",
      "saparov2022language",
      "kojima2022large",
      "gruver2023large",
      "ahuja2023mega",
      "hou2024large",
      "chen2024tree",
      "anil2023palm",
      "balaguer2024rag"
    ],
    "relevant_ids": [
      "suzgun2024meta",
      "jiang2024mixtral",
      "gao2023pal",
      "besta2024graph",
      "achiam2023gpt",
      "du2022glm",
      "wei2022chain",
      "anil2023palm",
      "zhang2022automatic",
      "zhou2022least",
      "brown2020language",
      "xu2023expertprompting",
      "wang2022selfConsistency",
      "yasunaga2023analogical",
      "yao2024tree",
      "touvron2023llama",
      "touvron2023llama2"
    ],
    "metrics": {
      "R@5": 0.0,
      "R@10": 0.058823529411764705,
      "R@20": 0.058823529411764705,
      "MRR": 0.14285714285714285,
      "hits": 3,
      "total_relevant": 17
    },
    "score": 0.06050420168067226,
    "timestamp": "2025-12-18T10:53:39.434570"
  },
  {
    "query": "Introduction Developing intelligent robotic agents capable of precise visuo-motor control is an important area of research. A standard paradigm of developing such agents is to train the visual encoder and control policy end-to-end, using domain-specific control data<|cite_0|>. However, this approach limits the applicability of visuo-motor control policies in real-world scenarios",
    "paper_id": "2406.06072",
    "retrieved_ids": [
      "levine2016e2e_visual",
      "schwab2019simultaneously",
      "zhan2022learning",
      "jing2023exploring",
      "q-attention",
      "hansen2022pre",
      "heravi2022visuomotor",
      "bousmalis2023robocat",
      "zhu2020ingredients",
      "chi2023diffusion",
      "seo2023multi",
      "mees2022calvin",
      "florence2019self",
      "zhu2023ghost",
      "parisi2022unsurprising",
      "li20213d",
      "xiao2022masked",
      "wang2021generalization",
      "finn2016deep",
      "Mees2022WhatMI",
      "yang2023learning",
      "peng2018sim",
      "Hafner2020DreamImagination",
      "gu2017deep",
      "andrychowicz2020learning",
      "wang2019deep",
      "ze2023rl3d",
      "hansen2022modem",
      "du2024learning",
      "dasari2020transformers",
      "lynch2022interactive",
      "duan2016benchmarking",
      "dreamer",
      "baranes2013active",
      "Truong2021BiDirectionalDA",
      "nagabandi2020deep",
      "matas2018sim",
      "qureshi2019composing",
      "popov2017data",
      "shah2021rrl",
      "zhang2018deep",
      "kim2022automating",
      "sharma2022state"
    ],
    "relevant_ids": [
      "mao2022towards",
      "yu2021rethinking",
      "yu2020metaworld",
      "bao2021beit",
      "levine2016e2e_visual",
      "nair2022r3m",
      "bommasani2021foundationmodels",
      "wang2022vrl3",
      "shah2021rrl",
      "majumdar2023vc1",
      "brown2020gpt3",
      "peng2021conformer",
      "he2022MAE",
      "he2020moco",
      "stone2023moo",
      "chu2021twins",
      "yuan2022pieg",
      "Hong2022RepresentationSF",
      "ma2022vip",
      "radosavovic2022mvp",
      "naseer2021intriguing",
      "parisi2022unsurprising",
      "he2016resnet",
      "devlin2018bert",
      "strudel2021segmenter",
      "kirillov2023sam",
      "Tassa2018DMC",
      "hansen2021svea",
      "radford2021clip",
      "brohan2023rt2",
      "li2022ViTDet",
      "Ranftl2021dpt",
      "liu2021swin",
      "rajeswaran2018adroit",
      "chen2022vit_adapter",
      "dosovitskiy2020image",
      "dehghani2023scalingvit",
      "Fang2022UnleashingVV"
    ],
    "metrics": {
      "R@5": 0.02631578947368421,
      "R@10": 0.02631578947368421,
      "R@20": 0.05263157894736842,
      "MRR": 1.0,
      "hits": 3,
      "total_relevant": 38
    },
    "score": 0.31842105263157894,
    "timestamp": "2025-12-18T10:53:41.355892"
  },
  {
    "query": "Introduction \\label{sec:intro} \\vspace{-1mm} Advancements in diffusion-based generative models<|cite_0|> have significantly improved text-driven image editing capabilities<|cite_1|>. Building on this success, there has been considerable effort to extend these technologies to the video domain<|cite_2|>, which holds practical potential across a broad range of applications, including Film/Entertainment and AR/VR. However, training a video",
    "paper_id": "2406.02541",
    "retrieved_ids": [
      "yang2022diffusion",
      "b8",
      "ho2022video",
      "zhang2023text",
      "ceylan2023pix2video",
      "chen2023videocrafter1",
      "liao2023text",
      "show1",
      "chai2023stablevideo",
      "zhang2022sine",
      "esser2023structure",
      "ref9",
      "po2023state",
      "moser2024diffusion",
      "nikankin2022sinfusion",
      "Schiappa-ACM-2023",
      "VIDiff",
      "videocrafter2",
      "song2023efficient",
      "qi2023fatezero",
      "zhang2023avid",
      "wu2023cvpr",
      "guo2024initno",
      "2312.04524",
      "molad2023dreamix",
      "tokenflow2023",
      "khachatryan2023text2video",
      "zeroshot_offtheshelf",
      "zhang2024moonshot",
      "simsar2023lime",
      "luoDiffFoleySynchronizedVideotoAudio2023",
      "park2023ed",
      "svd",
      "wang2023genlvideo",
      "levin2023differential",
      "liu2023video",
      "aldausari2020video",
      "avrahami2023blended",
      "sheynin2024emu",
      "du2023demofusion",
      "b51",
      "frolov2021adversarial",
      "pei2024deepfake"
    ],
    "relevant_ids": [
      "wu2023cvpr",
      "Avrahami_2022_CVPR",
      "kasten2021layered",
      "ceylan2023pix2video",
      "gal2022textual",
      "singer2022make",
      "rombach2022high",
      "ho2022video",
      "2312.04524",
      "zhang2023adding",
      "qi2023fatezero",
      "cao2023hexplane",
      "kerbl20233d",
      "yang2023deformable3dgs",
      "wu20234d",
      "tiny_cuda_nn",
      "ruiz2022dreambooth",
      "meng2022sdedit",
      "hertz2022prompttoprompt",
      "couairon2022diffedit",
      "tokenflow2023",
      "text2video-zero",
      "ho2020denoising",
      "brooks2022instructpix2pix",
      "bau2021paintbyword"
    ],
    "metrics": {
      "R@5": 0.08,
      "R@10": 0.08,
      "R@20": 0.12,
      "MRR": 0.3333333333333333,
      "hits": 6,
      "total_relevant": 25
    },
    "score": 0.156,
    "timestamp": "2025-12-18T10:53:44.002432"
  },
  {
    "query": "Introduction \\begin{figure} \\centering \\include{fig/params_accuracy} \\vspace{-0.8cm} \\caption{\\textbf{Parameter-accuracy characteristics of adaptation methods on the VTAB<|cite_0|> \\emph{test sets}.} We report original results and re-evaluations ($\\circlearrowright$) after a complete training schedule with suitable data normalization. Our \\adapter has clearly the best parameter-accuracy trade-off. The vertical, dashed line shows the possible minimal number of tunable",
    "paper_id": "2406.06820",
    "retrieved_ids": [
      "Zhai:2020:LSR",
      "wang2020tent",
      "wangOnceforallAdversarialTraining2020",
      "roy2019automatic",
      "liang2023comprehensive",
      "lim2023ttn",
      "he2023parameter",
      "boudiaf2022parameter",
      "hoffer2018normmatters",
      "liang2022no",
      "peters2019tune",
      "nado2020evaluating",
      "marsden2023universal",
      "chen2022adaptformer",
      "brock2021high",
      "Dou2022AnES",
      "luo2023towards",
      "Xu2019Understanding",
      "xu2020exploring",
      "sung2022vl",
      "lian2022scaling",
      "jang2024model_Stock",
      "ShiftAndScale",
      "qi2022parameterefficient",
      "mehta2024openelm",
      "adapter",
      "yu2019universally",
      "nagel2022overcoming",
      "he-etal-2023-blind",
      "Ivison2023CamelsIA",
      "tay2021scale",
      "lee2023holistic",
      "Wei2022QDropRD",
      "Taori2020MeasuringRT",
      "dettmers2023case",
      "liu2019roberta",
      "hagele2024scaling",
      "sukhbaatar-etal-2019-adaptive",
      "wu2023easyphoto",
      "mahabadi2021parameter",
      "guo2023black",
      "2208.05516",
      "dong2019bench",
      "vedaldi2016instance",
      "shu2022tpt",
      "tychsen2018improving",
      "zhu2017prune",
      "jin2021phototourism",
      "chen2021adaspeech",
      "park2019effect",
      "hao2023optimizing",
      "imagenet-v2",
      "jiang2022model",
      "hooker2019compressed"
    ],
    "relevant_ids": [
      "Pfeiffer:2021:AND",
      "Carion:2020:EEO",
      "Radford:2021:LTV",
      "Zhang:2022:NPS",
      "Donahue:2014:DDC",
      "Hu:2022:LLR",
      "Vaswani:2017:AAY",
      "Dosovitskiy:2021:IWW",
      "Ranftl:2021:VTD",
      "Jia:2022:VPT",
      "He:2016:DRL",
      "Houlsby:2019:PET",
      "Xie:2021:SSE",
      "He:2022:TUV",
      "Russakovsky:2015:ILS",
      "Rebuffi:2017:LMV",
      "Caron:2021:EPS",
      "Dehghani:2023:SVT",
      "Zhai:2020:LSR"
    ],
    "metrics": {
      "R@5": 0.05263157894736842,
      "R@10": 0.05263157894736842,
      "R@20": 0.05263157894736842,
      "MRR": 1.0,
      "hits": 1,
      "total_relevant": 19
    },
    "score": 0.3368421052631579,
    "timestamp": "2025-12-18T10:53:47.778005"
  },
  {
    "query": "Introduction Large Language Models~(LLMs) have achieved remarkable success in various natural language processing tasks, such as language understanding, reasoning, and generation, demonstrating superior performance and adaptability<|cite_0|>. However, the rapid growth in model size, with state-of-the-art LLMs containing billions of parameters, poses significant challenges to computational resources and memory consumption<|cite_1|>. The",
    "paper_id": "2406.08155",
    "retrieved_ids": [
      "kaddour2023challenges",
      "naveed2024comprehensivellms",
      "minaee2024large",
      "guo2024large",
      "Zhao2023ASO",
      "laskar2024systematic",
      "survey2",
      "Zhu2023LargeLM",
      "mmllms2",
      "xie2023translating",
      "ref:financialgpt",
      "qin2024large",
      "miao2023towards",
      "ref:edu1",
      "ref:med1",
      "hu2024rankprompt",
      "wang2023knowledge",
      "ref:ragsurvey2",
      "mmllms1",
      "Gao2023RetrievalAugmentedGF",
      "ref:ragsurvey7",
      "model_compression",
      "asai2024reliable",
      "llmsurvey3",
      "huang2023languages",
      "yuan2024llminferenceunveiledsurvey",
      "zheng2024",
      "du2022glamefficientscalinglanguage",
      "chia2024instructeval",
      "zhang2023FinEvalChineseFinancial",
      "jin2024comprehensive",
      "makatura2023large",
      "kim2023memoryefficient",
      "ref:med3",
      "mirzadeh2023relustrikesbackexploiting",
      "yu2023metamath",
      "jiang2023scaling",
      "ganesh2020compressing",
      "pang2024anchor",
      "chen2023videollm",
      "shao2023omniquant",
      "ahuja-etal-2023-mega",
      "tian2024gnp",
      "ahuja2023mega",
      "yu2024melo"
    ],
    "relevant_ids": [
      "aminabadi2022deepspeed",
      "pan2023smoothquant+",
      "dai2024deepseekmoe",
      "li2024merge",
      "krajewski2024scaling",
      "liu2023llm",
      "artetxe2022efficient",
      "kaplan2020scaling",
      "jiang2024mixtral",
      "lin2024awq",
      "brown2020language",
      "openai2024gpt4",
      "rajbhandari2022deepspeedmoe",
      "jiang2023mistral",
      "frantar2023optimal",
      "touvron2023llama",
      "xiao2024smoothquant",
      "frantar2023gptq",
      "fedus2022switch",
      "shoeybi2020megatronlm",
      "chen2022taskspecific",
      "shazeer2017outrageously",
      "sharify2024combining"
    ],
    "metrics": {
      "R@5": 0.0,
      "R@10": 0.0,
      "R@20": 0.0,
      "MRR": 0.0,
      "hits": 0,
      "total_relevant": 23
    },
    "score": 0.0,
    "timestamp": "2025-12-18T10:53:50.328656"
  },
  {
    "query": "Introduction In recent years, large language models (LLMs) have reached groundbreaking milestones, significantly advancing in areas such as semantic understanding, sentence translation, and more<|cite_0|>. These models not only facilitate enhanced interaction between computers and human language but also drive innovation across numerous applications. However, as these models become increasingly central",
    "paper_id": "2406.06140",
    "retrieved_ids": [
      "minaee2024large",
      "kaddour2023challenges",
      "naveed2024comprehensivellms",
      "Zhao2023ASO",
      "survey0",
      "gpt_summary",
      "mmllms1",
      "Zhu2023LargeLM",
      "ref:med2",
      "makatura2023large",
      "zhu2023multilingual",
      "laskar2024systematic",
      "ref:edu1",
      "ref:med1",
      "guo2024large",
      "ref:financialgpt",
      "sahoo2024systematic",
      "chu2023qwen",
      "xie2023translating",
      "qin2024large",
      "wang2024large",
      "2020arXiv200301200T",
      "shu2023llasm",
      "ref:med3",
      "qin2024multilingual",
      "miao2023towards",
      "yao2023survey",
      "llmsurvey3",
      "wang2023surveyfactualitylargelanguage",
      "Gao2023RetrievalAugmentedGF",
      "jiang2023scaling",
      "zhang-etal-2023-multilingual",
      "rae2021scaling",
      "wang2023chatcad",
      "ref:ragsurvey3",
      "pang2024anchor",
      "model_compression",
      "tang2023large",
      "Pan2023AutomaticallyCL",
      "huang2023survey",
      "levy2024same",
      "zhao2023bubogpt",
      "ref:ragsurvey2",
      "ahuja-etal-2023-mega",
      "ahuja2023mega",
      "bai2023qwen",
      "wang2023prompt"
    ],
    "relevant_ids": [
      "ge2023making",
      "openai2023gpt4",
      "liu2023llava",
      "anil2023palm",
      "kirillov2023segment",
      "team2023gemini",
      "qin2024infobench",
      "team2024gemma",
      "hendrycks2020measuring",
      "radford2021learning",
      "jiang2023mistral",
      "wei2023instructiongpt",
      "touvron2023llama",
      "jiang2023followbench",
      "bai2023qwen",
      "zhu2023minigpt",
      "fu2023mme",
      "zhong2023agieval",
      "zhou2023instruction",
      "sun2024trustllm",
      "yue2023mmmu",
      "wei2024large",
      "koh2023generating"
    ],
    "metrics": {
      "R@5": 0.0,
      "R@10": 0.0,
      "R@20": 0.0,
      "MRR": 0.021739130434782608,
      "hits": 1,
      "total_relevant": 23
    },
    "score": 0.006521739130434782,
    "timestamp": "2025-12-18T10:53:52.415530"
  },
  {
    "query": "Introduction \\label{sec:intro} Self-supervised learning (SSL) has significantly improved performance across various tasks. Despite not requiring supervision, SSL heavily depends on carefully selected data augmentations<|cite_0|>. Previous experimental literature has shown that removing even one augmentation such as random rescaling or color jittering reduces linear evaluation performance on ImageNet1k<|cite_1|> by at least",
    "paper_id": "2406.09294",
    "retrieved_ids": [
      "oquab2023dinov2",
      "hendrycks2019using",
      "chen23l_interspeech",
      "zhang2022leverage",
      "lee2024computer",
      "relic",
      "Azabou2021",
      "asano2020critical",
      "wang2022does",
      "ssl",
      "compress",
      "cui2022democratizing",
      "selvaraju2021casting",
      "colorization",
      "Tian2021UnderstandingSL",
      "liu2022improving",
      "zbontar2021barlow",
      "mu2021slip",
      "liu2020self",
      "khosla2020supervised",
      "liu2021contrastive",
      "seer",
      "deacl",
      "van2021revisiting",
      "cao2022synergistic",
      "elnouby2021largescale",
      "goyal2021selfsupervised",
      "huynh2022fnc",
      "sohn2020simple",
      "XDHLL20",
      "SBCZZRCKL20",
      "sohn2020fixmatch",
      "ermolov2020whitening",
      "ZWHWWOS21",
      "gyawali2019semi",
      "simclr",
      "graikos2023learned",
      "zhang2021flexmatch",
      "chaitanya2020contrastive",
      "xie2020unsupervised",
      "odin",
      "henaff2022object",
      "dangovski2021equivariant",
      "tian2020makes",
      "chen2022transmix"
    ],
    "relevant_ids": [
      "bao2021beit",
      "bojanowski2017unsupervised",
      "el2024scalable",
      "zhang2022rethinking",
      "richemond2020byol",
      "grill2020bootstrap",
      "asano2020critical",
      "oquab2023dinov2",
      "vonk\u00fcgelgen2022selfsupervised",
      "saunshi2022understanding",
      "elnouby2021largescale",
      "he2021masked",
      "dosovitskiy2016discriminative",
      "caron2020unsupervised",
      "tian2020makes",
      "doersch2015unsupervised",
      "gidaris2018unsupervised",
      "caron2021emerging",
      "caron2018deep",
      "chen2020simple",
      "eastwood2023self",
      "assran2023self",
      "purushwalkam2020demystifying",
      "chen2020improved",
      "he2020momentum",
      "xiao2021contrastive",
      "chen2020big",
      "bendidi2023free",
      "russakovsky2015imagenet",
      "geiping2023data"
    ],
    "metrics": {
      "R@5": 0.03333333333333333,
      "R@10": 0.06666666666666667,
      "R@20": 0.06666666666666667,
      "MRR": 1.0,
      "hits": 4,
      "total_relevant": 30
    },
    "score": 0.3333333333333333,
    "timestamp": "2025-12-18T10:53:54.858382"
  },
  {
    "query": "Introduction \\label{sec:intro} The development of Large Language Models (LLMs) has significantly advanced Natural Language Processing (NLP), especially in natural language generation, due to their advanced understanding capabilities. While prompt engineering initially showed promise<|cite_0|>, real-world applications have revealed significant challenges, particularly in following complex instructions. To overcome these, researchers have developed",
    "paper_id": "2406.11301",
    "retrieved_ids": [
      "he2024can",
      "kaddour2023challenges",
      "qin2024large",
      "2020arXiv200301200T",
      "minaee2024large",
      "gpt_summary",
      "naveed2024comprehensivellms",
      "Zhao2023ASO",
      "survey1",
      "conifer",
      "guo2024large",
      "ref:financialgpt",
      "sahoo2024systematic",
      "zhou2023instruction",
      "huang2023survey",
      "miao2023towards",
      "ref:edu1",
      "chia2024instructeval",
      "makatura2023large",
      "ref:ragsurvey2",
      "lian2023llmgrounded",
      "wen2024benchmarking",
      "ref:med3",
      "mmhal_survey",
      "llmsurvey3",
      "lyu2024probabilities",
      "ref:ragsurvey3",
      "yao2023survey",
      "Pan2023AutomaticallyCL",
      "Jiang2023LongLLMLinguaAA",
      "perez2022ignore",
      "model_compression",
      "zhou2022large",
      "wang2023prompt",
      "wu2023speechgen",
      "levy2024same",
      "ahuja-etal-2023-mega",
      "jin2023time",
      "li2024devbench",
      "ahuja2023mega",
      "bai2023qwen",
      "Liu2023IsCA"
    ],
    "relevant_ids": [
      "DPO",
      "RLHF",
      "KTO",
      "WizardMath",
      "LIMA",
      "survey2",
      "FollowBench",
      "survey0",
      "sahoo2024systematic",
      "brown2020language",
      "wang2023prompt",
      "instruction-tuning",
      "InFoBench",
      "WizardCoder",
      "IFEval",
      "conifer",
      "survey1",
      "wizardLM"
    ],
    "metrics": {
      "R@5": 0.0,
      "R@10": 0.1111111111111111,
      "R@20": 0.16666666666666666,
      "MRR": 0.1111111111111111,
      "hits": 4,
      "total_relevant": 18
    },
    "score": 0.06666666666666667,
    "timestamp": "2025-12-18T10:53:56.939236"
  },
  {
    "query": "Introduction \\vspace{-5pt} { \\begin{figure*}[h] \\centering \\includegraphics[scale=0.14]{sec/Images/Diffusion_inpainting_process.png} \\caption{An overview of the diffusion process for temporal consistent video editing. To make the video editing process temporally consistent, we extend the U-Net architecture by replacing original attention modules used in<|cite_0|> with Extended Attention modules \\vspace{-10pt}} \\label{fig:DiffusionProcess} \\end{figure*} In recent years, significant strides have",
    "paper_id": "2406.00272",
    "retrieved_ids": [
      "b8",
      "zhang2023avid",
      "tokenflow2023",
      "2312.04524",
      "ho2022video",
      "zhang2023towards",
      "ceylan2023pix2video",
      "zeng2020learning",
      "chai2023stablevideo",
      "simsar2023lime",
      "chu2023video",
      "kim2024tcan",
      "zeroshot_offtheshelf",
      "vmc",
      "wu2023harnessing",
      "molad2023dreamix",
      "Lumiere",
      "FGT",
      "song2023efficient",
      "po2023state",
      "cong2023flatten",
      "hu2023videocontrolnet",
      "mao2023guided",
      "shin2023editavideo",
      "guo2024initno",
      "li2023drivingdiffusion",
      "Zhou2024storydiffusion",
      "qi2023fatezero",
      "inpaint",
      "wimbauer2023cache",
      "Avrahami_2022_CVPR",
      "DMT",
      "customizeavideo",
      "neuralvideofieldsediting",
      "khachatryan2023text2video",
      "zhang2023text",
      "zhang2024moonshot",
      "levin2023differential",
      "fresco",
      "liu2022unsupervised",
      "webvid",
      "wang2023genlvideo",
      "lee2023syncdiffusion",
      "b52",
      "NilssonS18"
    ],
    "relevant_ids": [
      "YouTube-VOS_Dataset",
      "CLIP",
      "DALLE2",
      "GuidanceScale",
      "DAVIS_Dataset",
      "VIDiff",
      "ControlVideo",
      "Magicedit",
      "Imagen",
      "DDPM",
      "TuneAVideo",
      "Lumiere",
      "FGT",
      "LAMA",
      "LDMs",
      "ProPainter",
      "DMT",
      "Pix2Video",
      "E2FGVI",
      "TokenFlow",
      "DDIM",
      "ControlNet",
      "VideoComposer",
      "banmo"
    ],
    "metrics": {
      "R@5": 0.0,
      "R@10": 0.0,
      "R@20": 0.08333333333333333,
      "MRR": 0.058823529411764705,
      "hits": 3,
      "total_relevant": 24
    },
    "score": 0.01764705882352941,
    "timestamp": "2025-12-18T10:54:00.405616"
  },
  {
    "query": "Introduction Large language models (LLMs), like Llama<|cite_0|> and GPT<|cite_1|>, have exhibited remarkable proficiency in general-purpose language generation and understanding<|cite_2|>. These advancements are largely credited to the development of Transformer-based architectures<|cite_3|> with billions of parameters and to the extensive pre-training on web-sourced corpora with trillions of tokens<|cite_4|>. However, on the other",
    "paper_id": "2406.09179",
    "retrieved_ids": [
      "Zhao2023ASO",
      "generalpatternmachines2023",
      "naveed2024comprehensivellms",
      "minaee2024large",
      "lehman2022evolution",
      "ref:med1",
      "dolma",
      "ref:edu1",
      "mmllms1",
      "xia2024fofo",
      "makatura2023large",
      "lora",
      "wang2023knowledge",
      "xie2024me",
      "gui_automation1",
      "jordan_chinchilla_2022",
      "llmsurvey3",
      "ref:ragsurvey1",
      "xia2023sheared",
      "deepseekllm",
      "Hu:2022:LLR",
      "shoeybi2020megatronlm",
      "Gao2023RetrievalAugmentedGF",
      "llama-moe",
      "chia2024instructeval",
      "zeng2023lynx",
      "chowdhery2022palm",
      "wu2023bloomberggpt",
      "switch",
      "zheng2024",
      "scao2022bloom",
      "yu2024language_dare",
      "fingpt",
      "WizardMath",
      "refinedweb",
      "mehta2024openelm",
      "Touvron2023Llama2O",
      "Ivison2023CamelsIA",
      "wei2023skywork",
      "touvron2023llama",
      "dong2024promptpromptedadaptivestructuredpruning",
      "jamba",
      "gruver2023large",
      "conneau2020unsupervised",
      "xraygpt",
      "ge2023making",
      "hou2024large",
      "ref:qwen2"
    ],
    "relevant_ids": [
      "achiam2023gpt",
      "zhu2024decoupling",
      "maini2024tofu",
      "vaswani2017attention",
      "yao2023editing",
      "liu2023trustworthy",
      "azerbayev2023llemma",
      "roziere2023code",
      "bourtoule2021machine",
      "ji2023survey",
      "liu2024rethinking",
      "yao2023survey",
      "brown2020language",
      "arpit2017closer",
      "touvron2023llama",
      "gallegos2023bias",
      "carlini2021extracting",
      "ouyang2022training",
      "liu2023jailbreaking",
      "wu2023bloomberggpt",
      "touvron2023llama2",
      "yao2023large",
      "goodfellow2013empirical"
    ],
    "metrics": {
      "R@5": 0.0,
      "R@10": 0.0,
      "R@20": 0.0,
      "MRR": 0.03571428571428571,
      "hits": 2,
      "total_relevant": 23
    },
    "score": 0.010714285714285713,
    "timestamp": "2025-12-18T10:54:03.361708"
  },
  {
    "query": "Introduction Large Language Models (LLMs) exhibit the remarkable ability to capture plenty of factual knowledge into their parameters. However, knowledge of inner LLMs may become outdated or unsuitable over time<|cite_0|>. Unfortunately, naively re-training LLMs can be computationally intensive, and fine-tuning LLMs in several cases suffers the risk of catastrophic forgetting<|cite_1|>.",
    "paper_id": "2406.02882",
    "retrieved_ids": [
      "hu2023",
      "luo2023empirical",
      "tian2023fine",
      "dong2023abilities",
      "gupta2024model",
      "huang2022large",
      "wang2023surveyfactualitylargelanguage",
      "Zhao2023ASO",
      "felm",
      "naveed2024comprehensivellms",
      "Zhu2023LargeLM",
      "minaee2024large",
      "wang2023knowledge",
      "dola",
      "zhang2024comprehensive",
      "survey2",
      "kaddour2023challenges",
      "zheng2023can",
      "gu23minillm",
      "wang2024easyedit",
      "laskar2024systematic",
      "yao2023editing",
      "biderman2023emergent",
      "jaiswal2024compressing",
      "ding-et-al:scheme",
      "llmsurvey3",
      "yuan2023scaling",
      "wu2023evakellm",
      "Huang2023LargeLM",
      "cheng2023decouple",
      "mmllms1",
      "sun2023rankgpt",
      "he2022rethinking",
      "asai2024reliable",
      "Bao2023TALLRecAE",
      "liu2024rethinking",
      "Mallen2022WhenNT",
      "sun2024simpleeffectivepruningapproach",
      "Pan2023AutomaticallyCL",
      "Gao2023RetrievalAugmentedGF",
      "baek2023knowledge",
      "yu2024language_dare",
      "mazzia2023survey",
      "xie2023translating",
      "huang2023opera",
      "beniwal2024crosslingual",
      "wang2023retrievalaugmented",
      "tian2024gnp",
      "huang2023transformer"
    ],
    "relevant_ids": [
      "mitchell2021fast",
      "dhingra2022time",
      "huang2023transformer",
      "parisi2019continual",
      "wang2023knowledge",
      "yao2023editing",
      "li2023pmet",
      "zhang2024comprehensive",
      "de2021editing",
      "levy2017zero",
      "madaan2022memory",
      "elazar2021measuring",
      "yin2023history",
      "wang2023cross",
      "mitchell2022memory",
      "zheng2023can",
      "mazzia2023survey",
      "meng2022locating",
      "zhong2023mquake",
      "zhao2021calibrate",
      "dai2021knowledge"
    ],
    "metrics": {
      "R@5": 0.0,
      "R@10": 0.0,
      "R@20": 0.14285714285714285,
      "MRR": 0.07692307692307693,
      "hits": 6,
      "total_relevant": 21
    },
    "score": 0.023076923076923078,
    "timestamp": "2025-12-18T10:54:06.038519"
  },
  {
    "query": "Introduction Recent research extensively explores the development of robust predictors against adversarial perturbations, revealing the susceptibility of deep neural networks to imperceptible adversarial noise<|cite_0|>. Adversarial perturbations involve introducing limited noise $\\delta$ to an image ${x}$ (or more generally, $x'\\in \\calA(x)$ for a perturbation set $\\calA(x)$), resulting in visually indistinguishable yet",
    "paper_id": "2406.03458",
    "retrieved_ids": [
      "sur2",
      "metzen2017detecting",
      "tabacof2016exploring",
      "jin2015robust",
      "Gu2015",
      "grosse2016adversarial",
      "tramer2019adversarial",
      "mustafa2019adversarial",
      "he2019parametric",
      "zeng2019adversarial",
      "DBLP:conf/bmvc/LiTCBL18",
      "yan2018deep",
      "jia2020certified",
      "laidlaw2021perceptual",
      "xu2017feature",
      "Schott2018a",
      "fawzi2016robustness",
      "qin2019imperceptible",
      "adversarial_noise_layer",
      "overview",
      "simon2019first",
      "li2019certified",
      "Wong2017",
      "narodytska2016simple",
      "lamb2019interpolated",
      "wong2018provable",
      "chen2018ead",
      "akhtar2018threat",
      "gao2020patch",
      "daza2021towards",
      "Gowal2020UncoveringTL",
      "chen2018shapeshifter",
      "sharma2019effectiveness",
      "poursaeed2018generative",
      "su2019one",
      "hendrik2017universal",
      "fawzi2018adversarial",
      "wu2020adversarial",
      "moosavi2017universal",
      "xiao2018spatially",
      "blau2022threat",
      "tramer2017ensemble",
      "lyu2015unified",
      "dziugaite2016study",
      "ganeshan2019fda",
      "ding2019mma",
      "das2017keeping"
    ],
    "relevant_ids": [
      "cao2021invisible",
      "kurakin2016adversarial",
      "gowal2018effectiveness",
      "cohen2019certified",
      "levine2021improved",
      "szegedy2013intriguing",
      "goodfellow2014explaining",
      "wong2018provable",
      "chen2019towards",
      "salman2019provably",
      "zhai2020macer",
      "mohapatra2020higher",
      "madry2017towards",
      "raghunathan2018certified",
      "biggio2013evasion",
      "tjeng2017evaluating",
      "montasser2019vc",
      "carlini2017adversarial",
      "athalye2018robustness"
    ],
    "metrics": {
      "R@5": 0.0,
      "R@10": 0.0,
      "R@20": 0.0,
      "MRR": 0.038461538461538464,
      "hits": 1,
      "total_relevant": 19
    },
    "score": 0.011538461538461539,
    "timestamp": "2025-12-18T10:54:08.676930"
  },
  {
    "query": "Introduction \\label{sec:intro} The pursuit of ideal layout design, whether for user interfaces or graphical elements, is fundamentally iterative. Designers meticulously refine their creations through cycles of revision, gradually converging towards a solution that balances aesthetics and functionality. While recent advances in multimodal generative models have enabled automated design generation from",
    "paper_id": "2406.18559",
    "retrieved_ids": [
      "gupta2021layouttransformer",
      "xie2024leveraging",
      "kikuchi2021constrained",
      "guide_paper",
      "lin2023parse",
      "inoue2023layoutdm",
      "lin2024layoutprompter",
      "cheng2023play",
      "Feng2023LayoutGPTCV",
      "vonr\u00fctte2023fabric",
      "huang2024creativesynth",
      "liu2022design",
      "vtn",
      "yue2023mmmu",
      "blt",
      "fu2022shapecrafter",
      "chen2023trainingfreelayout",
      "gani2023llm",
      "zhao2023loco",
      "dahary2024yourself",
      "he2024dresscode",
      "kynk\u00e4\u00e4nniemi2019improved",
      "hertz2023style",
      "tang2023dreamgaussian",
      "hall2023dig",
      "hao2023optimizing",
      "makatura2023large",
      "liu2024glyph",
      "sun2024spatial",
      "xie2024hierarchical",
      "mikaeili2023sked",
      "sun2023dreamsync",
      "liu2023cones2",
      "Grid2Im",
      "sun20233dgpt",
      "gong2023talecrafter",
      "avrahami2023chosen",
      "jimenez2023mixture",
      "mao2023guided",
      "yu2023language"
    ],
    "relevant_ids": [
      "jain2022vectorfusion",
      "nair2022r3m",
      "stiennon2022learning",
      "team2023gemini",
      "adeniji2023language",
      "lee2021pebble",
      "heusel2017gans",
      "lee2023aligning",
      "blt",
      "inoue2023layoutdm",
      "chen2022pali",
      "cheng2023play",
      "hejna2022fewshot",
      "xie2024leveraging",
      "lin2024layoutprompter",
      "black2023training",
      "lin2023parse",
      "ouyang2022training",
      "fan2023dpok",
      "liu2023chain",
      "kikuchi2021constrained",
      "rafailov2023direct",
      "transformer",
      "casper2023open",
      "gupta2021layouttransformer",
      "xu2023imagereward",
      "vtn",
      "rombach2022highresolution",
      "bai2022training"
    ],
    "metrics": {
      "R@5": 0.13793103448275862,
      "R@10": 0.2413793103448276,
      "R@20": 0.3103448275862069,
      "MRR": 1.0,
      "hits": 9,
      "total_relevant": 29
    },
    "score": 0.4275862068965517,
    "timestamp": "2025-12-18T10:54:10.544099"
  },
  {
    "query": "Introduction Text-to-image diffusion models<|cite_0|> generate high quality realistic images, allowing users to specify the desired results using simple textual inputs. Furthermore, some variants receive images whose elements will appear in the results. Editing methods<|cite_1|> make partial changes in a scene while keeping the rest unchanged. ControlNet variants<|cite_2|> produce images with",
    "paper_id": "2406.07008",
    "retrieved_ids": [
      "kawar2023imagic",
      "nichol2021glide",
      "uni",
      "zhang2022sine",
      "zhang2023text",
      "jimenez2023mixture",
      "jia2023taming",
      "chen2023videocrafter1",
      "mo2023freecontrol",
      "avrahami2023chosen",
      "TextDiffuser",
      "farshad2023scenegenie",
      "diffeditor",
      "chen2024diffute",
      "tumanyan2023plug",
      "ho2022imagen",
      "brack2023leditspp",
      "po2023state",
      "orgad2023editing",
      "li2023layerdiffusion",
      "mokady2022nti",
      "DiffSTE",
      "mao2023guided",
      "self_guidance",
      "gu2022vector",
      "Avrahami_2022_CVPR",
      "2312.04524",
      "gafni2022make",
      "voynov2023p+",
      "kim2023reference",
      "purushwalkam2024bootpig",
      "sheynin2022knn",
      "wu2023selfcorrecting",
      "repaintnerf",
      "ceylan2023pix2video",
      "simsar2023lime",
      "liu2023more",
      "patashnik2023localizing",
      "brack2023sega",
      "dragon",
      "edit",
      "bar2023multidiffusion",
      "yu2023long",
      "tokenflow2023",
      "zhu2016generative",
      "wang2018high",
      "bahmani20234d",
      "parmar2023zero",
      "bhat2023loosecontrol",
      "chen2023trainingfreelayout"
    ],
    "relevant_ids": [
      "cross_image",
      "self_guidance",
      "chen2023anydoor",
      "lu2023tf",
      "stable",
      "diffuseit",
      "patashnik2023localizing",
      "hedlin2024unsupervised",
      "zhang2024tale",
      "dift",
      "diffedit",
      "parmar2023zero",
      "tumanyan2023plug",
      "paint",
      "freedom",
      "amir2021deep",
      "gu2024photoswap",
      "spliceVIT",
      "blended",
      "lee2024conditional",
      "masactrl",
      "swap_ae",
      "du2020energy",
      "diffeditor",
      "luo2024diffusion",
      "controlnet",
      "dragon",
      "hertz2022prompt"
    ],
    "metrics": {
      "R@5": 0.0,
      "R@10": 0.0,
      "R@20": 0.07142857142857142,
      "MRR": 0.07692307692307693,
      "hits": 6,
      "total_relevant": 28
    },
    "score": 0.023076923076923078,
    "timestamp": "2025-12-18T10:54:12.958261"
  },
  {
    "query": "Introduction Training modern Large Language Models (LLMs) with billions of parameters requires thousands of GPUs running in parallel. This is necessary to load the model and optimizer parameters in memory and reach the mini-batch size in the millions of tokens used to train them<|cite_0|>, relying on a distributed version of",
    "paper_id": "2406.02613",
    "retrieved_ids": [
      "minaee2024large",
      "kaddour2023challenges",
      "jordan_chinchilla_2022",
      "Zhao2023ASO",
      "survey2",
      "shoeybi2020megatronlm",
      "zheng2020distdgl",
      "naveed2024comprehensivellms",
      "sriram2022towards",
      "qin2024large",
      "mmllms1",
      "levy2024same",
      "shnitzer2023large",
      "llmsurvey3",
      "Zero1",
      "makatura2023large",
      "miao2023towards",
      "Hu:2022:LLR",
      "du2022glamefficientscalinglanguage",
      "ref:med1",
      "jiang2023llmlingua",
      "pang2024anchor",
      "xiao2024smoothquant",
      "Zerooffload",
      "kim2023memoryefficient",
      "malladi2023fine",
      "chen2023accelerating",
      "hu2024minicpm",
      "mehta2024openelm",
      "huang2019gpipe",
      "xia2023sheared",
      "meta-mm-scalinglaw",
      "narayanan2021memory",
      "svirschevski2024specexec",
      "ioffe2015batch",
      "rajbhandari2021zero",
      "britz2017massive",
      "Zhang2022OPTOP",
      "owq",
      "gshard",
      "zhang2024recurrentdrafterfastspeculative",
      "namburi2023cost",
      "shazeer2018mesh",
      "liu2019roberta",
      "chen23l_interspeech",
      "fu2024break",
      "jamba",
      "stich2018local",
      "shayer2017learning",
      "pham2021combined",
      "zafrir2019q8bert"
    ],
    "relevant_ids": [
      "Dutta2021",
      "mcmahan17a",
      "Wang2020SlowMo",
      "ortiz2021tradeoffs",
      "COCOSGD2019",
      "sun2024co",
      "OverlapSGD2020",
      "Zero1",
      "mishchenko2022asynchronous",
      "loshchilov2018decoupled",
      "touvron2023llama",
      "maranjyan2022gradskip",
      "KingBa15",
      "PytorchDDP2020",
      "EASGD2015",
      "stich2018local",
      "Konecn2016FederatedOD",
      "diskin2021distributed",
      "Zerooffload"
    ],
    "metrics": {
      "R@5": 0.0,
      "R@10": 0.0,
      "R@20": 0.05263157894736842,
      "MRR": 0.06666666666666667,
      "hits": 3,
      "total_relevant": 19
    },
    "score": 0.02,
    "timestamp": "2025-12-18T10:54:15.358615"
  },
  {
    "query": "Introduction \\label{introduction} \\renewcommand{\\dblfloatpagefraction}{.9} \\begin{figure*}[htbp] \\vspace{-3mm} \\centering \\includegraphics[width=0.30\\linewidth]{figures/law/all-NonEmb.pdf} \\includegraphics[width=0.30\\linewidth]{figures/law/all-NC-NonEmb.pdf} \\includegraphics[width=0.30\\linewidth]{figures/law/all-DC.pdf} \\includegraphics[width=0.30\\linewidth]{figures/law/llama-PFLOPS-NonEmb.pdf} \\includegraphics[width=0.30\\linewidth]{figures/law/llama-NC-NonEmb.pdf} \\includegraphics[width=0.30\\linewidth]{figures/law/llama-DC.pdf} \\includegraphics[width=0.30\\linewidth]{figures/law/tnl-PFLOPS-NonEmb.pdf} \\includegraphics[width=0.30\\linewidth]{figures/law/tnl-NC-NonEmb.pdf} \\includegraphics[width=0.30\\linewidth]{figures/law/tnl-DC.pdf} \\includegraphics[width=0.30\\linewidth]{figures/law/hgrn-PFLOPS-NonEmb.pdf} \\includegraphics[width=0.30\\linewidth]{figures/law/hgrn-NC-NonEmb.pdf} \\includegraphics[width=0.30\\linewidth]{figures/law/hgrn-DC.pdf} \\includegraphics[width=0.30\\linewidth]{figures/law/cosf-PFLOPS-NonEmb.pdf} \\includegraphics[width=0.30\\linewidth]{figures/law/cosf-NC-NonEmb.pdf} \\includegraphics[width=0.30\\linewidth]{figures/law/cosf-DC.pdf} \\vspace{-4mm} \\caption{\\textbf{Training Curve Fitting for Four Architectures.} In the master row, we present predicted training curves for various architectures, with each subsequent row representing a different architecture. On the",
    "paper_id": "2406.16690",
    "retrieved_ids": [
      "LeeS17",
      "wu2020iou",
      "yan2023multi",
      "hutter2021learning",
      "Henighan_scaling_2020",
      "liu2024neural",
      "DurfeeKPRS17",
      "YL1",
      "murphy2021implicit",
      "meta-mm-scalinglaw",
      "kaplan2020scaling",
      "2020arXiv200108361K",
      "cherti2023reproducible",
      "yang2023tensor",
      "voleti2022mcvd",
      "bahri2021explaining",
      "ye2024data",
      "Geiger2020",
      "zhang2020understanding",
      "voynov2023p+",
      "lian2023llmgrounded",
      "chen2024textto3dgsgen",
      "Pascanu2014",
      "2020arXiv201014701H",
      "chen2023pixartalpha",
      "hanin2018approximating",
      "Mao:ICCV15",
      "vaze2022generalized",
      "clark_scaling_law_moe_icml_2022",
      "li2022efficient",
      "berthelot2019remixmatch",
      "KARIMI2020101759",
      "dong2024promptpromptedadaptivestructuredpruning",
      "sorscher2022beyond",
      "show1",
      "bhat2023loosecontrol",
      "meng2020pruning",
      "Mad+18",
      "nguyen2020wide",
      "bartlett2018gradient",
      "tnt",
      "wizardLM",
      "2021arXiv210201293H",
      "xiao2021tackling",
      "hokamp2017lexically",
      "huberman2023ddpminv",
      "ding2022APQViTtowards",
      "rosenfeld2021predictability",
      "fang2020densely",
      "jorge2022NFGSM",
      "gowal2021improving",
      "bansal2022how",
      "sharir2021image",
      "Wan+23",
      "su2020locally",
      "nitzan2023domain",
      "chen2010super",
      "dift",
      "tian2020makes"
    ],
    "relevant_ids": [
      "orvieto2023resurrecting",
      "gu2021efficiently",
      "qin2023hierarchically",
      "qin2023toeplitz",
      "qin2024you",
      "jordan_chinchilla_2022",
      "vaswani_transformer_2017",
      "jared_scaling_law_openai_2020",
      "liu2022neural",
      "zhen2022cosformer",
      "hua2022transformer",
      "gu2022parameterization",
      "qin2023linearized",
      "mamba",
      "zheng2022linear",
      "qin2024hgrn2",
      "choromanski2021rethinking",
      "fu2022hungry",
      "2307.08621",
      "clark_scaling_law_moe_icml_2022",
      "qin2023transnormerllm",
      "katharopoulos2020transformers",
      "fu2023simple",
      "gpt4",
      "llama2_2023",
      "zheng2023efficient",
      "gu2020hippo",
      "yang2023gated",
      "Henighan_scaling_2020",
      "qin2024lightning",
      "qin2024various"
    ],
    "metrics": {
      "R@5": 0.03225806451612903,
      "R@10": 0.03225806451612903,
      "R@20": 0.03225806451612903,
      "MRR": 0.2,
      "hits": 2,
      "total_relevant": 31
    },
    "score": 0.08258064516129032,
    "timestamp": "2025-12-18T10:54:26.340262"
  },
  {
    "query": "Introduction \\label{intro} We consider the problem of customizing the outputs of text-to-image diffusion models using concepts depicted in user-supplied reference images with a particular focus on improving the quality of alignment between input text prompts and the images generated using such customized models. Building on top of the dramatic progress",
    "paper_id": "2406.18893",
    "retrieved_ids": [
      "kumari2022multi",
      "hao2023optimizing",
      "witteveen2022investigating",
      "chen2023photoverse",
      "ref9",
      "materzynska2023customizingmotion",
      "gu2024mix",
      "TextDiffuser",
      "customizeavideo",
      "balaji2022ediff",
      "zhang2023text",
      "vonr\u00fctte2023fabric",
      "ye2023ip-adapter",
      "wei2023elite",
      "lee2023holistic",
      "zhong2023adapter",
      "dreambooth",
      "zhao2023catversion",
      "kim2023towards",
      "guo2024initno",
      "patashnik2023localizing",
      "DiffSTE",
      "vmc",
      "avrahami2023chosen",
      "po2023state",
      "farshad2023scenegenie",
      "brack2023sega",
      "gandikota2024unified",
      "basu2023inspecting",
      "couairon2023zerolayout",
      "purushwalkam2024bootpig",
      "simsar2023lime",
      "mao2023guided",
      "sun2023dreamsync",
      "avrahami2023break",
      "sld",
      "li2024self",
      "chen2024diffute",
      "bar2023multidiffusion",
      "huang2024creativesynth",
      "li2024tuning",
      "mokady2022nti",
      "freecustom",
      "dong2022dreamartist",
      "seshadri2023bias",
      "tiue",
      "hertz2023style",
      "jimenez2023mixture",
      "liu2024glyph",
      "chen2023livephoto",
      "du2023demofusion",
      "gandikota2023erasing",
      "jeong2024visual"
    ],
    "relevant_ids": [
      "sohl2015deep",
      "gal2022image",
      "rassin2023linguistic",
      "han2023highly",
      "agarwal2023image",
      "zhu2020improved",
      "feng2022training",
      "rombach2022high",
      "tov2021designing",
      "agarwal2023star",
      "gal2023encoder",
      "song2020denoising",
      "tewel2023key",
      "wu2023harnessing",
      "chefer2023attend",
      "zhao2023catversion",
      "han2023svdiff",
      "kumari2023multi",
      "meng2022locating",
      "ho2020denoising",
      "phung2023grounded"
    ],
    "metrics": {
      "R@5": 0.0,
      "R@10": 0.0,
      "R@20": 0.047619047619047616,
      "MRR": 0.05555555555555555,
      "hits": 1,
      "total_relevant": 21
    },
    "score": 0.016666666666666666,
    "timestamp": "2025-12-18T10:54:28.983487"
  },
  {
    "query": "Introduction \\label{sec:introduction} The widespread adoption of machine learning in critical sectors such as healthcare, finance, and autonomous systems has highlighted the necessity for models that are both adaptable and secure. Traditional machine learning models are designed to continuously learn and retain information. Machine unlearning, as introduced in the literature<|cite_0|>, addresses",
    "paper_id": "2406.16986",
    "retrieved_ids": [
      "b2",
      "bourtoule2021machine",
      "b3",
      "liu2024rethinking",
      "Papernot2018",
      "b9",
      "song2017machine",
      "b13",
      "b30",
      "b1",
      "b19",
      "farahani2021brief",
      "b10",
      "overview",
      "wang2023prompt",
      "chen2020fedhealth",
      "biggio_2018",
      "yang2019federated",
      "d2020underspecification",
      "permuteattack",
      "Big+13",
      "yang2023introduction",
      "laousy2023certification",
      "b26",
      "tan2022towards",
      "cheplygina2019not",
      "liang2023comprehensive",
      "hendrycks2021unsolved",
      "li2017deep",
      "b18",
      "SplitFederatedGupta",
      "chen2022semi",
      "grativol2023federated",
      "zhao2018federated",
      "lim2020federated",
      "bulusu2020anomalous",
      "yang2023looped",
      "b22",
      "hein2017formal",
      "cao2019adversarial",
      "papernot2016crafting",
      "fawaz2019adversarial",
      "kumari2023trust",
      "PapernotM17"
    ],
    "relevant_ids": [
      "b2",
      "b9",
      "b31",
      "b12",
      "b18",
      "b10",
      "b30",
      "b1",
      "b29",
      "b22",
      "b26",
      "b13",
      "b27",
      "b19",
      "b32",
      "b3"
    ],
    "metrics": {
      "R@5": 0.125,
      "R@10": 0.375,
      "R@20": 0.5,
      "MRR": 1.0,
      "hits": 11,
      "total_relevant": 16
    },
    "score": 0.46249999999999997,
    "timestamp": "2025-12-18T10:54:31.151885"
  },
  {
    "query": "Introduction The advent of Large Language Models (LLMs) has brought about a significant transformation in artificial intelligence and natural language processing, enabling significant advancements in various tasks such as text generation, translation, and question-answering<|cite_0|>. With growing size and complexity, these models have evolved into highly effective standalone knowledge repositories, embedding",
    "paper_id": "2406.09979",
    "retrieved_ids": [
      "llmsurvey2",
      "alkhamissi2022review",
      "naveed2024comprehensivellms",
      "2020arXiv200301200T",
      "minaee2024large",
      "Zhao2023ASO",
      "gpt_summary",
      "kaddour2023challenges",
      "Zhu2023LargeLM",
      "xie2023translating",
      "mmllms1",
      "jiang2023scaling",
      "ref:ragsurvey3",
      "ref:edu1",
      "guo2024large",
      "wang2024large",
      "zhang2024comprehensive",
      "llmsurvey3",
      "miao2023towards",
      "ref:med1",
      "qin2024large",
      "ref:ragsurvey2",
      "ref:ragsurvey1",
      "wu2022autoformalization",
      "wang2023surveyfactualitylargelanguage",
      "ref:ragsurvey7",
      "zhang2023FinEvalChineseFinancial",
      "makatura2023large",
      "ref:med3",
      "Gao2023RetrievalAugmentedGF",
      "veagle_paper",
      "nambi2023breaking",
      "model_compression",
      "Pan2023AutomaticallyCL",
      "ahuja2023mega",
      "survey1",
      "hu2023",
      "ahuja-etal-2023-mega",
      "wu2023speechgen",
      "mordatch2017emergence",
      "ji2023survey",
      "bai2023qwen",
      "li2024devbench",
      "xraygpt",
      "bevilacqua2022autoregressive",
      "wang2023prompt"
    ],
    "relevant_ids": [
      "petroni2019language",
      "wang2024detect",
      "minaee2024large",
      "liu2024lost",
      "shuster2021retrieval",
      "lewis2020retrieval",
      "sarthi2024raptor",
      "he2024retriever",
      "kanagavalli2016study",
      "ram2023context",
      "bubeck2023sparks",
      "sun2021long",
      "levy2024same",
      "akyurek2022towards"
    ],
    "metrics": {
      "R@5": 0.07142857142857142,
      "R@10": 0.07142857142857142,
      "R@20": 0.07142857142857142,
      "MRR": 0.2,
      "hits": 1,
      "total_relevant": 14
    },
    "score": 0.10999999999999999,
    "timestamp": "2025-12-18T10:54:33.289400"
  },
  {
    "query": "Introduction Though large language models (LLMs) have delivered impressive results in a variety of natural language processing (NLP) tasks, their massive size often complicates deployment. One common method to compress LLMs is through the quantization of weight parameters, which reduces model sizes by lowering the precision of weight values<|cite_0|>. Existing",
    "paper_id": "2406.12311",
    "retrieved_ids": [
      "model_compression",
      "qin2024large",
      "minaee2024large",
      "llm_quant",
      "kaddour2023challenges",
      "jaiswal2024compressing",
      "jin2024comprehensive",
      "naveed2024comprehensivellms",
      "treviso2023efficient",
      "kim2023memoryefficient",
      "ganesh2020compressing",
      "xu2023qa",
      "quip",
      "hongdecoding",
      "survey1",
      "owq",
      "lin2024awq",
      "gpt_summary",
      "liu2023llm",
      "jiang2023llmlingua",
      "shao2023omniquant",
      "onebit",
      "pbllm",
      "billm",
      "zhang2023integer",
      "wang2019structured",
      "miao2023towards",
      "lu2024",
      "namburi2023cost",
      "pan2023smoothquant+",
      "du2022glamefficientscalinglanguage",
      "sqzllm",
      "ref:ragsurvey2",
      "fan2020training",
      "sharify2024combining",
      "pang2024anchor",
      "llmsurvey3",
      "defossez2021differentiable",
      "zhang2023benchmarking",
      "fusco2022pnlp",
      "zafrir2019q8bert",
      "Wu2018MixedPQ",
      "frantar2023gptq",
      "yu2024melo"
    ],
    "relevant_ids": [
      "xnor",
      "sqzllm",
      "gptq",
      "binary_survey",
      "owq",
      "stmoe",
      "model_compression",
      "billm",
      "pbllm",
      "quip",
      "bitnet",
      "switch_trans",
      "awq",
      "qlora",
      "onebit",
      "moe",
      "bireal",
      "llm_quant",
      "react"
    ],
    "metrics": {
      "R@5": 0.10526315789473684,
      "R@10": 0.10526315789473684,
      "R@20": 0.21052631578947367,
      "MRR": 1.0,
      "hits": 8,
      "total_relevant": 19
    },
    "score": 0.3736842105263158,
    "timestamp": "2025-12-18T10:54:35.375613"
  },
  {
    "query": "Introduction In recent years, diffusion models<|cite_0|> have demonstrated impressive capabilities to create high-quality visual content, capturing the interest and enthusiasm of both the general public and the academic community. Recently, diffusion models have brought forward milestone improvement for video synthesis<|cite_1|>. Most text-to-video (T2V) models usually leverage successful pre-trained text-to-image (T2I)<|cite_2|>",
    "paper_id": "2406.03215",
    "retrieved_ids": [
      "b8",
      "ho2022video",
      "esser2023structure",
      "ho2022imagen",
      "chen2023videocrafter1",
      "vivid",
      "b53",
      "po2023state",
      "videocrafter2",
      "wang2023lavie",
      "blattmann2023align",
      "luo2023videofusion",
      "xu2022versatile",
      "farshad2023scenegenie",
      "zhang2023text",
      "UdiffText",
      "liao2023text",
      "ge2023preserve",
      "he2022lvdm",
      "feng2022training",
      "chen2023pixartalpha",
      "wu2022tune",
      "vmc",
      "show1",
      "customizeavideo",
      "ceylan2023pix2video",
      "guo2023animatediff",
      "diffeditor",
      "molad2023dreamix",
      "b52",
      "wang2023modelscope",
      "khachatryan2023text2video",
      "2312.04524",
      "liang2023flowvid",
      "singer2022make",
      "ControlVideo",
      "svd",
      "liu2023improved",
      "wang2023genlvideo",
      "b51",
      "wu2023lamp"
    ],
    "relevant_ids": [
      "ddpm",
      "wang2019vatex",
      "customizeavideo",
      "multiconcept",
      "stylegan",
      "tuneavideo",
      "gan_tulyakov2018mocogan",
      "videocrafter2",
      "implicit_tian2021good",
      "vdm",
      "ceylan2023pix2video",
      "animageworthoneword",
      "zeroshot_offtheshelf",
      "dreambooth",
      "ramesh2022hierarchical",
      "saharia2022photorealistic",
      "ldm",
      "bartal2024lumiere",
      "renderavideo",
      "gu2022vector",
      "webvid",
      "show1",
      "gan_tian2021good",
      "vmc",
      "peng2024smoothvideo",
      "diffusionbeatgan",
      "makeavideo",
      "zhao2023motiondirector",
      "imagenvideo",
      "liang2023flowvid",
      "dreamvideo",
      "gan_saito2017temporal",
      "auto_srivastava2015unsupervised",
      "svd",
      "hu2021lora",
      "neuralvideofieldsediting",
      "tokenflow",
      "zhang2023motioncrafter",
      "gan_vondrick2016generating",
      "customizingmotion",
      "structure_content",
      "materzynska2023customizingmotion",
      "meng2021sdedit",
      "videoldm",
      "t2vzero",
      "yan2021videogpt",
      "gan"
    ],
    "metrics": {
      "R@5": 0.0,
      "R@10": 0.02127659574468085,
      "R@20": 0.02127659574468085,
      "MRR": 0.1111111111111111,
      "hits": 7,
      "total_relevant": 47
    },
    "score": 0.03971631205673759,
    "timestamp": "2025-12-18T10:54:37.759829"
  },
  {
    "query": "Introduction \\label{sec:intro} Recently, we witnessed a breakthrough in generative 3D content creation and painting<|cite_0|>, empowering content creators and 3D artists to recycle old 3D assets or prototype texture design ideas using simple text prompts. These methods either involve end-to-end optimization with the diffusion model as guidance through score distillation sampling",
    "paper_id": "2406.11202",
    "retrieved_ids": [
      "wang2023breathing",
      "richardson2023texture",
      "cao2023texfusion",
      "cao2022survey",
      "huang2023dreamtime",
      "yu2023text",
      "zhu2023hifa",
      "chen2023text2tex",
      "esser2023structure",
      "ho2022video",
      "zhang2023text",
      "wang2024prolificdreamer",
      "lin2022magic3d",
      "tsalicoglou2023textmesh",
      "tang2023dreamgaussian",
      "chen2023fantasia3d",
      "metzer2022latentnerf",
      "tang2023make",
      "liu2023unidream",
      "3dtopia",
      "huang2022draw",
      "zhang2024clay",
      "kim2023reference",
      "yi2023gaussiandreamer",
      "farshad2023scenegenie",
      "liao2023text",
      "bd23",
      "liu2022design",
      "zero12345plus",
      "po2023compositional",
      "latentnerf",
      "he2024dresscode",
      "chen2024textto3dgsgen",
      "zhang2023text2nerf",
      "zhao2023efficientdreamer",
      "li2024instant3d",
      "li2023focaldreamer",
      "bar2023multidiffusion",
      "im3d",
      "tang2024intex",
      "jimenez2023mixture",
      "ceylan2024matatlas",
      "dreamcraft3d",
      "hertz2023style",
      "du2023demofusion"
    ],
    "relevant_ids": [
      "zhang2023diffcollage",
      "cao2023texfusion",
      "chen2023text2tex",
      "poole2022dreamfusion",
      "song2023consistency",
      "mildenhall2021nerf",
      "luo2023latent",
      "gao2022get3d",
      "rombach2022high",
      "zhang2023adding",
      "kerbl20233d",
      "qian2023magic123",
      "richardson2023texture",
      "song2020denoising",
      "wang2023generativepowers",
      "song2020score",
      "tang2023dreamgaussian",
      "wang2023breathing",
      "decatur20233d",
      "bar2023multidiffusion",
      "lee2023syncdiffusion",
      "lu2022dpm",
      "podell2023sdxl",
      "metzer2023latent",
      "ho2020denoising",
      "chen2023fantasia3d",
      "wang2024prolificdreamer",
      "tsalicoglou2023textmesh"
    ],
    "metrics": {
      "R@5": 0.10714285714285714,
      "R@10": 0.14285714285714285,
      "R@20": 0.2857142857142857,
      "MRR": 1.0,
      "hits": 9,
      "total_relevant": 28
    },
    "score": 0.3857142857142857,
    "timestamp": "2025-12-18T10:54:39.961263"
  },
  {
    "query": "Introduction Large Language Models (LLMs) have shown impressive reasoning and decision-making capabilities in complex tasks like mathematical reasoning and creative writing by processing and generating thoughts based on token-level predictions<|cite_0|>. However, this approach limits their ability to extend higher-level or multi-perspective reasoning<|cite_1|>. Research on LLM reasoning indicates that implementing structured",
    "paper_id": "2406.02746",
    "retrieved_ids": [
      "huang2022large",
      "he2024can",
      "naveed2024comprehensivellms",
      "laskar2024systematic",
      "kaddour2023challenges",
      "levy2024same",
      "fu2023complexitybased",
      "zhang2023cumulative",
      "zhang2023multimodal",
      "yao2024tree",
      "yao2023react",
      "besta2024graph",
      "xu2023rewoo",
      "huang2022inner",
      "zhou2023navgpt",
      "wang2024large",
      "guo2024large",
      "fu23cotkd",
      "wang2023boosting",
      "ref:edu1",
      "wei2022chain",
      "ranaldi2023empowering",
      "yasunaga2023analogical",
      "lu2023chameleon",
      "Huang2023LargeLM",
      "xiao2023conditions",
      "ref:med3",
      "huang2023languages",
      "ref:financialgpt",
      "llmsurvey3",
      "anilexploring",
      "hao2023reasoning",
      "shi2022language",
      "lanchantin2023selfnote",
      "gao2023pal",
      "rae2021scaling",
      "model_compression",
      "xie2023translating",
      "wang2023knowledge",
      "lin2023swiftsage",
      "tang2023large",
      "wang2023large",
      "wang-etal-2022-iteratively",
      "makatura2023large",
      "fujisawa2022logical",
      "paul2024making",
      "liu2023agentbench",
      "wang-etal-2023-towards",
      "wang2024rat",
      "saparov2022language",
      "ravi2024small"
    ],
    "relevant_ids": [
      "yang2024leandojo",
      "wang2022self",
      "minaee2024large",
      "wang2024rat",
      "huang2022language",
      "huang2022inner",
      "chen2024masked",
      "wei2022chain",
      "ding2023everything",
      "zhang2022automatic",
      "lewis2020retrieval",
      "zhang2023planning",
      "shuster2021retrieval",
      "yao2024tree",
      "zhang2024instruct",
      "yu2023towards",
      "feng2024towards"
    ],
    "metrics": {
      "R@5": 0.0,
      "R@10": 0.058823529411764705,
      "R@20": 0.11764705882352941,
      "MRR": 0.1,
      "hits": 4,
      "total_relevant": 17
    },
    "score": 0.04764705882352941,
    "timestamp": "2025-12-18T10:54:42.605680"
  },
  {
    "query": "Introduction Very large language models (LLMs) such as GPT-3.5-Turbo \\& GPT-4<|cite_0|> show exceptional performance on a variety of NLP and reasoning tasks via \\textit{In-Context Learning} (ICL)<|cite_1|>. ICL feeds a task-specific instruction along with a few exemplars, appended with the test input, to the LLM. As LLMs can be highly sensitive",
    "paper_id": "2406.18880",
    "retrieved_ids": [
      "Zhao2023ASO",
      "wei2023larger",
      "huang2022large",
      "naveed2024comprehensivellms",
      "qin2024large",
      "minaee2024large",
      "survey1",
      "zhu2023multilingual",
      "xu2023retrieval",
      "hendel2023incontext",
      "khattab2023demonstratesearchpredict",
      "wang2023large",
      "conifer",
      "chia2024instructeval",
      "pan-etal-2023-context",
      "wizardLM",
      "wang2023label",
      "chen2023frugalgpt",
      "lawbench",
      "zhou2023navgpt",
      "li2024-Longcontextllmsstrugglelong",
      "tang2023struc",
      "Liu2023LLMRecBL",
      "lu2023chameleon",
      "dai2023can",
      "xie2023translating",
      "wang2024large",
      "WizardMath",
      "brown2020language",
      "honovich2022instruction",
      "makatura2023large",
      "nan2023enhancing",
      "chiang2023can",
      "2020arXiv200514165B",
      "chen-etal-2023-many",
      "chowdhery2022palm",
      "zheng2023can",
      "zhang2021differentiable",
      "tanwar2023multilingual",
      "hong20233dllm",
      "tanwar-etal-2023-multilingual",
      "pourreza2023din",
      "genegpt",
      "bueno2022induced",
      "llmsurvey3",
      "fu2023complexitybased",
      "zhang2024raft",
      "kojima2022large",
      "zhang-infinite-bench-2024",
      "saparov2022language",
      "wang2022super",
      "wan-etal-2023-universal"
    ],
    "relevant_ids": [
      "ouyang-etal-2022-impact",
      "achiam2023gpt",
      "muller2021being",
      "zhang2021differentiable",
      "Devlin2019BERTPO",
      "ahuja2023mega",
      "asai2023buffet",
      "winata2021language",
      "nllb2022",
      "brown2020language",
      "nambi2023breaking",
      "chen2023frustratingly",
      "conneau2020unsupervised",
      "ustun2020udapter",
      "alabi2022adapting",
      "rathore2023zgul",
      "Agrawal2022IncontextES",
      "garcia2023t",
      "wan-etal-2023-universal",
      "chowdhery2022palm",
      "reimers2020making",
      "pfeiffer2020mad",
      "zhao2021calibrate",
      "wan-etal-2023-better",
      "le2024constrained"
    ],
    "metrics": {
      "R@5": 0.0,
      "R@10": 0.0,
      "R@20": 0.0,
      "MRR": 0.034482758620689655,
      "hits": 4,
      "total_relevant": 25
    },
    "score": 0.010344827586206896,
    "timestamp": "2025-12-18T10:54:45.477330"
  },
  {
    "query": "Introduction The preservation of data privacy has long been a goal in the development of approaches to train machine learning models. In traditional machine learning, raw data from embedded systems are sent over a network to a powerful server for model training, raising concerns about confidentiality. Federated Learning (FL) has",
    "paper_id": "2406.14082",
    "retrieved_ids": [
      "Papernot2018",
      "li2020federated",
      "yang2019federated",
      "wuf2022edcg",
      "kairouz2021advances",
      "liu2020privacy",
      "nasr2019comprehensive",
      "arivazhagan2019federated",
      "sattler2019clustered",
      "lim2020federated",
      "SZHBFB19",
      "jayaraman_evaluating_2019",
      "tan2022towards",
      "qiu2022zerofl",
      "abadi2016deep",
      "jeong2018communication",
      "liang2020think",
      "kulkarni2020survey",
      "salem2018ml",
      "thapa2020splitfed",
      "jiang2022model",
      "nishio2019client",
      "li2019fedmd",
      "ramaswamy2020training",
      "xiong2022feddm",
      "perazzone2022communication",
      "luo2019real",
      "chen2018federated",
      "grativol2023federated",
      "fedsam-icml",
      "Fedlearning_edge_1",
      "SSSS17",
      "yu2019differentially",
      "SplitFederatedGupta",
      "chen2020wireless",
      "yu2020salvaging",
      "koda2020differentially",
      "lin2020ensemble",
      "zhu2021data",
      "liu2020accelerating",
      "b27"
    ],
    "relevant_ids": [
      "jiang2022model",
      "qiu2022zerofl",
      "bai2023unified",
      "wu2018training",
      "grativol2023federated",
      "mcmahan2017communication",
      "reisizadeh2020fedpaq",
      "hyeon2021fedpara",
      "babakniya2023slora",
      "cho2023heterogeneous",
      "cheng2017survey",
      "hu2021lora",
      "kairouz2021advances",
      "tessier2022rethinking",
      "tan2022towards",
      "bensaid2024novel"
    ],
    "metrics": {
      "R@5": 0.0625,
      "R@10": 0.0625,
      "R@20": 0.1875,
      "MRR": 0.2,
      "hits": 5,
      "total_relevant": 16
    },
    "score": 0.10375,
    "timestamp": "2025-12-18T10:54:47.216976"
  },
  {
    "query": "Introduction \\begin{figure}[ht] \\centering \\begin{minipage}[t]{0.49\\textwidth} \\centering \\includegraphics[width= \\textwidth]{images/main_figure/Umass_Fig1_v3.pdf} \\end{minipage} \\caption{We introduce \\methodname, which decomposes measurements of geographic disparities in text-to-image generation between \\textit{object} and \\textit{background} representations. Using \\methodname, we identify generation patterns that contribute to geographic disparities.} \\label{fig:SummaryFig1} \\end{figure} Recent advancements in text-to-image generative systems have driven immense progress both for",
    "paper_id": "2406.11988",
    "retrieved_ids": [
      "ControlGAN",
      "gligen",
      "hall2023dig",
      "hall2024geographic",
      "yu2022scaling",
      "gafni2022make",
      "huang2023t2icompbench",
      "ghosh2023geneval",
      "basu2023inspecting",
      "lakhanpal2024refining",
      "liu2023unidream",
      "sohn2023styledrop",
      "lee2024parrot",
      "lee2023holistic",
      "bianchi2023easily",
      "suti",
      "gani2023llm",
      "wang2024prolificdreamer",
      "li2024blip",
      "liu2022design",
      "voynov2023p+",
      "li2022upainting",
      "zhang2023text",
      "zhang2023text2nerf",
      "avrahami2023spatext",
      "yang2024mastering",
      "stylet2i",
      "zhang2018photographic",
      "chen2023fantasia3d",
      "ye2021improving",
      "zero12345plus",
      "diagonalgan",
      "liu2024glyph",
      "kang2023scaling",
      "ObjGAN",
      "frolov2021adversarial",
      "wu2023easyphoto",
      "dialog-gen",
      "TextDiffuser",
      "Hinz2019GeneratingMO",
      "xie2021fignerf",
      "conimgcap2",
      "lee2024compose",
      "attr_6_hierarchical",
      "webvid",
      "pointE",
      "chen2024textto3dgsgen",
      "yang2023introduction",
      "li2023unihuman",
      "yu2023text",
      "grounding2",
      "nabati2021centerfusion",
      "li2024llavamed",
      "lim2020federated",
      "xu2021e2e"
    ],
    "relevant_ids": [
      "tian2023stablerep",
      "lee2023holistic",
      "kirillov-etal-2023-segment",
      "ramesh2022hierarchical",
      "bansal2022how",
      "hemmat2023feedbackguided",
      "hall2024geographic",
      "basu2023inspecting",
      "yeo2024controlled",
      "hall2023dig",
      "naeem2020reliable",
      "kynk\u00e4\u00e4nniemi2019improved",
      "Rombach_2022_CVPR"
    ],
    "metrics": {
      "R@5": 0.15384615384615385,
      "R@10": 0.23076923076923078,
      "R@20": 0.3076923076923077,
      "MRR": 0.3333333333333333,
      "hits": 4,
      "total_relevant": 13
    },
    "score": 0.23076923076923078,
    "timestamp": "2025-12-18T10:54:52.050698"
  },
  {
    "query": "Introduction Recent advances in Large Language Models (LLMs) have revolutionized the field of natural language processing. These models have demonstrated remarkable capabilities in various tasks such as open-ended generation, question answering, and mathematical reasoning<|cite_0|>. However, despite their impressive performance, LLMs occasionally generate content that can be untruthful or harmful. This",
    "paper_id": "2406.07168",
    "retrieved_ids": [
      "Zhao2023ASO",
      "kaddour2023challenges",
      "naveed2024comprehensivellms",
      "creswell2022faithful",
      "mmllms1",
      "hu2023",
      "felm",
      "critic",
      "Zhu2023LargeLM",
      "Huang2023LargeLM",
      "wang2023surveyfactualitylargelanguage",
      "tian2023fine",
      "yao2023survey",
      "ref:edu1",
      "WizardMath",
      "xie2023translating",
      "lyu2024probabilities",
      "qin2024large",
      "ref:med1",
      "llmsurvey3",
      "rarr",
      "yu2023metamath",
      "huang2023survey",
      "Pan2023AutomaticallyCL",
      "check",
      "LLM4DriveAS",
      "Gao2023RetrievalAugmentedGF",
      "huang2023languages",
      "ref:med3",
      "openfunction",
      "ref:financialgpt",
      "Lin2021TruthfulQAMH",
      "makatura2023large",
      "chiang2023can",
      "jiang2020can",
      "zhong2023mquake",
      "he2022rethinking",
      "chen2023universal",
      "wu2023speechgen",
      "zhang2023hallucination",
      "ahuja-etal-2023-mega",
      "manakul",
      "seenivasan2023surgicalgpt",
      "ahuja2023mega",
      "saparov2022language",
      "bai2023qwen"
    ],
    "relevant_ids": [
      "Jaques2019WayOB",
      "Perez2022RedTL",
      "Ivison2023CamelsIA",
      "openai2023gpt4",
      "geminiteam2023gemini",
      "Weidinger2021EthicalAS",
      "Christiano2017DeepRL",
      "Scheurer2023TrainingLM",
      "Scheurer2022TrainingLM",
      "Valmeekam2023CanLL",
      "Shinn2023ReflexionLA",
      "hu2024rankprompt",
      "Wang2023LearningFM",
      "Ouyang2022TrainingLM",
      "Lu2023SELFSW",
      "Shen2023LargeLM",
      "Chen2023ImprovingCG",
      "Welleck2022GeneratingSB",
      "Myers2021LearningMR",
      "Bubeck2023SparksOA",
      "Chowdhery2022PaLMSL",
      "Brown2020LanguageMA",
      "Stiennon2020LearningTS",
      "casper2023open",
      "Rafailov2023DirectPO",
      "Yasunaga2020GraphbasedSP",
      "Pan2023AutomaticallyCL",
      "Chen2023TeachingLL",
      "Huang2023LargeLM",
      "Bai2022ConstitutionalAH",
      "Roit2023FactuallyCS",
      "Yu2023TeachingLM",
      "Madaan2023SelfRefineIR"
    ],
    "metrics": {
      "R@5": 0.0,
      "R@10": 0.030303030303030304,
      "R@20": 0.030303030303030304,
      "MRR": 0.1,
      "hits": 2,
      "total_relevant": 33
    },
    "score": 0.03909090909090909,
    "timestamp": "2025-12-18T10:54:54.431058"
  },
  {
    "query": "Introduction While large language models (LLMs) demonstrate promising success in various domains, from natural language understanding to creative content generation, their broad applications raise safety concerns for their ability to generate misleading, offensive, or otherwise harmful content<|cite_0|>, impacting millions worldwide, spanning all languages and cultural contexts. Despite extensive research and",
    "paper_id": "2406.13748",
    "retrieved_ids": [
      "kaddour2023challenges",
      "Zhao2023ASO",
      "survey0",
      "shen2024language",
      "naveed2024comprehensivellms",
      "yao2023survey",
      "huang2023survey",
      "laskar2024systematic",
      "gpt_summary",
      "Weidinger2021EthicalAS",
      "ref:med2",
      "kumar2022language",
      "mmhal_survey",
      "lyu2024probabilities",
      "ref:med1",
      "Shen2023LargeLM",
      "ref:edu1",
      "exaggeratedsafety",
      "wang2023surveyfactualitylargelanguage",
      "rarr",
      "chao2023jailbreaking",
      "zhang2024comprehensive",
      "ref:financialgpt",
      "zhang-etal-2023-multilingual",
      "ref:med3",
      "liu2024survey",
      "llmsurvey3",
      "miao2023towards",
      "Huang2023LargeLM",
      "ref:ragsurvey1",
      "zhang2023hallucination",
      "chiang2023can",
      "Pan2023AutomaticallyCL",
      "nasir2023llmatic",
      "model_compression",
      "makatura2023large",
      "ref:ragsurvey3",
      "Perez2022RedTL",
      "bianchi2023easily",
      "ref:ragsurvey2",
      "gehman2020realtoxicityprompts",
      "agrawal-2023",
      "shallowalign",
      "shi2023red",
      "ficler-goldberg-2017-controlling",
      "holtzman2019curious",
      "mao2024raidar"
    ],
    "relevant_ids": [
      "wu2023evakellm",
      "wang2023knowledge",
      "wang2024crosslingual",
      "sainz2023nlp",
      "yao2023editing",
      "huang2023languages",
      "schuster2019crosslingual",
      "li2023bactrianx",
      "Yao_2024",
      "k2020crosslingual",
      "shen2024language",
      "huang2023survey",
      "gallegos2024bias",
      "mitchell2022memorybased",
      "lin2022fewshot",
      "qi2023finetuning",
      "kalyan2021ammus",
      "Kotek_2023",
      "eldan2023whos",
      "li2022pretrained",
      "ge2023mart",
      "meng2023locating",
      "hubinger2024sleeper",
      "wei2023jailbroken",
      "lu2022quark",
      "yang2022improving",
      "golchin2024time",
      "shen2024do"
    ],
    "metrics": {
      "R@5": 0.03571428571428571,
      "R@10": 0.07142857142857142,
      "R@20": 0.07142857142857142,
      "MRR": 0.25,
      "hits": 2,
      "total_relevant": 28
    },
    "score": 0.11071428571428571,
    "timestamp": "2025-12-18T10:54:56.528284"
  },
  {
    "query": "Introduction \\label{Introduction} \\begin{figure}[t] \\begin{center} \\centerline{\\includegraphics[width=\\columnwidth]{icml2024/figures/Introduction.pdf}} \\caption{Illustration of video quality deterioration represented into two distinct categories: (a) content blur and (b) content flicker. For the comparison, we present our results in (c).} \\label{introduction} \\end{center} \\vskip -0.3in \\end{figure} \\begin{figure}[h!] \\begin{center} \\centerline{\\includegraphics[width=\\columnwidth]{icml2024/figures/Introduction3.pdf}} \\caption{(a) Frequency magnitude evaluations of videos according to low and high",
    "paper_id": "2406.06044",
    "retrieved_ids": [
      "dover",
      "b8",
      "ouyang2023codef",
      "chen2023videocrafter1",
      "zhang2023avid",
      "stylegan",
      "miech2020end",
      "kay2017kinetics",
      "voleti2022mcvd",
      "Wang_Yu_Yu_Dai_Tsvetkov_Cao_2021",
      "b53",
      "Han2020ASO",
      "KARIMI2020101759",
      "cheng2021modular",
      "wang2021survey",
      "liu2022design",
      "ren2024consisti2v",
      "webvid",
      "sariyildiz2020learning",
      "b49",
      "kingma2019introduction",
      "show1",
      "kobyzev2020normalizing",
      "chang2020devil",
      "kandala2024pix2gif",
      "ma2024follow",
      "nguyen2020wide",
      "xie2023boxdiff",
      "xie2021fignerf",
      "chen2023pixartalpha",
      "flow1",
      "wu2023easyphoto",
      "kang2020decoupling",
      "yang2023introduction",
      "mao2023guided",
      "zhang2020dive",
      "santa2017deeppermnet",
      "sharir2021image",
      "zhao2023clip",
      "zhang2020understanding",
      "elharrouss2020image",
      "ye2021disentangling",
      "2020Learning",
      "article17",
      "zhao2019object",
      "kitaev2018constituency",
      "huang2024classdiffusion",
      "bhat2023loosecontrol",
      "meng2020pruning",
      "li2024llavamed",
      "lim2020federated",
      "yan2023multi",
      "lee2023syncdiffusion",
      "patchesallyouneed",
      "inpaint",
      "tian2020makes"
    ],
    "relevant_ids": [
      "chai2023stablevideo",
      "kim2022diffusionclip",
      "hong2022cogvideo",
      "ronneberger2015u",
      "geyer2023tokenflow",
      "ramesh2022hierarchical",
      "rombach2022high",
      "ho2022video",
      "zhang2023adding",
      "qi2023fatezero",
      "bar2022text2live",
      "wu2023tune",
      "song2020denoising",
      "song2020score",
      "liu2023video",
      "dhariwal2021diffusion",
      "molad2023dreamix",
      "cong2023flatten",
      "ho2020denoising",
      "khachatryan2023text2video",
      "hertz2022prompt"
    ],
    "metrics": {
      "R@5": 0.0,
      "R@10": 0.0,
      "R@20": 0.0,
      "MRR": 0.0,
      "hits": 0,
      "total_relevant": 21
    },
    "score": 0.0,
    "timestamp": "2025-12-18T10:55:01.647590"
  },
  {
    "query": "Introduction \\label{sec:intro} Text-to-image diffusion models have captivated the creative world with their extraordinary capacity to turn textual descriptions into detailed, high-resolution images. These advancements have ushered in a new era of creativity, allowing for the generation of bespoke illustrations for storybooks, dynamic characters in video games, personalized content across digital",
    "paper_id": "2406.02820",
    "retrieved_ids": [
      "gal2022textual",
      "gong2022diffuseq",
      "DiffuSeq2022",
      "lian2023llmgrounded",
      "chen2023photoverse",
      "zhang2023text",
      "jeong2023zeroshot",
      "TextDiffuser",
      "li2023snapfusion",
      "dreambooth",
      "vonr\u00fctte2023fabric",
      "chen2023text2tex",
      "guo2024initno",
      "tewel2023key",
      "lin2022magic3d",
      "jimenez2023mixture",
      "zhong2023adapter",
      "gani2023llm",
      "po2023state",
      "b8",
      "shan2023glazeprotectingartistsstyle",
      "wu2023harnessing",
      "han2023highly",
      "avrahami2023chosen",
      "huang2024creativesynth",
      "jain2022vectorfusion",
      "li2024blip",
      "UdiffText",
      "chen2023videocrafter1",
      "du2023demofusion",
      "farshad2023scenegenie",
      "esser2023structure",
      "cao2022survey",
      "blattmann2023align",
      "somepalli2023diffusion",
      "tumanyan2023plug",
      "huang2022draw",
      "brack2023sega",
      "basu2023inspecting",
      "wang2023lavie",
      "jiang2023text2performer",
      "composer",
      "ma2024directed",
      "gafni2019vid2game",
      "chen2023seine",
      "wang2023imagen",
      "li2024tuning",
      "hall2023dig",
      "hertz2023style",
      "li2023your",
      "zhang2024clay"
    ],
    "relevant_ids": [
      "gal2022textual",
      "rahman2023makeastory",
      "ruiz2022dreambooth",
      "saharia2022photorealistic",
      "avrahami2023chosen",
      "chen2020uniter",
      "rombach2022high",
      "ye2023ip-adapter",
      "Zhou2024storydiffusion",
      "hu2021lora",
      "jeong2023zeroshot",
      "kim2021vilt",
      "tewel2024trainingfree",
      "nichol2021glide",
      "gong2023talecrafter"
    ],
    "metrics": {
      "R@5": 0.06666666666666667,
      "R@10": 0.13333333333333333,
      "R@20": 0.13333333333333333,
      "MRR": 1.0,
      "hits": 3,
      "total_relevant": 15
    },
    "score": 0.36666666666666664,
    "timestamp": "2025-12-18T10:55:03.988660"
  },
  {
    "query": "Introduction \\label{sec:intro} 3D head generation aims at creating high-quality 3D digital assets of human heads that align with user preferences utilizing various input data, which has significant application in film character creation, gaming, online meetings, education, etc. Conventional methods rely on multi-view geometric consistency<|cite_0|> or statistical 3D face prior model<|cite_1|>",
    "paper_id": "2406.16710",
    "retrieved_ids": [
      "zhang2024clay",
      "or2022stylesdf",
      "huang2023humannorm",
      "im3d",
      "fu2022stylegan",
      "videomv",
      "shi2023mvdream",
      "imagedream",
      "liu2023headartist",
      "humansd",
      "cheng2023sdfusion",
      "zero12345plus",
      "zhang2023avatarverse",
      "lin2023consistent123",
      "han2024headsculpt",
      "liu2023unidream",
      "raj2023dreambooth3d",
      "zhao2023efficientdreamer",
      "dreamcraft3d",
      "bd23",
      "xiang20233d",
      "cat3d",
      "tang2023make",
      "get3d",
      "huang2024consistentid",
      "animatabledreamer",
      "li2021topologically",
      "zhou2024dreamscene360",
      "li2024dreamscene",
      "ma2024fastscene",
      "zhang2023text2nerf",
      "avrahami2023chosen",
      "sun2022cgof++",
      "yi2023gaussiandreamer",
      "chen2024meshanything",
      "lin2022magic3d",
      "tex2shape",
      "sun2022ide",
      "bai2023learning",
      "zheng2023pointavatar",
      "lisweetdreamer",
      "kolotouros2023dreamhuman",
      "huang2023dreamwaltz",
      "li2023preim3d",
      "khwanmuang2023stylegan",
      "imavatar",
      "gafni2021dynamic",
      "grassal2022neural",
      "he2024dresscode",
      "ning2023picture"
    ],
    "relevant_ids": [
      "kirschstein2023nersemble",
      "munkberg2022extracting",
      "park2021nerfies",
      "wang2022clip",
      "han2023generalist",
      "liu2023headartist",
      "poole2022dreamfusion",
      "ramesh2022hierarchical",
      "saharia2022photorealistic",
      "rombach2022high",
      "lin2023magic3d",
      "yariv2021volume",
      "grassal2022neural",
      "tang2023make",
      "long2023wonder3d",
      "sanghi2022clip",
      "bai2023learning",
      "melas2023realfusion",
      "liu2023zero",
      "liu2023syncdreamer",
      "huang2023humannorm",
      "pei2024deepfake",
      "wang2024prolificdreamer",
      "chen2023fantasia3d",
      "zheng2023pointavatar",
      "han2024headsculpt",
      "Zheng_2023_CVPR",
      "ramesh2021zero"
    ],
    "metrics": {
      "R@5": 0.03571428571428571,
      "R@10": 0.07142857142857142,
      "R@20": 0.10714285714285714,
      "MRR": 0.3333333333333333,
      "hits": 7,
      "total_relevant": 28
    },
    "score": 0.1357142857142857,
    "timestamp": "2025-12-18T10:55:06.606800"
  },
  {
    "query": "Introduction \\begin{figure}[tbp!] \\begin{center} \\subfigure[Brute-force]{ \\includegraphics[width=0.31\\linewidth]{figures/illustrate_bruteforce.png} \\label{fig:PrepFree_Model_Selection} } \\hfill \\subfigure[Model Embedding]{ \\includegraphics[width=0.31\\linewidth]{figures/illustrate_model_embedding.png} \\label{fig:QueryEfficient_Model_Selection} } \\hfill \\subfigure[\\textbf{Isolated Model Embedding}]{ \\includegraphics[width=0.31\\linewidth]{figures/illustrate_isolated_model_embedding.png} \\label{fig:IndepPrep_QueryEfficient_Model_Selection} } \\caption{Illustrations for different families of model selection schemes. \\textbf{Isolated model embedding (ours)} is a family that supports asymptotically fast update and selection \\textbf{at the same time}.} \\label{fig:Formulation} \\end{center} \\vskip -0.1in",
    "paper_id": "2406.07536",
    "retrieved_ids": [
      "lian2022scaling",
      "ShiftAndScale",
      "yuan2024llminferenceunveiledsurvey",
      "xie2021fignerf",
      "dong2024promptpromptedadaptivestructuredpruning",
      "Task2Vec",
      "lu2023dpmsolver",
      "2020arXiv201014701H",
      "s3pet",
      "tang2022arbitrary",
      "liu2022design",
      "sun2024spectrfastspeculativedecoding",
      "wasserstein_task_embed",
      "LeeS17",
      "SS",
      "xu2022image2point",
      "chen2023pixartalpha",
      "firoozi2023foundation",
      "zhang2020understanding",
      "dong2019bench",
      "abdal2020image2stylegan++",
      "lian2023llmgrounded",
      "ding2021prototypical",
      "li2022efficient",
      "kandala2024pix2gif",
      "voynov2023p+",
      "wang2023patch",
      "Mao:ICCV15",
      "Wu2018MixedPQ",
      "hao2023optimizing",
      "yaida2022meta",
      "Han2020ASO",
      "anderson2017guided",
      "vaze2022generalized",
      "jin2021teachers",
      "yang2023change",
      "li2019leveraging",
      "hokamp2017lexically",
      "DurfeeKPRS17",
      "yuan2023text",
      "pfenet",
      "nguyen2020wide",
      "wang2024prolificdreamer",
      "b9",
      "liu2022practical",
      "Pascanu2014",
      "zou2024segment",
      "park2019effect",
      "levin2023differential",
      "xie2020adversarial",
      "nitzan2023domain",
      "yang2020randomized",
      "gurnee2023language",
      "white2021powerful",
      "meng2020pruning",
      "li2024llavamed",
      "dai2023can",
      "dani2023devil"
    ],
    "relevant_ids": [
      "HowStableAreMetrics",
      "zamir2018taskonomy",
      "leep",
      "EffTuned_task_embed",
      "H_score",
      "wasserstein_task_embed",
      "SelectBenchmark",
      "Task2Vec",
      "TransRate",
      "GBC",
      "LogMe"
    ],
    "metrics": {
      "R@5": 0.0,
      "R@10": 0.09090909090909091,
      "R@20": 0.18181818181818182,
      "MRR": 0.16666666666666666,
      "hits": 2,
      "total_relevant": 11
    },
    "score": 0.07727272727272727,
    "timestamp": "2025-12-18T10:55:14.421011"
  },
  {
    "query": "Introduction Graphs are pervasive in real world such as social networks<|cite_0|>, molecular graphs<|cite_1|>, and knowledge graphs<|cite_2|>. Graph Neural Networks (GNNs) have shown great ability in node representation learning on graphs. Generally, GNNs adopt a message-passing mechanism that updates a node's representation by iteratively aggregating information from its neighbors. The resulting",
    "paper_id": "2406.09836",
    "retrieved_ids": [
      "xu2018jknet",
      "hamilton2017representation",
      "you2020graph",
      "zhu2021survey",
      "gin",
      "Fey2020",
      "yang2021graphformers",
      "Zonghan2019",
      "Zhou2018",
      "xu2018powerful",
      "Sato2020ASO",
      "Zhang2017NetworkRL",
      "fan2019graph",
      "flam2020neural",
      "velivckovic2017graph",
      "liu2020towards",
      "random-features",
      "mavromatis2023train",
      "wang2020streaming",
      "rakaraddi2022reinforced",
      "article10",
      "morris2019weisfeiler",
      "grohewl",
      "nikolentzos2020k",
      "higher-order",
      "not_all_neighbor",
      "shao2022distributed",
      "wu2021graph",
      "liu2019hyperbolic",
      "zhu2021graph",
      "Thekumparampil2018AttentionbasedGN",
      "mad",
      "rusch2022g2gating",
      "fagcn",
      "wang2019heterogeneous",
      "pei2020geom",
      "he2023generalization",
      "rong2020self",
      "gcnlpa",
      "pprgo",
      "guo2020iterative"
    ],
    "relevant_ids": [
      "zhang2020gnnguard",
      "wang2021certified",
      "li2021anti",
      "zhang2021backdoor",
      "liu2022federated",
      "zhang2018link",
      "Dai_2023",
      "xu2018powerful",
      "zhang2024rethinking",
      "hamilton2017inductive",
      "kipf2016semi",
      "fan2019graph",
      "velickovic2017graph",
      "mansimov2019molecular",
      "xi2021graph"
    ],
    "metrics": {
      "R@5": 0.0,
      "R@10": 0.06666666666666667,
      "R@20": 0.13333333333333333,
      "MRR": 0.1,
      "hits": 2,
      "total_relevant": 15
    },
    "score": 0.05,
    "timestamp": "2025-12-18T10:55:16.711426"
  },
  {
    "query": "Introduction 3D human pose estimation is a critical task in computer vision that involves identifying and locating key points of the human body within images or video frames. This capability is essential for a wide range of applications, including analyzing human behavior, understanding intentions, and studying how individuals interact and",
    "paper_id": "2406.01196",
    "retrieved_ids": [
      "chen17",
      "Zheng20213DHP",
      "pavllo19",
      "CanonPoseSM",
      "JointformerSL",
      "Monocular3H",
      "nibali2019",
      "grinciunaite16",
      "rong2021frankmocap",
      "zhou16",
      "GyeongsikMoon2020hand4whole",
      "fabbri2020",
      "Tompson2014Joint",
      "kocabas2020",
      "toshev2014deeppose",
      "martinez17",
      "chou2017self",
      "SimpleBaseline",
      "SingleShotM3",
      "insafutdinov2016deepercut",
      "belagiannis2017recurrent",
      "pishchulin2016deepcut",
      "kanazawa18",
      "xiao2018simple",
      "cao2021openpose",
      "Ronchi2017Benchmarking",
      "POISE",
      "sun2018integral",
      "djrn",
      "fang2018pairwise",
      "drover18",
      "taheri2020grab",
      "bahl2023affordances",
      "wake2024gpt4vision",
      "han2023groundlink",
      "PanopticSA",
      "Awais-arxiv-2023",
      "DBLP:journals/corr/abs-1904-03181",
      "ju2023human",
      "HRNet",
      "fu2022stylegan",
      "sermanet2018tcn",
      "zang2023contextual",
      "feng2021collaborative",
      "choutas2020monocular"
    ],
    "relevant_ids": [
      "SpatialTG",
      "Zheng20213DHP",
      "sun2019deep",
      "bahdanau2014neural",
      "EndtoEndHP",
      "CanonPoseSM",
      "Pix2FaceD3",
      "Kipf2016SemiSupervisedCW",
      "PanopticSA",
      "SimpleBaseline",
      "SemGCN",
      "LargeP3",
      "AttentionIA",
      "SingleShotM3",
      "COCO",
      "LCRNetM2",
      "Monocular3H",
      "GraphSH",
      "Modelbased3H",
      "cao2017realtime",
      "Yang2018GraphRF",
      "mnih2014recurrent",
      "3DHS",
      "JointformerSL"
    ],
    "metrics": {
      "R@5": 0.125,
      "R@10": 0.16666666666666666,
      "R@20": 0.25,
      "MRR": 0.5,
      "hits": 7,
      "total_relevant": 24
    },
    "score": 0.25,
    "timestamp": "2025-12-18T10:55:18.533051"
  },
  {
    "query": "Introduction Large language models (LLMs) have achieved substantial advancements<|cite_0|>. Particularly noteworthy is the emergence of the Chain-of-Thought (CoT), which has further enhanced the ability of LLMs to handle complex reasoning tasks<|cite_1|>. In addition, as globalization continues to advance, aligning representations across different languages has become an urgent issue<|cite_2|>. This has",
    "paper_id": "2406.13940",
    "retrieved_ids": [
      "Zhao2023ASO",
      "naveed2024comprehensivellms",
      "survey0",
      "zhang2023multimodal",
      "kaddour2023challenges",
      "mmllms1",
      "wang2023boosting",
      "guo2024large",
      "Zhu2023LargeLM",
      "gpt_summary",
      "yu2023towards",
      "li2024faithful",
      "chai2023graphllm",
      "ref:edu1",
      "ref:med2",
      "turpin2023language",
      "zhang2022automatic",
      "wang2024large",
      "zheng2023progressivehint",
      "wu2023analyzing",
      "qin2023cross",
      "feng2024towards",
      "wang-etal-2023-towards",
      "ranaldi2023empowering",
      "yasunaga2023analogical",
      "llmsurvey3",
      "qin2024multilingual",
      "xu24llmkdsurvey",
      "zhang2024comprehensive",
      "chai2024xcot",
      "miao2023towards",
      "qin2024large",
      "wang2023knowledge",
      "sahoo2024systematic",
      "lanham2023measuring",
      "chia2024instructeval",
      "yuan2024llminferenceunveiledsurvey",
      "wang2023survey",
      "trivedi2022interleaving",
      "radhakrishnan2023question",
      "he2022rethinking",
      "tai2023exploring",
      "wang2024chain",
      "ref:ragsurvey2",
      "huang2023survey",
      "chen2023universal"
    ],
    "relevant_ids": [
      "wei2022chain",
      "wang2022self",
      "qin2024large",
      "zhang2022automatic",
      "shi2022language",
      "qin2023cross",
      "brown2020language",
      "touvron2023llama",
      "ranaldi2023empowering",
      "chen2021evaluating",
      "yao2023tree",
      "chai2024xcot",
      "huang2023not",
      "Mulcaire_Kasai_Smith_2019",
      "tanwar2023multilingual",
      "feng2023towards",
      "qin2024multilingual",
      "zhang2023multimodal",
      "Pires_Schlinger_Garrette_2019",
      "kojima2022large",
      "lin2021few",
      "tang2023large",
      "schaeffer2023emergent"
    ],
    "metrics": {
      "R@5": 0.043478260869565216,
      "R@10": 0.043478260869565216,
      "R@20": 0.08695652173913043,
      "MRR": 0.25,
      "hits": 7,
      "total_relevant": 23
    },
    "score": 0.10543478260869565,
    "timestamp": "2025-12-18T10:55:21.055531"
  },
  {
    "query": "Introduction \\begin{figure*} [htbp] \\centering \\includegraphics[width=1\\linewidth]{pic/intro.jpg} \\caption{\\label{fig:intro} Images generated by Character-Adapter. Character-Adapter can be seamlessly integrated with any preferred model, without extra training. This approach empowers the customization of concepts while preserving the high-fidelity appearance of given characters (without any quantitative limitations), encompassing attributes such as hairstyle, identity, attire, and others.}",
    "paper_id": "2406.16537",
    "retrieved_ids": [
      "zhang2024flashface",
      "gu2024mix",
      "tan2020michigan",
      "kharitonov2023speak",
      "chen2023photoverse",
      "hu2024animate",
      "edit",
      "gafni2022make",
      "roy2020stefann",
      "weng2019photo",
      "lee2017fully",
      "luddecke2022image",
      "han2024face",
      "wei2022hairclip",
      "bao2018towards",
      "zhao2023motiondirector",
      "saha2021loho",
      "li2023photomaker",
      "gal2023encoder",
      "ye2023ip-adapter",
      "kumari2022multi",
      "chen2024magiccloth",
      "sohn2023styledrop",
      "gao2021clip",
      "gong2023talecrafter",
      "gal2022textual",
      "lee2023holistic",
      "t2i-adapter",
      "shi2023instantbooth",
      "ramesh2021zero",
      "wu2023easyphoto",
      "voynov2023p+",
      "wang2024instantid",
      "gafni2019vid2game",
      "lee2024compose",
      "daras2022multiresolution",
      "zhang2023motioncrafter",
      "liu2024glyph",
      "Grid2Im",
      "jia2023taming",
      "ning2023picture",
      "UdiffText",
      "materzynska2023customizingmotion",
      "he2024dresscode",
      "kolotouros2023dreamhuman",
      "nitzan2023domain",
      "hao2023optimizing",
      "avrahami2023chosen",
      "wei2023hairclipv2",
      "an2022fastclipstyler",
      "guo2024pulid",
      "liu2023unidream",
      "blending",
      "creswell2018inverting"
    ],
    "relevant_ids": [
      "wu2023easyphoto",
      "gu2024mix",
      "kumari2023multi",
      "ye2023ip",
      "tewel2024training",
      "dahary2024yourself",
      "xiao2023fastcomposer",
      "li2024blip",
      "li2023photomaker",
      "hu2021lora",
      "wei2023elite",
      "hua2023dreamtuner",
      "kong2024omg",
      "wang2024instantid",
      "ruiz2023dreambooth",
      "zhang2023adding"
    ],
    "metrics": {
      "R@5": 0.0625,
      "R@10": 0.0625,
      "R@20": 0.125,
      "MRR": 0.5,
      "hits": 4,
      "total_relevant": 16
    },
    "score": 0.19374999999999998,
    "timestamp": "2025-12-18T10:55:24.661120"
  },
  {
    "query": "Introduction Existing large language models (LLMs) have exhibited strong generalization abilities on various tasks due to their huge model capacities<|cite_0|>. With faith in the scaling law<|cite_1|>, the amount of parameters in current LLMs is expanded steadily to achieve higher intelligence. However, the increasing parameters also bring high deployment costs in",
    "paper_id": "2406.17328",
    "retrieved_ids": [
      "Zhao2023ASO",
      "wei2022emergent",
      "minaee2024large",
      "naveed2024comprehensivellms",
      "kaddour2023challenges",
      "survey2",
      "yuan2023scaling",
      "dong2023abilities",
      "gpt_summary",
      "laskar2024systematic",
      "ref:edu1",
      "guo2024large",
      "jiang2023scaling",
      "zhu2023multilingual",
      "qin2024large",
      "ref:ragsurvey1",
      "survey1",
      "li2021scaling",
      "qin2024multilingual",
      "shen2023mixtureofexperts",
      "mmllms1",
      "miao2023towards",
      "deepseekllm",
      "llmsurvey3",
      "makatura2023large",
      "zhang2024scaling",
      "lu2024",
      "wang2023knowledge",
      "ref:med3",
      "model_compression",
      "jin2024comprehensive",
      "fu23cotkd",
      "pang2024anchor",
      "wei2024large",
      "ref:ragsurvey3",
      "namburi2023cost",
      "asai2024reliable",
      "hu2023",
      "ref:ragsurvey2",
      "tian2024gnp",
      "luo2023empirical",
      "pan2023smoothquant+",
      "shi2023dept",
      "yu2024melo",
      "ge2023making",
      "bentham2024chainofthought"
    ],
    "relevant_ids": [
      "wen23fdiv",
      "jiao20tinybert",
      "xu24llmkdsurvey",
      "touvron23llama",
      "gu23minillm",
      "sun19patientkd",
      "fu23cotkd",
      "hinton15kd",
      "sanh19distilbert",
      "sun20mobilebert",
      "chowdhery23palm",
      "wang21minilmv2",
      "ko24distillm",
      "kaplan20scaling",
      "wu2024rethinking",
      "openai23gpt4",
      "wang20minilm",
      "kim16seqkd"
    ],
    "metrics": {
      "R@5": 0.0,
      "R@10": 0.0,
      "R@20": 0.0,
      "MRR": 0.03125,
      "hits": 1,
      "total_relevant": 18
    },
    "score": 0.009375,
    "timestamp": "2025-12-18T10:55:26.792288"
  },
  {
    "query": "Introduction \\vspace{-4mm} In recent years, the rapid advancement of generative large language models (LLMs) has revolutionized their application across diverse fields such as automated code generation, conversational agents, and data analysis. Traditionally, evaluations of these models have predominantly relied on human judgment due to its comprehensive nature and ability to",
    "paper_id": "2406.07791",
    "retrieved_ids": [
      "chiang2023can",
      "kaddour2023challenges",
      "naveed2024comprehensivellms",
      "minaee2024large",
      "survey0",
      "Zhao2023ASO",
      "mmllms1",
      "gpt_summary",
      "wang2023survey",
      "Zhu2023LargeLM",
      "makatura2023large",
      "zhang2024llmeval",
      "ref:med2",
      "ref:med1",
      "shen-etal-2023-large",
      "zeng2023evaluating",
      "Pan2023AutomaticallyCL",
      "laskar2024systematic",
      "miao2023towards",
      "lyu2024probabilities",
      "ref:financialgpt",
      "liu2023agentbench",
      "ref:edu1",
      "feng2023synergistic",
      "LLM4DriveAS",
      "Ji2023GenRecLL",
      "chia2024instructeval",
      "ref:ragsurvey3",
      "yao2023survey",
      "wang2023knowledge",
      "k\u00f6pf2023openassistant",
      "rosati-etal-2024-long",
      "Zheng2023SecretsOR",
      "model_compression",
      "fingpt",
      "wu2023speechgen",
      "zhang2022survey",
      "huang2023survey",
      "llmsurvey3",
      "ahuja-etal-2023-mega",
      "ref:ragsurvey2",
      "ahuja2023mega",
      "Zonghan2019",
      "bai2023qwen",
      "liu2023comprehensive",
      "Liu2023IsCA"
    ],
    "relevant_ids": [
      "li2024devbench",
      "li2023generative",
      "li2023prd",
      "wei2022chain",
      "shen-etal-2023-large",
      "kocmi2023large",
      "karpinska2021perils",
      "liusie2024llm",
      "openai2024gpt4",
      "zheng2024large",
      "geminiteam2024gemini",
      "li2023split",
      "chen2024softtiger",
      "hou2024large",
      "khan2024debating",
      "zheng2024judging",
      "zeng2023evaluating",
      "chiang2023can",
      "zhu2023judgelm"
    ],
    "metrics": {
      "R@5": 0.05263157894736842,
      "R@10": 0.05263157894736842,
      "R@20": 0.15789473684210525,
      "MRR": 1.0,
      "hits": 3,
      "total_relevant": 19
    },
    "score": 0.3368421052631579,
    "timestamp": "2025-12-18T10:55:28.714433"
  },
  {
    "query": "Introduction \\label{sec:intro} \\begin{figure*}[!h] \\begin{minipage}[!t]{1\\linewidth} \\centering \\includegraphics[width=1\\textwidth]{imgs/intro_vis.jpg} \\caption{ Two denoising trajectories of a preferred image and a dispreferred image generated from the same prompt ``A \\redtext{cat} jumps on a \\redtext{dog}.'' The final image of the upper trajectory is preferred for correctly generating a dog and a cat in a correct spatial",
    "paper_id": "2406.04314",
    "retrieved_ids": [
      "luddecke2022image",
      "zhang2024hive",
      "hao2023optimizing",
      "qi2018semi",
      "daras2022multiresolution",
      "hu2022promptcap",
      "voynov2023p+",
      "li2023divide",
      "gal2022textual",
      "mao2023guided",
      "wu2023human",
      "liang2017generative",
      "guo2024initno",
      "kirillov2020pointrend",
      "sohn2023styledrop",
      "patashnik2023localizing",
      "kirstain2024pick",
      "xu2023imagereward",
      "paiss2023teaching",
      "armandpour2023re",
      "chen2022re",
      "yin2023dragnuwa",
      "priormdm",
      "Avrahami_2022_CVPR",
      "sun2024spatial",
      "dialog-gen",
      "agarwal2023star",
      "lee2023holistic",
      "CapDec",
      "tsalicoglou2023textmesh",
      "xie2023boxdiff",
      "jeong2024visual",
      "pinkney2022clip2latent",
      "gokaslan2018improving",
      "andreas2016neural",
      "kim2023consistency",
      "zero12345plus",
      "img2img2",
      "girdhar2023emu",
      "inpaint",
      "wu2023easyphoto",
      "voynov2022sketch",
      "kandala2024pix2gif",
      "bar2023multidiffusion",
      "park2023understanding",
      "rlhfv",
      "haque2023instruct",
      "TextDiffuser",
      "nitzan2023domain",
      "huberman2023ddpminv",
      "yi2023gaussiandreamer",
      "wu2023selfcorrecting",
      "liu2023unidream",
      "smilkov2017smoothgrad",
      "lee2023syncdiffusion"
    ],
    "relevant_ids": [
      "podell2023sdxl",
      "black2023training",
      "rafailov2024direct",
      "diffusion_dpo",
      "ouyang2022training",
      "kirstain2024pick",
      "clark2023directly",
      "xu2024imagereward",
      "rombach2022high",
      "chen2023enhancing",
      "schulman2017proximal",
      "prabhudesai2023aligning",
      "d3po",
      "hertz2022prompt",
      "lee2023aligning",
      "yang2024dense"
    ],
    "metrics": {
      "R@5": 0.0,
      "R@10": 0.0,
      "R@20": 0.0625,
      "MRR": 0.058823529411764705,
      "hits": 1,
      "total_relevant": 16
    },
    "score": 0.01764705882352941,
    "timestamp": "2025-12-18T10:55:32.427556"
  },
  {
    "query": "Introduction \\vspace{-0.1cm} Deep Neural Networks are known to be vulnerable to crafted imperceptible input-space perturbations known as \\emph{Adversarial attacks}<|cite_0|>, which can be used to fool classification networks into predicting any desired output, leading to disastrous consequences. Amongst the diverse attempts at improving the adversarial robustness of Deep Networks, Adversarial Training",
    "paper_id": "2406.05796",
    "retrieved_ids": [
      "fawaz2019adversarial",
      "liang2017deep",
      "yan2018deep",
      "metzen2017detecting",
      "Madry2017",
      "grosse2016adversarial",
      "chakraborty2018adversarial",
      "sur2",
      "fawzi2018adversarial",
      "Mad+18",
      "jakubovitz2018improving",
      "ross2018improving",
      "su2019one",
      "akhtar2018threat",
      "mustafa2019adversarial",
      "papernot2016crafting",
      "PMJ+16",
      "samangouei2018defense",
      "yuan2019adversarial",
      "jacobsen2018excessive",
      "narodytska2016simple",
      "discretization_based",
      "tramer2017ensemble",
      "papernot2016distillation",
      "huang2021exploring",
      "meng2017magnet",
      "xu2017feature",
      "kurakin2016adversarial",
      "moosavi2017universal",
      "isit2018",
      "chen2018shapeshifter",
      "Goodfellow2016",
      "wu2018understanding",
      "kang2021stable",
      "yu2021lafeat",
      "Tsuzuku2018",
      "shamir2019simple",
      "naseer2018task",
      "huang2019enhancing",
      "hendrik2017universal",
      "biggio_2018",
      "feinman2017detecting",
      "ganeshan2019fda",
      "blau2022threat",
      "dziugaite2016study"
    ],
    "relevant_ids": [
      "selfie",
      "kumar2022fine",
      "madry-iclr-2018",
      "simclr",
      "addepalli2022efficient",
      "tramer2020fundamental",
      "dynacl",
      "jigsaw",
      "rotnet",
      "sriramanan2020gama",
      "carlini2019evaluating",
      "cpc",
      "moco",
      "acl",
      "chen2020adversarial",
      "schmidt2018adversarially",
      "deacl",
      "advcl",
      "croce2020reliable",
      "zhang2019theoretically",
      "rocl",
      "intriguing-iclr-2014"
    ],
    "metrics": {
      "R@5": 0.0,
      "R@10": 0.0,
      "R@20": 0.0,
      "MRR": 0.0,
      "hits": 0,
      "total_relevant": 22
    },
    "score": 0.0,
    "timestamp": "2025-12-18T10:55:34.756733"
  },
  {
    "query": "Introduction The emergence of Large Language Models (LLMs) has improved the prospects for robotic tasks. However, existing benchmarks are still limited to single tasks, with each task being independently trained. This results in limited generalization capabilities of current research<|cite_0|>, only touching the surface of the inherent potential of these models.",
    "paper_id": "2406.03757",
    "retrieved_ids": [
      "wang2024large",
      "Zhao2023ASO",
      "kim2023language",
      "kaddour2023challenges",
      "minaee2024large",
      "gpt_summary",
      "naveed2024comprehensivellms",
      "generalpatternmachines2023",
      "laskar2024systematic",
      "qu2024tool",
      "wang2023survey",
      "mmllms2",
      "guo2024large",
      "zhu2023multilingual",
      "ref:financialgpt",
      "Liu2023LLMRecBL",
      "shnitzer2023large",
      "mmllms1",
      "lyu2024probabilities",
      "makatura2023large",
      "survey1",
      "ref:ragsurvey1",
      "chia2024instructeval",
      "qin2024large",
      "zheng2024",
      "llmsurvey3",
      "miao2023towards",
      "bai-etal-Longbench-2024",
      "zhang2023benchmarking",
      "zhu2023ghost",
      "rae2021scaling",
      "Zheng2023SecretsOR",
      "zhang-etal-2023-multilingual",
      "Gao2023RetrievalAugmentedGF",
      "xie2023translating",
      "xiao2023conditions",
      "levy2024same",
      "depalm",
      "scao2022bloom",
      "goddard2024arcee",
      "zhou2023navgpt",
      "wang2023large",
      "Wu2024HowED",
      "brohan2023rt1",
      "ref:ragsurvey2",
      "Bao2023TALLRecAE",
      "kwon2023language",
      "dubey2024llama",
      "mllm4mspeech",
      "brohan2022rt",
      "TRL",
      "francis2022core",
      "tian2024gnp",
      "bai2023qwen"
    ],
    "relevant_ids": [
      "silver2023generalized",
      "lin2023text2motion",
      "jiang2022vima",
      "madaan2023selfrefine",
      "zhang2021hierarchical",
      "codex",
      "liang2023code",
      "li2023starcoder",
      "singh2023progprompt",
      "zhang2023selfedit",
      "fried2022incoder",
      "nasir2023llmatic",
      "chen2023teaching",
      "touvron2023llama",
      "alphacode",
      "chen2021evaluating",
      "shridhar2022perceiver",
      "brohan2022rt",
      "chen2023evoprompting",
      "austin2021program",
      "guhur2022instruction",
      "liu2023llm+",
      "codeRL",
      "palme",
      "wang2023demo2code",
      "ding2023task",
      "robocodex",
      "isaac",
      "codellama",
      "wang2023voyager",
      "xie2023translating",
      "chowdhery2022palm",
      "eureka",
      "hendrycksapps2021",
      "lehman2022evolution",
      "codegen",
      "yu2023language",
      "huang2023instruct2act",
      "guo2023connecting",
      "jang2022bc"
    ],
    "metrics": {
      "R@5": 0.0,
      "R@10": 0.0,
      "R@20": 0.0,
      "MRR": 0.02857142857142857,
      "hits": 2,
      "total_relevant": 40
    },
    "score": 0.00857142857142857,
    "timestamp": "2025-12-18T10:55:37.076765"
  },
  {
    "query": "Introduction \\label{sec:intro} Current generative models showcase remarkable realism and unprecedented understanding of visual concepts, but at the same time, they lack control. While even a motorcycle riding panda can be generated persuasively, it is difficult to control exactly which jacket the panda should be wearing without drifting away from the",
    "paper_id": "2406.15331",
    "retrieved_ids": [
      "theis2016note",
      "plumerault2020controlling",
      "MAGIC",
      "liao2020towards",
      "Hinz2019GeneratingMO",
      "smplicit",
      "liu2020self",
      "panda",
      "shaham2019singan",
      "hu2023gaia",
      "alemohammad2023self",
      "Kingma2014SemisupervisedLW",
      "KowalskiECCV2020",
      "liu2022compositional",
      "cao2022survey",
      "liu2022design",
      "self_guidance",
      "dong2022dreamartist",
      "HabibMariooryadShannonBattenbergSkerryRyanStantonKaoBagby2019",
      "zhang2023text",
      "avrahami2023spatext",
      "kobyzev2020normalizing",
      "po2023state",
      "shoshan2021gan",
      "zhang2023controllable",
      "sun2024spatial",
      "goetschalckx2019ganalyze",
      "krause2021gedi",
      "su2023pandagpt",
      "chan2023generative",
      "chan2020cocon",
      "bansal2022how",
      "hu2022self",
      "niemeyer2021giraffe",
      "rissanen2022generative",
      "zhu2024sora",
      "brack2023sega",
      "xiao2021hallucination",
      "bianchi2023easily",
      "reed2016generative",
      "shi2023instantbooth",
      "zhuang2021enjoy",
      "gal2022textual",
      "du2023demofusion",
      "friedrich2023fair",
      "hall2023dig",
      "DPO",
      "huang2024classdiffusion",
      "basu2023inspecting",
      "avrahami2023chosen",
      "tewel2023key",
      "wang2021hijack",
      "alaluf2023neural",
      "mao2023guided",
      "Mansimov2015GeneratingIF",
      "evans2024fast"
    ],
    "relevant_ids": [
      "nitzan2024lazy",
      "gal2022image",
      "xie2023smartbrush",
      "huberman2023ddpminv",
      "tang2023emergent",
      "woo2021fedpara",
      "gal2022image2",
      "chen2024anydoor",
      "lugmayr2022repaint",
      "Ruiz_2023_CVPR",
      "zhu2023tryondiff",
      "10.1145/3610548.3618173",
      "wang2023imagen",
      "geyer2023tokenflow",
      "kim2023stableviton",
      "ramesh2022hierarchical",
      "kumari2022customdiff",
      "liu2023cones",
      "10.1145/3588432.3591506",
      "cao2023masactrl",
      "rombach2022high",
      "mokady2022nti",
      "meng2022sdedit",
      "song2022denoising",
      "hu2021lora",
      "alaluf2023crossimage",
      "dhariwal2021diffusion",
      "brack2023leditspp",
      "nichol2022glide",
      "nitzan2022mystyle",
      "morelli2023ladivton",
      "ho2020denoising",
      "ye2023ip-adapter",
      "garibi2024renoise",
      "qiu2023oft",
      "rombach2022highresolution",
      "Karras2021"
    ],
    "metrics": {
      "R@5": 0.0,
      "R@10": 0.0,
      "R@20": 0.0,
      "MRR": 0.0,
      "hits": 0,
      "total_relevant": 37
    },
    "score": 0.0,
    "timestamp": "2025-12-18T10:55:39.396582"
  },
  {
    "query": "Introduction \\label{intro} Generating 3D scenes from texts holds great promise for industries such as gaming, robotics, and VR/AR. Previous methods<|cite_0|>, which use score distillation sampling (SDS) to optimize 3D representations such as Neural Radiance Fields (NeRFs)<|cite_1|>, involve lengthy and unstable optimization processes. In contrast, newer approaches employ feed-forward networks<|cite_2|>, \\eg,",
    "paper_id": "2406.17601",
    "retrieved_ids": [
      "zhang2023text2nerf",
      "bahmani20234d",
      "mildenhall2020nerf",
      "wang2023nerf",
      "xie2021neural",
      "instancenerf",
      "pumarola2020d",
      "pumarola2021d",
      "tsalicoglou2023textmesh",
      "wang2024prolificdreamer",
      "yu2023text",
      "mirzaei2022laterf",
      "ma2024fastscene",
      "singer2023textto4d",
      "chen2024textto3dgsgen",
      "tschernezki2022neural",
      "yu2023edit",
      "khalid2023latenteditor",
      "park2023ed",
      "pixelnerf",
      "deng2022fov",
      "lin2022magic3d",
      "jeong2022perfception",
      "Cohen-Bar_2023_setthescene",
      "zhu2023hifa",
      "poole2022dreamfusion",
      "nguyen2022snerf",
      "chen2022sem2nerf",
      "hedman2021baking",
      "li2024instant3d",
      "deng2022gram",
      "tang2023dreamgaussian",
      "yu2021plenoctrees",
      "park2021hypernerf",
      "seolet",
      "chen2023single",
      "fang2022fast",
      "cao2023hexplane",
      "zhou2023feature",
      "dvgo",
      "kobayashi2022decomposing",
      "im3d",
      "chen2023gaussianeditor",
      "armandpour2023re",
      "zou2023sparse3d",
      "TMGS",
      "fan2022unified"
    ],
    "relevant_ids": [
      "li2023spacetime",
      "jain2022zero",
      "xu2023dmv3d",
      "shriram2024realmdreamer",
      "scaffoldgs",
      "yu2023mvimgnet",
      "sjc",
      "chung2023luciddreamer",
      "katzir2023noise",
      "guedon2023sugar",
      "muller2022instant",
      "bahmani20234d",
      "poole2022dreamfusion",
      "yi2023gaussiandreamer",
      "midas",
      "yu_and_fridovichkeil2021plenoxels",
      "yang2023gs4d",
      "mildenhall2021nerf",
      "chan2022efficient",
      "wu20234dgaussians",
      "shi2023mvdream",
      "li2024dreamscene",
      "rombach2022high",
      "long2023wonder3d",
      "deitke2023objaverse",
      "radford2021learning",
      "wang2023prolificdreamer",
      "tang2024lgm",
      "SceneScape",
      "szymanowicz24splatter",
      "liu2023unidream",
      "shue20233d",
      "wu2024hd",
      "zou2023sparse3d",
      "li2024instant3d",
      "Cohen-Bar_2023_setthescene",
      "jun2023shape",
      "jiang2023hifi4g",
      "liu2023syncdreamer",
      "tang2023dreamgaussian",
      "charatan23pixelsplat",
      "kerbl3Dgaussians",
      "chen2024mvsplat",
      "wang2023score",
      "barron2022mipnerf360",
      "ling2023dl3dv",
      "gslrm2024",
      "tang2023volumediffusion",
      "nichol2022pointe",
      "ma2024fastscene",
      "hoellein2023text2room",
      "ho2020denoising",
      "chen2024textto3dgsgen",
      "zhang2023text2nerf",
      "xu2024grm",
      "Lei_2023_rgbd2",
      "depthanything",
      "hong2023lrm",
      "yu2024gsdf",
      "zhou2024dreamscene360",
      "Huang2DGS2024",
      "wang2021neus",
      "tang2023mvdiffusion"
    ],
    "metrics": {
      "R@5": 0.031746031746031744,
      "R@10": 0.031746031746031744,
      "R@20": 0.06349206349206349,
      "MRR": 1.0,
      "hits": 9,
      "total_relevant": 63
    },
    "score": 0.3222222222222222,
    "timestamp": "2025-12-18T10:55:42.122214"
  },
  {
    "query": "Introduction \\vspace{-10pt} Transformer<|cite_0|> models have emerged as the dominant backbone in Natural Language Processing (NLP) and have successfully extended their influence to the vision domains. The pioneering work of Vision Transformer (ViT)<|cite_1|> and its follow-ups<|cite_2|> have showcased promising performances in various vision tasks, posing a formidable challenge to the established",
    "paper_id": "2406.02021",
    "retrieved_ids": [
      "khan2021transformers",
      "survey7",
      "survey3",
      "heo2021rethinking",
      "naveed2024comprehensivellms",
      "thrush2022winoground",
      "mmllms2",
      "mao2022towards",
      "wang2023visionllm",
      "han2020survey",
      "arnab2021vivit",
      "zhai2022scaling",
      "paul2022vision",
      "kim2021vilt",
      "bao2023u-vit",
      "li2022nextvit",
      "karamcheti2024prismatic",
      "mslongformer",
      "gu2022multi",
      "shao2022adversarial",
      "dosovitskiy2020vit",
      "sun_vvt_pami_2023",
      "wu2021rethinking",
      "yu2021rethinking",
      "dat",
      "T2T",
      "chen2022adaptformer",
      "li2021can",
      "dumpala2024sugarcrepe++",
      "vit",
      "xu2022vitpose",
      "li2023Ivit",
      "ding2022APQViTtowards",
      "tay2020long",
      "darcet2023vision",
      "wang2021knowledge",
      "liu2021swin",
      "do-VTs-see-like-CNNs",
      "t2tformer",
      "yuan2021incorporating",
      "survey1",
      "duan_visionRWKV_2024",
      "lee2022mpvit",
      "woodpecker",
      "yu2022metaformer",
      "Zheng20213DHP",
      "chen2024m3cotnovelbenchmarkmultidomain",
      "Jia:2022:VPT",
      "chen2023advancing",
      "beyer2022knowledge"
    ],
    "relevant_ids": [
      "mehta2021mobilevit",
      "hendrycks2016gelu",
      "guo2022cmt",
      "li2022efficientformer",
      "geva2020keyvalue",
      "cheng2021maskformer",
      "yun2024shvit",
      "shaker2023swiftformer",
      "sukhbaatar2019augmentingmemory",
      "bao2023u-vit",
      "he2016resnet",
      "sukhbaatar2015kvmemory",
      "carion2020detr",
      "Vaswani2017transformer",
      "yang2022moat",
      "dao2022flashattention",
      "liu2021swin",
      "ding2023unireplknet",
      "dosovitskiy2021vit",
      "liu2022convnet",
      "peebles2023dit",
      "vasu2023fastvit"
    ],
    "metrics": {
      "R@5": 0.0,
      "R@10": 0.0,
      "R@20": 0.045454545454545456,
      "MRR": 0.06666666666666667,
      "hits": 2,
      "total_relevant": 22
    },
    "score": 0.02,
    "timestamp": "2025-12-18T10:55:44.773129"
  },
  {
    "query": "Introduction \\label{sec:introduction} LiDAR sensors are used in a wide range of robotic applications. Compared to cameras and radars, they provide rich and accurate depth measurements in bright and dark lighting conditions. However, adverse weather effects like rain, snow, and fog severely degrade their performance. These common occurrences mainly impact LiDAR",
    "paper_id": "2406.09906",
    "retrieved_ids": [
      "dreissig2023survey",
      "piroli2023towards",
      "piroli2023energy",
      "heinzler2020cnn",
      "seppanen20224denoisenet",
      "kurup2021dsor",
      "lin2020depth",
      "li2022deepfusion",
      "ma2019self",
      "liang2022bevfusion",
      "xiao20233d",
      "bai2022transfusion",
      "long2021radar",
      "kim2022craft",
      "cao2021invisible",
      "kim2023crn",
      "Yang2020SurfelGANSR",
      "cao2019adversarial",
      "park2021elasticity",
      "RSN",
      "wang2022probabilistic",
      "behley2019semantickitti",
      "sirohi2021efficientlps",
      "pinggera2016lost",
      "10160674",
      "piroli2022detection",
      "object_tracking_in_autunomous_driving",
      "kong2023robo3d",
      "packnet",
      "caesar2020nuscenes",
      "ma2019exploiting",
      "vora2020pointpainting",
      "ravi2018real",
      "sun2020scalability",
      "park2021pseudo"
    ],
    "relevant_ids": [
      "oord2018representation",
      "piroli2023energy",
      "heinzler2020cnn",
      "kurup2021dsor",
      "wang2019symmetric",
      "10160674",
      "li2019learning",
      "dreissig2023survey",
      "myers2021generalized",
      "piroli2023towards",
      "xie2020pointcontrast",
      "tang2020searching",
      "piroli2022detection",
      "qi2017pointnet++",
      "seppanen20224denoisenet",
      "xiao20233d",
      "yang2022survey",
      "li2017learning",
      "jiang2021guided",
      "choy20194d",
      "kong2023robo3d",
      "liu2022less"
    ],
    "metrics": {
      "R@5": 0.22727272727272727,
      "R@10": 0.2727272727272727,
      "R@20": 0.3181818181818182,
      "MRR": 1.0,
      "hits": 10,
      "total_relevant": 22
    },
    "score": 0.4727272727272727,
    "timestamp": "2025-12-18T10:55:46.385801"
  },
  {
    "query": "Introduction \\label{sec:intro} Zero-shot text-to-speech (TTS) has gained attention in recent years due to its ability to synthesize speech that is similar to any voice without speaker-specific model adaptation<|cite_0|>. Although recent research in zero-shot TTS has made significant progress in achieving high audio quality and speaker similarity through the utilization of",
    "paper_id": "2406.02897",
    "retrieved_ids": [
      "vall-e",
      "NaturalSpeechEndtoEndText2022",
      "tan2021survey",
      "wu2022adaspeech",
      "ns3",
      "ramesh2021zero",
      "naturalspeech2",
      "clipforge",
      "peng2024voicecraft",
      "seedtss",
      "li2023styletts",
      "casanova2021sc",
      "kim2024clam",
      "dang2022training",
      "moss2020boffin",
      "chen2021adaspeech",
      "zhang2020adadurian",
      "jeong2021diff",
      "2021arXiv210212092R",
      "wang2023speechx",
      "yan2021adaspeech",
      "huang2022meta",
      "yan2021adaspeech3",
      "megatts",
      "simplespeech",
      "chen2018sample",
      "arik2018neural",
      "shimizu2023prompttts",
      "vall-ex",
      "kharitonov2023speak",
      "chen2020multispeech",
      "casanova2022yourtts",
      "zeroshot_offtheshelf",
      "gibiansky2017deep",
      "tewel2022zerocap",
      "vits"
    ],
    "relevant_ids": [
      "tacotron2",
      "vall-ex",
      "dac",
      "wang2023speechx",
      "audiolm",
      "BASETTS",
      "naturalspeech2",
      "soundstream",
      "musicgen",
      "wu2022adaspeech",
      "mqtts",
      "yang2023diffsound",
      "vall-e",
      "kharitonov2023speak",
      "encodec",
      "dang2022training",
      "ping2017deep",
      "casanova2021sc"
    ],
    "metrics": {
      "R@5": 0.1111111111111111,
      "R@10": 0.16666666666666666,
      "R@20": 0.3333333333333333,
      "MRR": 1.0,
      "hits": 8,
      "total_relevant": 18
    },
    "score": 0.39444444444444443,
    "timestamp": "2025-12-18T10:55:48.186565"
  },
  {
    "query": "Introduction \\label{sec:intro} Neural rendering has emerged as a generalizable, flexible, and powerful approach for photorealistic novel view synthesis (NVS) of any camera poses<|cite_0|>, underpinning a wide variety of applications in augmented/virtual/mixed reality<|cite_1|>, robotics<|cite_2|>, and generation<|cite_3|>, among more others. For example, taking a learning-based parametric idea, Neural Radiance Fields (NeRFs)<|cite_4|> implicitly",
    "paper_id": "2406.04251",
    "retrieved_ids": [
      "tewari2022advances",
      "tewari2020state",
      "mildenhall2020nerf",
      "tretschk2021non",
      "cao2022fwd",
      "yang2023cross",
      "nerf--",
      "hedman2021baking",
      "cheng2023tuvf",
      "niemeyer2022regnerf",
      "du2021neural",
      "truong2023sparf",
      "xu2021h",
      "martin2021nerf",
      "Pratul2021nerv",
      "deng2022fov",
      "zhang2020nerf++",
      "lin2021barf",
      "yuan2022nerf",
      "chen2021mvsnerf",
      "ziyang2023snerf",
      "wang2023nerf",
      "chen2020neural",
      "verbin2022ref",
      "pumarola2021d",
      "jeong2022perfception",
      "johari2022geonerf",
      "chen2023mobilenerf",
      "zhao2022humannerf",
      "byravan2022nerf2real",
      "nguyen2022snerf",
      "pan2021shading",
      "xu2022discoscene",
      "liu2022neural",
      "oechsle2021unisurf",
      "li2023spacetime",
      "ling2023dl3dv",
      "athar2022rignerf",
      "shi2023gir",
      "liu:nsvf:neurips2020",
      "boss2021nerd",
      "po2023state",
      "wang2023breathing",
      "v3d"
    ],
    "relevant_ids": [
      "li2023spacetime",
      "barron2021mip",
      "muller2022instant",
      "cheng2024gaussianpro",
      "turki2022mega",
      "poole2022dreamfusion",
      "thies2019deferred",
      "tancik2022block",
      "deng2022fov",
      "chen2022tensorf",
      "mildenhall2021nerf",
      "sitzmann2019deepvoxels",
      "yu2023mip",
      "barron2022mip",
      "kerbl20233d",
      "xu2022point",
      "wu20234d",
      "rota2024revising",
      "deng2022depth",
      "aliev2020neural",
      "lu2023scaffold",
      "tang2023dreamgaussian",
      "zhang2024pixel",
      "yang2023real",
      "lombardi2019neural",
      "barron2023zip",
      "liu2020neural",
      "yan2023multi",
      "yu2022monosdf",
      "yang2023unisim"
    ],
    "metrics": {
      "R@5": 0.0,
      "R@10": 0.0,
      "R@20": 0.03333333333333333,
      "MRR": 0.0625,
      "hits": 2,
      "total_relevant": 30
    },
    "score": 0.01875,
    "timestamp": "2025-12-18T10:55:51.022689"
  },
  {
    "query": "Introduction \\label{sec:intro} Depth estimation is a core technology for image and video processing, serving many downstream tasks, such as 3D scene reconstruction, video stabilization, applying a bokeh effect, etc. Compared to other geometric representations, such as voxels, point clouds, implicit neural representations, or truncated signed distance functions (TSDF), representation in",
    "paper_id": "2406.12048",
    "retrieved_ids": [
      "sinha2020deltas",
      "park2019deepsdf",
      "romanov2021gp",
      "mu2021sdf",
      "huang2021self",
      "lee2022gcvd",
      "varma2022transformers",
      "tewari2022advances",
      "zhou16",
      "wang2018mvdepthnet",
      "Depth2015Liu",
      "li2022voxsurf",
      "wang2021domain",
      "zhang2020adversarial",
      "takikawa2021neural",
      "murez2020atlas",
      "guo2022neural",
      "wang2021spline",
      "kopf2021rcvd",
      "long2021multiview",
      "davies2020effectiveness",
      "chibane2020neural",
      "lin2020depth",
      "luo2020cvd",
      "gropp2020implicit",
      "qiao2021vip",
      "wang2022probabilistic",
      "ma2019self",
      "Yin2019enforcing",
      "DMTET",
      "fu2022geo",
      "yu2024gsdf",
      "niemeyer:dvr:cvpr2020",
      "grinciunaite16",
      "implicit",
      "yu2022monosdf",
      "xie2021neural",
      "xu2021h",
      "venkatesh2021deep",
      "wang2023digging",
      "oechsle2021unisurf",
      "im2019dpsnet",
      "atzmon2020sal",
      "peng2021shape",
      "wu2024recent",
      "fan2022unified",
      "qiao2022NeuPhysics",
      "zeng2023efficient",
      "xian2021space"
    ],
    "relevant_ids": [
      "luo2020cvd",
      "ranftl2020midas",
      "romanov2021gp",
      "yao2018mvsnet",
      "bhat2023zoedepth",
      "duzceker2021deepvideomvs",
      "sayed2022simplerecon",
      "hou2019gpmvs",
      "Bae2022IronDepth",
      "shu2020featdepth",
      "lee2022gcvd",
      "im2019dpsnet",
      "long2021multiview",
      "wang2018mvdepthnet",
      "kopf2021rcvd",
      "huang2018deepmvs",
      "sinha2020deltas"
    ],
    "metrics": {
      "R@5": 0.11764705882352941,
      "R@10": 0.23529411764705882,
      "R@20": 0.35294117647058826,
      "MRR": 1.0,
      "hits": 8,
      "total_relevant": 17
    },
    "score": 0.41764705882352937,
    "timestamp": "2025-12-18T10:55:53.315000"
  },
  {
    "query": "Introduction The recently proposed neural rendering method 3D Gaussian Splatting (3DGS)<|cite_0|> has rapidly gained popularity and is being widely applied in various areas such as 3D reconstruction<|cite_1|>, 4D reconstruction<|cite_2|>, generation<|cite_3|>, and understanding<|cite_4|>. This is primarily due to its fast training, real-time rendering capabilities, and explicit point-based representation. Within the realm",
    "paper_id": "2406.02058",
    "retrieved_ids": [
      "wu2024recent",
      "tewari2022advances",
      "tewari2020state",
      "wu20234d",
      "kerbl20233d",
      "chen2024textto3dgsgen",
      "yu2024gsdf",
      "charatan23pixelsplat",
      "yang2023gs4d",
      "guedon2023sugar",
      "cheng2024gaussianpro",
      "zhou2023feature",
      "yan2024gsslam",
      "chen2023gaussianeditor",
      "TMGS",
      "yan2023multi",
      "tang2023dreamgaussian",
      "matsuki2024gaussian",
      "jo2024identifying",
      "dahmani2024swag",
      "lei2024gaussnav",
      "zhang2024pixel",
      "szymanowicz24splatter",
      "scaffoldgs",
      "Simon",
      "fastmesh",
      "malarz2024gaussiansplattingnerfbasedcolor",
      "yu2021plenoctrees",
      "shi2023gir",
      "Sharath",
      "Huang2DGS2024",
      "yang2023deformable3dgs",
      "Joo",
      "Wieland",
      "hong2022headnerf",
      "liu2020neural",
      "v4d",
      "ravi2020accelerating"
    ],
    "relevant_ids": [
      "li2023spacetime",
      "wu2024recent",
      "fridovich2022plenoxels",
      "shi2023language",
      "sam",
      "qin2023langsplat",
      "ruckert2022adop",
      "barron2021mip",
      "ding2023pla",
      "huang2023clip2point",
      "guo2024semantic",
      "muller2022instant",
      "clip",
      "yang2023deformable",
      "mip360",
      "mildenhall2021nerf",
      "takmaz2023openmask3d",
      "ye2023gaussian",
      "liu2023weakly",
      "deng2024compact",
      "yifan2019differentiable",
      "kerr2023lerf",
      "kerbl20233d",
      "wu20234d",
      "xue2023ulip",
      "zuo2024fmgs",
      "zhu2023pointclip",
      "lu2023open",
      "zhou2023feature",
      "aliev2020neural",
      "keetha2023splatam",
      "tang2023dreamgaussian",
      "sun2022direct",
      "barron2023zip",
      "luiten2023dynamic",
      "liao2024ov",
      "cen2023segment",
      "shi2023gir",
      "zhou2024hugs",
      "jiang2023alignerf",
      "dino",
      "ling2023align",
      "reiser2023merf"
    ],
    "metrics": {
      "R@5": 0.06976744186046512,
      "R@10": 0.06976744186046512,
      "R@20": 0.11627906976744186,
      "MRR": 1.0,
      "hits": 6,
      "total_relevant": 43
    },
    "score": 0.3488372093023256,
    "timestamp": "2025-12-18T10:55:55.704772"
  },
  {
    "query": "Introduction Recent advancements in Large Language Models (LLMs) have demonstrated the potential of LLM-based Artificial Intelligence (AI) in providing accurate answers to questions about world knowledge. These advancements are reflected in a series of studies and models, including but not limited to GPT-4, Gemini, Mistral, and Llama series<|cite_0|>. To benchmark",
    "paper_id": "2406.11328",
    "retrieved_ids": [
      "Zhao2023ASO",
      "xi2023rise",
      "minaee2024large",
      "naveed2024comprehensivellms",
      "Zhu2023LargeLM",
      "survey0",
      "mmllms1",
      "bubeck2023sparks",
      "schwenk2022okvqa",
      "wang2024large",
      "gpt_summary",
      "kaddour2023challenges",
      "guo2024large",
      "ref:edu1",
      "Shen2023LargeLM",
      "hu2023",
      "check",
      "ref:med2",
      "lawbench",
      "Liu2023LLMRecBL",
      "xia2024fofo",
      "wang2023surveyfactualitylargelanguage",
      "ref:financialgpt",
      "ref:ragsurvey1",
      "conifer",
      "xie2024me",
      "xie2023PIXIULargeLanguage",
      "chia2024instructeval",
      "qin2024large",
      "Zheng2023SecretsOR",
      "li2024mini",
      "tang2023salmonn",
      "miao2023towards",
      "llmsurvey3",
      "vu2023freshllms",
      "liu2023llm360",
      "makatura2023large",
      "mialon2023gaia",
      "ref:finalcialgpt2",
      "veagle_paper",
      "rae2021scaling",
      "ref:ragsurvey2",
      "jin2023time",
      "levy2024same",
      "hu2023bliva",
      "ahuja-etal-2023-mega",
      "ahuja2023mega",
      "yue2023mmmu",
      "bai2023qwen",
      "gruver2023large"
    ],
    "relevant_ids": [
      "Anil2023GeminiAF",
      "Pal2022MedMCQAA",
      "Nori2023CapabilitiesOG",
      "Wang2023CMBAC",
      "Jiang2023Mistral7",
      "hendrycks2020measuring",
      "chen2023huatuogptii",
      "Achiam2023GPT4TR",
      "singhal2023large",
      "kasai2023evaluating",
      "Clark2018ThinkYH",
      "Lin2021TruthfulQAMH",
      "jin2021disease",
      "xie2024me",
      "Cai2023MedBenchAL",
      "Touvron2023Llama2O"
    ],
    "metrics": {
      "R@5": 0.0,
      "R@10": 0.0,
      "R@20": 0.0,
      "MRR": 0.038461538461538464,
      "hits": 1,
      "total_relevant": 16
    },
    "score": 0.011538461538461539,
    "timestamp": "2025-12-18T10:55:57.872125"
  },
  {
    "query": "Introduction Large language models have achieved remarkable success in natural language processing<|cite_0|>. Nonetheless, in real-world scenarios, LLMs sometimes lack of domain-specific or latest information<|cite_1|>. To supplement necessary external knowledge, Retrieval-Augmented Generation (RAG) has been proposed and attracted much attention<|cite_2|>. The basic idea is to employ a two-step ``retrieve-then-generate'' process. First,",
    "paper_id": "2406.03963",
    "retrieved_ids": [
      "Zhao2023ASO",
      "Zhu2023LargeLM",
      "wang2023surveyfactualitylargelanguage",
      "Gao2023RetrievalAugmentedGF",
      "ref:ragsurvey7",
      "ref:ragsurvey3",
      "ram2023context",
      "ref:ragsurvey1",
      "ref:ragsurvey2",
      "qin2024large",
      "wang2023self",
      "wang2023knowledge",
      "zhang2024comprehensive",
      "feng2023synergistic",
      "wang2023retrievalaugmented",
      "ma2023query",
      "liu2023reta",
      "felm",
      "he2022rethinking",
      "Lewis2020RetrievalAugmentedGF",
      "Asai2023SelfRAGLT",
      "ref:ragsurvey6",
      "Pan2023AutomaticallyCL",
      "ref:gnn-rag",
      "Yan2024CorrectiveRA",
      "Barnett2024SevenFP",
      "Zhao2024RetrievalAugmentedGF",
      "cheng2023lift",
      "zhang2022survey",
      "chen2022corpusbrain",
      "Ke2024BridgingTP",
      "trivedi2022interleaving",
      "He2024GRetrieverRG",
      "su2024mitigating",
      "cuconasu2024power",
      "balaguer2024rag",
      "ref:graphrag"
    ],
    "relevant_ids": [
      "jiang2023llmlingua",
      "kandpal2023large",
      "weichain",
      "yu2022generate",
      "madaan2023selfrefine",
      "hatakeyama2023teaching",
      "xu2023retrieval",
      "lewis2020retrieval",
      "li2023structure",
      "lu2023chameleon",
      "litman2020scatter",
      "gao2023retrieval",
      "cheng2023lift",
      "balaguer2024rag",
      "wang2023self"
    ],
    "metrics": {
      "R@5": 0.0,
      "R@10": 0.0,
      "R@20": 0.06666666666666667,
      "MRR": 0.09090909090909091,
      "hits": 3,
      "total_relevant": 15
    },
    "score": 0.02727272727272727,
    "timestamp": "2025-12-18T10:55:59.970316"
  },
  {
    "query": "Introduction The emergence of large-scale pre-trained ``foundation'' vision-language models (VLM) such as CLIP<|cite_0|> and ALIGN<|cite_1|> has marked a paradigm shift in the field of computer vision. These models have demonstrated promising capacity for open-world generalization, and can be easily applied on novel tasks beyond the original training data. By harnessing",
    "paper_id": "2406.11309",
    "retrieved_ids": [
      "chen2024internvl",
      "survey7",
      "Awais-arxiv-2023",
      "yuan2021florence",
      "singh2022flava",
      "wang2023visionllm",
      "shrestha2023medical",
      "wang2022image",
      "azad2023foundational",
      "cui2022democratizing",
      "beal2021billion",
      "shen2021much",
      "bai2023qwen",
      "wu2023bidirectional",
      "gao2021clip",
      "bensaid2024novel",
      "wang2023cogvlm",
      "laion5b",
      "zhu2023pointclip",
      "yang2023foundation",
      "Wang_Yu_Yu_Dai_Tsvetkov_Cao_2021",
      "kuo2022fvlm",
      "xu2021e2e",
      "menon2022visual",
      "zhou2022cocoop",
      "ahn2024autort",
      "BLIP",
      "zhao2023clip",
      "kim2021vilt",
      "el2024scalable",
      "li2022mplug",
      "EVA",
      "conimgcap2",
      "Xiao2022RoboticSA",
      "mahajan2018exploring",
      "mllm4mspeech",
      "2021arXiv210300020R",
      "maniparambil2023enhancing",
      "depalm",
      "liu2022swin",
      "liu2021swin",
      "oquab2023dinov2",
      "brohan2023rt1",
      "chen2020lottery",
      "ghosh2023geneval",
      "sariyildiz2020learning",
      "brohan2022rt",
      "yadav2023ovrl",
      "li2022clip",
      "liu2023matcher",
      "beyer2022knowledge"
    ],
    "relevant_ids": [
      "liang2023comprehensive",
      "tanwisuth2023pouf",
      "zhou2022coop",
      "samadh2023align",
      "xuefeng2023reclip",
      "ge2023improving",
      "wang2020tent",
      "shu2022tpt",
      "niu2023towards",
      "zhou2022cocoop",
      "radford2021learning",
      "karmanov2024efficient",
      "guo2023calip",
      "jia2021scaling",
      "park2023robust",
      "udandarao2022sus"
    ],
    "metrics": {
      "R@5": 0.0,
      "R@10": 0.0,
      "R@20": 0.0,
      "MRR": 0.04,
      "hits": 1,
      "total_relevant": 16
    },
    "score": 0.012,
    "timestamp": "2025-12-18T10:56:02.270066"
  },
  {
    "query": "Introduction Recent advancements in diffusion-based text-to-image generative models<|cite_0|> have entitled common users to create high-fidelity photo-realistic images with minimal expertise. However, generating images from specific user elements, such as a personal dog of users, remains a considerable challenge<|cite_1|> due to the difficulty of preserving the ID of specific elements. To",
    "paper_id": "2406.11643",
    "retrieved_ids": [
      "zhang2023text",
      "kawar2023imagic",
      "xu2023ufogen",
      "wu2023harnessing",
      "balaji2022ediff",
      "zhang2022sine",
      "self_guidance",
      "po2023state",
      "zhang2024flashface",
      "kim2023towards",
      "kumari2023ablating",
      "han2023highly",
      "human_diffusion",
      "huang2023humannorm",
      "tumanyan2023plug",
      "wang2022diffusiondb",
      "van2023anti",
      "dahary2024yourself",
      "kumari2022multi",
      "ref9",
      "bar2023multidiffusion",
      "chen2023photoverse",
      "sheynin2022knn",
      "friedrich2023fair",
      "purushwalkam2024bootpig",
      "zhang2023forget",
      "mao2023guided",
      "zhao2023efficientdreamer",
      "avrahami2023chosen",
      "kim2023reference",
      "huang2024creativesynth",
      "UdiffText",
      "basu2023inspecting",
      "liao2023text",
      "jia2023taming",
      "li2023photomaker",
      "zhu2016generative",
      "chen2022re",
      "frolov2021adversarial",
      "han2023svdiff",
      "tiue",
      "simsar2023lime",
      "avrahami2023blended",
      "xing2023dynamicrafter",
      "chen2024it3d",
      "gal2023encoder",
      "tewel2024trainingfree",
      "wu2023easyphoto",
      "wu2024infinite",
      "huang2024classdiffusion",
      "li2023your",
      "parmar2023zero",
      "guo2024pulid",
      "jeong2024visual",
      "mahapatra2022controllable",
      "kim2023collaborative"
    ],
    "relevant_ids": [
      "chen2023anydoor",
      "peng2023portraitbooth",
      "tewel2022zerocap",
      "ramesh2022dalle",
      "li2023photomaker",
      "gal2022textinversion",
      "chefer2021transformerbeyond",
      "chen2023disenbooth",
      "zhang2023controlnet",
      "wang2024instantid",
      "rombach2022LDM",
      "ge2023expressive",
      "podell2023sdxl",
      "chen2024magiccloth",
      "pei2024deepfake",
      "ye2023ip",
      "chen2023pixartalpha",
      "peebles2023dit",
      "ruiz2023dreambooth",
      "huang2024consistentid"
    ],
    "metrics": {
      "R@5": 0.0,
      "R@10": 0.0,
      "R@20": 0.0,
      "MRR": 0.027777777777777776,
      "hits": 1,
      "total_relevant": 20
    },
    "score": 0.008333333333333333,
    "timestamp": "2025-12-18T10:56:05.043392"
  },
  {
    "query": "Introduction \"\\textit{Don't use a sledgehammer to crack a nut.}\" \\\\ \\rightline{~~------~~\\textit{Proverb}} Tool learning<|cite_0|> aims to arm large language models (LLMs)<|cite_1|> with real world tools, to alleviate hallucinations<|cite_2|> of LLMs. Existing tool learning methods focus on selecting the most effective tool from a large number of options<|cite_3|>, while overlooking the crucial",
    "paper_id": "2406.12429",
    "retrieved_ids": [
      "qu2024tool",
      "yang2023large",
      "xu2024hallucination",
      "qin2024large",
      "qin2023tool",
      "schick2024toolformer",
      "critic",
      "naveed2024comprehensivellms",
      "Huang2023LargeLM",
      "survey0",
      "qin2023toolllm",
      "survey2",
      "tian2024tinyllm",
      "ref:edu1",
      "zhang2023hallucination",
      "woodpecker",
      "yao2023editing",
      "tang2023toolalpaca",
      "zhang2024llmeval",
      "chen2023exploring",
      "Pan2023AutomaticallyCL",
      "ref:med3",
      "zhou2023instruction",
      "check",
      "tang2023struc",
      "zhou2023don",
      "lu2023chameleon",
      "wizardLM",
      "shnitzer2023large",
      "genegpt",
      "kwon2023language",
      "fu2023mme",
      "model_compression",
      "srivastava2023beyond",
      "chia2024instructeval",
      "liu2024rethinking",
      "openfunction",
      "sahoo2024systematic",
      "liu2023llavaplus",
      "xu2023rewoo",
      "llmsurvey3",
      "zhuang2024toolchain",
      "ref:ragsurvey2",
      "zhao2023mmicl",
      "zheng2024toolrerank",
      "chen2024tree",
      "tulu",
      "huang2023transformer",
      "guide_paper",
      "ref:edu2",
      "madaan2022memory"
    ],
    "relevant_ids": [
      "achiam2023gpt",
      "qin2023toolllm",
      "ji2023survey",
      "lu2023routing",
      "shnitzer2023large",
      "lewis2020retrieval",
      "tang2023toolalpaca",
      "qu2024tool",
      "du2024anytool",
      "zheng2024toolrerank",
      "touvron2023llama",
      "jiang2023llm",
      "bai2023qwen",
      "zhang2023hallucination",
      "gao2023retrieval",
      "vsakota2024fly",
      "chen2023frugalgpt",
      "schick2024toolformer",
      "qin2023tool"
    ],
    "metrics": {
      "R@5": 0.10526315789473684,
      "R@10": 0.15789473684210525,
      "R@20": 0.3157894736842105,
      "MRR": 1.0,
      "hits": 8,
      "total_relevant": 19
    },
    "score": 0.3894736842105263,
    "timestamp": "2025-12-18T10:56:08.011737"
  },
  {
    "query": "Introduction Passage ranking, which aims to rank each passage in a large corpus according to its relevance to the user's information need expressed in a short query, is an important task in IR and NLP and plays a crucial role in many applications such as web search and retrieval-augmented generation.",
    "paper_id": "2406.14848",
    "retrieved_ids": [
      "bai2023griprank",
      "ref:ragsurvey3",
      "maillard2021multi",
      "Zhu2023LargeLM",
      "feng2023synergistic",
      "ref:ragsurvey6",
      "ji2014information",
      "hofst\u00e4tter2022fidlight",
      "Yan2024CorrectiveRA",
      "sachan2022qg",
      "ref:ragsurvey2",
      "ren2023rocketqav2",
      "Xu2024ListawareRJ",
      "nogueira2019passage",
      "monot5",
      "khattab2020colbert",
      "paranjape2021hindsight",
      "fid",
      "liu2023reta",
      "google2020negative",
      "hu2019introductory",
      "khattab2023demonstratesearchpredict",
      "li2023multiview",
      "karpukhin-etal-2020-dense",
      "rankt5",
      "qu2021rocketqa",
      "nie2019revealing",
      "Asai2023SelfRAGLT",
      "sun2023rankgpt",
      "yu2022generate",
      "wu2022contextual",
      "listwisereranking",
      "chen2023exploring",
      "qu2020open",
      "izacard2020distilling",
      "hofstatter2020improving",
      "bajaj2018ms",
      "bevilacqua2022autoregressive",
      "cuconasu2024power",
      "hofstatter2021efficiently",
      "ref:graphrag",
      "Ke2024BridgingTP",
      "Wu2024HowED",
      "wang2023prompt"
    ],
    "relevant_ids": [
      "karpukhin2020dense",
      "pradeep2023rankvicuna",
      "jiang2023llmlingua",
      "morris2023vec2text",
      "chevalier2023autocompressor",
      "mu2024gist",
      "nogueira2019monobert",
      "pradeep2023rankzephyr",
      "openai2024gpt4",
      "sun2023rankgpt",
      "ge2023icae",
      "liang2022holistic",
      "qin2023pairwise",
      "sachan2022qg",
      "liu2024llava"
    ],
    "metrics": {
      "R@5": 0.0,
      "R@10": 0.06666666666666667,
      "R@20": 0.06666666666666667,
      "MRR": 0.1,
      "hits": 2,
      "total_relevant": 15
    },
    "score": 0.05,
    "timestamp": "2025-12-18T10:56:09.801305"
  },
  {
    "query": "Introduction \\vspace{-4pt} \\begin{figure}[ht] \\centering \\includegraphics[width=\\textwidth]{images/illu_fin.pdf} \\vspace{-20pt} \\caption{Illustration: The first row shows images generated from one seed. And we can identify the ``trigger patch'' located in the red box that tend to induce the generation of the object. If we inject the trigger patch into another noise, there will be objects",
    "paper_id": "2406.01970",
    "retrieved_ids": [
      "Hinz2019GeneratingMO",
      "shaham2019singan",
      "luddecke2022image",
      "CenterNet",
      "kirillov2020pointrend",
      "self_guidance",
      "dialog-gen",
      "yang2022modeling",
      "pinheiro2015learning",
      "brown2017adversarial",
      "zheng2023layoutdiffusion",
      "yin2017obj2text",
      "visprompt",
      "liu2018dpatch",
      "patashnik2023localizing",
      "voynov2023p+",
      "mao2023guided",
      "bar2023multidiffusion",
      "zhang2023text",
      "nitzan2023domain",
      "zero12345plus",
      "Avrahami_2022_CVPR",
      "xie2023boxdiff",
      "liu2023cones",
      "huang2023nlip",
      "zhang2020understanding",
      "sun2024spatial",
      "frolov2021adversarial",
      "guo2024initno",
      "jing2022learning",
      "wu2023easyphoto",
      "ding2024patched",
      "abdal2020image2stylegan++",
      "yi2023gaussiandreamer",
      "lian2023llmgrounded",
      "aich2022gama",
      "zhang2022sine",
      "nerfstudio",
      "po2023state",
      "pointE",
      "inpaint",
      "mehrabi2023flirt",
      "park2023understanding",
      "li2018exploring",
      "zhang2024rethinking",
      "universal_adversarial_triggers",
      "TextDiffuser",
      "esser2021taming",
      "simsek2021geometry",
      "li2021neural",
      "skorokhodov2022epigraf",
      "bhat2023loosecontrol",
      "Dai_2023",
      "lu2018neural",
      "li2021backdoor_shrinkpad"
    ],
    "relevant_ids": [
      "balaji2022ediff",
      "feng2022training",
      "nichol2021glide",
      "lin2014microsoft",
      "saharia2022photorealistic",
      "ren2015faster",
      "qi2017pointnet",
      "wojke2017simple",
      "rombach2022high",
      "zhang2023adding",
      "song2020denoising",
      "ho2022classifier",
      "guo2024initno",
      "voynov2023sketch",
      "chefer2023attend",
      "reading2021categorical",
      "zheng2023layoutdiffusion",
      "dhariwal2021diffusion",
      "lea2017temporal",
      "mao2023guided",
      "wang2022diffusiondb",
      "hertz2022prompt",
      "sun2024spatial"
    ],
    "metrics": {
      "R@5": 0.0,
      "R@10": 0.0,
      "R@20": 0.08695652173913043,
      "MRR": 0.09090909090909091,
      "hits": 4,
      "total_relevant": 23
    },
    "score": 0.02727272727272727,
    "timestamp": "2025-12-18T10:56:13.030527"
  },
  {
    "query": "Introduction \\label{sec:motivation} Recently, large language models (LLMs) have shown impressive performance in a range of natural language processing tasks<|cite_0|>. However, fine-tuning pre-trained language models (PLMs) is computationally and memory-intensive. To mitigate this, \\textit{parameter-efficient tuning} (PET) methods have been developed to fine-tune a small number of (extra) model parameters instead of",
    "paper_id": "2406.14956",
    "retrieved_ids": [
      "huang2022large",
      "zhong2022training",
      "minaee2024large",
      "ding-et-al:scheme",
      "nas4pet",
      "Zhao2023ASO",
      "qin2024large",
      "kim2023memoryefficient",
      "survey1",
      "survey2",
      "naveed2024comprehensivellms",
      "jordan_chinchilla_2022",
      "generalpatternmachines2023",
      "qi2022parameterefficient",
      "s3pet",
      "wang2019structured",
      "sun2024simpleeffectivepruningapproach",
      "dong2023abilities",
      "li2022pretrained",
      "kwon2022alphatuning",
      "zhang2024scaling",
      "lora",
      "chen2023parameter",
      "zhang2021differentiable",
      "shi2023dept",
      "malladi2023fine",
      "Hu:2022:LLR",
      "Sung2022LSTLS",
      "xia2023sheared",
      "wang2022adamix",
      "xiao2023lm_cocktail",
      "du2022glamefficientscalinglanguage",
      "xu2023qa",
      "mehta2024openelm",
      "adalora",
      "chang2022speechprompt",
      "zhang2023benchmarking",
      "He:2022:TUV",
      "yuan2023scaling",
      "wang2023nonintrusive",
      "lin2020exploring",
      "instruction-tuning",
      "han2022ptr",
      "chang2023speechprompt",
      "alabi2022adapting",
      "Lewis2020RetrievalAugmentedGF",
      "yu2024language_dare",
      "mahabadi2021compacter"
    ],
    "relevant_ids": [
      "darts",
      "squeezeandexcitation",
      "dylora",
      "llmsurvey",
      "bo",
      "paralleladapter",
      "lora",
      "serialadapter",
      "adalora",
      "prefixtuning",
      "bitfit",
      "nas4pet",
      "autopeft",
      "s3pet",
      "resnet",
      "densenet",
      "zerocostproxy"
    ],
    "metrics": {
      "R@5": 0.058823529411764705,
      "R@10": 0.058823529411764705,
      "R@20": 0.11764705882352941,
      "MRR": 0.2,
      "hits": 4,
      "total_relevant": 17
    },
    "score": 0.10117647058823528,
    "timestamp": "2025-12-18T10:56:15.770796"
  },
  {
    "query": "Introduction Transformer-based (<|cite_0|>) large language models (LLMs) are capable of implicitly internalizing a wide range of knowledge during pretraining (<|cite_1|>;<|cite_2|>). However, the potential for generating inaccurate and outdated responses limits the widespread applications of LLMs. One proposed solution to this problem is knowledge editing, which modifies specific factual knowledge in",
    "paper_id": "2406.11566",
    "retrieved_ids": [
      "de2021editing",
      "wang2023knowledge",
      "zhang2024comprehensive",
      "yao2023editing",
      "calinet",
      "wang2024easyedit",
      "alkhamissi2022review",
      "naveed2024comprehensivellms",
      "wang2023cross",
      "Zhao2023ASO",
      "zheng2023can",
      "wu2023evakellm",
      "yin2023history",
      "wang2023surveyfactualitylargelanguage",
      "mlake",
      "zhong2023mquake",
      "dola",
      "zhu2020modifying",
      "xu24llmkdsurvey",
      "survey2",
      "check",
      "llmsurvey2",
      "li2023pmet",
      "sahoo2024systematic",
      "hu2023",
      "felm",
      "ref:ragsurvey1",
      "lu2023emergent",
      "Gao2023RetrievalAugmentedGF",
      "wang2023retrievalaugmented",
      "Pan2023AutomaticallyCL",
      "halueval",
      "perez2022ignore",
      "wang2023self",
      "si2024mpn",
      "mitchell2022memory",
      "wang-etal-2022-iteratively",
      "huang2023transformer",
      "gupta2024model",
      "luo2023empirical",
      "su2024mitigating",
      "baek2023knowledge",
      "manakul",
      "su2024unsupervised",
      "verga2020facts"
    ],
    "relevant_ids": [
      "surveyofedit",
      "anisotropic",
      "beniwal2024crosslingual",
      "rome",
      "knowledge-neurons",
      "wang2023crosslingual",
      "alkhamissi2022review",
      "attentionisall",
      "petroni-etal-2019-language",
      "SERAC",
      "lm-multilingual-cot",
      "loramodule",
      "wang2023retrievalaugmented",
      "mend",
      "calinet",
      "mlake",
      "mllm4mspeech",
      "iclchangbaobao",
      "memit",
      "memprompt",
      "grace",
      "wang2024easyedit",
      "editing-factual-in-LM",
      "TPatcher"
    ],
    "metrics": {
      "R@5": 0.041666666666666664,
      "R@10": 0.125,
      "R@20": 0.16666666666666666,
      "MRR": 0.2,
      "hits": 5,
      "total_relevant": 24
    },
    "score": 0.11416666666666667,
    "timestamp": "2025-12-18T10:56:18.066234"
  },
  {
    "query": "Introduction \\label{Introduction} \\begin{figure}[t] \\centering \\includegraphics[width=1\\linewidth]{images/scatter4.pdf} \\caption{An overview of our proposed method. We introduce a spatial annealing strategy for a pre-filtering designed efficient NeRF architecture. With the addition of as few as one line of code, our technique substantially improves the rendering quality of the base architecture, outperforming TriMipRF by nearly",
    "paper_id": "2406.07828",
    "retrieved_ids": [
      "kirillov2020pointrend",
      "litman2020scatter",
      "metzer2022latentnerf",
      "nerfstudio",
      "chen2023dreg",
      "chen2023mobilenerf",
      "Cohen-Bar_2023_setthescene",
      "song2023efficient",
      "b8",
      "hu2023tri",
      "wang2023nerf",
      "zhang2022differentiable",
      "zhao2019object",
      "Kondo2021VaxNeRFRT",
      "yang2023freenerf",
      "kim2022infonerf",
      "pumarola2021d",
      "deng2023nerdi",
      "panopticnerf",
      "pureclipnerf",
      "rebain2021derf",
      "hedman2021baking",
      "verbin2022ref",
      "haque2023instruct",
      "nguyen2022snerf",
      "mao2023guided",
      "deng2023compressing",
      "maini2023t",
      "jain2021putting",
      "yan2023multi",
      "jang2024model_Stock",
      "Sharath",
      "Joo",
      "Wieland",
      "zhu2023hifa",
      "shen2024finetuning",
      "sun2022improved",
      "liu2024glyph",
      "brack2023leditspp",
      "poole2022dreamfusion",
      "wei2024meshlrm",
      "voynov2023p+",
      "li2022compressing",
      "mikaeili2023sked",
      "2208.05516",
      "chen2023gaussianeditor",
      "zhiwen",
      "nguyen2024bellman",
      "hao2023optimizing",
      "two_stage",
      "jo2024identifying",
      "wimbauer2023cache",
      "bolya2023token",
      "lee2023syncdiffusion",
      "si2024freeu"
    ],
    "relevant_ids": [
      "hu2023tri",
      "rematas2022urban",
      "jain2021putting",
      "park2021nerfies",
      "barron2021mip",
      "wang2023digging",
      "li2023neuralangelo",
      "wang2021ibrnet",
      "wang2023pet",
      "gu2023nerfdiff",
      "chen2021geosim",
      "fang2022fast",
      "roessle2022dense",
      "muller2022instant",
      "kwak2023geconerf",
      "poole2022dreamfusion",
      "chen2022tensorf",
      "mildenhall2021nerf",
      "cao2022fwd",
      "long2022sparseneus",
      "liu2022neural",
      "kim2022infonerf",
      "radford2021learning",
      "verbin2022ref",
      "barron2022mip",
      "shue20233d",
      "lao2024corresnerf",
      "chibane2021stereo",
      "ziyang2023snerf",
      "deng2022depth",
      "yang2023freenerf",
      "yu2021pixelnerf",
      "oechsle2021unisurf",
      "chen2021mvsnerf",
      "trevithick2021grf",
      "truong2023sparf",
      "lin2023vision",
      "wang2023sparsenerf",
      "liu2023robust",
      "wei2021nerfingmvs",
      "barron2023zip",
      "mildenhall2022nerf",
      "martin2021nerf",
      "hong2023lrm",
      "chen2023single",
      "niemeyer2022regnerf",
      "wang2021neus",
      "xian2021space"
    ],
    "metrics": {
      "R@5": 0.0,
      "R@10": 0.020833333333333332,
      "R@20": 0.0625,
      "MRR": 0.1,
      "hits": 6,
      "total_relevant": 48
    },
    "score": 0.03625,
    "timestamp": "2025-12-18T10:56:21.908068"
  },
  {
    "query": "Introduction Huge advances have recently been made in the application of convolutional networks to the localization of objects in images, allowing real-time deployment of such models even with restrictive hardware limitations such as on mobile devices. In applications which call for a high level of scene understanding, such as robotics",
    "paper_id": "1807.07674",
    "retrieved_ids": [
      "vgg",
      "howard2017mobilenets",
      "peleenet",
      "sermanet2013overfeat",
      "kendall2015posenet",
      "huang2016speed",
      "kim2015compression",
      "kang2016object",
      "Gidaris_2016",
      "zeng2019pgch",
      "zhou2014object",
      "cpm",
      "Tompson2014Joint",
      "GCN",
      "minaee2020image",
      "agrawal2014analyzing",
      "jiang2022model",
      "zhou2016learning",
      "zhang-shellnet-iccv19",
      "shaker2023swiftformer",
      "deeplabv1",
      "bency2016weakly",
      "lim2020federated",
      "HRNet",
      "szegedy2016rethinking",
      "swiftnet",
      "memory_guided",
      "alam2020survey",
      "graham20183d",
      "xie2021neural",
      "iandola2014densenet",
      "Lambdanetworks",
      "ref14",
      "yang2023learning",
      "li2022efficientformer",
      "orig",
      "jacob2018quantization",
      "roddick2020predicting",
      "gu2017deep",
      "seichter2022efficient",
      "Charles_2017",
      "sandler2018mobilenetv2",
      "baskin2021nice",
      "YuKF17",
      "figurnov2017spatially",
      "valin2019lpcnet",
      "jiang2023rtmpose",
      "vasu2023fastvit",
      "lite_transformer",
      "aguinaldo2019compressing",
      "lin2024awq"
    ],
    "relevant_ids": [
      "associative",
      "instancecut",
      "Deeplabv3",
      "depth-layering",
      "bridging",
      "Deeplabv3+",
      "Retinanet",
      "proposal-free",
      "discriminative",
      "Unet",
      "DCM",
      "cascades",
      "masklab",
      "SSD",
      "recurrent-embedding",
      "FCIS",
      "YOLO9000",
      "FCN",
      "MaskRCNN",
      "metric",
      "PersonLab"
    ],
    "metrics": {
      "R@5": 0.0,
      "R@10": 0.0,
      "R@20": 0.0,
      "MRR": 0.0,
      "hits": 0,
      "total_relevant": 21
    },
    "score": 0.0,
    "timestamp": "2025-12-18T10:56:23.916082"
  },
  {
    "query": "Introduction Graph modeling has recently received broad interest {because of} the increasing number of non-Euclidean data that needs to be analyzed across various areas, including social networks, physics, and bioinformatics<|cite_0|>. The graph neural network (GNN)<|cite_1|>, a deep learning-based method, has been reported to be a powerful tool for graph representation",
    "paper_id": "2110.13567",
    "retrieved_ids": [
      "xu2018powerful",
      "bronstein2017geometric",
      "gin",
      "liu2020towards",
      "hamilton2017representation",
      "gnn",
      "Zhou2018",
      "zhao2021data",
      "Zonghan2019",
      "you2020graph",
      "zhu2021survey",
      "Zhang2017NetworkRL",
      "Sato2020ASO",
      "xu2018jknet",
      "GCN",
      "chen2019powerful",
      "benchmarking-gnn",
      "Jaume2019",
      "grohewl",
      "dwivedi2020benchmarking",
      "10.1145/3394486.3403237",
      "random-features",
      "graphite",
      "morris2019weisfeiler",
      "han2022g",
      "wu2021graph",
      "higher-order",
      "monti2017geometric",
      "frasca2020sign",
      "liu2019hyperbolic",
      "liu2024neural",
      "wang2022lifelong",
      "pareja2020evolvegcn",
      "chenWHDL2020gcnii",
      "shao2022distributed",
      "pei2020geom",
      "wang2019heterogeneous",
      "levie2017cayleynets",
      "atwood2016diffusion",
      "Yang2018GraphRF",
      "brannon2023congrat",
      "zhou2021overcoming",
      "oono2019graph"
    ],
    "relevant_ids": [
      "hu2019strategies",
      "sun2019infograph",
      "hamilton2017representation",
      "hassani2020contrastive",
      "li2020deepergcn",
      "hamilton2017inductive",
      "zhang2020motif",
      "kipf2016semi",
      "velivckovic2018deep",
      "rong2020self",
      "hu2020gpt",
      "you2020graph",
      "liu2020towards",
      "velivckovic2017graph",
      "liu2020self"
    ],
    "metrics": {
      "R@5": 0.13333333333333333,
      "R@10": 0.2,
      "R@20": 0.2,
      "MRR": 0.25,
      "hits": 3,
      "total_relevant": 15
    },
    "score": 0.18833333333333335,
    "timestamp": "2025-12-18T10:56:26.003770"
  },
  {
    "query": "Introduction Federated learning (FL) is a collaborative learning technique to build machine learning (ML) models from data distributed among several participants (\"agents\"). The objective is to generate a common, robust model not by exchanging the data between agents, but rather through exchanging parameter updates for the common model that all",
    "paper_id": "2110.06978",
    "retrieved_ids": [
      "li2020federated",
      "yang2019federated",
      "Donahue2020stable_coalitions",
      "kulkarni2020survey",
      "briggs2020federated",
      "lim2020federated",
      "bonawitz2019towards",
      "li2021fedh2l",
      "diao2020heterofl",
      "Fedlearning_edge_1",
      "itahara2020distillation",
      "sattler2019clustered",
      "lin2020ensemble",
      "kairouz2021advances",
      "nishio2019client",
      "perazzone2022communication",
      "hanzely2020federated",
      "thapa2020splitfed",
      "xiong2022feddm",
      "yu2020salvaging",
      "fallah_personalized_MAML_2020",
      "wuf2022edcg",
      "zhang2021personalized",
      "jeong2018communication",
      "wu2021federated",
      "konevcny2016federated",
      "jiang2019improving",
      "li2019fedmd",
      "liu2020privacy",
      "zhu2021data",
      "fedsam-icml",
      "abad2020hierarchical",
      "chen2020wireless",
      "liu2020accelerating",
      "wang2019federated",
      "grativol2023federated",
      "nguyen2020fast",
      "chen2018federated",
      "reddi2020adaptive",
      "chang2019cronus",
      "mohri2019agnostic",
      "grimberg2021optimal",
      "seo2020federated",
      "melis2019exploiting",
      "DingAssist",
      "lu2023robust"
    ],
    "relevant_ids": [
      "Finn2017",
      "kairouz2019_FL_advances_open_problems",
      "zhang2021personalized",
      "deng2020apfl",
      "li_ditto_FL_2021",
      "grimberg2021optimal",
      "karimireddy2020scaffold",
      "Smith2017MOCHA",
      "mcmahan2017fedavg",
      "kulkarni2020survey",
      "Donahue2020stable_coalitions",
      "fallah_personalized_MAML_2020"
    ],
    "metrics": {
      "R@5": 0.16666666666666666,
      "R@10": 0.16666666666666666,
      "R@20": 0.16666666666666666,
      "MRR": 0.3333333333333333,
      "hits": 5,
      "total_relevant": 12
    },
    "score": 0.21666666666666667,
    "timestamp": "2025-12-18T10:56:27.822408"
  },
  {
    "query": "Introduction Self-attention-based models, especially vision transformers \\citep[ViTs; Figure \\ref{fig:vit};][]{dosovitskiy2020image}, are an alternative to convolutional neural networks (CNNs) to learn visual representations. Briefly, ViT divides an image into a sequence of non-overlapping patches and then learns inter-patch representations using multi-headed self-attention in transformers<|cite_0|>. The general trend is to increase the number",
    "paper_id": "2110.02178",
    "retrieved_ids": [
      "li2021can",
      "do-VTs-see-like-CNNs",
      "park2023self",
      "park2022vision",
      "khan2021transformers",
      "han2020survey",
      "chu2021twins",
      "li2021efficient",
      "naseer2021intriguing",
      "wang2023closer",
      "tnt",
      "zhou2022understanding",
      "Caron:2021:EPS",
      "chen2022regionvit",
      "yang2021lite",
      "dino",
      "dat",
      "lin2022cat",
      "zhou2021deepvit",
      "yao2022wave",
      "tu2022maxvit",
      "mao2022towards",
      "dosovitskiy2020vit",
      "jelassi2022vision",
      "vit",
      "lee2021vision",
      "wu2021pale",
      "xu2021vitae",
      "chen2021crossvit",
      "meng2022adavit",
      "Hong2022RepresentationSF",
      "paul2022vision",
      "mehta2021mobilevit",
      "qin2021understanding",
      "xiao2023patch-wise",
      "DUAL-VIT",
      "lee2022mpvit",
      "Lambdanetworks",
      "xcit",
      "patchesallyouneed",
      "wang2022anti",
      "suchengCVPR22",
      "gberta_2021_ICML",
      "melas2021resmlp"
    ],
    "relevant_ids": [
      "touvron2021training",
      "heo2021rethinking",
      "zhou2021deepvit",
      "vaswani2017attention",
      "tan2019efficientnet",
      "d2021convit",
      "tan2019mixconv",
      "howard2019searching",
      "ma2018shufflenet",
      "mehta2019espnetv2",
      "sandler2018mobilenetv2",
      "rao2021dynamicvit",
      "xiao2021early",
      "he2016deep",
      "dai2021coatnet",
      "ranftl2021vision",
      "wu2021cvt",
      "touvron2021going",
      "chen2017rethinking",
      "tan2019mnasnet",
      "wang2021pyramid",
      "howard2017mobilenets",
      "chollet2017xception",
      "zhong2020random",
      "srinivas2021bottleneck",
      "russakovsky2015imagenet",
      "chen2021mobile",
      "dosovitskiy2020image"
    ],
    "metrics": {
      "R@5": 0.0,
      "R@10": 0.0,
      "R@20": 0.03571428571428571,
      "MRR": 0.05263157894736842,
      "hits": 1,
      "total_relevant": 28
    },
    "score": 0.015789473684210523,
    "timestamp": "2025-12-18T10:56:30.576394"
  },
  {
    "query": "Introduction \\begin{figure*}[t] \\centering \\subfigure[FPN]{ \\includegraphics[width=0.22\\linewidth]{fa.pdf}} \\label{fig:fpna} \\subfigure[PANet]{ \\includegraphics[width=0.2599\\linewidth]{fb.pdf}} \\label{fig:fpnb} \\subfigure[BiFPN]{ \\includegraphics[width=0.2599\\linewidth]{fc.pdf}} \\label{fig:fpnc} \\subfigure[RevFP (Ours)]{ \\includegraphics[width=0.22\\linewidth]{fd.pdf}} \\label{fig:fpnd} \\caption{Illustration of different feature pyramid designs, including (a) FPN, (b) PANet, (c) BiFPN, (d) RevFP. FPN is limited by the unidirectional top-down information flow. PANet and BiFPN stack multiple feature pyramid layers to allow",
    "paper_id": "2110.12130",
    "retrieved_ids": [
      "kirillov2019pafpn",
      "DeGeus2018a",
      "chen2020feature",
      "BMVC2016_87",
      "lin2017fpn",
      "yang2020feature",
      "frido",
      "tang2022arbitrary",
      "wu2020visual",
      "han2016deep",
      "ghiasi2019fpn",
      "nguyen2020global",
      "li2018pyramid_attention",
      "panet",
      "valanarasu2021kiu",
      "Pascanu2014",
      "yang2023tensor",
      "scenecomposer",
      "pfenet",
      "chen2023pixartalpha",
      "murphy2021implicit",
      "xie2021fignerf",
      "ding2024patched",
      "kirillov2020pointrend",
      "guo2020augfpn",
      "Joo",
      "Xie:2021:SSE",
      "nguyen2020wide",
      "fang2020multi",
      "liu2018path",
      "dong2021cswin",
      "bhat2023loosecontrol",
      "yaida2020non",
      "Lin2020GAS",
      "cheng2020higherhrnet",
      "chen2021you",
      "dani2023devil",
      "Mao:ICCV15",
      "egeunet",
      "chen2010super",
      "fang2020densely",
      "li2022efficient",
      "tian2019decoders",
      "hurtik2022poly",
      "yang2023reco",
      "nair2023unite",
      "tan2020efficientdet",
      "yan2023multi",
      "tnt",
      "meng2020pruning"
    ],
    "relevant_ids": [
      "lin2019tsm",
      "zhou2019objects",
      "tian2019fcos",
      "girshick2015fast",
      "li2020netnet",
      "redmon2018yolov3",
      "zhu2019feature",
      "he2017mask",
      "wu2018shift",
      "lin2017focal",
      "kong2018deep",
      "lin2017feature",
      "pang2019libra",
      "qiao2021detectors",
      "tan2020efficientdet",
      "zhao2019m2det",
      "chen2019all",
      "ren2016faster",
      "duan2019centernet",
      "ghiasi2019fpn",
      "law2018cornernet",
      "guo2020augfpn",
      "you2020shiftaddnet",
      "liu2018path",
      "wang2020scale"
    ],
    "metrics": {
      "R@5": 0.0,
      "R@10": 0.0,
      "R@20": 0.04,
      "MRR": 0.09090909090909091,
      "hits": 4,
      "total_relevant": 25
    },
    "score": 0.02727272727272727,
    "timestamp": "2025-12-18T10:56:36.119516"
  },
  {
    "query": "Introduction It is well-known that human can learn new tasks without forgetting old ones. However, Deep Neural Networks (DNN) may forget the ``old'' knowledge when it is trained to learn a single task. For example, given a backbone DNN model, conventional model fine-tuning for new tasks could easily result in",
    "paper_id": "2110.00908",
    "retrieved_ids": [
      "li2017learning",
      "toneva2018an",
      "Arpit2017",
      "hooker2019compressed",
      "chen2020recall",
      "aljundi2018memory",
      "mazzia2023survey",
      "kirkpatrick2017overcoming",
      "hung2019compacting",
      "shin2017continual",
      "von2019continual",
      "rusu2016progressive",
      "zenke2017continual",
      "liu2021overcoming",
      "shmelkov2017incremental_ilod",
      "li2019learn",
      "xu2018reinforced_IL",
      "masarczyk2020reducing",
      "chen2016net2net",
      "dodge2017study",
      "fineprune2018",
      "kaiser2017one",
      "davidrolnicketal2018",
      "sinitsin2020editable",
      "wang2021afec",
      "liu2019multi",
      "goodfellow2013empirical",
      "schwarzschild2021can",
      "chen2017zoo",
      "zhai2024fine",
      "lee2017overcoming",
      "aljundi2017expert",
      "End-To-End",
      "zhu2020modifying",
      "veniat2020efficient",
      "guo2020adafilter",
      "douillard2022dytox",
      "gem",
      "mallya2018packnet",
      "lee2019overcoming",
      "wang2021knowledge",
      "li2019rilod",
      "zhang2020side",
      "zhou2024expandable",
      "peng2020faster",
      "zhao2020maintaining",
      "DistillIncre",
      "yu2020semantic"
    ],
    "relevant_ids": [
      "veniat2020efficient",
      "pomponi2020efficient",
      "zenke2017continual",
      "wen2020autogrow",
      "rusu2016progressive",
      "riemer2018learning",
      "aljundi2018memory",
      "yoon2017lifelong",
      "yuan2020growing",
      "rosenfeld2018incremental",
      "rebuffi2017icarl",
      "yoon2019scalable",
      "dhar2019learning",
      "hung2019compacting",
      "ritter2018online",
      "li2019learn",
      "kirkpatrick2017overcoming",
      "lee2017overcoming",
      "wei2016network",
      "shin2017continual",
      "li2017learning",
      "schwarz2018progress",
      "fernando2017pathnet",
      "chaudhry2018riemannian"
    ],
    "metrics": {
      "R@5": 0.041666666666666664,
      "R@10": 0.20833333333333334,
      "R@20": 0.3333333333333333,
      "MRR": 1.0,
      "hits": 10,
      "total_relevant": 24
    },
    "score": 0.37916666666666665,
    "timestamp": "2025-12-18T10:56:38.108740"
  },
  {
    "query": "Introduction Invertible flow networks emerged as powerful deep learning models to learn maps between distributions<|cite_0|>. They generate high-quality samples<|cite_1|> and facilitate solving scientific inference problems<|cite_2|>. By design, however, invertible flows are bijective and may not be a natural choice when the target distribution has low-dimensional support. This problem can be",
    "paper_id": "2110.04227",
    "retrieved_ids": [
      "NalisnickMTGL19",
      "ardizzone2018analyzing",
      "jacobsen2018irevnet",
      "cunningham2020normalizing",
      "kruse2021benchmarking",
      "papamakarios2019normalizing",
      "bronstein2017geometric",
      "liu2022flow",
      "kothari2021trumpets",
      "kingma2016improving",
      "teshima2020couplingbased",
      "zimmermann2022variational",
      "kobyzev2020normalizing",
      "rezende15variational",
      "i-resnet",
      "ishikawa2022universal",
      "benchmarking-gnn",
      "mena2018learning",
      "jain2023gflownets",
      "lipman2022flow",
      "liu2018learning",
      "kingma2018glow",
      "Yin_2020_CVPR",
      "wu2018moleculenet",
      "kruse2019hint",
      "dwivedi2020benchmarking",
      "eqFlows",
      "sorrenson2019disentanglement",
      "chen2019residual",
      "creswell2018inverting",
      "deleu2024discrete",
      "durkan2019cubic",
      "durkan2019neural",
      "nguyen2016synthesizing",
      "koehler2021representational",
      "teshima2020coupling",
      "yang2019pointflow",
      "hernandez2023multi",
      "jaini2019sum",
      "albergo2023building",
      "sorrenson2024lifting",
      "su2023dual",
      "mobahi2020self",
      "zhu2020domain",
      "zhu2017toward"
    ],
    "relevant_ids": [
      "teshima2020coupling",
      "kruse2019hint",
      "kingma2016improving",
      "kruse2021benchmarking",
      "cunningham2020normalizing",
      "grathwohl2018ffjord",
      "kothari2021trumpets",
      "bora2017compressed",
      "ardizzone2018analyzing",
      "huang2018neural",
      "papamakarios2019normalizing",
      "brehmer2020flows",
      "kingma2018glow",
      "sun2020deep",
      "durkan2019cubic",
      "jaini2019sum",
      "kobyzev2020normalizing"
    ],
    "metrics": {
      "R@5": 0.17647058823529413,
      "R@10": 0.35294117647058826,
      "R@20": 0.4117647058823529,
      "MRR": 0.5,
      "hits": 12,
      "total_relevant": 17
    },
    "score": 0.3264705882352941,
    "timestamp": "2025-12-18T10:56:40.212900"
  },
  {
    "query": "Introduction Many real-world problems, in our case autonomous driving, can be modeled as high-dimensional control problems. In recent years, there has been much research effort to solve such problems in an end-to-end fashion. While solutions based on imitation learning try to mimic the behavior of an expert, approaches based on",
    "paper_id": "2110.00808",
    "retrieved_ids": [
      "chen2023end",
      "codevilla2018end",
      "cheng2023rethinking",
      "roach",
      "liu2022improved",
      "huang2022efficient",
      "codevilla2019exploring",
      "pini2023safe",
      "hu2023gaia",
      "SimNet2021",
      "lu2023imitation",
      "Chen2021InterpretableLearning",
      "prakash2021multi",
      "chitta2021neat",
      "hu2022st",
      "scheel2022urban",
      "dulac2020empirical",
      "de2019causal",
      "liang2018cirl",
      "Bewley2018LearningLabels",
      "jiang2023vad",
      "wang2019deep",
      "ltd",
      "Schulman2016a",
      "OKelly2018ScalableEA",
      "Li2019AADSAA",
      "JLB",
      "toromanoff2020end",
      "hu2023imitation",
      "sharma2023self",
      "Nagabandi2018",
      "kendall2019learning",
      "sharma2020emergent",
      "zhu2023learning",
      "chatzilygeroudis2019survey",
      "shan2018pixel",
      "dosovitskiy2017carla",
      "sharma2022state",
      "paster2020planning",
      "matas2018sim",
      "chaffre2020sim",
      "popov2017data",
      "thor_target_driven",
      "Samvelyan2019TheSM",
      "Foerster2018CounterfactualMP"
    ],
    "relevant_ids": [
      "huang2018multimodal",
      "sim2sim",
      "Janner2019WhenOptimization",
      "unit",
      "dqn",
      "Tassa2018DeepMindSuite",
      "wang2019learning",
      "Liu2017UnsupervisedNetworks",
      "pla_net",
      "Bewley2018LearningLabels",
      "Chua2018DeepModels",
      "rl_cycle_gan",
      "dreamer",
      "ltd",
      "Kurutach2018Model-EnsembleOptimization",
      "Chen2021InterpretableLearning",
      "mu_zero",
      "Brockman2016OpenAIGym",
      "ha2018world",
      "volpi2018generalizing",
      "dreamer_v2",
      "toromanoff2020end",
      "Hafner2020DreamImagination",
      "Zhu2017UnpairedNetworks",
      "Bellemare2013TheAgents",
      "random_convolutions",
      "Ganin2015UnsupervisedBackpropagation",
      "James2019Sim-to-realNetworks",
      "qiao2020learning",
      "Hafner2018LearningPixels"
    ],
    "metrics": {
      "R@5": 0.0,
      "R@10": 0.0,
      "R@20": 0.06666666666666667,
      "MRR": 0.08333333333333333,
      "hits": 4,
      "total_relevant": 30
    },
    "score": 0.024999999999999998,
    "timestamp": "2025-12-18T10:56:42.115497"
  },
  {
    "query": "Introduction Self-supervised learning aims to learn useful representations of the input data without relying on human annotations. Recent advances in self-supervised visual representation learning based on joint embedding methods<|cite_0|> show that self-supervised representations have competitive performances compared with supervised ones. These methods generally aim to learn representations invariant to data",
    "paper_id": "2110.09348",
    "retrieved_ids": [
      "Gidaris2019",
      "zhang2022leverage",
      "misra2020pirl",
      "compress",
      "tsai2020self",
      "bendidi2023free",
      "data2vec",
      "Schiappa-ACM-2023",
      "fei2023self",
      "Bardes2021VICRegVR",
      "purushwalkam2020demystifying",
      "Lee2020PredictingWY",
      "doersch2017multi",
      "simclr",
      "Tian2021UnderstandingSL",
      "ze2023rl3d",
      "relicv1",
      "liu2020self",
      "zhao2020distilling",
      "newell2020how",
      "ermolov2020whitening",
      "miech2020end",
      "kim2020mixco",
      "relic",
      "yang2022survey",
      "lee2020self",
      "mu2021slip",
      "chen2022semi",
      "hendrycks2019using",
      "Assran2021SemiSupervisedLO",
      "zhuang2019local",
      "o2020unsupervised",
      "Modelbased3H",
      "ye2019unsupervised",
      "jing2021understanding",
      "odin",
      "Tosh2021ContrastiveLM",
      "iclr2020sela",
      "ericsson2021well",
      "henaff2022object",
      "assran2022masked",
      "CCA_SSG"
    ],
    "relevant_ids": [
      "Caron2018DeepCF",
      "HaoChen2021ProvableGF",
      "Saxe2019AMT",
      "Li2021PrototypicalCL",
      "Dwibedi2021WithAL",
      "Gunasekar2018ImplicitRI",
      "He2020MomentumCF",
      "Oord2018RepresentationLW",
      "Ji2019GradientDA",
      "grill2020byol",
      "Chen2020ImprovedBW",
      "Assran2021SemiSupervisedLO",
      "Neyshabur2019TowardsUT",
      "Barrett2021ImplicitGR",
      "Tian2021UnderstandingSL",
      "Hua2021OnFD",
      "Arora2019ATA",
      "Lee2020PredictingWY",
      "he2016resnet",
      "chen2020simsiam",
      "chen2020simclr",
      "caron2020swav",
      "zbontar2021barlow",
      "Tosh2021ContrastiveLM",
      "Arora2019ImplicitRI",
      "Caron2021EmergingPI",
      "Soudry2018TheIB",
      "misra2020pirl",
      "Misra2020SelfSupervisedLO",
      "Bardes2021VICRegVR"
    ],
    "metrics": {
      "R@5": 0.03333333333333333,
      "R@10": 0.06666666666666667,
      "R@20": 0.13333333333333333,
      "MRR": 0.3333333333333333,
      "hits": 6,
      "total_relevant": 30
    },
    "score": 0.13333333333333333,
    "timestamp": "2025-12-18T10:56:44.127561"
  },
  {
    "query": "Introduction Machine learning technologies are widely popular and we can find them as part of the services provided by major companies such as Amazon or Google. It is well-known that machine learning models memorize information about the data they are trained with, and this makes them vulnerable to different attacks<|cite_0|>.",
    "paper_id": "2110.05524",
    "retrieved_ids": [
      "song2017machine",
      "GDG17",
      "SM21",
      "SSSS17",
      "song2021systematic",
      "SZHBFB19",
      "salem2018ml",
      "chakraborty2018adversarial",
      "papernot2017practical",
      "pang2021security",
      "akhtar2018threat",
      "Big+13",
      "b27",
      "carlini2021extracting",
      "Papernot2018",
      "mireshghallah2022memorization",
      "papernot2016transferability",
      "kurakin_nips2017competition",
      "kandpal2022deduplicating",
      "xu2019adversarial",
      "overview",
      "yeom2017privacy",
      "b2",
      "mahloujifar2019curse",
      "boucher2021bad",
      "b19",
      "bourtoule2021machine",
      "b3",
      "biggio_2018",
      "yang2019federated",
      "li2021backdoor_shrinkpad",
      "NSH18",
      "li2023backdoorbox",
      "b13",
      "bulusu2020anomalous",
      "nasr2018machine",
      "geiping2021witches",
      "Li2020BackdoorLA",
      "b26",
      "abadi2016deep",
      "li2018textbugger"
    ],
    "relevant_ids": [
      "nasr2018machine",
      "yu2019differentially",
      "long2018understanding",
      "jayaraman_evaluating_2019",
      "shokri2016membership",
      "chaudhuri2011differentially",
      "tramer2020differentially",
      "song2017machine",
      "song2021systematic",
      "salem2018ml",
      "abadi2016deep",
      "jia2019memguard",
      "li2020membership",
      "choquette2021label",
      "carlini2019secret",
      "yeom2017privacy"
    ],
    "metrics": {
      "R@5": 0.125,
      "R@10": 0.1875,
      "R@20": 0.1875,
      "MRR": 1.0,
      "hits": 6,
      "total_relevant": 16
    },
    "score": 0.40625,
    "timestamp": "2025-12-18T10:56:45.858595"
  },
  {
    "query": "Introduction In many real-world applications such as robotics there can be large disparities in the size of agent's observation space (for example, the image generated by agent's camera), and a much smaller latent state space (for example, the agent's location and orientation) governing the rewards and dynamics. This size disparity",
    "paper_id": "2110.08847",
    "retrieved_ids": [
      "tabacof2016exploring",
      "gibson",
      "Hafner2018LearningPixels",
      "kim2021exploiting",
      "bousmalis2023robocat",
      "pla_net",
      "park2023understanding",
      "liu2023agentbench",
      "peng2018sim",
      "park2023generative",
      "gelada2019deepmdp",
      "shridhar2022perceiver",
      "zhang2022lovis",
      "hill2019environmental",
      "Hafner2020DreamImagination",
      "pillai2017towards",
      "chaffre2020sim",
      "lexa2021",
      "kinose2022multi",
      "paster2020planning",
      "chi2023diffusion",
      "iqbal2019actor",
      "Mees2022WhatMI",
      "dreamer",
      "Trauble_the-role-of_2022",
      "Sunehag2018ValueDecompositionNF",
      "huang-action-2022",
      "pathak2017curiosity",
      "li20213d",
      "chatzilygeroudis2019survey",
      "finn2016deep",
      "kim2022automating",
      "gilmer2018adversarial",
      "salzmann2020trajectron++",
      "ivanovic2020multimodal",
      "heravi2022visuomotor",
      "dreamer_v2",
      "zhang2020learning",
      "khanna2023habitat",
      "imagine_explore",
      "ding2020mutual",
      "zhao2020mutual",
      "Schulman2016a",
      "sharma2022state",
      "luo2016understanding",
      "sun2020scalability",
      "yahya2017collective",
      "driess2022reinforcement",
      "majumdar2022zson",
      "thor_target_driven",
      "du2024learning"
    ],
    "relevant_ids": [
      "pathak2017curiosity",
      "paster2020planning",
      "laskin2020curl",
      "tang2017exploration",
      "dietterich2018discovering",
      "hafner2019learning",
      "gregor2016variational",
      "du2019provably",
      "misra2020kinematic",
      "zhang2020learning",
      "jiang2017contextual",
      "gelada2019deepmdp",
      "agarwal2020flambe",
      "burda2018large"
    ],
    "metrics": {
      "R@5": 0.0,
      "R@10": 0.0,
      "R@20": 0.14285714285714285,
      "MRR": 0.09090909090909091,
      "hits": 4,
      "total_relevant": 14
    },
    "score": 0.02727272727272727,
    "timestamp": "2025-12-18T10:56:48.061908"
  },
  {
    "query": "Introduction Over the past decade, deep learning has made tremendous progress in multiple fields, especially in vision<|cite_0|> and natural language processing<|cite_1|>. However, several important issues remain unsolved, including the ability to generalize well to novel, out-of-distribution data<|cite_2|>. A particularly challenging situation involves simultaneous changes at test time in both the",
    "paper_id": "2110.06990",
    "retrieved_ids": [
      "zhang2017",
      "2021arXiv210302667A",
      "li2022data",
      "liang2023comprehensive",
      "sun2020test",
      "survey3",
      "ye-et-al:scheme",
      "zhou2021review",
      "yang2022openood",
      "krueger2021out",
      "domain_confusion",
      "sun2019optimization",
      "tan2018survey",
      "menghani2021efficient",
      "zhou-et-al:scheme",
      "farahani2021brief",
      "chen2022bootstrap",
      "poursaeed2021robustness",
      "ben2019demystifying",
      "jin2023time",
      "limitations",
      "r2r",
      "zenke2017continual",
      "song2023comprehensive",
      "chen2022semi",
      "wang2021knowledge",
      "jiang2017mentornet",
      "2020Learning",
      "white2023neural",
      "redko2020survey",
      "chen2018fastgcn",
      "xie2021neural",
      "nuriel2022textadain",
      "cui2022democratizing",
      "Han2020ASO",
      "2020arXiv200708558D",
      "zhang2023slca",
      "deletang2023neural",
      "biggio_2018",
      "clipforge",
      "hou2022batch",
      "mazzia2023survey",
      "Schiappa-ACM-2023",
      "cherti2023reproducible",
      "cao2021open",
      "pang2021image",
      "jakubovitz2018improving",
      "liu2022swin",
      "blalock2020state",
      "pan2020dynamic",
      "campari",
      "lu2023synthetic",
      "chen2020lottery",
      "chen-et-al:scheme"
    ],
    "relevant_ids": [
      "2013arXiv1312.6114K",
      "2021arXiv210300020R",
      "2018arXiv181103259Z",
      "2017arXiv171200409H",
      "2020arXiv201014701H",
      "2020arXiv200301200T",
      "Goodfellow-et-al-2016",
      "2017arXiv170603762V",
      "2021arXiv210305247L",
      "2020arXiv200708558D",
      "2017arXiv170305175S",
      "2020arXiv200514165B",
      "2021arXiv210302667A",
      "2016arXiv160604080V",
      "2021arXiv210201293H",
      "2021arXiv210212092R",
      "alam2020survey",
      "2014arXiv1406.2661G",
      "2020arXiv200108361K"
    ],
    "metrics": {
      "R@5": 0.05263157894736842,
      "R@10": 0.05263157894736842,
      "R@20": 0.05263157894736842,
      "MRR": 0.5,
      "hits": 2,
      "total_relevant": 19
    },
    "score": 0.1868421052631579,
    "timestamp": "2025-12-18T10:56:50.878351"
  },
  {
    "query": "Introduction \\label{sec:intro} The rise of Graph Neural Networks (GNNs) has brought the modeling of complex graph data into a new era. Using message-passing, GNNs iteratively share information between neighbors in a graph to make predictions of node labels, edge labels, or graph-level properties. A number of powerful GNN architectures<|cite_0|> have",
    "paper_id": "2110.14363",
    "retrieved_ids": [
      "gin",
      "xu2018powerful",
      "Zhou2018",
      "you2020graph",
      "zhao2021data",
      "flam2020neural",
      "Zonghan2019",
      "liu2020towards",
      "Sato2020ASO",
      "chen2019powerful",
      "alon2021on",
      "grohewl",
      "higher-order",
      "zhu2021survey",
      "10.1145/3394486.3403237",
      "gnn",
      "morris2019weisfeiler",
      "dehmamy2019understanding",
      "velivckovic2017graph",
      "random-features",
      "wu2021graph",
      "fey2021gnnautoscale",
      "Jaume2019",
      "liu2021elastic",
      "zhang2020gnnguard",
      "gcnlpa",
      "hu2020strategies",
      "pei2020geom",
      "pareja2020evolvegcn",
      "nikolentzos2020k",
      "liu2019hyperbolic",
      "shao2022distributed",
      "zhu2021graph",
      "wang2021certified",
      "pprgo",
      "choi2022finding",
      "ying2018hierarchical",
      "chenWHDL2020gcnii",
      "zhang2019_bgcn",
      "rong2020self",
      "rusch2022g2gating",
      "he2023generalization"
    ],
    "relevant_ids": [
      "hu2020open",
      "zou2019layer",
      "xu2018powerful",
      "chen2018fastgcn",
      "hamilton2017inductive",
      "kipf2016semi",
      "chen2018stochastic",
      "chiang2019cluster",
      "huang2018adaptive",
      "zeng2019graphsaint",
      "velivckovic2017graph"
    ],
    "metrics": {
      "R@5": 0.09090909090909091,
      "R@10": 0.09090909090909091,
      "R@20": 0.18181818181818182,
      "MRR": 0.5,
      "hits": 2,
      "total_relevant": 11
    },
    "score": 0.21363636363636362,
    "timestamp": "2025-12-18T10:56:53.204494"
  },
  {
    "query": "Introduction \\label{introduction} Convolutional Neural Networks (CNN) dominate the learning of visual representations and show effectiveness on various downstream tasks, including image classification, object detection, semantic segmentation, etc. Recently, convolution-free backbones show impressive performances on image classification. Vision Transformer (ViT)<|cite_0|> firstly shows that pure Transformer architecture can attain state-of-the-art performance when",
    "paper_id": "2110.04035",
    "retrieved_ids": [
      "kang2016object",
      "li2022ViTDet",
      "bhojanapalli2021understanding",
      "li2021can",
      "cspnet",
      "wu2021cvt",
      "han2020survey",
      "do-VTs-see-like-CNNs",
      "guo2022cmt",
      "paul2022vision",
      "vgg",
      "gu2022multi",
      "strudel2021segmenter",
      "wang2021pvt",
      "huang2022lightvit",
      "Szegedy2014",
      "li2021involution",
      "uniformer",
      "liu2022more",
      "wu2020visual",
      "naseer2021intriguing",
      "li2022nextvit",
      "ding2021diverse",
      "li2021benchmarking",
      "dosovitskiy2020vit",
      "liu2022convnet",
      "t2tformer",
      "vit",
      "farispaper",
      "peng2021conformer",
      "mao2022towards",
      "T2T",
      "bai2021transformers",
      "botnet",
      "zhou2021deepvit",
      "chu2021twins",
      "chen2022vision",
      "Lee2021ViTGANTG",
      "jiang2021transgan",
      "Yan2021ConTNetWN",
      "heo2021rethinking",
      "yang2021lite",
      "lee2022mpvit",
      "ding2023unireplknet",
      "liu2021swin",
      "Ranftl2021dpt",
      "Zheng20213DHP"
    ],
    "relevant_ids": [
      "carafe++",
      "yuan2021incorporating",
      "coco",
      "resmlp",
      "cvt",
      "lip",
      "mixer",
      "inception",
      "zhang2019shiftinvar",
      "efficientnet",
      "pvt",
      "deit",
      "vit",
      "resnet",
      "dpp",
      "convit"
    ],
    "metrics": {
      "R@5": 0.0,
      "R@10": 0.0,
      "R@20": 0.0,
      "MRR": 0.03571428571428571,
      "hits": 1,
      "total_relevant": 16
    },
    "score": 0.010714285714285713,
    "timestamp": "2025-12-18T10:56:55.933187"
  },
  {
    "query": "Introduction Learning intrinsic low-dimensional representations from high-dimensional data without human supervision, i.e., unsupervised representation learning (URL), is a long-standing problem. As the high-dimensional data is usually highly redundant and non-Euclidean, a widespread assumption is that data lies on a low-dimensional ambient space. Given a finite set of samples $X=[x_1,...,x_n]\\in \\mathbb{R}^{n\\times",
    "paper_id": "2110.14553",
    "retrieved_ids": [
      "gmmdavid",
      "Li2021PrototypicalCL",
      "Bengio2013representation",
      "Arora2019ATA",
      "niemeyer:dvr:cvpr2020",
      "noroozi2016unsupervised",
      "rotation",
      "shen2022unmix",
      "gelada2019deepmdp",
      "bronstein2017geometric",
      "ermolov2020whitening",
      "yamada2022task",
      "agarwal2020flambe",
      "zbontar2021barlow",
      "bojanowski2017unsupervised",
      "kulkarni2019unsupervised",
      "du2020few",
      "Azabou2021",
      "zhou2020bypassing",
      "Facebook",
      "awasthi2021adversarially",
      "Modi2021modelfree",
      "shih2019video",
      "article1",
      "hu2017learning",
      "hill2016learning",
      "jampani:cvpr:2016",
      "liu2021behavior",
      "WangHLX19",
      "yang2021representation",
      "jing2021understanding",
      "cunningham2020normalizing",
      "shi-et-al:scheme",
      "liu2019large",
      "fei2023self",
      "tancik2020fourier",
      "lrank",
      "Schulman2016a",
      "yang2017towards",
      "Trauble_the-role-of_2022",
      "overview3",
      "baranes2013active",
      "thekumparampil2021sample",
      "shen2021towards",
      "softtriple",
      "dai2016discriminative",
      "elhamifar2013sparse",
      "driess2022reinforcement",
      "oza2019c2ae",
      "subspace",
      "venturi2018spurious",
      "bora2017compressed"
    ],
    "relevant_ids": [
      "iclr2017attkd",
      "eccv2016coloring",
      "cvpr2019rkd",
      "2021Barlow",
      "nips2020swav",
      "2018UMAP",
      "iccv019iic",
      "eccv2016jigsaw",
      "iccv2015relativeloc",
      "eccv2018pkt",
      "cvpr2020noisy",
      "2020simclr",
      "iccv2019sp",
      "cvpr2021simsiam",
      "eccv2020SSKD",
      "eccv2018deepcluster",
      "cvpr2020moco",
      "iclr2020CRD",
      "nips2020byol",
      "iclr2020sela",
      "nips2014kd",
      "cvpr2018dml",
      "iclr2021seed",
      "cvpr2020odc",
      "iclr2018rotation"
    ],
    "metrics": {
      "R@5": 0.0,
      "R@10": 0.0,
      "R@20": 0.0,
      "MRR": 0.0,
      "hits": 0,
      "total_relevant": 25
    },
    "score": 0.0,
    "timestamp": "2025-12-18T10:56:58.764388"
  },
  {
    "query": "Introduction In reinforcement learning (RL), the agent sequentially interacts with the environment and receives reward from it. In many real-world RL problems, the reward function is manually designed to encourage the desired behavior of the agent. Thus, engineers have to change the reward function time by time and train the",
    "paper_id": "2110.06394",
    "retrieved_ids": [
      "dulac2020empirical",
      "kaelbling1996reinforcement",
      "zhu2020ingredients",
      "aubret-survey-2019",
      "lin2023survey",
      "janner2021offline",
      "li2017deep",
      "adeniji2023language",
      "li2023survey",
      "yu-explainable-2023",
      "reddy2019sqil",
      "qin2022neorl",
      "casper2023open",
      "lambert2023history",
      "florensa2018automatic",
      "lee2021pebble",
      "vecerik2017leveraging",
      "eysenbach2017leave",
      "learningtolearn",
      "liu2018reinforcement",
      "eysenbach2020off",
      "sharma2020emergent",
      "sharma2022state",
      "Christiano2017DeepRL",
      "cabi2020scaling",
      "zhao2020mutual",
      "peng2019advantage",
      "kim2022automating",
      "sharma2021autonomous",
      "wang2020reward",
      "zhang2018study",
      "fujimoto2021minimalist",
      "sharma2021vaprl",
      "toromanoff2020end",
      "eureka",
      "jin2020reward",
      "eysenbach2021maximum",
      "huang2022efficient",
      "burda2018large",
      "brandfonbrener2022does",
      "Pang2022RewardGI",
      "wang2020SERL",
      "zhang2020automatic",
      "lu2020reset",
      "pathak2017curiosity",
      "Schulman2017",
      "Zhuang2021ConsequencesOM",
      "imagine_explore",
      "her"
    ],
    "relevant_ids": [
      "liu2020sharp",
      "ayoub2020model",
      "wang2020reward",
      "kaufmann2020adaptive",
      "zanette2020provably",
      "zhang2020nearly",
      "jin2020reward",
      "modi2020sample",
      "zanette2020frequentist",
      "zhou2020provably",
      "zanette2019tighter",
      "jia2020model",
      "menard2020fast",
      "yang2019sample",
      "zanette2020learning",
      "zhou2020nearly",
      "jin2020provably",
      "wang2019optimism"
    ],
    "metrics": {
      "R@5": 0.0,
      "R@10": 0.0,
      "R@20": 0.0,
      "MRR": 0.03333333333333333,
      "hits": 2,
      "total_relevant": 18
    },
    "score": 0.01,
    "timestamp": "2025-12-18T10:57:00.670382"
  },
  {
    "query": "Introduction Aside from model quality, the efficiency<|cite_0|> of a model is often an important aspect to consider and is commonly used to measure the relative utility of different methods. After all, training time spent on accelerators is directly linked to financial costs and environmental impact. Meanwhile, the speed of a",
    "paper_id": "2110.12894",
    "retrieved_ids": [
      "isomobile",
      "teney2022id",
      "menghani2021efficient",
      "treviso2023efficient",
      "theis2016note",
      "Janner2019WhenOptimization",
      "qurating",
      "dehghani2021efficiency",
      "Sharath",
      "tan2019efficientnet",
      "huang2019gpipe",
      "tay2021scale",
      "goodfellow2015efficient",
      "cheng2017survey",
      "Strubell:2019uv",
      "fingpt",
      "qiu2022zerofl",
      "maranjyan2022gradskip",
      "faiz2024llmcarbon",
      "joseph2021going",
      "wang2023surveyfactualitylargelanguage",
      "bo",
      "Simon2019",
      "hendrycks2019augmix",
      "wu2018moleculenet",
      "bucilu2006model",
      "sun2024comprehensive",
      "levy2024same",
      "kilonerf",
      "namburi2023cost",
      "choi2021dance",
      "Ronchi2017Benchmarking",
      "dong2024promptpromptedadaptivestructuredpruning",
      "hagele2024scaling",
      "hooker2020characterising",
      "zhou2023don",
      "chen2022resgrad",
      "dwivedi2020benchmarking",
      "fsrcnn",
      "niemeyer2024radsplat",
      "liu2019roberta",
      "pan202iared2",
      "strubell2019energy",
      "bentham2024chainofthought",
      "newell2020how",
      "blalock2020state",
      "hu2022online"
    ],
    "relevant_ids": [
      "riquelme2021scaling",
      "wightman2021resnet",
      "lee2021fnet",
      "tan2019efficientnet",
      "lan2019albert",
      "shazeer2018mesh",
      "fedus2021switch",
      "simonyan2014very",
      "he2016deep",
      "devlin2018bert",
      "menghani2021efficient",
      "feichtenhofer2019slowfast",
      "xue2021byt5",
      "szegedy2015going",
      "dehghani2018universal",
      "huang2017densely",
      "liu2019roberta",
      "fan2021multiscale",
      "tay2020synthesizer",
      "mehta2020delight"
    ],
    "metrics": {
      "R@5": 0.05,
      "R@10": 0.1,
      "R@20": 0.1,
      "MRR": 0.3333333333333333,
      "hits": 3,
      "total_relevant": 20
    },
    "score": 0.15,
    "timestamp": "2025-12-18T10:57:02.618628"
  },
  {
    "query": "Introduction \\label{sec:introduction} The ability of an agent to learn an informative mapping from complex observations to a succinct representation is one of the essential factors for the success of machine learning in fields such as computer vision, language modeling, and more broadly in deep learning<|cite_0|>. In supervised learning, it is",
    "paper_id": "2110.14798",
    "retrieved_ids": [
      "Bengio2013representation",
      "zhang2020dive",
      "survey3",
      "sun2019optimization",
      "geirhos2021partial",
      "khosla2020supervised",
      "relicv1",
      "trlvln",
      "hong2023learning",
      "pfeiffer2023modular",
      "chen2022weakly",
      "yang2023introduction",
      "kingma2019introduction",
      "yamada2022task",
      "tan2018survey",
      "flow1",
      "Albrecht_2018",
      "bendidi2023free",
      "menghani2021efficient",
      "Awais-arxiv-2023",
      "foerster2016learning",
      "li-etal-2023-zero",
      "chen2022semi",
      "bronstein2017geometric",
      "le2020contrastive",
      "liu2020self",
      "grover2018learning",
      "battaglia2018relational",
      "KARIMI2020101759",
      "ICLRsub2022",
      "Wang_Yu_Yu_Dai_Tsvetkov_Cao_2021",
      "hong2020language",
      "mottaghi2019adversarial",
      "Zheng2023SecretsOR",
      "Kingma2014SemisupervisedLW",
      "Wu_2016_CVPR",
      "white2023neural",
      "wang2021knowledge",
      "TRL",
      "ze2023gnfactor",
      "Ghorbani2019",
      "haochen2022theoretical",
      "wang2023images",
      "beyer2022knowledge",
      "ash2021investigating",
      "greff2020binding",
      "2016arXiv160604080V",
      "mscoco",
      "DiffsionDecisionMaking2022",
      "yoo2022groundtruth",
      "nic",
      "he2024active",
      "vinyals2017matching",
      "muller2019does",
      "perarnau2016invertible",
      "smilkov2017smoothgrad"
    ],
    "relevant_ids": [
      "Jin2020linear",
      "AgarwalKKS20",
      "Zanette2020low",
      "Modi2021modelfree",
      "hao2020adaptive",
      "DuKJAD019",
      "Bengio2013representation",
      "lu2021power",
      "papini2021leveraging"
    ],
    "metrics": {
      "R@5": 0.1111111111111111,
      "R@10": 0.1111111111111111,
      "R@20": 0.1111111111111111,
      "MRR": 1.0,
      "hits": 1,
      "total_relevant": 9
    },
    "score": 0.37777777777777777,
    "timestamp": "2025-12-18T10:57:04.977034"
  },
  {
    "query": "Introduction \\label{sec:intro} Graph Convolutional Networks (GCNs) has gained popularity in recent years. Researchers have shown that non-localized multi-hop GCNs<|cite_0|> have better performance than localized one-hop GCNs<|cite_1|>. However, in Convolutional Neural Networks (CNNs), the localized $3 \\times 3$ kernels' expressiveness has been shown in image recognition both experimentally<|cite_2|> and theoretically<|cite_3|>. These",
    "paper_id": "2110.07141",
    "retrieved_ids": [
      "Zhou2018",
      "Zonghan2019",
      "sgc",
      "gin",
      "nazir2021survey",
      "article34",
      "li2018deeper",
      "liu2020towards",
      "liu2021sampling",
      "pei2020geom",
      "zeng2019pgch",
      "clustergcn",
      "10.1145/3292500.3330925",
      "li2019deepgcns",
      "chenWHDL2020gcnii",
      "fastgcn",
      "pareja2020evolvegcn",
      "defferrard2016convolutional",
      "chen2018fastgcn",
      "fagcn",
      "ladies",
      "rong2019dropedge",
      "dehmamy2019understanding",
      "luan2019break",
      "yidingNeurIPS20",
      "grohewl",
      "higher-order",
      "morris2019weisfeiler",
      "vrgcn",
      "wang2021dissecting",
      "oono2019graph",
      "yan2022sides",
      "dissecting",
      "zou2019layerdependent",
      "nikolentzos2020k",
      "song2018exploring",
      "dong2021equivalence",
      "chen2018stochastic",
      "chen2019powerful",
      "li2020deepergcn",
      "10_Wang_Xiaoyun",
      "cao2019gcnet",
      "asgcn",
      "gatanalysis",
      "he2023generalization",
      "liu2021overcoming",
      "11_Wang_Jihong"
    ],
    "relevant_ids": [
      "maron2018invariant",
      "chenWHDL2020gcnii",
      "monti2017geometric",
      "wu2019simplifying",
      "maron2019provably",
      "bruna2013spectral",
      "bianchi2019graph",
      "lecun2015deep",
      "xu2018jknet",
      "rong2019dropedge",
      "fey2018splinecnn",
      "kipf2016semi",
      "liao2019lanczosnet",
      "gilmer2017neural",
      "pei2020geom",
      "hammond2011wavelets",
      "luan2019break",
      "abu2019mixhop",
      "defferrard2016convolutional",
      "xu2018powerful",
      "simonyan2014vggnet",
      "hamilton2017inductive",
      "zhou2020universality",
      "li2016gated",
      "hoang2019revisiting",
      "oono2019graph",
      "klicpera2018predict",
      "li2018deeper",
      "cai2020note",
      "dwivedi2020benchmarking",
      "morris2019weisfeiler"
    ],
    "metrics": {
      "R@5": 0.0,
      "R@10": 0.06451612903225806,
      "R@20": 0.12903225806451613,
      "MRR": 0.14285714285714285,
      "hits": 8,
      "total_relevant": 31
    },
    "score": 0.06221198156682027,
    "timestamp": "2025-12-18T10:57:07.917093"
  },
  {
    "query": "Introduction The scale-permuted network proposed by Du \\etal<|cite_0|> opens up the design of a new family of meta-architecture that allows wiring features with a scale-permuted ordering in convolutional neural network. The scale-permuted architecture achieves promising results on visual recognition and localization by significantly outperforming its scale-decreased counterpart when using the",
    "paper_id": "2010.11426",
    "retrieved_ids": [
      "munkhdalai2017meta",
      "radosavovic2019network",
      "tan2019efficientnet",
      "Du2020SpineNet",
      "Szegedy2014",
      "szegedy2015going",
      "vgg",
      "permutator",
      "santa2017deeppermnet",
      "luan2019break",
      "yaida2022meta",
      "chen2019all",
      "xie2019exploring",
      "abs-1809-04184",
      "morris2019weisfeiler",
      "liu2022more",
      "ma2020weightnet",
      "lrnet",
      "zou_stability",
      "wu2019pointconv",
      "ding2021diverse",
      "zhang-shellnet-iccv19",
      "chen2021dynamicregion",
      "karras2019style",
      "orig",
      "universal-invariant-equivariant-gnn",
      "settransformer",
      "li2019scale",
      "chen2019drop",
      "janossy",
      "liu2017hierarchical",
      "real2019regularized",
      "tatro2020optimizing",
      "heo2021rethinking",
      "zhuang2019local",
      "brea2019weight",
      "deepsets",
      "xie2017resnext",
      "ScorebasedGraph",
      "chen2020feature",
      "Jordan2022REPAIRRP",
      "do-VTs-see-like-CNNs",
      "newell2016stacked",
      "Lambdanetworks",
      "li2021localvit",
      "fang2021msg",
      "huang2021shuffle",
      "yu2018deep",
      "zhang2020resnet",
      "lu2021fantastically",
      "benzing2022random"
    ],
    "relevant_ids": [
      "mobilenetv3",
      "Liang2020Computation",
      "Chen_2018_deeplabv3plus",
      "autodeeplab",
      "Du2020SpineNet",
      "tan2020efficientdet",
      "mnasnet",
      "nasfpn",
      "tan2019efficientnet",
      "zhao2019m2det",
      "coco",
      "fpn",
      "mobilenetv2",
      "liu2018panet",
      "Chen2017deeplabv3"
    ],
    "metrics": {
      "R@5": 0.13333333333333333,
      "R@10": 0.13333333333333333,
      "R@20": 0.13333333333333333,
      "MRR": 0.3333333333333333,
      "hits": 2,
      "total_relevant": 15
    },
    "score": 0.19333333333333333,
    "timestamp": "2025-12-18T10:57:10.584582"
  },
  {
    "query": "Introduction Multi-person pose estimation has been intensely investigated in computer vision. It is a challenging task due to occluded, self-occluded, and invisible keypoints. There have been growing interests in developing deep neural network methods for multi-person pose estimation. In general, these methods can be categorized into top-down and bottom-up methods.",
    "paper_id": "2010.14076",
    "retrieved_ids": [
      "li2020simple",
      "toshev2014deeppose",
      "mspn",
      "cheng2020higherhrnet",
      "insafutdinov2016deepercut",
      "rsn",
      "PersonLab",
      "fang2017rmpe",
      "pishchulin2016deepcut",
      "chen2018cascaded",
      "kocabas2018multiposenet",
      "scarb",
      "sun2019deep",
      "tian2019directpose",
      "fabbri2020",
      "kreiss2019pifpaf",
      "nie2019single",
      "iqbal2016multi",
      "Papandreou2017Towards",
      "cao2021openpose",
      "SingleShotM3",
      "newell2016stacked",
      "belagiannis2017recurrent",
      "LCRNetM2",
      "zhou16",
      "chou2017self",
      "chen17",
      "Zheng20213DHP",
      "guler2018densepose",
      "cao2017realtime",
      "Wang_2020_CVPR",
      "associative"
    ],
    "relevant_ids": [
      "chen2018cascaded",
      "szegedy2017inception",
      "lin2019online",
      "sun2019deep",
      "papandreou2018personlab",
      "redmon2016you",
      "xia2017joint",
      "he2017mask",
      "li2019scale",
      "lim2019fast",
      "lin2017feature",
      "fang2017rmpe",
      "he2016deep",
      "hu2018squeeze",
      "newell2017associative",
      "insafutdinov2016deepercut",
      "liu2016ssd",
      "ho2019population",
      "cao2017realtime",
      "xiao2018simple",
      "newell2016stacked",
      "cubuk2018autoaugment",
      "shrivastava2016training"
    ],
    "metrics": {
      "R@5": 0.043478260869565216,
      "R@10": 0.13043478260869565,
      "R@20": 0.17391304347826086,
      "MRR": 0.2,
      "hits": 6,
      "total_relevant": 23
    },
    "score": 0.11652173913043477,
    "timestamp": "2025-12-18T10:57:12.553799"
  },
  {
    "query": "Introduction \\vspace{-0.15cm} Adversarial training (AT) has been one of the most effective defense strategies against adversarial attacks<|cite_0|>. Based on the primary AT frameworks like PGD-AT<|cite_1|>, many improvements have been proposed from different perspectives, and demonstrate promising results (detailed in Sec.~\\ref{sec2}). However, the recent benchmarks<|cite_2|> find that simply early stopping the",
    "paper_id": "2010.00467",
    "retrieved_ids": [
      "bai2021recent",
      "chakraborty2018adversarial",
      "raghunathan2018certified",
      "meunier2019yet",
      "Raghunathan2018",
      "zhang2019defense",
      "zhang2020attacks",
      "meng2017magnet",
      "sriramanan2020gama",
      "liao2018defense",
      "sriramanan2020GAT",
      "wang2019convergence",
      "wong2020fast",
      "Pang2021BagOT",
      "samangouei2018defense",
      "chen2022adversarial",
      "liu2018adv",
      "zheng2020efficient",
      "randomized_discretization",
      "croce2020reliable",
      "DBLP:conf/ccs/SharadMTK20",
      "stutz2020confidence",
      "song2018improving",
      "VMI-FGSM",
      "jorge2022NFGSM",
      "croce2021robustbench",
      "Cro+21",
      "jia2019comdefend",
      "xie2017mitigating",
      "chen2020adversarial",
      "liu2022practical",
      "Dong_2018",
      "rebuffi2021fixing",
      "arnab2018robustness",
      "rakin2018defend",
      "shafahi2018are",
      "liu2019zk",
      "liu2019extending",
      "zhang2019you",
      "mao2022enhance",
      "strauss2017ensemble",
      "dong2019evading",
      "deacl",
      "tack2022consistency",
      "singla2021curvature",
      "zou2020improving",
      "wang2019enhancing",
      "qinRandomNoiseDefense",
      "yasunaga2018robust",
      "dynacl",
      "shafahi2019adversarial",
      "li2019certified",
      "wu2020adversarial"
    ],
    "relevant_ids": [
      "lee2020adversarial",
      "alayrac2019labels",
      "pang2019rethinking",
      "atzmon2019controlling",
      "zhang2019you",
      "wang2019convergence",
      "Goodfellow2014",
      "Szegedy2013",
      "mao2019metric",
      "zhang2020attacks",
      "cheng2020cat",
      "chen2020rays",
      "cai2018curriculum",
      "shafahi2019adversarial",
      "rice2020overfitting",
      "xu2020exploring",
      "biggio2013evasion",
      "madry2018towards",
      "ding2019mma",
      "croce2020reliable",
      "zhang2019theoretically",
      "wong2020fast",
      "carmon2019unlabeled",
      "zhang2019defense",
      "qin2019adversarial",
      "huang2020self"
    ],
    "metrics": {
      "R@5": 0.0,
      "R@10": 0.07692307692307693,
      "R@20": 0.19230769230769232,
      "MRR": 0.16666666666666666,
      "hits": 7,
      "total_relevant": 26
    },
    "score": 0.07307692307692307,
    "timestamp": "2025-12-18T10:57:15.601222"
  },
  {
    "query": "Introduction Adversarial examples are perturbed inputs that are designed to deceive machine learning classifiers by adding adversarial noises to the original data. Although such perturbations are sufficiently subtle and undetectable by humans, they result in an incorrect classification. Since deep-learning models were found to be vulnerable to adversarial examples<|cite_0|>, a",
    "paper_id": "2010.01799",
    "retrieved_ids": [
      "lu2017adversarial",
      "overview",
      "yuan2019adversarial",
      "Goodfellow2014",
      "goodfellow2015laceyella",
      "bubeck2019adversarial",
      "metzen2017detecting",
      "grosse2017statistical",
      "kurakin2016adversarial",
      "shafahi2018are",
      "xiao2018generating",
      "alzantot2018generating",
      "carlini2017adversarial",
      "PMJ+16",
      "li2017adversarial",
      "Goodfellow2016",
      "meng2017magnet",
      "grosse2016adversarial",
      "wu2018understanding",
      "eykholt2018physical",
      "biggio_2018",
      "li2019certified",
      "akhtar2018threat",
      "Carlini_Dill_2018",
      "strauss2017ensemble",
      "ilyas2019adversarial",
      "ford2019adversarial",
      "moosavi2017universal",
      "zhao2017generating",
      "sur2",
      "Eykholt2018Robust",
      "papernot2016crafting",
      "feinman2017detecting",
      "tramer2020fundamental",
      "qin2019imperceptible",
      "fowl2021adversarial",
      "hendrik2017universal",
      "hosseini2017google",
      "evtimov2017robust",
      "dziugaite2016study",
      "PapernotM17"
    ],
    "relevant_ids": [
      "vivek2020single",
      "gowal2018effectiveness",
      "cohen2019certified",
      "szegedy2013intriguing",
      "alayrac2019labels",
      "goodfellow2014explaining",
      "wong2018provable",
      "andriushchenko2020understanding",
      "li2020towards",
      "salman2019provably",
      "najafi2019robustness",
      "lamb2019interpolated",
      "shafahi2019adversarial",
      "madry2017towards",
      "zhang2017mixup",
      "pang2019mixup",
      "croce2020reliable",
      "zhang2019towards",
      "wong2020fast",
      "carmon2019unlabeled",
      "tramer2017ensemble"
    ],
    "metrics": {
      "R@5": 0.0,
      "R@10": 0.0,
      "R@20": 0.0,
      "MRR": 0.0,
      "hits": 0,
      "total_relevant": 21
    },
    "score": 0.0,
    "timestamp": "2025-12-18T10:57:17.386519"
  },
  {
    "query": "Introduction Batch normalization (BN)<|cite_0|> has been considered as a milestone technique in the development of deep neural networks (DNNs) pushing the frontier in computer vision due to improved convergence. Numerous works have attempted to understand the impact of BN on DNNs from various perspectives. In contrast to previous works, investigating",
    "paper_id": "2010.03316",
    "retrieved_ids": [
      "luo2018towards",
      "ioffe2015batch",
      "chen2021bn",
      "hoffer2018normmatters",
      "Shen2020PowerNorm",
      "lim2023ttn",
      "han2021dynamic",
      "chang2019domain",
      "calib",
      "santurkar2018does",
      "wu2018group",
      "benz2020revisiting",
      "binary_survey",
      "szegedy2015going",
      "cheng2017survey",
      "keskar2016large",
      "salimans2016weightnorm",
      "Courbariaux:2016tm",
      "white2023neural",
      "ba2016layer",
      "Szegedy2014",
      "heo2021rethinking",
      "Xu2019",
      "li2019nattack",
      "orthDNN",
      "mocov3",
      "huang2019iterative",
      "galloway2019batch",
      "huang2021exploring",
      "brock2021high",
      "rakin2018defend",
      "Chen2021VisformerTV",
      "you2021test",
      "zhang2017convergent",
      "ben2019demystifying",
      "nguyen2016synthesizing",
      "zhang2019fixup",
      "cspnet",
      "xu2023resilient",
      "wightman2021resnet",
      "yan2019adversarial",
      "ding2023unireplknet",
      "xie2019intriguing",
      "cartl",
      "richemond2020byol",
      "xiao2018characterizing"
    ],
    "relevant_ids": [
      "mahloujifar2019curse",
      "kurakin2016adversarial",
      "santurkar2018does",
      "cohen2019certified",
      "szegedy2013intriguing",
      "carlini2017towards",
      "galloway2019batch",
      "dong2019evading",
      "goodfellow2014explaining",
      "luo2018towards",
      "xie2020adversarial",
      "vedaldi2016instance",
      "fawzi2016robustness",
      "ba2016layer",
      "dong2018boosting",
      "ilyas2019adversarial",
      "tanay2016boundary",
      "tsipras2018robustness",
      "xie2019improving",
      "wu2018group",
      "ford2019adversarial",
      "ioffe2015batch",
      "gilmer2018adversarial",
      "schmidt2018adversarially",
      "moosavi2019robustness",
      "shafahi2018adversarial",
      "schneider2020improving",
      "benz2020revisiting",
      "xie2019intriguing",
      "jiang2020robust",
      "zhang2019theoretically",
      "rebuffi2017learning",
      "qin2019adversarial"
    ],
    "metrics": {
      "R@5": 0.06060606060606061,
      "R@10": 0.09090909090909091,
      "R@20": 0.18181818181818182,
      "MRR": 1.0,
      "hits": 8,
      "total_relevant": 33
    },
    "score": 0.3515151515151515,
    "timestamp": "2025-12-18T10:57:19.513286"
  },
  {
    "query": "Introduction Convolutional neural networks (CNNs) have achieved the state-of-the-art performance on many computer vision tasks. However, the requirement of computational resources remains a major obstacle which prevents CNNs' deployment on mobile devices. Various model compression techniques have been developed to balance the resource consumption and metrics of specific tasks. As",
    "paper_id": "2010.14714",
    "retrieved_ids": [
      "bucilu2006model",
      "He:2018vj",
      "howard2017mobilenets",
      "amc",
      "yang2023introduction",
      "kim2015compression",
      "tang2023elasticvit",
      "Yang:2018tp",
      "cheng2017survey",
      "netadapt",
      "polino2018distillation",
      "Zhang_2019",
      "Han:2016uf",
      "pruning",
      "zhang2020dynet",
      "Tan:2018vw",
      "tan2019mnasnet",
      "mehta2021mobilevit",
      "cspnet",
      "agrawal2014analyzing",
      "kondratyuk2021movinets",
      "Liu:2017wj",
      "liu2017learning",
      "shaker2023swiftformer",
      "li2022efficientformer",
      "ref14",
      "binary_survey",
      "ashok2017n2n",
      "denton2014exploiting",
      "wang2020gan",
      "gale2019state",
      "chen2020dynamic",
      "li2022QViT",
      "grativol2023federated",
      "gou2021knowledge",
      "Jaderberg_2014",
      "jha2020lightlayers",
      "cai2019WNQ",
      "iandola2014densenet",
      "Srinivas2017TrainingSN",
      "gong2014compressing",
      "frantar2023optimal",
      "baskin2021nice",
      "hooker2019compressed",
      "li2016ternary",
      "wang2021addernet",
      "ghostnet"
    ],
    "relevant_ids": [
      "he2017channel",
      "liu2019darts",
      "wen2016learning",
      "gao2019dynamic",
      "he2019filter",
      "liu2017learning",
      "shu2019co",
      "liu2018rethinking",
      "zhuang2018discrimination",
      "huang2018data",
      "dong2019network",
      "he2018amc",
      "wu2019fbnet",
      "blalock2020state"
    ],
    "metrics": {
      "R@5": 0.0,
      "R@10": 0.0,
      "R@20": 0.0,
      "MRR": 0.043478260869565216,
      "hits": 1,
      "total_relevant": 14
    },
    "score": 0.013043478260869565,
    "timestamp": "2025-12-18T10:57:21.653685"
  },
  {
    "query": "Introduction \\label{sec:introduction} Adversarial examples, which contain perturbations to the input of a model that elicit large changes in the output, have been shown to be an effective way of assessing the robustness of models in natural language processing (NLP)<|cite_0|>. Adversarial training, in which models are trained on adversarial examples, has",
    "paper_id": "2010.01610",
    "retrieved_ids": [
      "alzantot2018generating",
      "bai2021recent",
      "goodfellow2015laceyella",
      "Goodfellow2014",
      "overview",
      "cheng2018seq2sick",
      "carlini2017towards",
      "bubeck2019adversarial",
      "tsipras2018robustness",
      "Gu2015",
      "Ebrahimi:18b",
      "acl",
      "huang2015learning",
      "zhao2017generating",
      "sur2",
      "zhang2019generating",
      "wu2018understanding",
      "yuan2019adversarial",
      "grosse2017statistical",
      "kurakin2016adversarial",
      "arnab2018robustness",
      "schmidt2018adversarially",
      "michel2019evaluation",
      "ross2018improving",
      "PMJ+16",
      "Goodfellow2016",
      "papernot2016crafting",
      "potdevin19",
      "ford2019adversarial",
      "wu2020adversarial",
      "narodytska2016simple",
      "qin2019imperceptible",
      "lu2017adversarial",
      "tramer2020fundamental",
      "li2019certified",
      "samangouei2018defense",
      "boucher2021bad",
      "pang2020mixup"
    ],
    "relevant_ids": [
      "miyato2016adversarial",
      "samanta2017towards",
      "yasunaga2018robust",
      "michel2019evaluation",
      "zhang2019generating",
      "papernot2016crafting",
      "belinkov2017synthetic",
      "tramer2017ensemble",
      "devlin2019bert",
      "goodfellow2014explaining",
      "alzantot2018generating",
      "ebrahimi2018hotflip",
      "jia2017adversarial",
      "wang2019improving",
      "hosseini2017deceiving"
    ],
    "metrics": {
      "R@5": 0.06666666666666667,
      "R@10": 0.06666666666666667,
      "R@20": 0.13333333333333333,
      "MRR": 1.0,
      "hits": 4,
      "total_relevant": 15
    },
    "score": 0.3466666666666667,
    "timestamp": "2025-12-18T10:57:23.440103"
  },
  {
    "query": "Introduction Human-object interaction (HOI) detection aims to recognize and localize the interactions between human-object pairs (\\textit{e.g.} sitting on a chair, riding a horse, eating an apple, \\textit{etc.}). As a fundamental task of image semantic understanding, it plays a vital role in many other computer vision fields such as image captioning<|cite_0|>,",
    "paper_id": "2010.01005",
    "retrieved_ids": [
      "Gkioxari2017Detecting",
      "chao2018learning",
      "gao2018ican",
      "2020UnionDet",
      "liu2020consnet",
      "fang2018pairwise",
      "qpic",
      "hotr",
      "liu2022interactiveness",
      "wang2020contextual",
      "ulutan2020vsgnet",
      "hoitrans",
      "li2019transferable",
      "liao2019ppdm",
      "vcl",
      "li2020hoi",
      "DBLP:journals/corr/abs-1904-03181",
      "hoiclip",
      "Wang2020IPNet",
      "qi2018learning",
      "hou2021affordance",
      "cdn",
      "hou2021detecting",
      "kang2024intent3d",
      "asnet",
      "djrn",
      "mscoco",
      "gupta2015visual",
      "LinMBHPRDZ14",
      "visualgenome",
      "lu2016visual",
      "SAS",
      "lu2021omnimatte"
    ],
    "relevant_ids": [
      "Wang2020IPNet",
      "2019Relation",
      "zhou2020cascaded",
      "fang2018pairwise",
      "DBLP:journals/corr/LinDGHHB16",
      "2020UnionDet",
      "DBLP:journals/corr/abs-1806-07243",
      "DBLP:journals/corr/abs-1808-05864",
      "2019Aligning",
      "DBLP:journals/corr/abs-1904-03181",
      "liao2019ppdm",
      "li2020pastanet",
      "chao2018learning",
      "gupta2018nofrills",
      "gao2018ican",
      "DBLP:journals/corr/abs-1708-02002",
      "gupta2015visual",
      "lu2016visual",
      "qi2018learning",
      "Shao_2020_CVPR",
      "fang2018weakly",
      "li2019transferable",
      "wan2019pose",
      "CVPR2019_ARG"
    ],
    "metrics": {
      "R@5": 0.125,
      "R@10": 0.16666666666666666,
      "R@20": 0.375,
      "MRR": 0.5,
      "hits": 11,
      "total_relevant": 24
    },
    "score": 0.25,
    "timestamp": "2025-12-18T10:57:25.089892"
  },
  {
    "query": "Introduction Dialogue state tracking (DST) plays a key role in task-oriented dialogue systems to track the user's intentional state and convert it into a set of slot-value pairs, i.e., dialogue state. As the basis for selecting the next system action, the accurate prediction of dialogue state is critical. Traditionally, DST",
    "paper_id": "2010.10811",
    "retrieved_ids": [
      "gao2019dialog",
      "xu2018end",
      "hosseini2020simple",
      "rastogi2018multi",
      "ren2018towards",
      "goel2018flexible",
      "mrkvsic2017neural",
      "goel2019hyst",
      "shan2020contextual",
      "heck2020trippy",
      "Balaraman2019ScalableND",
      "zhu2020efficient",
      "rastogi2017scalable",
      "zhang2019find",
      "Zhong2018GlobalLocallySD",
      "chao2019bert",
      "chen2017survey",
      "Wu2019TransferableMS",
      "Ren2019ScalableAA",
      "kim2020somdst",
      "ubuntu_corpus",
      "lee2019sumbt",
      "Ramadan2018LargeScaleMB"
    ],
    "relevant_ids": [
      "vinyals2015pointer",
      "rastogi2017scalable",
      "lee2019sumbt",
      "zhang2019find",
      "devlin2018bert",
      "shan2020contextual",
      "chao2019bert",
      "gao2019dialog",
      "rastogi2018multi",
      "mrkvsic2017neural",
      "goel2018flexible",
      "ren2018towards",
      "heck2020trippy",
      "xu2018end"
    ],
    "metrics": {
      "R@5": 0.2857142857142857,
      "R@10": 0.5714285714285714,
      "R@20": 0.7857142857142857,
      "MRR": 1.0,
      "hits": 12,
      "total_relevant": 14
    },
    "score": 0.5857142857142856,
    "timestamp": "2025-12-18T10:57:26.242882"
  },
  {
    "query": "Introduction Larger model capacity has brought improvement in accuracy by enabling better modeling of data. However, increasing model capacity causes a significant increase in computational cost at both training and inference time despite better accuracy. To address this issue,<|cite_0|> propose product key memory (PKM) that enables very efficient and exact",
    "paper_id": "2010.03881",
    "retrieved_ids": [
      "pkn",
      "sun2017revisiting",
      "abnar2021exploring",
      "hu2019triple",
      "xiao2024smoothquant",
      "miao2023towards",
      "cho2014exponentially",
      "menghani2021efficient",
      "Zero1",
      "aminabadi2022deepspeed",
      "dong2023abilities",
      "faiz2024llmcarbon",
      "bucilu2006model",
      "yuan2024llminferenceunveiledsurvey",
      "DBLP:journals/corr/GruslysMDLG16",
      "rajbhandari2022deepspeedmoe",
      "shen2021efficient",
      "du2022glamefficientscalinglanguage",
      "jin2024comprehensive",
      "compressedcontext",
      "sharify2024combining",
      "pang2024anchor",
      "mirzadeh2023relustrikesbackexploiting",
      "onebit",
      "yuan2023scaling",
      "levy2024same",
      "namburi2023cost",
      "teerapittayanon2016branchynet",
      "biderman2023emergent",
      "flashattention2",
      "zhang2024relu2winsdiscoveringefficient",
      "gshard",
      "gao2022parameterefficient",
      "schaefer2023mixed",
      "zafrir2019q8bert",
      "amc",
      "Mallen2022WhenNT",
      "smith2023coda",
      "li2021scaling",
      "zhang2020resnet",
      "twostageMC",
      "ge2023model",
      "choi2018pact",
      "bondarenko2023quantizable",
      "carlini2022quantifying",
      "moe",
      "wu2022memorizing",
      "bulatov2023scaling",
      "he2018soft",
      "brix2020successfully",
      "you2020shiftaddnet",
      "puigcerver2023sparse"
    ],
    "relevant_ids": [
      "wang2018glue",
      "he2016deep",
      "devlin2018bert",
      "sukhbaatar2019augmenting",
      "fevry2020entities",
      "guu2020realm",
      "vaswani2017attention",
      "raffel2019exploring",
      "chandar2016hierarchical",
      "brown2020language",
      "lample2019large",
      "weston2014memory",
      "verga2020facts",
      "sukhbaatar2015end",
      "khandelwal2019generalization",
      "liu2019roberta",
      "rae2016scaling"
    ],
    "metrics": {
      "R@5": 0.0,
      "R@10": 0.0,
      "R@20": 0.0,
      "MRR": 0.0,
      "hits": 0,
      "total_relevant": 17
    },
    "score": 0.0,
    "timestamp": "2025-12-18T10:57:28.503861"
  },
  {
    "query": "Introduction Since the seminal paper of<|cite_0|>, adversarial examples arose much attention in machine learning, with various attacks (e.g.<|cite_1|>) and defence methods (e.g.<|cite_2|>) being developed, as well as various attempts to explain their presence (e.g.<|cite_3|>). Yet, it is still not clear why adversarial examples exist, and why they can be found",
    "paper_id": "2010.14927",
    "retrieved_ids": [
      "overview",
      "yuan2019adversarial",
      "goodfellow2015laceyella",
      "Goodfellow2014",
      "shafahi2018are",
      "chakraborty2018adversarial",
      "kurakin2016adversarial",
      "shamir2019simple",
      "bubeck2019adversarial",
      "kurakin_nips2017competition",
      "grosse2017statistical",
      "DBLP:journals/corr/abs-1901-00532",
      "hendrycks2019natural",
      "ilyas2019adversarial",
      "he2017adversarial",
      "tabacof2016exploring",
      "potdevin19",
      "zeng2019adversarial",
      "carlini2017adversarial",
      "biggio_2018",
      "chen2018ead",
      "zhang2022beyond",
      "bai2021recent",
      "meng2017magnet",
      "Madry2017",
      "xu2019adversarial",
      "akhtar2018threat",
      "cheng2018seq2sick",
      "lyu2015unified",
      "li2017adversarial",
      "Cro+21",
      "b2",
      "lu2017adversarial",
      "croce2021robustbench",
      "xue2024pixelbarrierdiffusionmodels",
      "xiao2018characterizing",
      "chen2019stateful",
      "papernot2017practical",
      "huang2021exploring",
      "chen2019towards",
      "xie2017mitigating",
      "tramer2019adversarial",
      "fowl2021adversarial",
      "Li2020BackdoorLA",
      "isit2018",
      "li2018generative",
      "carlini2017towards",
      "kumari2023trust",
      "zhang2020does",
      "sharma2019effectiveness"
    ],
    "relevant_ids": [
      "fawzi2018adversarial",
      "papernot2016distillation",
      "papernot2017practical",
      "shamir2019simple",
      "madry2017towards",
      "schmidt2018adversarially",
      "feinman2017detecting",
      "grosse2017statistical",
      "shafahi2018adversarial",
      "carlini2018audio",
      "bubeck2019adversarial",
      "goodfellow2014explaining",
      "wong2018provable",
      "athalye2018obfuscated",
      "carlini2017adversarial",
      "szegedy2014intriguing"
    ],
    "metrics": {
      "R@5": 0.0,
      "R@10": 0.125,
      "R@20": 0.25,
      "MRR": 0.125,
      "hits": 5,
      "total_relevant": 16
    },
    "score": 0.075,
    "timestamp": "2025-12-18T10:57:31.067571"
  },
  {
    "query": "Introduction Modern Convolutional Neural Network (CNN) models have shown outstanding performance in many computer vision tasks. However, due to their numerous parameters and computation, it remains challenging to deploy them to mobile phones or edge devices. One of the widely used methods to lighten and accelerate the network is pruning.",
    "paper_id": "2010.13160",
    "retrieved_ids": [
      "howard2017mobilenets",
      "amc",
      "jiang2022model",
      "li2017pruning",
      "he2017channel",
      "He:2017vq",
      "tang2023elasticvit",
      "zhou2019accelerate",
      "molchanov2016pruning",
      "liu2017learning",
      "zhang2016acceleratingvd",
      "zhu2017prune",
      "cheng2017survey",
      "kim2015compression",
      "blalock2020state",
      "Liu:2017wj",
      "li2020eagleeye",
      "tessier2022rethinking",
      "yang2019multi",
      "liu2021fisher",
      "Jaderberg_2014",
      "ye2018rethinking",
      "Tan:2018vw",
      "lim2020federated",
      "Zhang_2019",
      "tan2019mnasnet",
      "Han:2015vn",
      "cai2019once",
      "mehta2021mobilevit",
      "BMVC2015_31",
      "li2022efficientformer",
      "cspnet",
      "dong2017learning",
      "jha2020lightlayers",
      "chen2019all",
      "binary_survey",
      "chen2020dynamic",
      "gong2014compressing",
      "baskin2021nice",
      "guo2016dynamic",
      "wang2021addernet",
      "gou2021knowledge",
      "Courbariaux2015BinaryConnectTD",
      "dynamic_sparsity",
      "huang2018data",
      "addernet",
      "iofinova2021well"
    ],
    "relevant_ids": [
      "liu2017learning",
      "SimonyanZ14a",
      "myssay2020coreset",
      "he2017channel",
      "wang2020picking",
      "han2016eie",
      "lebedev2014speeding",
      "li2016pruning",
      "denil2013predicting",
      "yu2018nisp",
      "he2018soft",
      "he2019filter",
      "lee2018snip",
      "BMVC2015_31",
      "you2019gate",
      "ye2018rethinking",
      "BMVC2016_87",
      "molchanov2016pruning",
      "kim2015compression",
      "luo2017thinet"
    ],
    "metrics": {
      "R@5": 0.05,
      "R@10": 0.15,
      "R@20": 0.2,
      "MRR": 0.2,
      "hits": 6,
      "total_relevant": 20
    },
    "score": 0.125,
    "timestamp": "2025-12-18T10:57:33.093872"
  },
  {
    "query": "Introduction The advances in deep representation learning and the development of large-scale dataset<|cite_0|> have inspired a number of pioneering approaches in visual question answering (VQA). Though neural networks are powerful, flexible and robust, recent work has repeatedly demonstrated their flaws, showing how they struggle to generalize in a systematic manner<|cite_1|>,",
    "paper_id": "2010.04913",
    "retrieved_ids": [
      "antol2015vqa",
      "kafle2017analysis",
      "agrawal2016analyzing",
      "ren2015exploring",
      "veagle_paper",
      "ECCV16baseline",
      "zhu2017structured",
      "robustvqa",
      "teney2017graph",
      "hudson2019gqa",
      "balanced_vqa_v2",
      "FVQA",
      "xiong2016dynamic",
      "okvqa",
      "goyal2016towards",
      "yu2019deep",
      "chen2015abc",
      "showaskattend",
      "shih2016att",
      "yu2018beyond",
      "Xu2016AskAA",
      "DBLP:journals/corr/IlievskiYF16",
      "murel",
      "jang2017tgif",
      "causalvqa",
      "rubi",
      "wang2020general",
      "jiang2020defense",
      "iclr2",
      "andreas2016neural",
      "yu2017mfb",
      "vqahat",
      "nsvqa",
      "wang2021knowledge",
      "Zhu2015Visual7WGQ",
      "lu2016hierarchical",
      "blind2",
      "Changpinyo2022AllYM",
      "santoro2017simple",
      "Wang_Yu_Yu_Dai_Tsvetkov_Cao_2021",
      "nguyen2020movie"
    ],
    "relevant_ids": [
      "lxmert",
      "zellers2018neural",
      "nsvqa",
      "mac",
      "clevr",
      "lake2017generalization",
      "xnm",
      "santoro2017simple",
      "teney2017graph",
      "yin2017obj2text",
      "antol2015vqa",
      "nmn",
      "xu2017scene",
      "hu2017learning",
      "gqa",
      "li2018factorizable",
      "bahdanau2014neural",
      "agrawal2016analyzing"
    ],
    "metrics": {
      "R@5": 0.1111111111111111,
      "R@10": 0.16666666666666666,
      "R@20": 0.16666666666666666,
      "MRR": 1.0,
      "hits": 5,
      "total_relevant": 18
    },
    "score": 0.39444444444444443,
    "timestamp": "2025-12-18T10:57:35.110380"
  },
  {
    "query": "Introduction \\input{figures/normalized_perspective_schematic} The improvements in 2D human pose estimation (HPE) over the last years have been one of the most prominent successes of deep learning in computer vision tasks. Consequently, there is a still growing ambition to transition those advancements into the next logical step in human pose estimation: the",
    "paper_id": "2010.12317",
    "retrieved_ids": [
      "sun2019deep",
      "xiao2018simple",
      "toshev2014deeppose",
      "bulat2016human",
      "mspn",
      "belagiannis2017recurrent",
      "chen17",
      "sun2017revisiting",
      "grinciunaite16",
      "Tompson2014Joint",
      "nibali2019",
      "swahr",
      "chen2017adversarial",
      "Zheng20213DHP",
      "kreiss2019pifpaf",
      "zhang2020distribution",
      "drover18",
      "chou2017self",
      "li2020simple",
      "newell2016stacked",
      "guler2018densepose",
      "pavllo19",
      "popa17",
      "insafutdinov2016deepercut",
      "pavlakos18",
      "varamesh2020mixture",
      "cao2021openpose",
      "wandt19",
      "SimpleBaseline",
      "CanonPoseSM",
      "pavlakos17",
      "JointformerSL",
      "Monocular3H",
      "martinez17",
      "zhou16",
      "jin2020whole",
      "spgnet",
      "ju2023human",
      "elsken2019neural"
    ],
    "relevant_ids": [
      "fabbri2020",
      "nibali2019",
      "martinez17",
      "pavllo19",
      "kocabas2019",
      "chen17",
      "pavlakos17",
      "zhou16",
      "kanazawa18",
      "kocabas2020",
      "kanazawa19",
      "popa17",
      "wandt19",
      "rogez16",
      "sun18",
      "drover18",
      "pavlakos18",
      "tung17",
      "grinciunaite16"
    ],
    "metrics": {
      "R@5": 0.0,
      "R@10": 0.10526315789473684,
      "R@20": 0.21052631578947367,
      "MRR": 0.14285714285714285,
      "hits": 11,
      "total_relevant": 19
    },
    "score": 0.0744360902255639,
    "timestamp": "2025-12-18T10:57:36.939575"
  },
  {
    "query": "Introduction Overparameterized neural networks have infinitely many solutions that achieve zero training error, and such global minima have different generalization performance. Moreover, training a neural network is a high-dimensional nonconvex problem, which is typically intractable to solve. However, the success of deep learning indicates that first-order methods such as gradient",
    "paper_id": "2010.02501",
    "retrieved_ids": [
      "zhang2017",
      "zou2019improved",
      "Neyshabur2019TowardsUT",
      "Geiger2020",
      "nagarajan2019uniform",
      "zhang2019type",
      "arora2019fine",
      "dziugaite2017computing",
      "keskar2016large",
      "dauphin2014identifying",
      "goodfellow2014qualitatively",
      "evci2019difficulty",
      "du2018gradientB",
      "oymak2020towards",
      "zou2021understanding",
      "du2018gradientA",
      "zhang2019identity",
      "shevchenko2020landscape",
      "arora2018optimization",
      "liu2022loss",
      "belkin2018understand",
      "power2022grokking",
      "cooper2018loss",
      "zou2018stochastic",
      "sun2019optimization",
      "simsek2021geometry",
      "lee2019wide",
      "ren2018learning",
      "liu2018learning",
      "arora2019convergence",
      "wilson2017marginal",
      "Zhang2019WhyGC",
      "he2020milenas",
      "lafon2024understanding",
      "Barrett2021ImplicitGR",
      "liang2018understanding",
      "brea2019weight",
      "lucas2021analyzing",
      "mirzadeh2021linear",
      "NitandaSuzuki_ICLR2021optimal"
    ],
    "relevant_ids": [
      "jacot2018neural",
      "gissin2020implicit",
      "gunasekar2017implicit",
      "woodworth2020kernel",
      "arora2019implicit",
      "zou2018stochastic",
      "arora2018optimization",
      "wu2019global",
      "soudry2018implicit",
      "gunasekar2018implicit",
      "hu2020provable",
      "chizat2020implicit",
      "ji2020directional",
      "ji2019gradient",
      "gunasekar2018characterizing",
      "bartlett2018gradient",
      "du2019width",
      "du2018gradientB",
      "allen2018convergence",
      "oymak2020towards",
      "du2018gradientA",
      "arora2019convergence"
    ],
    "metrics": {
      "R@5": 0.0,
      "R@10": 0.0,
      "R@20": 0.18181818181818182,
      "MRR": 0.07692307692307693,
      "hits": 6,
      "total_relevant": 22
    },
    "score": 0.023076923076923078,
    "timestamp": "2025-12-18T10:57:38.785777"
  },
  {
    "query": "Introduction \\label{sec::introduction} The advent of deep learning has greatly improved the performances of natural language processing (NLP) models. Consequently, the models are becoming more complex<|cite_0|>, rendering it difficult to understand the \\red{rationale} behind their predictions. To use deep neural networks (DNNs) for making high-stakes decisions, the interpretability must be guaranteed",
    "paper_id": "2010.13984",
    "retrieved_ids": [
      "2020arXiv200301200T",
      "young2018recent",
      "feng2018pathologies",
      "MIR-2022-03-068",
      "zintgraf2017visualizing",
      "ross2018improving",
      "survey0",
      "gilpin2018explaining",
      "papernot2018deep",
      "thulasidasan2019mixup",
      "Dombrowski2022",
      "madsen2021posthoc",
      "wu2021graph",
      "bastings2019interpretable",
      "zhang2020interpretable",
      "Strubell:2019uv",
      "sur2",
      "sun2019optimization",
      "deyoung2020eraser",
      "strubell2019energy",
      "lawbench",
      "yu2019playing",
      "wang2022rationale",
      "Zheng2023SecretsOR",
      "deletang2023neural",
      "guo2016dynamic",
      "srinidhi2021deep",
      "chen2019ncv",
      "battaglia2018relational",
      "white2023neural",
      "li2016understanding",
      "hu2018explainable",
      "frosst2017distilling",
      "KARIMI2020101759",
      "Kingma2014SemisupervisedLW",
      "bubeck2023sparks",
      "thorough",
      "han2016eie",
      "rationale",
      "wiegreffe2019attention",
      "DetNet",
      "chen2020lottery",
      "limitations",
      "zellers2019hellaswag",
      "xu2019adversarial"
    ],
    "relevant_ids": [
      "li2016understanding",
      "hendrycks2016baseline",
      "vaswani2017attention",
      "devlin2019bert",
      "arras2017explaining",
      "bahdanau2014neural",
      "zintgraf2017visualizing",
      "gilpin2018explaining",
      "prabhakaran2019perturbation",
      "feng2018pathologies",
      "wiegreffe2019attention",
      "yang2019xlnet",
      "chang2018explaining",
      "lundberg2017unified",
      "jain2019attention",
      "liu2019roberta",
      "yi2020informationtheoretic",
      "jin2019towards",
      "simonyan2013deep",
      "zeiler2014visualizing"
    ],
    "metrics": {
      "R@5": 0.1,
      "R@10": 0.15,
      "R@20": 0.15,
      "MRR": 0.3333333333333333,
      "hits": 5,
      "total_relevant": 20
    },
    "score": 0.185,
    "timestamp": "2025-12-18T10:57:41.126764"
  },
  {
    "query": "Introduction \\label{sec:intro} A sentence can be pronounced in many different ways: fast or slow, happy or angry, with emphasis on certain words. We can say much more than what is written because we are able to control some fundamental aspects of the production of speech. In this work, we call",
    "paper_id": "2010.15084",
    "retrieved_ids": [
      "BaeBaeJooLeeLeeCho2020",
      "zhou2020bert",
      "RenRuanTanQinZhaoZhaoLiu2019",
      "chorowski2015attention",
      "RaitioRasipuramCastellani2020",
      "yan2021adaspeech3",
      "ji2023textrolspeech",
      "rarr",
      "martinez2021attention",
      "Collobert2011",
      "zhou2023controlled",
      "wu2020clear",
      "HabibMariooryadShannonBattenbergSkerryRyanStantonKaoBagby2019",
      "parallelwavenet",
      "wang2017tacotron",
      "liu2023pre",
      "voita-etal-2019-good",
      "zhang2018sentence",
      "spangher2023sequentially",
      "movie_dialogue",
      "yoo2022groundtruth",
      "humeau2019poly",
      "shimizu2023prompttts",
      "rubenstein2023audiopalm",
      "dugan2023real",
      "hu2019introductory",
      "yang2023instructtts",
      "fathullah2023prompting",
      "plug_play_gen",
      "survey0",
      "wei2023instructiongpt",
      "ethayarajh2019contextual",
      "chong2022jojogan",
      "liao2020improving",
      "chen2023frustratingly",
      "ValleLiPrengerCatanzaro2019",
      "duan2023botchat",
      "toxigen",
      "park2023linear",
      "instruction-tuning",
      "chen2020say",
      "li2022diffusion",
      "KARIMI2020101759",
      "min2022rethinking",
      "pratt2023does",
      "xi2023rise",
      "evans2024fast",
      "chu2024qwen2audiotechnicalreport",
      "2020Learning",
      "athiwaratkun2019there",
      "muller2019does",
      "videomv",
      "Martens2015"
    ],
    "relevant_ids": [
      "PrengerValleCatanzaro2018",
      "ZhaiGaoXueRothchildWuGonzalezKeutzer2020",
      "PingPengZhaoSong2019",
      "DonahueMcAuleyPuckette2019",
      "HsuZhangWeissZenWuWangCaoJiaChenShenetal2018",
      "WangStantonZhangSkerryRyanBattenbergShorXiaoRenJiaSaurous2018",
      "HabibMariooryadShannonBattenbergSkerryRyanStantonKaoBagby2019",
      "BinkowskiDonahueDielemanClarkElsenCasagrandeCoboSimonyan2019",
      "parallelwavenet",
      "parallelwavegan",
      "OordDielemanZenSimonyanVinyalsGravesKalchbrennerSeniorKavukcuoglu2016",
      "ValleShihPrengerCatanzaro2020",
      "RenHuTanQinZhaoZhaoLiu2020",
      "EngelHantrakulGuRoberts2020",
      "KimKimKongYoon2020",
      "RenRuanTanQinZhaoZhaoLiu2019",
      "ShenJiaChrzanowskiZhangEliasZenWu2020",
      "RaitioRasipuramCastellani2020",
      "WangTakakiYamagishi2019",
      "ValleLiPrengerCatanzaro2019",
      "KimLeeSongKimYoon2019",
      "melgan",
      "BaeBaeJooLeeLeeCho2020"
    ],
    "metrics": {
      "R@5": 0.13043478260869565,
      "R@10": 0.13043478260869565,
      "R@20": 0.21739130434782608,
      "MRR": 1.0,
      "hits": 6,
      "total_relevant": 23
    },
    "score": 0.3913043478260869,
    "timestamp": "2025-12-18T10:57:43.228131"
  },
  {
    "query": "Introduction In recent years, neural models have led to state-of-the-art results in machine translation (MT)<|cite_0|>. Many of these systems can broadly be characterized as following a multi-layer encoder-decoder neural network design: both the encoder and decoder learn representations of word sequences by a stack of layers<|cite_1|>, building on an interesting",
    "paper_id": "2010.03737",
    "retrieved_ids": [
      "Chen:18",
      "cho2014properties",
      "Chen:2018vf",
      "wang-etal-2018-multi-layer",
      "gu2017learning",
      "Cheng:18",
      "barone2017deep",
      "Bahdanau:2015vz",
      "wang-etal-2019-learning",
      "ZhouCWLX16",
      "gehring2016convolutional",
      "maruf-haffari-2018-document",
      "Bahdanau2014NeuralMT",
      "mccann2017learned",
      "sutskever2014sequence",
      "Tu2017NeuralMT",
      "vnmt",
      "wu2016google",
      "Wu:2016wt",
      "kalchbrenner2016neural",
      "Kalchbrenner:2016vf",
      "gru",
      "liu2020very",
      "kasai2021deep",
      "wang:2017:aaai",
      "knnmt",
      "artetxe2017unsupervised",
      "NLI_NSE",
      "bastings2017graph",
      "kiros2014unifying",
      "wangEtAl2017",
      "huang2021non",
      "kaiser2018fast",
      "badrinarayanan2015segnet",
      "hypernetworks",
      "ha2016hypernetworks",
      "zheng2020gman",
      "dehghani2018universal",
      "Yu2022CoCaCC",
      "hatamizadeh2022unetr",
      "ma2019monotonic",
      "vaswani2017attention"
    ],
    "relevant_ids": [
      "gehring2017convs2s",
      "wu-etal-2019-depth",
      "he2016deep",
      "lei2016layer",
      "liu2020understanding",
      "wang-etal-2019-learning",
      "wang-etal-2018-multi-layer",
      "wu2016google",
      "zhang-etal-2019-improving",
      "sutskever2014sequence",
      "vaswani2017attention",
      "bapna-etal-2018-training",
      "wei2004multiscale",
      "xu2019lipschitz",
      "bahdanau2014neural"
    ],
    "metrics": {
      "R@5": 0.06666666666666667,
      "R@10": 0.13333333333333333,
      "R@20": 0.26666666666666666,
      "MRR": 0.25,
      "hits": 5,
      "total_relevant": 15
    },
    "score": 0.14166666666666666,
    "timestamp": "2025-12-18T10:57:45.268919"
  },
  {
    "query": "Introduction Mobile devices and the Internet of Things (IoT) devices are becoming the primary computing resource for billions of users worldwide<|cite_0|>. These devices generate a significant amount of data that can be used to improve numerous existing applications<|cite_1|>. From the privacy and economic point of view, due to these devices'",
    "paper_id": "2010.01264",
    "retrieved_ids": [
      "lin2020mcunet",
      "Papernot2018",
      "thomee2016yfcc100m",
      "Zhang2015ASO",
      "gou2021knowledge",
      "li2023snapfusion",
      "wang2019federated",
      "lim2020federated",
      "Konecn2016FederatedOD",
      "jeong2018communication",
      "ha2022feasibility",
      "ruan2021towards",
      "howard2017mobilenets",
      "cho2023heterogeneous",
      "gu2021fast",
      "zhao2018federated",
      "mcmahan2017fedavg",
      "optimising_resource",
      "hard2018federated",
      "li2020federated",
      "mcmahan2017communication",
      "Fedlearning_edge_1",
      "jiang2022model",
      "wang2021addernet",
      "ramaswamy2020training",
      "liu2023largest",
      "b2",
      "wang2019deep",
      "chen2020wireless",
      "baumgartner2020pushshift",
      "zhu2024sora",
      "chen2020fedhealth",
      "retail_analytics",
      "grativol2023federated",
      "fu20212",
      "alam2020survey",
      "overview3",
      "b3",
      "levine2020offline",
      "acar2020federated",
      "lin2024awq",
      "strubell2019energy",
      "koda2020differentially"
    ],
    "relevant_ids": [
      "bonawitz2019towards",
      "li2020federated",
      "mcmahan2017communication",
      "ivkin2019communication",
      "konevcny2016federated",
      "liang2020think",
      "mansour2020three",
      "hard2018federated",
      "khodak2019adaptive",
      "jiang2019improving",
      "alistarh2017qsgd",
      "DingAssist",
      "thapa2020splitfed",
      "zhao2020idlg",
      "melis2019exploiting",
      "ben2019demystifying",
      "nishio2019client",
      "li2020lotteryfl",
      "ioffe2015batch",
      "zhu2019deep",
      "wang2019federated",
      "smith2017federated",
      "lim2020federated",
      "li2019fedmd"
    ],
    "metrics": {
      "R@5": 0.0,
      "R@10": 0.08333333333333333,
      "R@20": 0.16666666666666666,
      "MRR": 0.14285714285714285,
      "hits": 5,
      "total_relevant": 24
    },
    "score": 0.06785714285714285,
    "timestamp": "2025-12-18T10:57:47.185386"
  },
  {
    "query": "Introduction In the last decade, Machine Learning (ML) models have achieved remarkable success in various tasks such as classification, regression and decision making. However, recently they have been found vulnerable to adversarial examples that are legitimate inputs altered by small and often imperceptible perturbations<|cite_0|>. These carefully curated examples are correctly",
    "paper_id": "1907.11932",
    "retrieved_ids": [
      "overview",
      "Goodfellow2014",
      "goodfellow2015laceyella",
      "metzen2017detecting",
      "huang2015learning",
      "Carlini_Dill_2018",
      "grosse2017statistical",
      "kurakin2016adversarial",
      "yuan2019adversarial",
      "brown2018unrestricted",
      "fawzi2018adversarial",
      "Goodfellow2016",
      "carlini2017adversarial",
      "alzantot2018generating",
      "xie2019improving",
      "meng2017magnet",
      "chakraborty2018adversarial",
      "ilyas2019adversarial",
      "zhao2017generating",
      "tramer2017space",
      "biggio_2018",
      "qin2019imperceptible",
      "sur2",
      "fawaz2019adversarial",
      "samangouei2018defense",
      "akhtar2018threat",
      "tramer2020fundamental",
      "schmidt2018adversarially",
      "moosavi2017universal",
      "papernot2016crafting",
      "ma2021understanding",
      "ford2019adversarial",
      "papernot2017practical",
      "ross2018improving",
      "boucher2021bad",
      "lyu2015unified",
      "jakubovitz2018improving",
      "poursaeed2021robustness",
      "li2020towards",
      "tramer2017ensemble",
      "dziugaite2016study",
      "jin2020bert",
      "Papernot2018"
    ],
    "relevant_ids": [
      "moosavi2017universal",
      "papernot2017practical",
      "kurakin2016adversarial",
      "li2018textbugger",
      "li2016understanding",
      "ebrahimi2017hotflip",
      "zhao2017generating",
      "gao2018black",
      "feng2018pathologies",
      "szegedy2013intriguing",
      "liang2017deep",
      "DBLP:journals/corr/KurakinGB16a",
      "carlini2018audio",
      "goodfellow2014explaining",
      "alzantot2018generating",
      "43405"
    ],
    "metrics": {
      "R@5": 0.0,
      "R@10": 0.0625,
      "R@20": 0.1875,
      "MRR": 0.125,
      "hits": 5,
      "total_relevant": 16
    },
    "score": 0.056249999999999994,
    "timestamp": "2025-12-18T10:57:49.079785"
  },
  {
    "query": "Introduction Invertible neural networks have many applications in machine learning. They have been employed to investigate representations of deep classifiers<|cite_0|>, understand the cause of adversarial examples<|cite_1|>, learn transition operators for MCMC<|cite_2|>, create generative models that are directly trainable by maximum likelihood<|cite_3|>, and perform approximate inference<|cite_4|>. Many applications of invertible neural",
    "paper_id": "1907.07945",
    "retrieved_ids": [
      "goodfellow2015laceyella",
      "Goodfellow2014",
      "yuan2019adversarial",
      "li2017adversarial",
      "kurakin2016adversarial",
      "Ebrahimi:18b",
      "jacobsen2018irevnet",
      "GAN",
      "samangouei2018defense",
      "ncsn",
      "Zinan2017",
      "i-resnet",
      "song2019generative",
      "creswell2018inverting",
      "ardizzone2018analyzing",
      "mino2018logan",
      "grosse2016adversarial",
      "Zhou2018",
      "teshima2020couplingbased",
      "sinitsin2020editable",
      "kruse2021benchmarking",
      "maaloe2019biva",
      "GDG17",
      "ishikawa2022universal",
      "NalisnickMTGL19",
      "li2018generative",
      "nguyen2017plug",
      "kuzina2022alleviating",
      "xu2019adversarial",
      "chen2019stateful",
      "luhman2021knowledge",
      "sehwag2021improving",
      "teshima2020coupling",
      "cGAN",
      "deleu2022bayesian",
      "zhang2020approximation",
      "dataaugmentation",
      "zimmermann2021score",
      "Srinivas2017TrainingSN",
      "mena2018learning",
      "shao2022adversarial",
      "nguyen2016synthesizing",
      "bronstein2017geometric",
      "b29",
      "perarnau2016invertible",
      "kothari2021trumpets",
      "nalisnick",
      "baradad2021learning",
      "jin2021teachers",
      "lossy_vae"
    ],
    "relevant_ids": [
      "i-resnet",
      "he2016deep",
      "levy2018generalizing",
      "rezende15variational",
      "FFJORD",
      "song2017nice",
      "jacobsen2018irevnet",
      "maf",
      "dinh2016density",
      "berg2018sylvester",
      "glow",
      "nvp",
      "jacobsen2018excessive"
    ],
    "metrics": {
      "R@5": 0.0,
      "R@10": 0.07692307692307693,
      "R@20": 0.15384615384615385,
      "MRR": 0.14285714285714285,
      "hits": 2,
      "total_relevant": 13
    },
    "score": 0.06593406593406592,
    "timestamp": "2025-12-18T10:57:51.713255"
  },
  {
    "query": "Introduction Neural Network (NN)-based algorithms have made significant progresses in natural language generation tasks, including language modeling<|cite_0|>, machine translation<|cite_1|> and dialog systems<|cite_2|>. Despite the huge variety of applications and model architectures, natural language generation mostly relies on predicting the next word given previous contexts and other conditional information. A standard",
    "paper_id": "1907.12009",
    "retrieved_ids": [
      "llmsurvey2",
      "chen2017survey",
      "young2018recent",
      "context_conversation",
      "ferrando-etal-2023-explaining",
      "petroni-contextaffectslanguagemodels-2020",
      "journals/corr/JeanLFC17",
      "thoppilan-lamdalanguagemodelsdialog-2022",
      "minaee2024large",
      "cho2014properties",
      "maruf-etal-2019-selective",
      "neural_conversation",
      "Collobert2011",
      "zhou2023controlled",
      "costa2016character",
      "lewis2019bart",
      "ficler-goldberg-2017-controlling",
      "ref:ragsurvey2",
      "externVQA",
      "white2023neural",
      "elsken2019neural",
      "dong2019unified",
      "sun2021long",
      "li2022pretrained",
      "kim2016character",
      "feng2017memory",
      "li2016simple",
      "wang:2017:aaai",
      "maskp",
      "huang2017finish",
      "santra2020hierarchical",
      "ahuja2023mega",
      "gao2019dialog",
      "welleckneural",
      "zhou2019understanding",
      "ran2021guiding",
      "ren2020astudy",
      "bahdanau2016actor",
      "Gulcehre2016Pointing",
      "ahuja-etal-2023-mega",
      "kasai2020non",
      "ji2023survey",
      "qian2020glancing",
      "guo2019non",
      "showandtell",
      "fan2018hierarchical",
      "cho2016noisy",
      "Ranzato2015SequenceLT",
      "lee2018deterministic",
      "Shu2020LaNMT",
      "font2019equalizing"
    ],
    "relevant_ids": [
      "gehring2017convolutional",
      "inan2016tying",
      "mikolov2013distributed",
      "britz2017massive",
      "E17-2025",
      "wu2016google",
      "kim2016character",
      "yang2017breaking",
      "vaswani2017attention",
      "ba2016layer",
      "bahdanau2014neural",
      "shang2015neural",
      "dauphin2017language",
      "mccann2017learned",
      "merity2017regularizing",
      "jozefowicz2016exploring"
    ],
    "metrics": {
      "R@5": 0.0,
      "R@10": 0.0,
      "R@20": 0.0,
      "MRR": 0.04,
      "hits": 1,
      "total_relevant": 16
    },
    "score": 0.012,
    "timestamp": "2025-12-18T10:57:54.083901"
  },
  {
    "query": "Introduction \\label{sec:introduction} Object detection has achieved significant advances with the introduction of convolutional neural networks (CNN). The recent detection frameworks can be divided into two categories: (\\romannumeral 1) two-stage detectors<|cite_0|> and (\\romannumeral 2) one-stage detectors<|cite_1|>. In two-stage detectors, the first stage proposes a sparse set of candidate object regions. After",
    "paper_id": "1907.12736",
    "retrieved_ids": [
      "kang2016tcnn",
      "relationnetwork",
      "yu2016unitbox",
      "kim2016pvanet",
      "li2018detnet",
      "kang2016object",
      "li2017light",
      "CascadeRCNN",
      "pang2019libra",
      "fcos",
      "cai2018cascade",
      "cascade_rcnn",
      "sun2021sparse",
      "zhao2019object",
      "wang2021end",
      "FCOS",
      "zhang2020dynamic",
      "cascade",
      "zhang2018refinedet",
      "chen2021pix2seq",
      "qiao2021detectors",
      "FastRCNN",
      "cdn",
      "Keypoint",
      "agrawal2014analyzing",
      "xu2022fusionrcnn",
      "zhou2014object",
      "cspnet",
      "zhao2019m2det",
      "chen2021i3net",
      "han2016seqnms",
      "peng2020faster",
      "oza2021unsupervised",
      "law2018cornernet",
      "yang2019fast",
      "wu2023stmixer",
      "qiu2020borderdet",
      "2020UnionDet",
      "sun2018fishnet",
      "iandola2014densenet",
      "pan2020dynamic",
      "shrivastava2016training",
      "RetinaNet",
      "li2018exploring",
      "zhang2020bridging"
    ],
    "relevant_ids": [
      "liu2016ssd",
      "lin2014mscoco",
      "girshick2015fast",
      "redmon2016you",
      "dai2016rfcn",
      "ren2015faster",
      "he2017mask",
      "huang2017speed",
      "lin2017fpn",
      "redmon2016yolo9000",
      "lin2017focal",
      "sermanet2013overfeat",
      "luo2016understanding",
      "zhang2018refinedet"
    ],
    "metrics": {
      "R@5": 0.0,
      "R@10": 0.0,
      "R@20": 0.07142857142857142,
      "MRR": 0.05263157894736842,
      "hits": 1,
      "total_relevant": 14
    },
    "score": 0.015789473684210523,
    "timestamp": "2025-12-18T10:57:56.620038"
  },
  {
    "query": "Introduction \\begin{figure}[t] \\centering \\includegraphics[width=6cm]{intro.png} \\caption{Contrastive examples provide positive examples that illustrate the true intent, along with negative examples that show what characteristics we want LLMs to avoid.} \\label{fig-intro} \\end{figure} In recent years, large language models like GPT, Llama, and PaLM series have made significant progress in natural language processing<|cite_0|>, enabling",
    "paper_id": "2401.17390",
    "retrieved_ids": [
      "qin2024large",
      "minaee2024large",
      "survey0",
      "naveed2024comprehensivellms",
      "zhou2023instruction",
      "gpt_summary",
      "kaddour2023challenges",
      "Wu2023ASO",
      "mmllms2",
      "zhu2023multilingual",
      "makatura2023large",
      "fu2023mme",
      "lian2023llmgrounded",
      "wang2024large",
      "tang2023struc",
      "cd-reasoning",
      "Liu2023LLMRecBL",
      "ref:ragsurvey2",
      "openfunction",
      "qi2023finetuning",
      "jin2023time",
      "Pan2023AutomaticallyCL",
      "MIR-2022-03-068",
      "touvron2023llama",
      "woodpecker",
      "WizardMath",
      "gruver2023large",
      "wang2023visionllm",
      "huang2023survey",
      "wang2023cross",
      "Jiang2023LongLLMLinguaAA",
      "bansal2022how",
      "shrivastava2023llamas",
      "zeng2023lynx",
      "llmsurvey3",
      "contrastive-cot",
      "hu2023advancing",
      "chen2022controllable",
      "iti",
      "li2024llavamed",
      "dai2023can",
      "guide_paper",
      "agrawal-2023",
      "brohan2023rt2",
      "hu2022promptcap",
      "xie2021fignerf",
      "guo2023pointbind",
      "bai2023qwen",
      "allal2023santacoder",
      "madaan2022memory"
    ],
    "relevant_ids": [
      "brown2020gpt3",
      "meng2021coco",
      "honovich2022instruction",
      "alayrac2022flamingo",
      "sun2023autohint",
      "bommasani2021opportunities",
      "zhou2022large",
      "touvron2023llama",
      "radford2021clip",
      "su2022selective",
      "gao2020making",
      "gao2020dialoguerpt"
    ],
    "metrics": {
      "R@5": 0.0,
      "R@10": 0.0,
      "R@20": 0.0,
      "MRR": 0.041666666666666664,
      "hits": 1,
      "total_relevant": 12
    },
    "score": 0.012499999999999999,
    "timestamp": "2025-12-18T10:57:59.737004"
  },
  {
    "query": "Introduction \\label{sec:intro} The sound effects artists work with different types of sounds, some you could see on-screen (like footsteps or a car passing) you couldn't see but hear (like background noises that make the video feel more real and add to the story). On-screen sounds match what's happening in the",
    "paper_id": "2401.04394",
    "retrieved_ids": [
      "baradad2021learning",
      "soundnet",
      "xiao2020background_challenge_bgc",
      "lu2021omnimatte",
      "arandjelovic2017look",
      "Schiappa-ACM-2023",
      "cuiVarietySoundTimbreControllableVideo2023",
      "iashinTamingVisuallyGuided2021",
      "peng2024smoothvideo",
      "tang2023salmonn",
      "shefferHearYourTrue2023",
      "karacan2018manipulating",
      "gong2023listen",
      "huang2023audiogpt",
      "huang2024creativesynth",
      "yang2023diffsound",
      "CapDec",
      "zhu2015aligning",
      "choi2023towards",
      "yu2022metaformer",
      "gberta_2021_ICML",
      "huangNoise2MusicTextconditionedMusic2023",
      "tokenflow2023",
      "zhang2023avid",
      "zhang2020seqmix",
      "smilkov2017smoothgrad",
      "wang2023audit",
      "peng2024voicecraft",
      "weng2019photo",
      "yuan2023text",
      "garbin2022voltemorph",
      "stylegan",
      "KARIMI2020101759",
      "pons2021upsampling",
      "tewari2020state",
      "sharir2021image",
      "inpaint",
      "ma2024follow",
      "hall2023dig",
      "duConditionalGenerationAudio2023a",
      "EngelHantrakulGuRoberts2020",
      "esmaeilpour2020detection",
      "baltruvsaitis2018multimodal",
      "evans2024fast",
      "bhat2023loosecontrol",
      "wang2023seal",
      "huang2017like",
      "arjovsky2017wasserstein",
      "murphy2019relational",
      "goel2023interactive"
    ],
    "relevant_ids": [
      "chenMiniGPTv2LargeLanguage2023",
      "kreukAudioGen",
      "wang2023visionllm",
      "renFastSpeechFastHighQuality2022",
      "huangNoise2MusicTextconditionedMusic2023",
      "shefferHearYourTrue2023",
      "iashinTamingVisuallyGuided2021",
      "luoDiffFoleySynchronizedVideotoAudio2023",
      "agostinelliMusicLMGeneratingMusic2023",
      "touvron2023llama",
      "copetSimpleControllableMusic2023a",
      "huang2023make",
      "leePriorGradImprovingConditional2022",
      "liuAudioLDMTexttoAudioGeneration2023",
      "ghosalTexttoAudioGenerationUsing2023",
      "NaturalSpeechEndtoEndText2022",
      "lamEfficientNeuralMusic2023b",
      "Infergrad",
      "Grad-TTS",
      "peng2023kosmos",
      "cuiVarietySoundTimbreControllableVideo2023",
      "duConditionalGenerationAudio2023a",
      "chen2022resgrad",
      "lamBilateralDenoisingDiffusion2021"
    ],
    "metrics": {
      "R@5": 0.0,
      "R@10": 0.08333333333333333,
      "R@20": 0.125,
      "MRR": 0.14285714285714285,
      "hits": 5,
      "total_relevant": 24
    },
    "score": 0.06785714285714285,
    "timestamp": "2025-12-18T10:58:02.089971"
  },
  {
    "query": "Introduction \\label{sec:intro} \\begin{figure} \\centering \\includegraphics[width=\\linewidth]{figures/grad_vis-crop.pdf} \\caption{Visualization of SDS and LMC-SDS gradients \\wrt pixel values and estimated denoised images for the given image $\\image$, the prompt $\\cond=\\text{\\emph{``autumn''}}$, and $t=0.5$. We visualize the negative gradient, \\ie the direction of change. See text for details.} \\label{fig:grad_vis} \\end{figure} Image diffusion models<|cite_0|> have recently become",
    "paper_id": "2401.05293",
    "retrieved_ids": [
      "ddpm",
      "hu2022self",
      "ho2020denoising",
      "kazerouni2022diffusion",
      "croitoru2023diffusion",
      "miyake2023negative",
      "DiffSTE",
      "TextDiffuser",
      "levin2023differential",
      "armandpour2023re",
      "saharia2022palette",
      "mao2023guided",
      "simonyan2013deep",
      "humansd",
      "wimbauer2023cache",
      "chen2023importance",
      "po2023state",
      "balaji2022ediff",
      "guo2024initno",
      "moser2024diffusion",
      "liu2023instaflow",
      "lee2022progressive",
      "kirillov2020pointrend",
      "farshad2023scenegenie",
      "park2023understanding",
      "Niu2023CDPMSRCD",
      "Grad-TTS",
      "zhang2023text",
      "poole2022dreamfusion",
      "SAS",
      "li2022upainting",
      "chen2023pixartalpha",
      "visprompt",
      "hertz2023delta",
      "lee2023holistic",
      "pixelbert",
      "prabhudesai2023aligning",
      "lee2023syncdiffusion",
      "ghiasi2022scaling",
      "xie2021fignerf",
      "voynov2023p+",
      "liu2022design",
      "MAGIC",
      "nerfstudio",
      "doodl",
      "santa2017deeppermnet",
      "dialog-gen",
      "wu2024hd",
      "youwang2024paint",
      "inpaint",
      "wu2023easyphoto",
      "katzir2023noise",
      "li2024llavamed",
      "huberman2023ddpminv",
      "smilkov2017smoothgrad"
    ],
    "relevant_ids": [
      "saharia2022palette",
      "lugmayr2022repaint",
      "chung2022improving",
      "gao2023implicit",
      "poole2022dreamfusion",
      "nichol2021glide",
      "hertz2023delta",
      "saharia2022photorealistic",
      "ramesh2022hierarchical",
      "rombach2022high",
      "lin2023magic3d",
      "chung2022diffusion",
      "wang2023prolificdreamer",
      "jain2023vectorfusion",
      "kawar2023imagic",
      "zou2023sparse3d",
      "kawar2022denoising",
      "croitoru2023diffusion",
      "meng2022sdedit",
      "zhou2023sparsefusion",
      "kim2023collaborative",
      "wang2023score",
      "metzer2023latent",
      "ho2020denoising",
      "liu2016stein",
      "ruiz2023dreambooth",
      "Liu_2023_ICCV",
      "kolotouros2023dreamhuman"
    ],
    "metrics": {
      "R@5": 0.07142857142857142,
      "R@10": 0.07142857142857142,
      "R@20": 0.10714285714285714,
      "MRR": 0.3333333333333333,
      "hits": 5,
      "total_relevant": 28
    },
    "score": 0.15,
    "timestamp": "2025-12-18T10:58:06.494942"
  },
  {
    "query": "Introduction \\label{sec:intro} \\begin{table}[ht] \\setlength{\\tabcolsep}{1.0mm} \\caption{Our method enhances the quality of 3D reconstruction based on various state-of-the-art (SOTA) multi-view generators. We evaluate geometric quality with Chamfer Distance (CD) and Volume Intersection over Union (IoU), and RGB quality with PSNR, SSIM, and LPIPS, using GSO dataset for SyncDreamer and Zero123, and ShapeNet",
    "paper_id": "2401.15841",
    "retrieved_ids": [
      "pix2vox",
      "yu2024gsdf",
      "reizenstein2021common",
      "wang2021neus",
      "im3d",
      "neus",
      "popov2020corenet",
      "hong2023lrm",
      "epidiff",
      "liu2023syncdreamer",
      "murez2020atlas",
      "szymanowicz24splatter",
      "xie2020pix2vox++",
      "xu2019disn",
      "shi2023zero123++",
      "sayed2022simplerecon",
      "rezatofighi2019generalized",
      "v3d",
      "zero12345plus",
      "or2022stylesdf",
      "ling2023dl3dv",
      "wei2024meshlrm",
      "fastmesh",
      "wu2024texro",
      "ye2021improving",
      "skorokhodov2022epigraf",
      "zhao2023efficientdreamer",
      "bd23",
      "spring",
      "wang2021tuta",
      "zheng2022sdf",
      "jiang20233d",
      "deitke2023objaverse",
      "long2023wonder3d",
      "sun2022benchmarking",
      "chen2024textto3dgsgen",
      "mo2023dit",
      "lee20223d",
      "yan2023multi",
      "gao2022dynamic",
      "li2023preim3d",
      "ye2023gaussian",
      "zhao2022generative",
      "tang2022self",
      "zhang2024gsw",
      "chen2024hac",
      "cascade",
      "Huang2DGS2024",
      "dreamfields",
      "garrido2022duality",
      "lee2023codi",
      "yu2023text",
      "liu2023instaflow",
      "zhang2022dynamic",
      "bhat2023loosecontrol"
    ],
    "relevant_ids": [
      "shi2023zero123++",
      "chan2022efficient",
      "ramesh2022hierarchical",
      "karras2021alias",
      "ho2020denoising",
      "rombach2022high",
      "lin2023magic3d",
      "liu2023syncdreamer",
      "ren2020deep",
      "liu2023zero",
      "karras2019style",
      "wang2023prolificdreamer",
      "wang2021neus",
      "lin2023consistent123",
      "poole2022dreamfusion"
    ],
    "metrics": {
      "R@5": 0.06666666666666667,
      "R@10": 0.13333333333333333,
      "R@20": 0.2,
      "MRR": 0.25,
      "hits": 3,
      "total_relevant": 15
    },
    "score": 0.14166666666666666,
    "timestamp": "2025-12-18T10:58:10.057096"
  },
  {
    "query": "Introduction As artificial neural networks and the datasets used to train them have rapidly increased in size and complexity, it has become increasingly important to identify which examples affect learning and how. Indeed, in recent years a growing number of proposals have appeared for scoring and ranking individual examples in",
    "paper_id": "2401.01867",
    "retrieved_ids": [
      "paul2021deep",
      "baldock2021deep",
      "yuan2019adversarial",
      "overview",
      "wistuba2019survey",
      "hendrycks2016baseline",
      "carlini2017adversarial",
      "elsken2019neural",
      "teney2022id",
      "sun2019optimization",
      "white2023neural",
      "power2022grokking",
      "real2020automlzero",
      "real2017large",
      "sur2",
      "Zonghan2019",
      "agrawal2014analyzing",
      "yu2019evaluating",
      "xie2017mitigating",
      "carlini2019distribution",
      "lim2020federated",
      "gao2018ican",
      "goodfellow2015efficient",
      "Carlini_Dill_2018",
      "radosavovic2019network",
      "akhtar2018threat",
      "overview3",
      "le2020contrastive",
      "li2016understanding",
      "dhamdhere2018important",
      "frosst2017distilling",
      "b1",
      "ash2021investigating",
      "jiang2019fantastic",
      "nohyun2023data",
      "barsbey2021heavy",
      "twins",
      "mellor2021neural",
      "carlini2022quantifying",
      "Alvarez_2017",
      "barrett2018measuring",
      "elsken2018efficient",
      "chakraborty2018adversarial",
      "jiang2020characterizing",
      "MIR-2022-03-068",
      "zhang2022and",
      "blalock2020state",
      "white2021powerful",
      "wu2020adversarial",
      "yosinski2014transferable",
      "perera2023analyzing",
      "xu2019adversarial",
      "kurakin_nips2017competition",
      "imagenet-v2",
      "huh2016what"
    ],
    "relevant_ids": [
      "carlini2019distribution",
      "siddiqui2022metadata",
      "yang2023identifying",
      "agarwal2020estimating",
      "sorscher2022beyond",
      "hooker2019compressed",
      "baldock2021deep",
      "nohyun2023data",
      "toneva2018an",
      "jiang2019accelerating",
      "jiang2020characterizing",
      "swayamdipta2020dataset",
      "Pruthi2020",
      "paul2021deep",
      "feldman2020neural",
      "jin2022pruning",
      "birodkar2019semantic",
      "northcutt2021confident",
      "hooker2020characterising",
      "pleiss2020identifying"
    ],
    "metrics": {
      "R@5": 0.1,
      "R@10": 0.1,
      "R@20": 0.15,
      "MRR": 1.0,
      "hits": 5,
      "total_relevant": 20
    },
    "score": 0.37,
    "timestamp": "2025-12-18T10:58:12.297479"
  },
  {
    "query": "Introduction Multi-agent collaborative perception promotes better and more holistic perception by enabling multiple agents to share complementary perceptual information with each other<|cite_0|>. This task can fundamentally overcome several long-standing issues in single-agent perception, such as occlusion<|cite_1|>. The related methods and systems have tremendous potential in many applications, including multi-UAVs (unmanned",
    "paper_id": "2401.13964",
    "retrieved_ids": [
      "li2022v2x",
      "li2021learning",
      "xi2023rise",
      "li2023amongus",
      "liu2020when2com",
      "xu2022v2xvit",
      "liu2020who2com",
      "hu2022where2comm",
      "agarwal2019learning",
      "wei2023asynchronyrobust",
      "lei2022latency",
      "xiang2023hm",
      "song2018collaborative",
      "gibson",
      "wang2020v2vnet",
      "iqbal2019actor",
      "BEV-survey",
      "zhang2022beverse",
      "Du2018TheUA",
      "zhu2022uni",
      "kinose2022multi",
      "xu2022cobevt",
      "lu2023robust",
      "francis2022core",
      "gu2022vip3d",
      "dreissig2023survey",
      "hill2019environmental",
      "wu2023mars",
      "tian2019contrastive",
      "hu2023collaboration",
      "liu2024uncertainty",
      "vadivelu2021learning",
      "yahya2017collective",
      "zhu2024sora",
      "graph_topo"
    ],
    "relevant_ids": [
      "liu2020when2com",
      "yu2022dair",
      "ma2019self",
      "wang2020v2vnet",
      "hu2023collaboration",
      "xu2022opv2v",
      "cui2022coopernaut",
      "liu2020who2com",
      "zhu2023learning",
      "bai2022transfusion",
      "yang2022deepinteraction",
      "li2022homogeneous",
      "lei2022latency",
      "vora2020pointpainting",
      "xu2022fusionrcnn",
      "vadivelu2021learning",
      "xiang2023hm",
      "lu2023robust",
      "li2021learning",
      "wei2023asynchronyrobust",
      "liu2022bevfusion",
      "borse2023x",
      "chen2022bevdistill",
      "li2022v2x",
      "chen2022autoalignv2",
      "li2023amongus",
      "chen2022learning",
      "li2022deepfusion",
      "hu2022where2comm",
      "li2022voxel"
    ],
    "metrics": {
      "R@5": 0.13333333333333333,
      "R@10": 0.23333333333333334,
      "R@20": 0.3333333333333333,
      "MRR": 1.0,
      "hits": 13,
      "total_relevant": 30
    },
    "score": 0.42333333333333334,
    "timestamp": "2025-12-18T10:58:14.264389"
  },
  {
    "query": "Introduction \\label{sec:introduction} \\begin{figure}[ht!] \\centering \\includegraphics[width=\\linewidth]{Figures/fig_teaser.pdf} \\caption{\\textbf{An Overview of GRAM.} We suggest an interleaved encoder architecture combining page- with document-attention layers, allowing information to propagate between different pages. An optional compression transformer (C-former) is introduced to allow a trade-off between quality and latency.} \\label{Fig:fig_teaser} \\end{figure} Document understanding, particularly in the context",
    "paper_id": "2401.03411",
    "retrieved_ids": [
      "yang2023introduction",
      "appalaraju2021docformer",
      "powalski2021going",
      "tnt",
      "obelics",
      "liu2019text",
      "wang2020cluster",
      "Yang:CVPR19",
      "cornia2020meshed",
      "humeau2019poly",
      "ga",
      "branchformer",
      "wu2021rethinking",
      "zheng2020document",
      "gal2023encoder",
      "unconstrained",
      "DiffSTE",
      "yao2023dual",
      "huang2019attentio",
      "liu2019hierarchical",
      "MEBERT",
      "fang2021msg",
      "rethinking",
      "maruf-etal-2019-selective",
      "namburi2023cost",
      "fang2020cert",
      "vedaldi2016instance",
      "zheng2021rethinking_seg",
      "tito2022hierarchical",
      "Xie:2021:SSE",
      "dai2023can",
      "xie2021fignerf",
      "podell2023sdxl",
      "nogueira2019monobert",
      "lee2023holistic",
      "hu2023advancing",
      "kitaev2018constituency",
      "wang2023knowledge",
      "UdiffText",
      "yuan2024llminferenceunveiledsurvey",
      "voynov2023p+",
      "dialog-gen",
      "li2022upainting",
      "wei2023elite",
      "dreamllm",
      "kandala2024pix2gif",
      "jin2019towards",
      "zhang2024moonshot",
      "liang2022nuwa",
      "nitzan2023domain",
      "xia2023speculative",
      "ma2019monotonic",
      "schneider2023archisound",
      "dong2024promptpromptedadaptivestructuredpruning",
      "bolya2023token",
      "Kandpal2022LargeLM",
      "sharir2021image"
    ],
    "relevant_ids": [
      "bulatov2023scaling",
      "biten2022latr",
      "peng2022ernie",
      "aberdam2023clipter",
      "vaswani2017attention",
      "mathew2021docvqa",
      "aberdam2021sequence",
      "raffel2020exploring",
      "landeghem2023document",
      "litman2020scatter",
      "liu2023visual",
      "powalski2021going",
      "tang2023unifying",
      "xu2020layoutlm",
      "appalaraju2021docformer",
      "openai2023gpt",
      "ainslie2023colt5",
      "xu2020layoutlmv2",
      "appalaraju2023docformerv2",
      "ganz2023towards",
      "dai2019transformer",
      "zaheer2020big",
      "huang2022layoutlmv3",
      "press2021train",
      "tito2022hierarchical",
      "chen2023extending",
      "nuriel2022textadain",
      "li2023blip",
      "aberdam2022multimodal",
      "beltagy2020longformer"
    ],
    "metrics": {
      "R@5": 0.06666666666666667,
      "R@10": 0.06666666666666667,
      "R@20": 0.06666666666666667,
      "MRR": 0.5,
      "hits": 3,
      "total_relevant": 30
    },
    "score": 0.19666666666666666,
    "timestamp": "2025-12-18T10:58:18.383002"
  },
  {
    "query": "Introduction While neural networks have achieved state-of-the-art accuracy in classification problems, it is by now well known that networks trained with standard \\emph{error risk minimization} (ERM) can be exceedingly brittle<|cite_0|>. As a result, various works have suggested replacing ERM with alternative training procedures that enforce robustness. In this paper, we",
    "paper_id": "2401.09191",
    "retrieved_ids": [
      "tsipras2018robustness",
      "raghunathanUnderstandingMitigatingTradeoff2020",
      "xu2020exploring",
      "weng2018evaluating",
      "jung2023neural",
      "laugros2020addressing",
      "huang2020self",
      "Hendrycks2019BenchmarkingNN",
      "sur2",
      "YL1",
      "hendrycks2019augmix",
      "goodfellow2014qualitatively",
      "optimal-transport",
      "croce2018provable",
      "liang2018understanding",
      "wu2021wider",
      "calib",
      "rosenfeld2020risks",
      "hein2017formal",
      "li2021neural",
      "zhai2019adversarially",
      "tsp2019",
      "zhangefficient",
      "piratla2021focus",
      "andriushchenko2022towards",
      "hendrycks2016baseline",
      "zhang2019type",
      "Yang2020",
      "brau",
      "symmetric",
      "chaudhuri2011differentially",
      "zhang-shellnet-iccv19",
      "Tsuzuku_Sato_Sugiyama_2018",
      "ziyin2020neural",
      "miyashita2016logquant",
      "GCE",
      "huang2020combining",
      "hein2019relu",
      "yang2021taxonomizing",
      "lucas2021analyzing",
      "kamath2021does",
      "izmailov2022feature",
      "liu2022devil",
      "murugesan2022calibrating",
      "cha2021swad",
      "lafon2024understanding",
      "Dinh2017",
      "chidambaram2022provably",
      "singla2021curvature",
      "raghunathan2018certified",
      "zhou2020bypassing",
      "ma2020normalized",
      "kairouz2020fast",
      "Raghunathan2018"
    ],
    "relevant_ids": [
      "qin2019imperceptible",
      "yin2019rademacher",
      "madry2017towards",
      "chen2017zoo",
      "Bhagoji2019LowerBO",
      "tramer2018ensemble",
      "Pydi2021AdversarialRV",
      "bungert2022gamma",
      "Trillos2020AdversarialCN",
      "trillos2023multimarginal",
      "dai2023characterizing",
      "khim2019adversarial",
      "goodfellow2014explaining",
      "cai2021zeroth",
      "Altschuler2022",
      "tsipras2018robustness",
      "weng2018evaluating"
    ],
    "metrics": {
      "R@5": 0.11764705882352941,
      "R@10": 0.11764705882352941,
      "R@20": 0.11764705882352941,
      "MRR": 1.0,
      "hits": 2,
      "total_relevant": 17
    },
    "score": 0.38235294117647056,
    "timestamp": "2025-12-18T10:58:21.163307"
  },
  {
    "query": "Introduction Consider $\\mathcal{S}$ as an action characterizing a quantum field theory, with $S$ representing its discretization to a lattice.<|cite_0|> suggest a method for sampling from the lattice quantum field theory described by $S$. This involves a normalizing flow parameterizing a probability density $q_\\theta(\\phi)$ of discrete fields over the lattice, and",
    "paper_id": "2401.00828",
    "retrieved_ids": [
      "flow1",
      "Albergo_2021",
      "kobyzev2020normalizing",
      "papamakarios2019normalizing",
      "flow2",
      "nicoli2023detecting",
      "flow3",
      "gerdes2022learning",
      "rezende15variational",
      "albergo2023building",
      "cnf1",
      "koehler2021representational",
      "albergo2019flow",
      "liu2021sampling",
      "chen2022sampling",
      "zhang2022generative",
      "mate2023learning",
      "nicoli2021estimation",
      "eqFlows",
      "OD10",
      "albergo2023stochastic",
      "zhang2022deis",
      "algSP0",
      "yang2023introduction",
      "STLattice2018CVPR",
      "mildenhall2020nerf",
      "basri2020frequency",
      "tishby2000information",
      "liu2019rao",
      "hammond2011wavelets",
      "lim2023scorebased",
      "hokamp2017lexically",
      "NG11"
    ],
    "relevant_ids": [
      "flow1",
      "flow3",
      "kovachki2021neural",
      "mate2023learning",
      "albergo2019flow",
      "eqFlows",
      "nicoli2021estimation",
      "nicoli2023detecting",
      "voleti2021multi",
      "Albergo_2021",
      "flow2",
      "BoltzmannGenerators",
      "Nicoli_2020",
      "chen2018neural",
      "cnf1",
      "gerdes2022learning",
      "hagemann2023multilevel"
    ],
    "metrics": {
      "R@5": 0.17647058823529413,
      "R@10": 0.35294117647058826,
      "R@20": 0.6470588235294118,
      "MRR": 1.0,
      "hits": 11,
      "total_relevant": 17
    },
    "score": 0.4764705882352941,
    "timestamp": "2025-12-18T10:58:22.871834"
  },
  {
    "query": "Introduction Deep neural networks (DNNs) have been shown to be susceptible to adversarial examples<|cite_0|>, which are generated by adding small, human-imperceptible perturbations to natural images, but completely change the prediction results to DNNs, leading to disastrous implications<|cite_1|>. Since then, numerous methods have been proposed to defend against adversarial examples. Notably,",
    "paper_id": "2401.16352",
    "retrieved_ids": [
      "xiao2018generating",
      "Gu2015",
      "yuan2019adversarial",
      "sur2",
      "PMJ+16",
      "rakin2018defend",
      "zhao2017generating",
      "alzantot2018generating",
      "overview",
      "kurakin2016adversarial",
      "ma2021understanding",
      "jia2019comdefend",
      "meng2017magnet",
      "xu2017feature",
      "chen2018ead",
      "papernot2016distillation",
      "li2019nattack",
      "potdevin19",
      "akhtar2018threat",
      "narodytska2016simple",
      "yan2018deep",
      "wu2018understanding",
      "duan2021advdrop",
      "xu2019adversarial",
      "chakraborty2018adversarial",
      "xiao2018spatially",
      "kang2021stable",
      "dong2020adversarially",
      "jakubovitz2018improving",
      "NRP",
      "samangouei2018defense",
      "vijaykeerthy2018hardening",
      "li2021neural",
      "su2019one",
      "dong2019evading",
      "biggio_2018",
      "xiao2019meshadv",
      "xue2024pixelbarrierdiffusionmodels",
      "lamb2019interpolated",
      "nguyen2015deep",
      "xiao2018characterizing",
      "LID",
      "naseer2018task",
      "ganeshan2019fda",
      "das2017keeping"
    ],
    "relevant_ids": [
      "deng2020analysis",
      "tack2022consistency",
      "yang2019me",
      "nie2022diffusion",
      "tramer2020adaptive",
      "chen2022adversarial",
      "zhang2019theoretically",
      "madry2017towards",
      "szegedy2013intriguing",
      "laidlaw2021perceptual",
      "carlini2017towards",
      "poursaeed2021robustness",
      "croce2020reliable",
      "xiao2018spatially",
      "wudenoising",
      "goodfellow2014explaining",
      "madry2018towards",
      "shi2021online"
    ],
    "metrics": {
      "R@5": 0.0,
      "R@10": 0.0,
      "R@20": 0.0,
      "MRR": 0.038461538461538464,
      "hits": 1,
      "total_relevant": 18
    },
    "score": 0.011538461538461539,
    "timestamp": "2025-12-18T10:58:25.156675"
  },
  {
    "query": "Introduction \\label{sec:intro} In recent years, tremendous progress has been made for re-renderable real-world environment reconstruction using the neural radiance field (NeRF)<|cite_0|>. Concurrently, text-guided object generation<|cite_1|> has demonstrated significant potential in creating novel 3D contents. In this work, we investigate a novel problem: generate 3D objects that harmonize with given 3D",
    "paper_id": "2401.05750",
    "retrieved_ids": [
      "tewari2022advances",
      "zhang2023text2nerf",
      "xie2021fignerf",
      "wang2023nerf",
      "mildenhall2020nerf",
      "lin2022magic3d",
      "wu2024recent",
      "gordon2023blended",
      "pumarola2020d",
      "metzer2022latentnerf",
      "wang2022clip",
      "pumarola2021d",
      "latentnerf",
      "xu2022discoscene",
      "yu2023edit",
      "tsalicoglou2023textmesh",
      "BungeeNeRF",
      "Joo",
      "nguyen2022snerf",
      "liu2023unidream",
      "park2023ed",
      "Zhang2023Nerflets",
      "blending",
      "mikaeili2023sked",
      "jeong2022perfception",
      "martin2021nerf",
      "rematas2022urban",
      "Cohen-Bar_2023_setthescene",
      "li2024dreamscene",
      "martinbrualla2020nerfw",
      "earle2024dreamcraft",
      "zhou2024dreamscene360",
      "zero12345plus",
      "park2021hypernerf",
      "lin2021barf",
      "verbin2022ref",
      "jiang2023alignerf",
      "hu2023tri",
      "gu2023nerfdiff",
      "wu2024reconfusion",
      "wang2023breathing",
      "zhao2023efficientdreamer",
      "repaintnerf",
      "cheng2023tuvf",
      "tang2022compressible",
      "liu2021editing",
      "boss2021nerd",
      "bokhovkin2023mesh2tex",
      "chen2022hanerf",
      "chen2023gaussianeditor",
      "kobayashi2022decomposing",
      "zhou2023feature"
    ],
    "relevant_ids": [
      "fridovich2022plenoxels",
      "jain2022zero",
      "park2021nerfies",
      "barron2021mip",
      "wang2022clip",
      "muller2022instant",
      "mirzaei2023spin",
      "poole2022dreamfusion",
      "mildenhall2021nerf",
      "yuan2022nerf",
      "huang2022stylizednerf",
      "suvorov2021resolution",
      "lin2023magic3d",
      "tang2023make",
      "wang2023prolificdreamer",
      "bai2023learning",
      "pumarola2021d",
      "barron2022mip",
      "mirzaei2023reference",
      "xu2022point",
      "gordon2023blended",
      "mildenhall2019llff",
      "dai2023hybrid",
      "liu2021editing",
      "yu2021pixelnerf",
      "raj2023dreambooth3d",
      "po2023compositional",
      "yu2023text",
      "barron2023zip",
      "haque2023instruct",
      "yang2022neumesh",
      "kobayashi2022decomposing",
      "liu2023zero1to3",
      "Rombach_2022_CVPR",
      "kolotouros2023dreamhuman"
    ],
    "metrics": {
      "R@5": 0.0,
      "R@10": 0.02857142857142857,
      "R@20": 0.08571428571428572,
      "MRR": 0.125,
      "hits": 5,
      "total_relevant": 35
    },
    "score": 0.04607142857142857,
    "timestamp": "2025-12-18T10:58:28.601711"
  },
  {
    "query": "Introduction Today, the advent of large language models (LLMs) and their advanced prompting strategies has marked a significant progression, especially in classical NLP tasks<|cite_0|>. A key innovation among these is the Chain of Thought (CoT) prompting technique<|cite_1|>, known for its efficacy in multi-step problem solving. This technique, reflecting human sequential",
    "paper_id": "2401.04925",
    "retrieved_ids": [
      "naveed2024comprehensivellms",
      "zhang2022automatic",
      "kim2023language",
      "wei2022chain",
      "zhang2023multimodal",
      "besta2024graph",
      "reynolds2021prompt",
      "qin2024large",
      "qu2024tool",
      "yu2023towards",
      "sahoo2024systematic",
      "zheng2023progressivehint",
      "bbh",
      "gpt_summary",
      "wang2023boosting",
      "jagerman2023query",
      "ranaldi2023empowering",
      "turpin2023language",
      "Pan2023AutomaticallyCL",
      "shao2023synthetic",
      "wang-etal-2022-iteratively",
      "wang-etal-2023-towards",
      "trivedi2022interleaving",
      "wu2023analyzing",
      "feng2024towards",
      "qin2023cross",
      "llmsurvey3",
      "yasunaga2023analogical",
      "Zheng2023SecretsOR",
      "madaan2022text",
      "yeo2024interpretable",
      "chen2022program",
      "suzgun2024meta",
      "tai2023exploring",
      "Huang2023LargeLM",
      "ref:ragsurvey2",
      "lanham2023measuring",
      "lyu-etal-2023-faithful",
      "wang2023prompt",
      "kojima2022large",
      "ravi2024small"
    ],
    "relevant_ids": [
      "wang2023selfconsistency",
      "merrill2023expressive",
      "yao2023tree",
      "lyu2023faithful",
      "madaan2022text",
      "wei2022chain",
      "zhang2022automatic",
      "fu2023complexitybased",
      "shao2023synthetic",
      "brown2020language",
      "wu2023analyzing",
      "tang2023large",
      "schaeffer2023emergent",
      "kojima2023large"
    ],
    "metrics": {
      "R@5": 0.14285714285714285,
      "R@10": 0.14285714285714285,
      "R@20": 0.21428571428571427,
      "MRR": 0.5,
      "hits": 5,
      "total_relevant": 14
    },
    "score": 0.25,
    "timestamp": "2025-12-18T10:58:30.955143"
  },
  {
    "query": "Introduction In recent years, computer vision and computer graphics have achieved notable advancements, particularly in the neural rendering area. Neural Radiance Field(NeRF)<|cite_0|> and its subsequent methods have propelled the development of neural scene representations, which have shown significant capabilities for novel view synthesis. Besides learning a radiance field, some methods",
    "paper_id": "2401.05925",
    "retrieved_ids": [
      "mildenhall2020nerf",
      "tretschk2021non",
      "nerfstudio",
      "du2021neural",
      "hedman2021baking",
      "yang2021learning",
      "cao2022fwd",
      "niemeyer2022regnerf",
      "yen2022nerf",
      "BungeeNeRF",
      "xie2021neural",
      "xu2021h",
      "pumarola2020d",
      "wu2024recent",
      "pumarola2021d",
      "martinbrualla2020nerfw",
      "chen2023mobilenerf",
      "zhang2020nerf++",
      "martin2021nerf",
      "Guo_2022_CVPR",
      "jang2022dtensorf",
      "nerf--",
      "lin2021barf",
      "chen2020neural",
      "deng2022fov",
      "yuan2022nerf",
      "verbin2022ref",
      "wei2021nerfingmvs",
      "truong2023sparf",
      "xu2022point",
      "liu2022neural",
      "kerbl20233d",
      "barron2023zip",
      "wang2023digging",
      "cao2023hexplane",
      "plenoxels",
      "rebain2021derf",
      "liu2021editing",
      "chen2022hanerf",
      "smith2022unsupervised",
      "kobayashi2022decomposing",
      "malarz2024gaussiansplattingnerfbasedcolor",
      "zhou2023feature"
    ],
    "relevant_ids": [
      "hu2023tri",
      "fridovich2022plenoxels",
      "barron2021mip",
      "goel2023interactive",
      "kirillov2023segment",
      "muller2022instant",
      "chen2022tensorf",
      "cheng2022masked",
      "mildenhall2021nerf",
      "ye2023gaussian",
      "qi2017pointnet",
      "tschernezki2022neural",
      "wang2022dm",
      "landrieu2018large",
      "barron2022mip",
      "kerbl20233d",
      "hu2020randla",
      "zhou2023feature",
      "caron2021emerging",
      "zhang2020nerf++",
      "sun2022direct",
      "siddiqui2023panoptic",
      "qi2017pointnet++",
      "kobayashi2022decomposing",
      "cen2023segment",
      "zhi2021place"
    ],
    "metrics": {
      "R@5": 0.0,
      "R@10": 0.0,
      "R@20": 0.038461538461538464,
      "MRR": 0.05555555555555555,
      "hits": 4,
      "total_relevant": 26
    },
    "score": 0.016666666666666666,
    "timestamp": "2025-12-18T10:58:33.031207"
  },
  {
    "query": "Introduction \\thispagestyle{plain} \\label{sec:intro} While current methods outperform humans in recognizing images of diverse styles using large-scale labeled benchmarks, such extensive human annotations are not always available for training. Therefore, a large number of recognition models focus on extracting information from abundant unlabeled data. Semi-supervised learning (SSL)<|cite_0|> is a well-known and",
    "paper_id": "2401.13325",
    "retrieved_ids": [
      "joulin2016weakly",
      "zhao2022exploiting",
      "tian2021divide",
      "article31",
      "wang2022does",
      "yalniz2019billion",
      "sohn2020simple",
      "yang2022survey",
      "ZWHWWOS21",
      "cao2021open",
      "SBCZZRCKL20",
      "chen2022semi",
      "liu2020self",
      "Kingma2014SemisupervisedLW",
      "iscen2019label",
      "zhu2021residual",
      "graikos2023learned",
      "cheplygina2019not",
      "aberdam2022multimodal",
      "lee2024computer",
      "datasetgan",
      "gyawali2019semi",
      "zhang2022leverage",
      "2020Learning",
      "chen23l_interspeech",
      "Li2021SemanticSW",
      "sohn2020fixmatch",
      "zhang2021flexmatch",
      "Tian2021UnderstandingSL",
      "ke2020guided",
      "Schiappa-ACM-2023",
      "singh2022swag",
      "chaitanya2020contrastive",
      "seer",
      "huang2022survey",
      "liu2021unbiased",
      "smith2020building",
      "ericsson2021well",
      "fogel2020scrabblegan",
      "goyal2021selfsupervised",
      "oquab2023dinov2",
      "akata2016multi",
      "chen2020mixtext",
      "newell2020how",
      "vaze2022generalized",
      "an2022generalized",
      "pham2021meta",
      "nakashima2021can"
    ],
    "relevant_ids": [
      "vaze2022generalized",
      "han2021autonovel",
      "fini2021unified",
      "zhong2021openmix",
      "wang2021combating",
      "zhao2020distilling",
      "he2020momentum",
      "zhao2021novel",
      "zhang2021flexmatch",
      "caron2021emerging",
      "sohn2020fixmatch",
      "wang2020double",
      "cao2021open",
      "berthelot2019mixmatch",
      "gidaris2018unsupervised"
    ],
    "metrics": {
      "R@5": 0.0,
      "R@10": 0.06666666666666667,
      "R@20": 0.06666666666666667,
      "MRR": 0.1,
      "hits": 4,
      "total_relevant": 15
    },
    "score": 0.05,
    "timestamp": "2025-12-18T10:58:36.218639"
  },
  {
    "query": "Introduction \\label{sec:intro} Adversarial Training (AT)<|cite_0|> and TRADES<|cite_1|> have emerged as prominent training methods for training robust architectures. However, these training mechanisms involve solving an inner optimization problem per training step, often requiring an order of magnitude more time per iteration in comparison to standard training<|cite_2|>. To address the computational overhead",
    "paper_id": "2401.11618",
    "retrieved_ids": [
      "bai2021recent",
      "wang2019convergence",
      "andriushchenko2020understanding",
      "xu2020exploring",
      "addepalli2022efficient",
      "huang2015learning",
      "yao2018hessian",
      "miyato2016adversarial",
      "huang2021exploring",
      "Pang2021BagOT",
      "wong2020fast",
      "kurakin2016adversarial",
      "cheng2020cat",
      "wangOnceforallAdversarialTraining2020",
      "Deb+23",
      "mottaghi2019adversarial",
      "wang2019bilateral",
      "ding2019mma",
      "zheng2020efficient",
      "jorge2022NFGSM",
      "Goodfellow2016",
      "li2023less",
      "li2021neural",
      "Shafahi2020Adversarially",
      "zhang2019you",
      "rebuffi-et-al:scheme",
      "rebuffi2021fixing",
      "mok2021advrush",
      "vivek2020single",
      "guo2020meets",
      "xu2020adversarial",
      "tack2022consistency",
      "zhang2019defense",
      "deacl",
      "shafahi2019adversarial",
      "tjeng2017evaluating",
      "croce2021-multiple",
      "pmlr-v119-singla20a",
      "Li20AAAI",
      "Xie2020SmoothAT",
      "hu2019triple",
      "watson2021learning",
      "liu2022towards",
      "li2020gan",
      "shin2020two",
      "yao2019trust",
      "DBLP:journals/corr/GruslysMDLG16",
      "johnson2016perceptual",
      "zhou2022dataset",
      "Sharath",
      "declip",
      "hedman2021baking"
    ],
    "relevant_ids": [
      "shafahi2019ATFree",
      "Zhang2019TRADES",
      "tram\u00e8r2018R-FGSM",
      "qin2019adversarial",
      "li2023understanding",
      "sriramanan2020GAT",
      "xu2023dydart",
      "andriushchenko2020GradAlign",
      "moosavi2019robustness",
      "singla2021curvature",
      "etmann2019double_backprop",
      "ross2018improving",
      "simon2019first",
      "jorge2022NFGSM",
      "madry2018AT",
      "wong2020FAT"
    ],
    "metrics": {
      "R@5": 0.0,
      "R@10": 0.0,
      "R@20": 0.0625,
      "MRR": 0.05,
      "hits": 1,
      "total_relevant": 16
    },
    "score": 0.015,
    "timestamp": "2025-12-18T10:58:39.400315"
  },
  {
    "query": "Introduction \\label{sec:introduction} \\begin{figure} \\centering \\includegraphics[width=0.85\\columnwidth]{figures/graphical_abstract.png} \\caption{ Method overview. Our approach involves two main steps: i) {\\color[HTML]{e03131}\\textit{learn the concept}} and ii) {\\color[HTML]{e03131}\\textit{learn the condition}}. For the first step, we utilise a high-capacity model that has previously learned a general-purpose image prior. For simplicity, we use Stable Diffusion, which we inject with",
    "paper_id": "2401.03152",
    "retrieved_ids": [
      "kazerouni2022diffusion",
      "saharia2022palette",
      "zhang2023forget",
      "balaji2022ediff",
      "hu2022self",
      "b8",
      "wang2023instructedit",
      "lian2023llmgrounded",
      "chen2022analog",
      "mao2023guided",
      "guo2024initno",
      "moser2024diffusion",
      "li2023your",
      "wang2023patch",
      "chen2023pixartalpha",
      "wimbauer2023cache",
      "meng2022distillation",
      "zhang2023text",
      "ye2023ip-adapter",
      "luo2023latent",
      "gui_automation1",
      "voleti2022mcvd",
      "mokady2022nti",
      "ceylan2023pix2video",
      "kumari2022multi",
      "shen2024finetuning",
      "liu2023improved",
      "li2022upainting",
      "mogadala2017describing",
      "liu2021pndm",
      "hoogeboom2021argmax",
      "ChandraK16",
      "ghosh2023geneval",
      "instructpix2pix",
      "sauer2023stylegan",
      "agarwal2023image",
      "armandpour2023re",
      "Bel21",
      "cui2022democratizing",
      "ruta2023diffnst",
      "voynov2023p+",
      "Mao:ICCV15",
      "jin2021teachers",
      "shaker2022unetr++",
      "swiftnet",
      "dialog-gen",
      "furuta2023multimodal",
      "desai2021virtex",
      "nitzan2023domain",
      "li2024llavamed",
      "wu2023easyphoto",
      "zhao2023clip",
      "haque2023instruct",
      "pmlr-v97-backurs19a",
      "gur2024a",
      "smilkov2017smoothgrad"
    ],
    "relevant_ids": [
      "gal2022image",
      "avrahami2022blended",
      "tang2023emergent",
      "saharia2022palette",
      "mu2022coordgan",
      "lugmayr2022repaint",
      "mou2023dragondiffusion",
      "zhang2019self",
      "ruiz2023hyperdreambooth",
      "song2023comprehensive",
      "nichol2021glide",
      "huang2022survey",
      "saharia2022photorealistic",
      "rombach2022high",
      "park2019semantic",
      "antoniou2017data",
      "wang2022semantic",
      "zhang2023adding",
      "touvron2023llama",
      "chen2022semi",
      "peebles2022gan",
      "cheplygina2019not",
      "croitoru2023diffusion",
      "elharrouss2020image",
      "hu2021lora",
      "dhariwal2021diffusion",
      "levin2023differential",
      "ho2020denoising",
      "meng2021sdedit",
      "yang2022diffusion",
      "zhou2021review",
      "brock2018large",
      "ruiz2023dreambooth",
      "lorenz2019unsupervised"
    ],
    "metrics": {
      "R@5": 0.029411764705882353,
      "R@10": 0.029411764705882353,
      "R@20": 0.029411764705882353,
      "MRR": 0.5,
      "hits": 1,
      "total_relevant": 34
    },
    "score": 0.17058823529411765,
    "timestamp": "2025-12-18T10:58:44.198962"
  },
  {
    "query": "Introduction \\vspace{-3pt} The creation of 3D assets, crucial across various domains such as gaming, AR/VR, graphic design, anime, and movies, is notably laborious and time-consuming. Despite the use of machine learning techniques to aid 3D artists in some aspects of the task, the development of efficient end-to-end 3D asset generation",
    "paper_id": "2401.07727",
    "retrieved_ids": [
      "tang2023dreamgaussian",
      "zhang2024clay",
      "groueix2018atlasnet",
      "jin2017towards",
      "xu2024instantmesh",
      "hash3d",
      "im3d",
      "cheng2023sdfusion",
      "3dtopia",
      "po2023compositional",
      "wang2023breathing",
      "bd23",
      "chen2023fantasia3d",
      "zhou2024dreamscene360",
      "zero12345plus",
      "chen2024meshanything",
      "li2024instant3d",
      "pavllo2020convolutional",
      "tsalicoglou2023textmesh",
      "crowson2022vqgan",
      "get3d",
      "yi2023gaussiandreamer",
      "tewari2020state",
      "lin2023consistent123",
      "zhang2023avatarverse",
      "sun20233dgpt",
      "v3d",
      "nash2020polygen",
      "jun2023shape",
      "garbin2022voltemorph",
      "lorraine2023att3d",
      "pointE",
      "liu2023meshdiffusion",
      "deitke2023objaverse",
      "cao2023texfusion",
      "chen2023gaussianeditor",
      "cao2023large",
      "deng2022fov",
      "li2024image",
      "brock2016generative",
      "elsken2019neural",
      "zhang2023text",
      "he2024dresscode",
      "yu2023text",
      "clipmesh",
      "li2021topologically",
      "downs2022google",
      "zhang2023getavatar",
      "mirzaei2022laterf"
    ],
    "relevant_ids": [
      "cao2023texfusion",
      "chen2023text2tex",
      "lorraine2023att3d",
      "liu2023one",
      "gu2023nerfdiff",
      "kang2023scaling",
      "yu2022scaling",
      "kim2023neuralfield",
      "poole2022dreamfusion",
      "watson2022novel",
      "mildenhall2021nerf",
      "saharia2022photorealistic",
      "instant3d2023",
      "shi2023mvdream",
      "rombach2022high",
      "lin2023magic3d",
      "zhao2023michelangelo",
      "long2023wonder3d",
      "tang2023make",
      "jun2023shap",
      "po2023state",
      "shue20233d",
      "chan2023generative",
      "zou2023sparse3d",
      "luo2021diffusion",
      "richardson2023texture",
      "gupta20233dgen",
      "liu2023syncdreamer",
      "zhou2023sparsefusion",
      "schuhmann2022laion",
      "zhang20233dshape2vecset",
      "podell2023sdxl",
      "nichol2022point",
      "metzer2023latent",
      "ho2020denoising",
      "tsalicoglou2023textmesh",
      "ma2023x",
      "liu2023zero1to3",
      "chen2023single",
      "ramesh2021zero"
    ],
    "metrics": {
      "R@5": 0.0,
      "R@10": 0.0,
      "R@20": 0.025,
      "MRR": 0.05263157894736842,
      "hits": 3,
      "total_relevant": 40
    },
    "score": 0.015789473684210523,
    "timestamp": "2025-12-18T10:58:46.663681"
  },
  {
    "query": "Introduction Diffusion Probabilistic Models (DPM) have demonstrated high capability in generating high-quality images<|cite_0|>. DPM approaches image generation as a multi-step sampling process, involving the use of a denoiser network to progressively transform a Gaussian noise map into an output image. Compared to 2D images, videos have an additional time dimension,",
    "paper_id": "2401.01256",
    "retrieved_ids": [
      "ho2022video",
      "lu2022dpmsolver",
      "sasaki2021unit",
      "lu2022dpm-solver",
      "rombach2022high",
      "ddpm",
      "nichol2021improved",
      "luo2023videofusion",
      "improved_ddpm",
      "lu2023dpmsolver",
      "kazerouni2022diffusion",
      "moser2024diffusion",
      "zhao2023unipc",
      "po2023state",
      "Choi2021ILVRCM",
      "croitoru2023diffusion",
      "latentdiffusion",
      "Niu2023CDPMSRCD",
      "wang2022semantic",
      "huang2023wavedm",
      "zheng2023dpm",
      "song2020denoising",
      "hoogeboom2023simple",
      "Bao2022AnalyticDPMAA",
      "ddim",
      "nikankin2022sinfusion",
      "lamBilateralDenoisingDiffusion2021",
      "Zhang_2023_CVPR",
      "mao2023guided",
      "wimbauer2023cache",
      "kawar2022enhancing",
      "wang2022sindiffusion",
      "yang2023diffusion",
      "CycleDiffusion",
      "austin2021structured",
      "DiffusionGraphBenefitDiscrete",
      "meng2022distillation",
      "liu2021pndm",
      "huberman2023ddpminv",
      "garibi2024renoise",
      "chung2022come",
      "luhman2021knowledge",
      "preechakul2022diffusion",
      "baranchuk2021label"
    ],
    "relevant_ids": [
      "sohl2015deep",
      "guo2023animatediff",
      "long2023BCN",
      "he2022lvdm",
      "liang2022nuwa",
      "qiu2017learning",
      "lin2023videodirectorgpt",
      "shin2023editavideo",
      "khachatryan2023text2videozero",
      "singer2022make",
      "geyer2023tokenflow",
      "ramesh2022hierarchical",
      "mou2023t2iadapter",
      "ho2022imagen",
      "rombach2022high",
      "yao2022wave",
      "yin2023dragnuwa",
      "zhang2023adding",
      "qi2023fatezero",
      "wang2023genlvideo",
      "long2022dynamic",
      "long2019gaussian",
      "nichol2021improved",
      "lu2022dpmsolver",
      "song2022denoising",
      "wu2023tuneavideo",
      "esser2023structure",
      "ho2022classifier",
      "lu2023dpmsolver",
      "dhariwal2021diffusion",
      "song2019generative",
      "nichol2022glide",
      "villegas2022phenaki",
      "yin2023nuwa",
      "luo2023videofusion",
      "ho2020denoising",
      "blattmann2023align",
      "long2022sifa",
      "li2022CoT",
      "hu2023videocontrolnet",
      "ouyang2023codef",
      "wang2023videocomposer",
      "yao2023dual"
    ],
    "metrics": {
      "R@5": 0.046511627906976744,
      "R@10": 0.11627906976744186,
      "R@20": 0.11627906976744186,
      "MRR": 0.5,
      "hits": 5,
      "total_relevant": 43
    },
    "score": 0.20348837209302326,
    "timestamp": "2025-12-18T10:58:49.006074"
  },
  {
    "query": "Introduction \\label{intro} The integration of software in computational design and manufacturing has revolutionized various engineering sectors, offering significant advancements and efficiencies. However, this integration necessitates a deep understanding of domain-specific knowledge, underscoring the necessity for the development of more accessible interfaces, such as natural language processing, in industrial applications. In",
    "paper_id": "2401.06437",
    "retrieved_ids": [
      "2020arXiv200301200T",
      "treviso2023efficient",
      "Collobert2011",
      "makatura2023large",
      "qin2024large",
      "llmsurvey",
      "wolf2019huggingface",
      "ref:ragsurvey7",
      "ref:ragsurvey2",
      "sahoo2024systematic",
      "survey3",
      "Geng2022RecommendationAL",
      "li2024devbench",
      "ref:med2",
      "qu2024tool",
      "miao2023towards",
      "ref:financialgpt",
      "wu2021fashion",
      "wang2023knowledge",
      "elnaggar2021codetrans",
      "zhang2024comprehensive",
      "yu2023towards",
      "rae2021scaling",
      "peng2023openscene",
      "guide_paper",
      "naveed2024comprehensivellms",
      "Bengio2013representation",
      "wang2023prompt",
      "wang2023chatcad",
      "sun20233dgpt",
      "ref:ragsurvey3",
      "wang2024large",
      "hu2023advancing",
      "retail_analytics",
      "huang2023survey",
      "Gao2023RetrievalAugmentedGF",
      "yuan2023advancing",
      "liu2021gpt",
      "levy2024same",
      "ref:med3",
      "oza2021unsupervised",
      "alam2020survey",
      "b3",
      "lambert2023history",
      "hendrycks2021unsolved"
    ],
    "relevant_ids": [
      "openai2023gpt4",
      "zhou2023language",
      "khalid2022clipmesh",
      "sun20233dgpt",
      "poole2022dreamfusion",
      "shi2023mvdream",
      "wang2023prolificdreamer",
      "kerbl20233d",
      "chen2021humaneval",
      "mildenhall2020nerf",
      "tang2023dreamgaussian",
      "du2023classeval",
      "raj2023dreambooth3d",
      "li2023instant3d",
      "siddiqui2023meshgpt",
      "austin2021mbpp",
      "ho2020denoising",
      "chen2023fantasia3d",
      "lin2022magic3d",
      "makatura2023large",
      "rozi\u00e8re2023code"
    ],
    "metrics": {
      "R@5": 0.047619047619047616,
      "R@10": 0.047619047619047616,
      "R@20": 0.047619047619047616,
      "MRR": 0.25,
      "hits": 2,
      "total_relevant": 21
    },
    "score": 0.10833333333333334,
    "timestamp": "2025-12-18T10:58:51.263646"
  },
  {
    "query": "Introduction With the advance in pre-trained language models<|cite_0|>, the Natural Language Processing (NLP) technology is evolving fast, so as its applications in finanical and business domains<|cite_1|>. With the release of ChatGPT series, decoder-only Large Language Models (LLMs) like GPT-4<|cite_2|> and LLaMa<|cite_3|> have rapidly become a cornerstone of modern artificial intelligence,",
    "paper_id": "2401.02982",
    "retrieved_ids": [
      "llmsurvey2",
      "llmsurvey",
      "Zhao2023ASO",
      "qin2024large",
      "ref:finalcialgpt2",
      "ref:financialgpt",
      "kaddour2023challenges",
      "minaee2024large",
      "naveed2024comprehensivellms",
      "bubeck2023sparks",
      "gpt_summary",
      "Zhu2023LargeLM",
      "survey0",
      "MIR-2022-03-068",
      "ref:med2",
      "mmllms2",
      "xie2023translating",
      "lu2023bbt",
      "zhu2023multilingual",
      "dubey2024llama",
      "wu2023decoder",
      "llmsurvey3",
      "zhang2022survey",
      "touvron2023llama",
      "xie2023PIXIULargeLanguage",
      "Touvron2023Llama2O",
      "ref:ragsurvey2",
      "b2",
      "makatura2023large",
      "xie2024me",
      "Zheng2023SecretsOR",
      "deepseekllm",
      "Liu2023LLMRecBL",
      "chen2023videollm",
      "shen-etal-2023-large",
      "WizardMath",
      "qin2024multilingual",
      "ahuja-etal-2023-mega",
      "ahuja2023mega",
      "bai2023qwen",
      "wang2023prompt"
    ],
    "relevant_ids": [
      "llmsurvey2",
      "multimedqa",
      "gsm8k",
      "halueval",
      "xie2023PIXIULargeLanguage",
      "llmsurvey1",
      "du2022glm",
      "zhang2023FinEvalChineseFinancial",
      "zeng2022glm",
      "touvron2023llama",
      "llmsurvey3",
      "bai2023QwenTechnicalReport",
      "fingpt",
      "felm",
      "openai2023GPT4TechnicalReport",
      "touvron2023llama2",
      "devlin2019BERTPretrainingDeep",
      "yang2020finbert",
      "lawbench",
      "huang2023CEvalMultiLevelMultiDiscipline",
      "zhong2023AGIEvalHumanCentricBenchmark",
      "bbh",
      "mmlu",
      "lu2023bbt"
    ],
    "metrics": {
      "R@5": 0.041666666666666664,
      "R@10": 0.041666666666666664,
      "R@20": 0.08333333333333333,
      "MRR": 1.0,
      "hits": 5,
      "total_relevant": 24
    },
    "score": 0.32916666666666666,
    "timestamp": "2025-12-18T10:58:53.696115"
  },
  {
    "query": "Introduction \\label{introduction} Generative transformer-based language models (LMs) have achieved state-of-the-art results across many tasks, including in high-stakes domains such as medicine and education<|cite_0|>. These general-purpose models have an enormous output space, and may respond to input prompts in ways which may induce wide-ranging harms. For example, an LM may output",
    "paper_id": "2401.16656",
    "retrieved_ids": [
      "huang2022large",
      "ref:med1",
      "ref:med3",
      "ref:edu1",
      "llmsurvey2",
      "kaddour2023challenges",
      "naveed2024comprehensivellms",
      "rubin2021learning",
      "schick2024toolformer",
      "gpt_summary",
      "yang2023generative",
      "Zhao2023ASO",
      "genegpt",
      "sahoo2024systematic",
      "srivastava2023beyond",
      "wu2023speechgen",
      "perez2022ignore",
      "kalyan2021ammus",
      "suzgun2024meta",
      "Weidinger2021EthicalAS",
      "tsai2018learning",
      "zhou2023instruction",
      "jiang2020can",
      "Nori2023CapabilitiesOG",
      "survey1",
      "rae2021scaling",
      "chia2024instructeval",
      "xu24llmkdsurvey",
      "akyurek2022towards",
      "mialon2023augmented",
      "chevalier2023autocompressor",
      "makatura2023large",
      "Pan2023AutomaticallyCL",
      "Dathathri2020Plug",
      "gehman2020realtoxicityprompts",
      "sun2024trustllm",
      "zeng2023lynx",
      "liu2021makes",
      "shen2024language",
      "mu2024gist",
      "ahuja-etal-2023-mega",
      "kwon2023language",
      "nambi2023breaking",
      "vsakota2024fly",
      "perceiverio",
      "Perez2022RedTL",
      "llmsurvey3",
      "ahuja2023mega",
      "ma2023can",
      "deng-etal-2022-rlprompt",
      "tandon2022learning",
      "Jaegle2021PerceiverIA",
      "wang2022rationale",
      "plug_play_gen",
      "lu2021fantastically",
      "xie2021explanation",
      "kasai2023evaluating"
    ],
    "relevant_ids": [
      "openai2023gpt4",
      "lee2023query",
      "anil2023palm",
      "singhal2023large",
      "universal_adversarial_triggers",
      "gumbel-softmax-2",
      "plug_play_gen",
      "mudgal2023controlled",
      "grad_adver_attack",
      "auto_audit_discrete",
      "red_team_LM",
      "imperceptible_toxicity_triggers",
      "toxigen",
      "universal_transfer_adv_attacks",
      "touvron2023llama",
      "casper2023explore",
      "LM_discrim",
      "deng-etal-2022-rlprompt",
      "mehrabi2023flirt",
      "hong2024curiositydriven",
      "ouyang2022training",
      "yang2021fudge",
      "christiano2017deep",
      "gumbel_softmax",
      "direct_pref_opt",
      "text_gen_attribute_control",
      "bai2022training"
    ],
    "metrics": {
      "R@5": 0.0,
      "R@10": 0.0,
      "R@20": 0.0,
      "MRR": 0.02,
      "hits": 2,
      "total_relevant": 27
    },
    "score": 0.006,
    "timestamp": "2025-12-18T10:58:56.685505"
  },
  {
    "query": "Introduction Image captioning is a fundamental task in vision-language understanding that involves generating natural language descriptions for a given image. It plays a critical role in facilitating more complex vision-language tasks, such as visual question answering<|cite_0|> and visual dialog<|cite_1|>. The mainstream image captioning methods<|cite_2|> require expensive human annotation of image-text",
    "paper_id": "2401.02347",
    "retrieved_ids": [
      "das2017visual",
      "sariyildiz2020learning",
      "antol2015vqa",
      "conimgcap1",
      "Cho2021UnifyingVT",
      "ujain2018two",
      "wuKb",
      "nic",
      "hossain2019comprehensive",
      "bai2023qwen",
      "mscoco",
      "conimgcap2",
      "visualgenome",
      "hu2022promptcap",
      "Mokady2021ClipCapCP",
      "Dalle3",
      "BLIP",
      "showaskattend",
      "schwenk2022okvqa",
      "mogadala2017describing",
      "okvqa",
      "2019Aligning",
      "showandtell",
      "chen2023pali",
      "kottur2019clevr",
      "Changpinyo2022AllYM",
      "cc12",
      "chen2022pali",
      "Changpinyo",
      "Wang_Yu_Yu_Dai_Tsvetkov_Cao_2021",
      "honda2021removing",
      "liu2018show",
      "hu2023tifa",
      "jia2021scaling",
      "herdade2019image",
      "mao2016generation",
      "mil",
      "johnson2016densecap",
      "anderson2018partially",
      "laina2019towards",
      "veagle_paper",
      "ganz2023towards",
      "merullo2022linearlylimber",
      "zang2023contextual",
      "lu2018neural"
    ],
    "relevant_ids": [
      "Agrawal2015VQAVQ",
      "conimgcap3",
      "CapDec",
      "pointclip",
      "Wang2020UnderstandingCR",
      "maskclip",
      "Changpinyo2022AllYM",
      "Zhang2022OPTOP",
      "clip",
      "Niu2018RecursiveVA",
      "miniGPT4",
      "styleclip",
      "MAGIC",
      "conimgcap1",
      "glip",
      "conimgcap2",
      "Wang2020UnderstandingTB",
      "Wang_Yu_Yu_Dai_Tsvetkov_Cao_2021",
      "glip2",
      "okvqa",
      "Guo2022FromIT",
      "MindGap",
      "ZeroCap",
      "Flickr30k",
      "gpt3",
      "gqa",
      "mscoco",
      "calip",
      "clipbagofwords",
      "Das2016VisualD",
      "conimgcap4",
      "Tiong2022PlugandPlayVZ",
      "denclip",
      "align",
      "hoiclip",
      "flamingo",
      "Changpinyo",
      "llava",
      "vqav2",
      "blip",
      "DeCap"
    ],
    "metrics": {
      "R@5": 0.024390243902439025,
      "R@10": 0.024390243902439025,
      "R@20": 0.07317073170731707,
      "MRR": 0.25,
      "hits": 7,
      "total_relevant": 41
    },
    "score": 0.09207317073170732,
    "timestamp": "2025-12-18T10:58:59.353099"
  },
  {
    "query": "Introduction In recent years, Transformer models<|cite_0|> have achieved astounding success across vastly different domains: e.g., vision<|cite_1|>, NLP<|cite_2|>, chemistry, and many others. However, without massive training datasets, their performance can quickly saturate as model depth increases<|cite_3|>. This appears to be caused by a fundamental property of Transformer models: a recent line",
    "paper_id": "2401.04301",
    "retrieved_ids": [
      "khan2021transformers",
      "touvron2021going",
      "tay2020efficient",
      "mensink2021factors",
      "Chen2021VisformerTV",
      "luo2022your",
      "qin2023-nlp_effectiveness_long-range",
      "wang2023learning",
      "zhai2022scaling",
      "ganesh2020compressing",
      "han2020survey",
      "limitations",
      "zhou2021deepvit",
      "shamshad2022transformers",
      "lin2022cat",
      "yu2022metaformer",
      "tay2020long",
      "survey3",
      "survey1",
      "chen2022adaptformer",
      "wang-etal-2019-learning",
      "liu2020very",
      "wang2022anti",
      "white2023neural",
      "bondarenko2021understanding",
      "zhang2022minivit",
      "liu2022convnet",
      "wu2021rethinking",
      "benchmarking-gnn",
      "cui2022democratizing",
      "fan2019reducing",
      "ruckle2021adapterdrop",
      "he2020realformer",
      "guo2019star",
      "noci2022signal",
      "b8",
      "liu2021pay",
      "wu2022memorizing",
      "bondarenko2023quantizable",
      "yu2021rethinking",
      "bendidi2023free",
      "zhu2020modifying",
      "yang2023change",
      "hatamizadeh2022swin",
      "qin_transnormer_emnlp_2022",
      "lite_transformer",
      "mazzia2023survey",
      "wang2021knowledge",
      "kocsis2022unreasonable",
      "zhang2020pegasus",
      "dwivedi2020benchmarking",
      "chen2020lottery",
      "evanas",
      "de2023reliability",
      "zhao2022decoupled",
      "beyer2022knowledge",
      "liu2024neural",
      "cherti2023reproducible"
    ],
    "relevant_ids": [
      "dosovitskiy2021image",
      "touvron2021training",
      "wei2023chainofthought",
      "kaplan2020scaling",
      "kocsis2022unreasonable",
      "kaddour2023challenges",
      "yu2022efficient",
      "ali2023centered",
      "vaswani2023attention",
      "park2022vision",
      "liu2021pay",
      "yu2022metaformer",
      "wang2022anti",
      "noci2022signal",
      "dong2021attention",
      "touvron2023llama",
      "guo2023contranorm"
    ],
    "metrics": {
      "R@5": 0.0,
      "R@10": 0.0,
      "R@20": 0.058823529411764705,
      "MRR": 0.0625,
      "hits": 5,
      "total_relevant": 17
    },
    "score": 0.01875,
    "timestamp": "2025-12-18T10:59:03.013236"
  },
  {
    "query": "Introduction \\label{sec:intro} Music auto-tagging is the automated process of attaching relevant semantic labels such as genre, mood, or instrument to musical tracks, usually enabled by machine learning algorithms. This function is crucial for effective music information retrieval, personalization, and recommendation systems, predominantly in music-streaming platforms such as Spotify. These services",
    "paper_id": "2401.15323",
    "retrieved_ids": [
      "won2019toward",
      "choi2016automatic",
      "won2021semi",
      "kim2018sample",
      "lee2017sample",
      "pons2017end",
      "kingma2019introduction",
      "han2021autonovel",
      "Wu2023ASO",
      "doh2023lp",
      "b3",
      "baltruvsaitis2018multimodal",
      "choi2023towards",
      "yuan2023marble",
      "dhariwal2020jukebox",
      "Zhang2023CoLLMIC",
      "siyao2022bailando",
      "li2023mert",
      "Gao2023ChatRECTI",
      "yang2022survey",
      "mccallum2022supervised",
      "agostinelliMusicLMGeneratingMusic2023",
      "lee2019dancing",
      "lamEfficientNeuralMusic2023b",
      "li2021ai",
      "musicgen",
      "choi2017convolutional",
      "huangNoise2MusicTextconditionedMusic2023",
      "huang2018music",
      "Collobert2011",
      "lim2019fast",
      "chen2019behavior",
      "gru4rec",
      "gru4rec2",
      "hu2019introductory",
      "MIR-2022-03-068",
      "evans2024fast"
    ],
    "relevant_ids": [
      "ajakan2014domain",
      "choi2023towards",
      "pons2017end",
      "yuan2023marble",
      "huang2022improving",
      "lee2017sample",
      "mccallum2022supervised",
      "li2023mert",
      "choi2017convolutional",
      "vaswani2017attention",
      "kim2018sample",
      "won2021semi",
      "won2019toward",
      "choi2016automatic",
      "ganin2016domain"
    ],
    "metrics": {
      "R@5": 0.3333333333333333,
      "R@10": 0.4,
      "R@20": 0.6,
      "MRR": 1.0,
      "hits": 11,
      "total_relevant": 15
    },
    "score": 0.5533333333333332,
    "timestamp": "2025-12-18T10:59:04.881288"
  },
  {
    "query": "Introduction \\label{sec:intro} \\IEEEPARstart{C}{oordinate}-based networks, typically MLPs (Multilayer Perceptrons) taking the coordinates of points in a low dimensional space as inputs, have emerged as a general representation of encoding implicit fields for 2D and 3D contents<|cite_0|>. These neural implicit representations enjoy several major benefits over their traditional counterparts. They offer a",
    "paper_id": "2401.01391",
    "retrieved_ids": [
      "tewari2022advances",
      "lindell2022bacon",
      "trevithick2021grf",
      "martel2021acorn",
      "chen2023neurbf",
      "zhang20233dshape2vecset",
      "riegler2017octnet",
      "zheng2021rethinking",
      "chen2024meshxl",
      "takikawa2021neural",
      "sitzmann2020implicit",
      "genova2020local",
      "xie2021neural",
      "tancik2020fourier",
      "chen2022transformers",
      "erkocc2023hyperdiffusion",
      "wang2021spline",
      "davies2020effectiveness",
      "venkatesh2021deep",
      "chen2019learning",
      "MAXIM",
      "yang2022neumesh",
      "xiao2023unsupervised",
      "jun2023shape",
      "binary_survey",
      "Sucar_2021_ICCV",
      "srinivas2019fullgrad",
      "kundu2022panoptic",
      "wang2023pet",
      "gropp2020implicit",
      "chen2023factor",
      "zeiler2014visualizing",
      "yu2022monosdf",
      "chen2021learning",
      "walker2023explicit",
      "fan2022unified",
      "jampani:cvpr:2016",
      "tolstikhin2021mlp",
      "Lambdanetworks",
      "Gao-portraitnerf",
      "v4d",
      "lin2023vision",
      "scardapane2020should"
    ],
    "relevant_ids": [
      "atzmon2020sal",
      "sitzmann2020implicit",
      "liu2020neural",
      "gropp2020implicit",
      "rahaman2019spectral",
      "takikawa2021neural",
      "mildenhall2020nerf",
      "martel2021acorn",
      "park2019deepsdf",
      "xu2019frequency",
      "wang2021spline",
      "davies2020effectiveness",
      "tancik2020fourier",
      "wang2021neus",
      "xie2021neural"
    ],
    "metrics": {
      "R@5": 0.06666666666666667,
      "R@10": 0.13333333333333333,
      "R@20": 0.4666666666666667,
      "MRR": 0.25,
      "hits": 8,
      "total_relevant": 15
    },
    "score": 0.14166666666666666,
    "timestamp": "2025-12-18T10:59:07.400566"
  },
  {
    "query": "Introduction The significance of 3D assets has grown remarkably due to the expansion of virtual reality and the gaming industry. Creating these assets, however, involves considerable labor costs. Recently, the popularity of methods based on differentiable rendering<|cite_0|> and the explosion of text-to-image models<|cite_1|> provides a new line of methods for",
    "paper_id": "2401.16764",
    "retrieved_ids": [
      "zhang2024clay",
      "wu2024recent",
      "chen2023fantasia3d",
      "lin2022magic3d",
      "kang2023scaling",
      "raj2023dreambooth3d",
      "tewari2020state",
      "3dtopia",
      "chen2024textto3dgsgen",
      "wang2023breathing",
      "li2024instant3d",
      "cao2023texfusion",
      "hasselgren2022shape",
      "liu2023unidream",
      "avrahami2023chosen",
      "chen2024it3d",
      "lin2023consistent123",
      "po2023state",
      "zhou2024dreamscene360",
      "clipmesh",
      "zero12345plus",
      "lorraine2023att3d",
      "yi2023gaussiandreamer",
      "song2023efficient",
      "tang2023volumediffusion",
      "wu2024hd",
      "sun20233dgpt",
      "scaffoldgs",
      "im3d",
      "v3d",
      "movshovitz-attias2016how",
      "zhao2023efficientdreamer",
      "deng2022fov",
      "fastmesh",
      "garbin2022voltemorph",
      "munkberg2022extracting",
      "chen2023gaussianeditor",
      "li2022voxsurf",
      "jun2023shape",
      "xie2021neural",
      "ravi2020accelerating",
      "neff2021donerf",
      "chen2024meshanything",
      "Li2019AADSAA",
      "zhu2024sora",
      "xu2022deforming",
      "rebain2021derf"
    ],
    "relevant_ids": [
      "lorraine2023att3d",
      "zhao2023efficientdreamer",
      "poole2022dreamfusion",
      "mildenhall2021nerf",
      "liu2023meshdiffusion",
      "shi2023mvdream",
      "rombach2022high",
      "lin2023magic3d",
      "zhou20213d",
      "deitke2023objaverse",
      "wang2023prolificdreamer",
      "zhang2023adding",
      "sanghi2023clip",
      "shen2021dmtet",
      "qian2023magic123",
      "jain2022dreamfields",
      "liu2023one2345",
      "gupta20233dgen",
      "jun2023shape",
      "melas2023realfusion",
      "liu2023zero",
      "kerbl3Dgaussians",
      "wang2023score",
      "zeng2022lion",
      "nichol2022pointe",
      "metzer2023latent",
      "chen2023fantasia3d",
      "schuhmann2022laion5b",
      "chen2023single",
      "ntavelis2023autodecoding",
      "wang2021neus",
      "tang2023mvdiffusion"
    ],
    "metrics": {
      "R@5": 0.03125,
      "R@10": 0.03125,
      "R@20": 0.03125,
      "MRR": 0.3333333333333333,
      "hits": 4,
      "total_relevant": 32
    },
    "score": 0.12187499999999998,
    "timestamp": "2025-12-18T10:59:10.245037"
  },
  {
    "query": "Introduction Large language models have garnered considerable attention from both academic and industrial communities, providing artificial intelligence with the capability to adeptly integrate into myriad downstream applications. The deployment of large language models typically involves two discernible phases. As depicted in Figure~\\ref{fig:overall}, the initial stage focuses on pre-training the model",
    "paper_id": "2401.14027",
    "retrieved_ids": [
      "minaee2024large",
      "kaddour2023challenges",
      "naveed2024comprehensivellms",
      "Zhao2023ASO",
      "gpt_summary",
      "sahoo2024systematic",
      "laskar2024systematic",
      "wang2024large",
      "xi2023rise",
      "ref:med1",
      "survey2",
      "qu2024tool",
      "ref:edu1",
      "llmsurvey2",
      "alkhamissi2022review",
      "wang2023survey",
      "llmsurvey",
      "zhao2023chatbridge",
      "survey1",
      "wang2023knowledge",
      "shu2023llasm",
      "peng2023kosmos",
      "rae2021scaling",
      "ref:med3",
      "miao2023towards",
      "ref:finalcialgpt2",
      "Gao2023RetrievalAugmentedGF",
      "gunter2024apple",
      "ref:ragsurvey3",
      "zhang2023speechgpt",
      "ahuja2023mega",
      "Pan2023AutomaticallyCL",
      "ahuja-etal-2023-mega",
      "solaiman2019release",
      "sink",
      "ref:ragsurvey2",
      "llmsurvey3",
      "wolf2019huggingface",
      "b3",
      "li2022mplug",
      "bai2023qwen",
      "wang2022lilt",
      "guo2023black",
      "gupta2024model",
      "tan2021survey"
    ],
    "relevant_ids": [
      "chen-et-al:scheme",
      "li-et-al:scheme",
      "ding-et-al:scheme",
      "ye-et-al:scheme",
      "cai-et-al:scheme",
      "wang2022does",
      "brown-et-al:scheme",
      "houlsby-et-al:scheme",
      "touvron2023llama",
      "devlin2018bert",
      "rebuffi-et-al:scheme",
      "wang-et-al:scheme",
      "zhou-et-al:scheme",
      "Muandet-et-al:scheme",
      "zhou-et-al:scheme1",
      "ji-et-al:scheme",
      "kumar-et-al:scheme",
      "croce-et-al:scheme",
      "hu-et-al:scheme",
      "wortsman-et-al:scheme",
      "shi-et-al:scheme",
      "luo-et-al:scheme",
      "t5raffel2020exploring",
      "zaken-et-al:scheme"
    ],
    "metrics": {
      "R@5": 0.0,
      "R@10": 0.0,
      "R@20": 0.0,
      "MRR": 0.0,
      "hits": 0,
      "total_relevant": 24
    },
    "score": 0.0,
    "timestamp": "2025-12-18T10:59:13.154553"
  },
  {
    "query": "Introduction \\label{sec:intro} Vision Transformers (ViTs) have achieved state-of-the-art (SOTA) performance across various vision tasks, including image classification<|cite_0|>, object detection<|cite_1|> and segmentation<|cite_2|>. However, their computational demands, high memory footprint, and significant energy consumption make them impractical for deployment on resource constrained platforms. Compression and acceleration techniques have been investigated for ViTs,",
    "paper_id": "2401.11243",
    "retrieved_ids": [
      "naseer2021intriguing",
      "park2022vision",
      "ecc",
      "wu2021cvt",
      "li2022nextvit",
      "zhai2022scaling",
      "heo2021rethinking",
      "wang2023closer",
      "lee2021vision",
      "dosovitskiy2020vit",
      "mslongformer",
      "li2021benchmarking",
      "lin2022super",
      "li2022efficientformer",
      "gu2022multi",
      "ref14",
      "jelassi2022vision",
      "li2022QViT",
      "yun2024shvit",
      "li2021can",
      "xu2022evo_token",
      "Lee2021ViTGANTG",
      "paul2022vision",
      "pan202iared2",
      "vit",
      "shamshad2022transformers",
      "zhang2022minivit",
      "li2023Ivit",
      "yao2023dual",
      "wang2021pvt",
      "dong2021cswin",
      "DUAL-VIT",
      "lee2022mpvit",
      "darcet2023vision",
      "chen2022vision",
      "liu2021swin",
      "yu2023boost",
      "shaker2022unetr++",
      "ref17",
      "wang2022advancing",
      "swiftnet",
      "huang2016speed",
      "NekrasovS018",
      "fanet",
      "wang2021addernet",
      "liu2022convnet",
      "efficientvmamba",
      "Yao2020HAWQV3DN",
      "liu2024swin"
    ],
    "relevant_ids": [
      "touvron2021training_DeiT",
      "xu2022evo_token",
      "chefer2021LRP",
      "liu2021post",
      "chen2022dear_KD",
      "wang2021PVT",
      "mao2022exploring_OD",
      "wu2022tinyvit",
      "carion2020DETR",
      "li2022efficientformer",
      "smilkov2017smoothgrad",
      "wang2019haq",
      "voita2019analyzing",
      "LRP_chefer2021transformer",
      "yuan2022ptq4vit",
      "binder2016layerLRP_NN",
      "abnar2020quantifying_attentionrollout",
      "ding2022APQViTtowards",
      "liu2023noisyquant",
      "li2022QViT",
      "he2018soft_prun",
      "dai2021upDETR",
      "xu2022vitpose",
      "rao2021dynamicvit",
      "zheng2021rethinking_seg",
      "zhang2022minivit",
      "denil2013predicting_lw",
      "srinivas2019fullgrad",
      "li2023Ivit",
      "jain2023oneformer",
      "xiao2023patch-wise",
      "dong2019hawq",
      "carion2020end_end_OD",
      "liu2021swin",
      "lin2022fq_vit",
      "li2023repq",
      "lin2022knowledge_KD",
      "dong2020hawqv2",
      "liu2021post_Rank_Aware_PTQ",
      "gu2019understanding",
      "lou2020autoq",
      "strudel2021segmenter_seg",
      "image_16x16_dosovitskiy2021an_ViT"
    ],
    "metrics": {
      "R@5": 0.0,
      "R@10": 0.0,
      "R@20": 0.046511627906976744,
      "MRR": 0.07142857142857142,
      "hits": 6,
      "total_relevant": 43
    },
    "score": 0.021428571428571425,
    "timestamp": "2025-12-18T10:59:16.665079"
  },
  {
    "query": "Introduction Nowadays, deep learning-based text-guided image morphing has been showing unprecedented high qualities in many real-world applications, such as image editing<|cite_0|>, and style transfer<|cite_1|>. Especially, text-guided image morphing only uses text to give guidance on the given images and does not require any additional target images to guide how to",
    "paper_id": "2401.10526",
    "retrieved_ids": [
      "ManiGAN",
      "li2024tuning",
      "bai2023itstyler",
      "Zhou2019",
      "edit",
      "kawar2023imagic",
      "patashnik2021styleclip",
      "nichol-2021",
      "nichol2021glide",
      "wang2023imagen",
      "zhang2022sine",
      "tumanyan2023plug",
      "crowson2022vqgan",
      "li2023stylediffusion",
      "wu2019editing",
      "xia2021tedigan",
      "huang2022diffstyler",
      "lee2024conditional",
      "chong2021stylegan",
      "zhang2024magicbrush",
      "fang2023gaussianeditor",
      "brack2023sega",
      "song2022clipvg",
      "paint",
      "sohn2023styledrop",
      "Avrahami_2022_CVPR",
      "chefer2022image",
      "masactrl",
      "ref9",
      "voynov2022sketch",
      "matsunaga2022fine",
      "self_guidance",
      "li2023layerdiffusion",
      "mokady2022nti",
      "ma2018exemplar",
      "zhang2023text",
      "diffuseit",
      "simsar2023lime",
      "liao2023text",
      "ceylan2023pix2video",
      "blending",
      "UdiffText",
      "sine",
      "parmar2023zero",
      "lin2023regeneration",
      "repaintnerf",
      "dragon"
    ],
    "relevant_ids": [
      "gal2022stylegan",
      "crowson2022vqgan",
      "huang2022diffstyler",
      "patashnik2021styleclip",
      "kwon2022clipstyler",
      "tevet2022motionclip",
      "chefer2022image",
      "sauer2023stylegan",
      "kirkpatrick2017overcoming",
      "li2017learning",
      "simon2021learning",
      "rebuffi2017icarl",
      "radford2021learning",
      "kim2022diffusionclip",
      "song2022clipvg",
      "zhu2016generative",
      "bar2022text2live",
      "nitzan2023domain"
    ],
    "metrics": {
      "R@5": 0.0,
      "R@10": 0.05555555555555555,
      "R@20": 0.16666666666666666,
      "MRR": 0.14285714285714285,
      "hits": 5,
      "total_relevant": 18
    },
    "score": 0.05952380952380952,
    "timestamp": "2025-12-18T10:59:19.318966"
  },
  {
    "query": "Introduction The ability of incremental learning is critical for deep neural networks to accommodate real-world dynamics, but is limited by catastrophic forgetting of old knowledge<|cite_0|>. Especially in object detection tasks, there is usually a large number of object instances from new classes that need to be incorporated and recognized. Numerous",
    "paper_id": "2401.05362",
    "retrieved_ids": [
      "toneva2018an",
      "kirkpatrick2017overcoming",
      "istrate2018incremental",
      "li2017learning",
      "shmelkov2017incremental_ilod",
      "MCIncre",
      "feng2022overcoming",
      "li2019learn",
      "serra2018overcoming",
      "lee2017overcoming",
      "dhar2019learning",
      "matthiasdelangeetal2021",
      "li2019rilod",
      "masarczyk2020reducing",
      "gissin2020implicit",
      "wang2021afec",
      "aljundi2018memory",
      "peng2020faster",
      "wang2023incorporating",
      "zenke2017continual",
      "zhai2024fine",
      "parisi2019continual",
      "End-To-End",
      "cermelli2020modeling",
      "belouadah2020scail",
      "DistillIncre",
      "rebuffi2017icarl",
      "zhou2024expandable",
      "veniat2020efficient",
      "hung2019compacting",
      "wang2023comprehensive",
      "wang2020streaming",
      "davidrolnicketal2018",
      "rakaraddi2022reinforced",
      "yu2020semantic",
      "zhao2020maintaining",
      "wang2021memory"
    ],
    "relevant_ids": [
      "feng2022overcoming",
      "wang2022coscl",
      "shmelkov2017incremental_ilod",
      "parisi2019continual",
      "liu2022unbiased",
      "wang2021afec",
      "wang2021ordisco",
      "li2019rilod",
      "redmon2016you",
      "ren2015faster",
      "he2017mask",
      "rebuffi2017icarl",
      "laine2016temporal",
      "tarvainen2017mean",
      "lin2017feature",
      "wang2021memory",
      "kirkpatrick2017overcoming",
      "wang2023consistent",
      "berthelot2019mixmatch",
      "wang2023comprehensive",
      "serra2018overcoming",
      "peng2020faster",
      "joseph2021towards",
      "xu2021end",
      "wang2023incorporating",
      "sohn2020simple",
      "wang2023hierarchical",
      "liu2021unbiased"
    ],
    "metrics": {
      "R@5": 0.07142857142857142,
      "R@10": 0.14285714285714285,
      "R@20": 0.2857142857142857,
      "MRR": 0.5,
      "hits": 12,
      "total_relevant": 28
    },
    "score": 0.22142857142857142,
    "timestamp": "2025-12-18T10:59:21.429605"
  },
  {
    "query": "Introduction \\label{sec:introduction} A proposition is a boolean statement, meaning it has a truth (true or false) value. Factual information/knowledge pertains to the truth values of propositions. Large Language Models (LLMs) contain a wealth of factual information, which can be modified by editing methods. For example, the proposition {\\bf There has",
    "paper_id": "2401.07526",
    "retrieved_ids": [
      "marks2023geometry",
      "de2021editing",
      "ref:edu1",
      "hu2023",
      "yoo2022groundtruth",
      "naveed2024comprehensivellms",
      "yuan2024whispers",
      "survey0",
      "carta2023grounding",
      "thoppilan-lamdalanguagemodelsdialog-2022",
      "azerbayev2023llemma",
      "liu2023trustworthy",
      "kaddour2023challenges",
      "wang2023surveyfactualitylargelanguage",
      "zhong2023mquake",
      "lawbench",
      "calinet",
      "liu2024evedit",
      "felm",
      "wu2023evakellm",
      "akyurek2022towards",
      "zheng2023can",
      "tian2023fine",
      "casper2023explore",
      "min2023factscore",
      "thorne2018fever",
      "Mallen2022WhenNT",
      "dola",
      "wang2023retrievalaugmented",
      "manakul",
      "li2023pmet",
      "lee2022factuality",
      "llmsurvey3",
      "chen2019tabfact",
      "verga2020facts",
      "deyoung2020eraser",
      "meng2022locating",
      "chen2021dataset",
      "Valmeekam2023CanLL",
      "elazar2021measuring",
      "min2022rethinking",
      "Kotek_2023",
      "prabhakaran2019perturbation",
      "madsen2021posthoc",
      "llava_rlhf",
      "guo2023pointbind",
      "agrawal-2023"
    ],
    "relevant_ids": [
      "mitchell2021fast",
      "roberts2020much",
      "goodfellow2013empirical",
      "meng2022locating",
      "de2021editing",
      "mitchell2022memory",
      "meng2022mass",
      "hu2021lora",
      "ding2022delta",
      "zaken2021bitfit",
      "dai2021knowledge",
      "DBLP:journals/corr/abs-2305-13172",
      "zheng2023can",
      "guo2020parameter"
    ],
    "metrics": {
      "R@5": 0.07142857142857142,
      "R@10": 0.07142857142857142,
      "R@20": 0.07142857142857142,
      "MRR": 0.5,
      "hits": 3,
      "total_relevant": 14
    },
    "score": 0.19999999999999998,
    "timestamp": "2025-12-18T10:59:24.254279"
  },
  {
    "query": "Introduction In recent years, machine learning models have shown impressive performance on fixed distributions<|cite_0|>. However, the distribution from which examples are drawn at inference time is not always stationary or overlapping with the training distribution. In these cases where inference examples are far from the training set, not only does",
    "paper_id": "2401.12129",
    "retrieved_ids": [
      "teney2022id",
      "2021arXiv210302667A",
      "liang2023comprehensive",
      "SZHBFB19",
      "carlini2019distribution",
      "salem2018ml",
      "sun2020test",
      "paul2021deep",
      "koh2021wilds",
      "ilyas2019adversarial",
      "Agrawal2022IncontextES",
      "swayamdipta2020dataset",
      "nijkamp2020learning",
      "confidence",
      "hendrycks2016baseline",
      "farahani2021brief",
      "nagarajan2020understanding",
      "yao2022improving",
      "levy2022diverse",
      "sun2022out",
      "besnier2020dataset",
      "maxlogit",
      "kaplun2022knowledge",
      "tu2020empirical",
      "khurana2021sita",
      "Miller2021AccuracyOT",
      "b1",
      "xie2021explanation",
      "NSH18",
      "toneva2018an",
      "ye-et-al:scheme",
      "redko2020survey",
      "ovadia2019can",
      "haochen2022theoretical",
      "hendrycks2019natural",
      "lee2018simple",
      "ryabinin2021scaling",
      "vendrow2023dataset",
      "waic",
      "Dao:2021",
      "kuleshov2022calibrated",
      "samangouei2018defense",
      "zheng2024toward",
      "malinin2018predictive",
      "jiang2020characterizing",
      "phuong2019towards",
      "garg2022can",
      "nasr2018machine",
      "feldman2020neural",
      "muller2019does"
    ],
    "relevant_ids": [
      "hendrycks2016baseline",
      "dai2016r",
      "feichtenhofer2020x3d",
      "sun2022out",
      "girshick2015fast",
      "redmon2016you",
      "ren2015faster",
      "djurisic2022extremely",
      "radford2021learning",
      "katz2022training",
      "lin2017focal",
      "devlin2018bert",
      "ovadia2019can",
      "nguyen2015deep",
      "huang2021importance",
      "sun2021react",
      "hsu2020generalized",
      "guo2017calibration",
      "liu2016ssd",
      "liu2020energy",
      "liang2017enhancing",
      "dosovitskiy2020image"
    ],
    "metrics": {
      "R@5": 0.0,
      "R@10": 0.0,
      "R@20": 0.09090909090909091,
      "MRR": 0.06666666666666667,
      "hits": 3,
      "total_relevant": 22
    },
    "score": 0.02,
    "timestamp": "2025-12-18T10:59:26.826647"
  }
]