{
  "query": "Introduction A series of Large Language Models (LLMs)<|cite_0|> like GPT-4<|cite_1|>, PaLM<|cite_2|> and LLaMA<|cite_3|> have showcased the impressive performance in various reasoning tasks. In addition to scaling up the model size to improve the reasoning performance, there are more effective prompting methods that further enhance the functionality and performance of LLMs.",
  "paper_id": "2406.04271",
  "retrieved_ids": [
    "Zhao2023ASO",
    "huang2022large",
    "survey0",
    "zheng2023progressivehint",
    "minaee2024large",
    "naveed2024comprehensivellms",
    "wei2022chain",
    "zhang2023cumulative",
    "sahoo2024systematic",
    "hu2024rankprompt",
    "zheng2023take",
    "wang2024large",
    "sun2023pearl",
    "jiang2023scaling",
    "cd-reasoning",
    "mmllms1",
    "lu2023chameleon",
    "WizardMath",
    "xie2024me",
    "bubeck2023sparks",
    "gao2023pal",
    "stechly2023gpt",
    "deepseekllm",
    "chia2024instructeval",
    "huang2023languages",
    "tang2023struc",
    "llmsurvey3",
    "Liu2023LLMRecBL",
    "hao2023reasoning",
    "qin2024large",
    "makatura2023large",
    "liu2023reta",
    "yu2023metamath",
    "fu2023complexitybased",
    "wang-etal-2023-towards",
    "petroni2021kilt",
    "wan-etal-2023-better",
    "openfunction",
    "ref:ragsurvey2",
    "he2022rethinking",
    "zhou2022teaching",
    "zhou2023solving",
    "ahuja-etal-2023-mega",
    "singhal2023large",
    "saparov2022language",
    "kojima2022large",
    "gruver2023large",
    "ahuja2023mega",
    "hou2024large",
    "chen2024tree",
    "anil2023palm",
    "balaguer2024rag"
  ],
  "relevant_ids": [
    "suzgun2024meta",
    "jiang2024mixtral",
    "gao2023pal",
    "besta2024graph",
    "achiam2023gpt",
    "du2022glm",
    "wei2022chain",
    "anil2023palm",
    "zhang2022automatic",
    "zhou2022least",
    "brown2020language",
    "xu2023expertprompting",
    "wang2022selfConsistency",
    "yasunaga2023analogical",
    "yao2024tree",
    "touvron2023llama",
    "touvron2023llama2"
  ],
  "metrics": {
    "R@5": 0.0,
    "R@10": 0.058823529411764705,
    "R@20": 0.058823529411764705,
    "MRR": 0.14285714285714285,
    "hits": 3,
    "total_relevant": 17
  },
  "score": 0.06050420168067226,
  "timestamp": "2025-12-18T10:53:39.434570"
}