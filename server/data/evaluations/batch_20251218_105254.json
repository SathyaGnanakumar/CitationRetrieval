{
  "query": "Introduction Reducing the storage and computational requirements of the state-of-the-art deep neural networks (DNNs) is of great practical importance. An effective way to compress DNNs is through model quantization<|cite_0|>. A quantization strategy that starts with a pre-trained model is known as post-training quantization (PTQ). Typically, PTQ requires only a small",
  "paper_id": "2307.05657",
  "retrieved_ids": [
    "bucilu2006model",
    "cheng2017survey",
    "pruning",
    "Han:2016uf",
    "polino2018distillation",
    "Alvarez_2017",
    "gong2014compressing",
    "Nagel2021AWP",
    "frantar2023optimal",
    "cai2019WNQ",
    "zhou2023dataset",
    "li2021brecq",
    "elthakeb2018releq",
    "zhang2018lq",
    "choukroun2019lowbit",
    "zhu2016ttq",
    "banner2018posttraining",
    "yuan2022ptq4vit",
    "choi2018bridging",
    "frantar2023gptq",
    "Wei2022QDropRD",
    "schaefer2023mixed",
    "dong2017learning",
    "llm_quant",
    "Dong2019HAWQHA",
    "zhou2017incremental",
    "li2023repq",
    "liu2023qllm",
    "zhao2019ocs",
    "pan2023smoothquant+",
    "shao2023omniquant",
    "rakin2018defend",
    "gong2019dsq",
    "hooker2019compressed",
    "choi2018pact",
    "Wang:2019tj",
    "Cai2020ZeroQAN",
    "pbllm",
    "Wang2019HAQHA",
    "liu2023llm"
  ],
  "relevant_ids": [
    "Nagel2020UpOD",
    "Cai2020ZeroQAN",
    "Nagel2021AWP",
    "Lou2020AutoQAK",
    "Courbariaux2015BinaryConnectTD",
    "Wei2022QDropRD",
    "Dong2020HAWQV2HA",
    "Chen2021TowardsMQ",
    "Dong2019HAWQHA",
    "Yang2020FracBitsMP",
    "Tang2022MixedPrecisionNN",
    "Wang2019HAQHA",
    "Yao2020HAWQV3DN",
    "Guo2020SinglePO",
    "Kim2021IBERTIB",
    "Wu2018MixedPQ",
    "Deng2022VariabilityAwareTA"
  ],
  "metrics": {
    "R@5": 0.0,
    "R@10": 0.058823529411764705,
    "R@20": 0.058823529411764705,
    "MRR": 0.125,
    "hits": 5,
    "total_relevant": 17
  },
  "score": 0.05514705882352941,
  "timestamp": "2025-12-18T10:52:54.782567"
}