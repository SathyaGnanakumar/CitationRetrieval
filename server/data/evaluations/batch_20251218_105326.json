{
  "query": "Introduction \\label{sec:intro} \\begin{wrapfigure}[14]{r}{0.35\\textwidth} \\centering \\vspace{-1.5em} \\includegraphics[width=\\linewidth]{plots/cascade_pattern_visualize_intro.pdf} \\vspace{-2.0em} \\caption{\\small Attention matrices from Streaming LLM<|cite_0|> and Cascading KV Cache (Ours), \\textbf{both with the same total cache size}.} \\label{fig:intro_visualization} \\end{wrapfigure} A key problem in deploying large language models (LLMs) is the compute cost necessary to perform the attention<|cite_1|> operation during inference. While there",
  "paper_id": "2406.17808",
  "retrieved_ids": [
    "generalpatternmachines2023",
    "sink",
    "naveed2024comprehensivellms",
    "chen2023frugalgpt",
    "survey2",
    "ge2023model",
    "jiang2023llmlingua",
    "llm_quant",
    "hsieh2023distilling",
    "SEED",
    "pang2024anchor",
    "model_compression",
    "mehta2024openelm",
    "qin2023transnormerllm",
    "yuan2024llminferenceunveiledsurvey",
    "lian2023llmgrounded",
    "qin2024lightning",
    "liu2023llm",
    "onebit",
    "kim2023memoryefficient",
    "qin2024various",
    "jiang2023scaling",
    "jaiswal2024compressing",
    "ashkboos2024slicegptcompresslargelanguage",
    "miao2023specinfer",
    "qlora",
    "namburi2023cost",
    "hassid2022does",
    "llmsurvey3",
    "flashattention2",
    "phung2023grounded",
    "wimbauer2023cache",
    "fu2024break",
    "karmanov2024efficient",
    "dong2024promptpromptedadaptivestructuredpruning",
    "aminabadi2022deepspeed",
    "sqzllm",
    "lin2024awq",
    "jamba",
    "li2022efficient",
    "cherti2023reproducible",
    "infiniattention",
    "switch",
    "dao2022flashattention",
    "Pan2021OnTI",
    "han2023imagebindllm",
    "MaddahAliCentralized",
    "dettmers2023case",
    "ShirinWiggerYenerCacheAssingment",
    "dynabert"
  ],
  "relevant_ids": [
    "flashattention2",
    "landmarkattention",
    "longformer",
    "reformer",
    "compressedcontext",
    "mistral",
    "performer",
    "sink",
    "infiniattention",
    "pg19",
    "wikitext",
    "attention"
  ],
  "metrics": {
    "R@5": 0.08333333333333333,
    "R@10": 0.08333333333333333,
    "R@20": 0.08333333333333333,
    "MRR": 0.5,
    "hits": 3,
    "total_relevant": 12
  },
  "score": 0.20833333333333331,
  "timestamp": "2025-12-18T10:53:26.179340"
}