{
  "query": "Introduction In recent years, Transformer models<|cite_0|> have achieved astounding success across vastly different domains: e.g., vision<|cite_1|>, NLP<|cite_2|>, chemistry, and many others. However, without massive training datasets, their performance can quickly saturate as model depth increases<|cite_3|>. This appears to be caused by a fundamental property of Transformer models: a recent line",
  "paper_id": "2401.04301",
  "retrieved_ids": [
    "khan2021transformers",
    "touvron2021going",
    "tay2020efficient",
    "mensink2021factors",
    "Chen2021VisformerTV",
    "luo2022your",
    "qin2023-nlp_effectiveness_long-range",
    "wang2023learning",
    "zhai2022scaling",
    "ganesh2020compressing",
    "han2020survey",
    "limitations",
    "zhou2021deepvit",
    "shamshad2022transformers",
    "lin2022cat",
    "yu2022metaformer",
    "tay2020long",
    "survey3",
    "survey1",
    "chen2022adaptformer",
    "wang-etal-2019-learning",
    "liu2020very",
    "wang2022anti",
    "white2023neural",
    "bondarenko2021understanding",
    "zhang2022minivit",
    "liu2022convnet",
    "wu2021rethinking",
    "benchmarking-gnn",
    "cui2022democratizing",
    "fan2019reducing",
    "ruckle2021adapterdrop",
    "he2020realformer",
    "guo2019star",
    "noci2022signal",
    "b8",
    "liu2021pay",
    "wu2022memorizing",
    "bondarenko2023quantizable",
    "yu2021rethinking",
    "bendidi2023free",
    "zhu2020modifying",
    "yang2023change",
    "hatamizadeh2022swin",
    "qin_transnormer_emnlp_2022",
    "lite_transformer",
    "mazzia2023survey",
    "wang2021knowledge",
    "kocsis2022unreasonable",
    "zhang2020pegasus",
    "dwivedi2020benchmarking",
    "chen2020lottery",
    "evanas",
    "de2023reliability",
    "zhao2022decoupled",
    "beyer2022knowledge",
    "liu2024neural",
    "cherti2023reproducible"
  ],
  "relevant_ids": [
    "dosovitskiy2021image",
    "touvron2021training",
    "wei2023chainofthought",
    "kaplan2020scaling",
    "kocsis2022unreasonable",
    "kaddour2023challenges",
    "yu2022efficient",
    "ali2023centered",
    "vaswani2023attention",
    "park2022vision",
    "liu2021pay",
    "yu2022metaformer",
    "wang2022anti",
    "noci2022signal",
    "dong2021attention",
    "touvron2023llama",
    "guo2023contranorm"
  ],
  "metrics": {
    "R@5": 0.0,
    "R@10": 0.0,
    "R@20": 0.058823529411764705,
    "MRR": 0.0625,
    "hits": 5,
    "total_relevant": 17
  },
  "score": 0.01875,
  "timestamp": "2025-12-18T10:59:03.013236"
}