{
  "query": "Introduction Large language models (LLMs), like Llama<|cite_0|> and GPT<|cite_1|>, have exhibited remarkable proficiency in general-purpose language generation and understanding<|cite_2|>. These advancements are largely credited to the development of Transformer-based architectures<|cite_3|> with billions of parameters and to the extensive pre-training on web-sourced corpora with trillions of tokens<|cite_4|>. However, on the other",
  "paper_id": "2406.09179",
  "retrieved_ids": [
    "Zhao2023ASO",
    "generalpatternmachines2023",
    "naveed2024comprehensivellms",
    "minaee2024large",
    "lehman2022evolution",
    "ref:med1",
    "dolma",
    "ref:edu1",
    "mmllms1",
    "xia2024fofo",
    "makatura2023large",
    "lora",
    "wang2023knowledge",
    "xie2024me",
    "gui_automation1",
    "jordan_chinchilla_2022",
    "llmsurvey3",
    "ref:ragsurvey1",
    "xia2023sheared",
    "deepseekllm",
    "Hu:2022:LLR",
    "shoeybi2020megatronlm",
    "Gao2023RetrievalAugmentedGF",
    "llama-moe",
    "chia2024instructeval",
    "zeng2023lynx",
    "chowdhery2022palm",
    "wu2023bloomberggpt",
    "switch",
    "zheng2024",
    "scao2022bloom",
    "yu2024language_dare",
    "fingpt",
    "WizardMath",
    "refinedweb",
    "mehta2024openelm",
    "Touvron2023Llama2O",
    "Ivison2023CamelsIA",
    "wei2023skywork",
    "touvron2023llama",
    "dong2024promptpromptedadaptivestructuredpruning",
    "jamba",
    "gruver2023large",
    "conneau2020unsupervised",
    "xraygpt",
    "ge2023making",
    "hou2024large",
    "ref:qwen2"
  ],
  "relevant_ids": [
    "achiam2023gpt",
    "zhu2024decoupling",
    "maini2024tofu",
    "vaswani2017attention",
    "yao2023editing",
    "liu2023trustworthy",
    "azerbayev2023llemma",
    "roziere2023code",
    "bourtoule2021machine",
    "ji2023survey",
    "liu2024rethinking",
    "yao2023survey",
    "brown2020language",
    "arpit2017closer",
    "touvron2023llama",
    "gallegos2023bias",
    "carlini2021extracting",
    "ouyang2022training",
    "liu2023jailbreaking",
    "wu2023bloomberggpt",
    "touvron2023llama2",
    "yao2023large",
    "goodfellow2013empirical"
  ],
  "metrics": {
    "R@5": 0.0,
    "R@10": 0.0,
    "R@20": 0.0,
    "MRR": 0.03571428571428571,
    "hits": 2,
    "total_relevant": 23
  },
  "score": 0.010714285714285713,
  "timestamp": "2025-12-18T10:54:03.361708"
}