{
  "query": "Introduction Today, the advent of large language models (LLMs) and their advanced prompting strategies has marked a significant progression, especially in classical NLP tasks<|cite_0|>. A key innovation among these is the Chain of Thought (CoT) prompting technique<|cite_1|>, known for its efficacy in multi-step problem solving. This technique, reflecting human sequential",
  "paper_id": "2401.04925",
  "retrieved_ids": [
    "naveed2024comprehensivellms",
    "zhang2022automatic",
    "kim2023language",
    "wei2022chain",
    "zhang2023multimodal",
    "besta2024graph",
    "reynolds2021prompt",
    "qin2024large",
    "qu2024tool",
    "yu2023towards",
    "sahoo2024systematic",
    "zheng2023progressivehint",
    "bbh",
    "gpt_summary",
    "wang2023boosting",
    "jagerman2023query",
    "ranaldi2023empowering",
    "turpin2023language",
    "Pan2023AutomaticallyCL",
    "shao2023synthetic",
    "wang-etal-2022-iteratively",
    "wang-etal-2023-towards",
    "trivedi2022interleaving",
    "wu2023analyzing",
    "feng2024towards",
    "qin2023cross",
    "llmsurvey3",
    "yasunaga2023analogical",
    "Zheng2023SecretsOR",
    "madaan2022text",
    "yeo2024interpretable",
    "chen2022program",
    "suzgun2024meta",
    "tai2023exploring",
    "Huang2023LargeLM",
    "ref:ragsurvey2",
    "lanham2023measuring",
    "lyu-etal-2023-faithful",
    "wang2023prompt",
    "kojima2022large",
    "ravi2024small"
  ],
  "relevant_ids": [
    "wang2023selfconsistency",
    "merrill2023expressive",
    "yao2023tree",
    "lyu2023faithful",
    "madaan2022text",
    "wei2022chain",
    "zhang2022automatic",
    "fu2023complexitybased",
    "shao2023synthetic",
    "brown2020language",
    "wu2023analyzing",
    "tang2023large",
    "schaeffer2023emergent",
    "kojima2023large"
  ],
  "metrics": {
    "R@5": 0.14285714285714285,
    "R@10": 0.14285714285714285,
    "R@20": 0.21428571428571427,
    "MRR": 0.5,
    "hits": 5,
    "total_relevant": 14
  },
  "score": 0.25,
  "timestamp": "2025-12-18T10:58:30.955143"
}