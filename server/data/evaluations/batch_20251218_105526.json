{
  "query": "Introduction Existing large language models (LLMs) have exhibited strong generalization abilities on various tasks due to their huge model capacities<|cite_0|>. With faith in the scaling law<|cite_1|>, the amount of parameters in current LLMs is expanded steadily to achieve higher intelligence. However, the increasing parameters also bring high deployment costs in",
  "paper_id": "2406.17328",
  "retrieved_ids": [
    "Zhao2023ASO",
    "wei2022emergent",
    "minaee2024large",
    "naveed2024comprehensivellms",
    "kaddour2023challenges",
    "survey2",
    "yuan2023scaling",
    "dong2023abilities",
    "gpt_summary",
    "laskar2024systematic",
    "ref:edu1",
    "guo2024large",
    "jiang2023scaling",
    "zhu2023multilingual",
    "qin2024large",
    "ref:ragsurvey1",
    "survey1",
    "li2021scaling",
    "qin2024multilingual",
    "shen2023mixtureofexperts",
    "mmllms1",
    "miao2023towards",
    "deepseekllm",
    "llmsurvey3",
    "makatura2023large",
    "zhang2024scaling",
    "lu2024",
    "wang2023knowledge",
    "ref:med3",
    "model_compression",
    "jin2024comprehensive",
    "fu23cotkd",
    "pang2024anchor",
    "wei2024large",
    "ref:ragsurvey3",
    "namburi2023cost",
    "asai2024reliable",
    "hu2023",
    "ref:ragsurvey2",
    "tian2024gnp",
    "luo2023empirical",
    "pan2023smoothquant+",
    "shi2023dept",
    "yu2024melo",
    "ge2023making",
    "bentham2024chainofthought"
  ],
  "relevant_ids": [
    "wen23fdiv",
    "jiao20tinybert",
    "xu24llmkdsurvey",
    "touvron23llama",
    "gu23minillm",
    "sun19patientkd",
    "fu23cotkd",
    "hinton15kd",
    "sanh19distilbert",
    "sun20mobilebert",
    "chowdhery23palm",
    "wang21minilmv2",
    "ko24distillm",
    "kaplan20scaling",
    "wu2024rethinking",
    "openai23gpt4",
    "wang20minilm",
    "kim16seqkd"
  ],
  "metrics": {
    "R@5": 0.0,
    "R@10": 0.0,
    "R@20": 0.0,
    "MRR": 0.03125,
    "hits": 1,
    "total_relevant": 18
  },
  "score": 0.009375,
  "timestamp": "2025-12-18T10:55:26.792288"
}