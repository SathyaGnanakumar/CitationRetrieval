{
  "query": "Introduction In recent years, neural models have led to state-of-the-art results in machine translation (MT)<|cite_0|>. Many of these systems can broadly be characterized as following a multi-layer encoder-decoder neural network design: both the encoder and decoder learn representations of word sequences by a stack of layers<|cite_1|>, building on an interesting",
  "paper_id": "2010.03737",
  "retrieved_ids": [
    "Chen:18",
    "cho2014properties",
    "Chen:2018vf",
    "wang-etal-2018-multi-layer",
    "gu2017learning",
    "Cheng:18",
    "barone2017deep",
    "Bahdanau:2015vz",
    "wang-etal-2019-learning",
    "ZhouCWLX16",
    "gehring2016convolutional",
    "maruf-haffari-2018-document",
    "Bahdanau2014NeuralMT",
    "mccann2017learned",
    "sutskever2014sequence",
    "Tu2017NeuralMT",
    "vnmt",
    "wu2016google",
    "Wu:2016wt",
    "kalchbrenner2016neural",
    "Kalchbrenner:2016vf",
    "gru",
    "liu2020very",
    "kasai2021deep",
    "wang:2017:aaai",
    "knnmt",
    "artetxe2017unsupervised",
    "NLI_NSE",
    "bastings2017graph",
    "kiros2014unifying",
    "wangEtAl2017",
    "huang2021non",
    "kaiser2018fast",
    "badrinarayanan2015segnet",
    "hypernetworks",
    "ha2016hypernetworks",
    "zheng2020gman",
    "dehghani2018universal",
    "Yu2022CoCaCC",
    "hatamizadeh2022unetr",
    "ma2019monotonic",
    "vaswani2017attention"
  ],
  "relevant_ids": [
    "gehring2017convs2s",
    "wu-etal-2019-depth",
    "he2016deep",
    "lei2016layer",
    "liu2020understanding",
    "wang-etal-2019-learning",
    "wang-etal-2018-multi-layer",
    "wu2016google",
    "zhang-etal-2019-improving",
    "sutskever2014sequence",
    "vaswani2017attention",
    "bapna-etal-2018-training",
    "wei2004multiscale",
    "xu2019lipschitz",
    "bahdanau2014neural"
  ],
  "metrics": {
    "R@5": 0.06666666666666667,
    "R@10": 0.13333333333333333,
    "R@20": 0.26666666666666666,
    "MRR": 0.25,
    "hits": 5,
    "total_relevant": 15
  },
  "score": 0.14166666666666666,
  "timestamp": "2025-12-18T10:57:45.268919"
}