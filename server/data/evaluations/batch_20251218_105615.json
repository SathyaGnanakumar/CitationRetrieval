{
  "query": "Introduction \\label{sec:motivation} Recently, large language models (LLMs) have shown impressive performance in a range of natural language processing tasks<|cite_0|>. However, fine-tuning pre-trained language models (PLMs) is computationally and memory-intensive. To mitigate this, \\textit{parameter-efficient tuning} (PET) methods have been developed to fine-tune a small number of (extra) model parameters instead of",
  "paper_id": "2406.14956",
  "retrieved_ids": [
    "huang2022large",
    "zhong2022training",
    "minaee2024large",
    "ding-et-al:scheme",
    "nas4pet",
    "Zhao2023ASO",
    "qin2024large",
    "kim2023memoryefficient",
    "survey1",
    "survey2",
    "naveed2024comprehensivellms",
    "jordan_chinchilla_2022",
    "generalpatternmachines2023",
    "qi2022parameterefficient",
    "s3pet",
    "wang2019structured",
    "sun2024simpleeffectivepruningapproach",
    "dong2023abilities",
    "li2022pretrained",
    "kwon2022alphatuning",
    "zhang2024scaling",
    "lora",
    "chen2023parameter",
    "zhang2021differentiable",
    "shi2023dept",
    "malladi2023fine",
    "Hu:2022:LLR",
    "Sung2022LSTLS",
    "xia2023sheared",
    "wang2022adamix",
    "xiao2023lm_cocktail",
    "du2022glamefficientscalinglanguage",
    "xu2023qa",
    "mehta2024openelm",
    "adalora",
    "chang2022speechprompt",
    "zhang2023benchmarking",
    "He:2022:TUV",
    "yuan2023scaling",
    "wang2023nonintrusive",
    "lin2020exploring",
    "instruction-tuning",
    "han2022ptr",
    "chang2023speechprompt",
    "alabi2022adapting",
    "Lewis2020RetrievalAugmentedGF",
    "yu2024language_dare",
    "mahabadi2021compacter"
  ],
  "relevant_ids": [
    "darts",
    "squeezeandexcitation",
    "dylora",
    "llmsurvey",
    "bo",
    "paralleladapter",
    "lora",
    "serialadapter",
    "adalora",
    "prefixtuning",
    "bitfit",
    "nas4pet",
    "autopeft",
    "s3pet",
    "resnet",
    "densenet",
    "zerocostproxy"
  ],
  "metrics": {
    "R@5": 0.058823529411764705,
    "R@10": 0.058823529411764705,
    "R@20": 0.11764705882352941,
    "MRR": 0.2,
    "hits": 4,
    "total_relevant": 17
  },
  "score": 0.10117647058823528,
  "timestamp": "2025-12-18T10:56:15.770796"
}