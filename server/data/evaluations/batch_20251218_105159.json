{
  "query": "Introduction Transformers<|cite_0|>, characterized by stacked self-attention modules and feed-forward transformations, have become a staple in modern deep learning, natural language processing<|cite_1|> and even computer vision<|cite_2|>. One key defining characteristics in the self-attention mechanism is the global receptive field in which each token is accessible to every other token in the",
  "paper_id": "2103.01075",
  "retrieved_ids": [
    "vig-belinkov-2019-analyzing",
    "khan2021transformers",
    "SEED",
    "zhou2022understanding",
    "Sukhbaatar:2019adaptive",
    "abnar2020quantifying_attentionrollout",
    "chu2021twins",
    "tay2020efficient",
    "sukhbaatar-etal-2019-adaptive",
    "melas2021resmlp",
    "yang2021focal",
    "chen2022regionvit",
    "dat",
    "lin2022cat",
    "ding2022davit",
    "katharopoulos2020transformers",
    "dosovitskiy2020vit",
    "ICLRsub2022",
    "han2020survey",
    "vit",
    "chefer2021transformerbeyond",
    "zhu2019empirical",
    "paul2022vision",
    "lee2021vision",
    "sukhbaatar2019augmentingmemory",
    "wu2021pale",
    "ergen2022convexifying",
    "ali2023centered",
    "Sukhbaatar:2019augmenting",
    "edelman2022inductive",
    "yang2022scalablevit",
    "xu2022evo_token",
    "xu2021vitae",
    "dehghani2018universal",
    "lu2021soft",
    "DUAL-VIT",
    "darcet2023vision",
    "selfconv",
    "zhang2022vitaev2",
    "li2021localvit",
    "suchengCVPR22",
    "xcit",
    "li2022CoT",
    "shen2018reinforced",
    "yu2021rethinking",
    "yu2022metaformer",
    "Lambdanetworks",
    "lou2022crosstoken",
    "mamba"
  ],
  "relevant_ids": [
    "tay2020efficient",
    "vaswani2017attention",
    "srivastava2015highway",
    "wang2020linformer",
    "bahdanau2014neural",
    "liu2020very",
    "bapna2018training",
    "raffel2019exploring",
    "brown2020language",
    "chelba2013one",
    "parikh2016decomposable",
    "devlin2018bert",
    "he2016deep",
    "choromanski2020rethinking",
    "tay2018densely",
    "dai2019transformer",
    "zaheer2020big",
    "dehghani2018universal",
    "he2020realformer",
    "huang2017densely",
    "tay2020long",
    "dosovitskiy2020image"
  ],
  "metrics": {
    "R@5": 0.0,
    "R@10": 0.045454545454545456,
    "R@20": 0.045454545454545456,
    "MRR": 0.125,
    "hits": 2,
    "total_relevant": 22
  },
  "score": 0.05113636363636363,
  "timestamp": "2025-12-18T10:51:59.015006"
}