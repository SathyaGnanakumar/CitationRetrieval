{
  "query": "Introduction Very large language models (LLMs) such as GPT-3.5-Turbo \\& GPT-4<|cite_0|> show exceptional performance on a variety of NLP and reasoning tasks via \\textit{In-Context Learning} (ICL)<|cite_1|>. ICL feeds a task-specific instruction along with a few exemplars, appended with the test input, to the LLM. As LLMs can be highly sensitive",
  "paper_id": "2406.18880",
  "retrieved_ids": [
    "Zhao2023ASO",
    "wei2023larger",
    "huang2022large",
    "naveed2024comprehensivellms",
    "qin2024large",
    "minaee2024large",
    "survey1",
    "zhu2023multilingual",
    "xu2023retrieval",
    "hendel2023incontext",
    "khattab2023demonstratesearchpredict",
    "wang2023large",
    "conifer",
    "chia2024instructeval",
    "pan-etal-2023-context",
    "wizardLM",
    "wang2023label",
    "chen2023frugalgpt",
    "lawbench",
    "zhou2023navgpt",
    "li2024-Longcontextllmsstrugglelong",
    "tang2023struc",
    "Liu2023LLMRecBL",
    "lu2023chameleon",
    "dai2023can",
    "xie2023translating",
    "wang2024large",
    "WizardMath",
    "brown2020language",
    "honovich2022instruction",
    "makatura2023large",
    "nan2023enhancing",
    "chiang2023can",
    "2020arXiv200514165B",
    "chen-etal-2023-many",
    "chowdhery2022palm",
    "zheng2023can",
    "zhang2021differentiable",
    "tanwar2023multilingual",
    "hong20233dllm",
    "tanwar-etal-2023-multilingual",
    "pourreza2023din",
    "genegpt",
    "bueno2022induced",
    "llmsurvey3",
    "fu2023complexitybased",
    "zhang2024raft",
    "kojima2022large",
    "zhang-infinite-bench-2024",
    "saparov2022language",
    "wang2022super",
    "wan-etal-2023-universal"
  ],
  "relevant_ids": [
    "ouyang-etal-2022-impact",
    "achiam2023gpt",
    "muller2021being",
    "zhang2021differentiable",
    "Devlin2019BERTPO",
    "ahuja2023mega",
    "asai2023buffet",
    "winata2021language",
    "nllb2022",
    "brown2020language",
    "nambi2023breaking",
    "chen2023frustratingly",
    "conneau2020unsupervised",
    "ustun2020udapter",
    "alabi2022adapting",
    "rathore2023zgul",
    "Agrawal2022IncontextES",
    "garcia2023t",
    "wan-etal-2023-universal",
    "chowdhery2022palm",
    "reimers2020making",
    "pfeiffer2020mad",
    "zhao2021calibrate",
    "wan-etal-2023-better",
    "le2024constrained"
  ],
  "metrics": {
    "R@5": 0.0,
    "R@10": 0.0,
    "R@20": 0.0,
    "MRR": 0.034482758620689655,
    "hits": 4,
    "total_relevant": 25
  },
  "score": 0.010344827586206896,
  "timestamp": "2025-12-18T10:54:45.477330"
}