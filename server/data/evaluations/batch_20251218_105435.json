{
  "query": "Introduction Though large language models (LLMs) have delivered impressive results in a variety of natural language processing (NLP) tasks, their massive size often complicates deployment. One common method to compress LLMs is through the quantization of weight parameters, which reduces model sizes by lowering the precision of weight values<|cite_0|>. Existing",
  "paper_id": "2406.12311",
  "retrieved_ids": [
    "model_compression",
    "qin2024large",
    "minaee2024large",
    "llm_quant",
    "kaddour2023challenges",
    "jaiswal2024compressing",
    "jin2024comprehensive",
    "naveed2024comprehensivellms",
    "treviso2023efficient",
    "kim2023memoryefficient",
    "ganesh2020compressing",
    "xu2023qa",
    "quip",
    "hongdecoding",
    "survey1",
    "owq",
    "lin2024awq",
    "gpt_summary",
    "liu2023llm",
    "jiang2023llmlingua",
    "shao2023omniquant",
    "onebit",
    "pbllm",
    "billm",
    "zhang2023integer",
    "wang2019structured",
    "miao2023towards",
    "lu2024",
    "namburi2023cost",
    "pan2023smoothquant+",
    "du2022glamefficientscalinglanguage",
    "sqzllm",
    "ref:ragsurvey2",
    "fan2020training",
    "sharify2024combining",
    "pang2024anchor",
    "llmsurvey3",
    "defossez2021differentiable",
    "zhang2023benchmarking",
    "fusco2022pnlp",
    "zafrir2019q8bert",
    "Wu2018MixedPQ",
    "frantar2023gptq",
    "yu2024melo"
  ],
  "relevant_ids": [
    "xnor",
    "sqzllm",
    "gptq",
    "binary_survey",
    "owq",
    "stmoe",
    "model_compression",
    "billm",
    "pbllm",
    "quip",
    "bitnet",
    "switch_trans",
    "awq",
    "qlora",
    "onebit",
    "moe",
    "bireal",
    "llm_quant",
    "react"
  ],
  "metrics": {
    "R@5": 0.10526315789473684,
    "R@10": 0.10526315789473684,
    "R@20": 0.21052631578947367,
    "MRR": 1.0,
    "hits": 8,
    "total_relevant": 19
  },
  "score": 0.3736842105263158,
  "timestamp": "2025-12-18T10:54:35.375613"
}