{
  "query": "Introduction \\label{intro_ewn} Deep neural networks are susceptible to {\\em adversarial attacks} -- the presentation of almost imperceptibly manipulated ``adversarial'' inputs that cause the system to misclassify them<|cite_0|>. A variety of defense techniques have been proposed in the literature to address this problem. The most successful line of work is {\\em",
  "paper_id": "2103.08265",
  "retrieved_ids": [
    "fawaz2019adversarial",
    "yuan2019adversarial",
    "Madry2017",
    "chakraborty2018adversarial",
    "PMJ+16",
    "Mad+18",
    "sur2",
    "akhtar2018threat",
    "ma2021understanding",
    "liang2017deep",
    "potdevin19",
    "xu2017feature",
    "he2019parametric",
    "yan2018deep",
    "strauss2017ensemble",
    "samangouei2018defense",
    "fineprune2018",
    "papernot2016distillation",
    "xu2019adversarial",
    "metzen2017detecting",
    "vijaykeerthy2018hardening",
    "overview",
    "liu2019extending",
    "meng2017magnet",
    "carlini2017adversarial",
    "2_Dai",
    "ghosh2019resisting",
    "shafahi2018are",
    "su2019one",
    "prakash2018deflecting",
    "NRP",
    "chou2020senti",
    "yu2021lafeat",
    "xiao2020one",
    "cheng2018seq2sick",
    "biggio_2018",
    "IJADD17",
    "Carlini_Dill_2018",
    "ganeshan2019fda",
    "das2017keeping",
    "PapernotM17"
  ],
  "relevant_ids": [
    "papernot2016distillation",
    "moosavi2016deepfool",
    "kurakin2016adversarial",
    "yang2019me",
    "apernot2017practical",
    "gilmer2019adversarial",
    "cisse2017parseval",
    "zhang2019theoretically",
    "carmon2019unlabeled",
    "xie2019feature",
    "szegedy2013properties",
    "goodfellow2014explaining",
    "hendrycks2019using",
    "madry2018towards",
    "xu2012robustnessvi",
    "tsipras2018robustness"
  ],
  "metrics": {
    "R@5": 0.0,
    "R@10": 0.0,
    "R@20": 0.0625,
    "MRR": 0.05555555555555555,
    "hits": 1,
    "total_relevant": 16
  },
  "score": 0.016666666666666666,
  "timestamp": "2025-12-18T10:52:16.263461"
}