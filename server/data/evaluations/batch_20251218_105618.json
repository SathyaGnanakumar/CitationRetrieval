{
  "query": "Introduction Transformer-based (<|cite_0|>) large language models (LLMs) are capable of implicitly internalizing a wide range of knowledge during pretraining (<|cite_1|>;<|cite_2|>). However, the potential for generating inaccurate and outdated responses limits the widespread applications of LLMs. One proposed solution to this problem is knowledge editing, which modifies specific factual knowledge in",
  "paper_id": "2406.11566",
  "retrieved_ids": [
    "de2021editing",
    "wang2023knowledge",
    "zhang2024comprehensive",
    "yao2023editing",
    "calinet",
    "wang2024easyedit",
    "alkhamissi2022review",
    "naveed2024comprehensivellms",
    "wang2023cross",
    "Zhao2023ASO",
    "zheng2023can",
    "wu2023evakellm",
    "yin2023history",
    "wang2023surveyfactualitylargelanguage",
    "mlake",
    "zhong2023mquake",
    "dola",
    "zhu2020modifying",
    "xu24llmkdsurvey",
    "survey2",
    "check",
    "llmsurvey2",
    "li2023pmet",
    "sahoo2024systematic",
    "hu2023",
    "felm",
    "ref:ragsurvey1",
    "lu2023emergent",
    "Gao2023RetrievalAugmentedGF",
    "wang2023retrievalaugmented",
    "Pan2023AutomaticallyCL",
    "halueval",
    "perez2022ignore",
    "wang2023self",
    "si2024mpn",
    "mitchell2022memory",
    "wang-etal-2022-iteratively",
    "huang2023transformer",
    "gupta2024model",
    "luo2023empirical",
    "su2024mitigating",
    "baek2023knowledge",
    "manakul",
    "su2024unsupervised",
    "verga2020facts"
  ],
  "relevant_ids": [
    "surveyofedit",
    "anisotropic",
    "beniwal2024crosslingual",
    "rome",
    "knowledge-neurons",
    "wang2023crosslingual",
    "alkhamissi2022review",
    "attentionisall",
    "petroni-etal-2019-language",
    "SERAC",
    "lm-multilingual-cot",
    "loramodule",
    "wang2023retrievalaugmented",
    "mend",
    "calinet",
    "mlake",
    "mllm4mspeech",
    "iclchangbaobao",
    "memit",
    "memprompt",
    "grace",
    "wang2024easyedit",
    "editing-factual-in-LM",
    "TPatcher"
  ],
  "metrics": {
    "R@5": 0.041666666666666664,
    "R@10": 0.125,
    "R@20": 0.16666666666666666,
    "MRR": 0.2,
    "hits": 5,
    "total_relevant": 24
  },
  "score": 0.11416666666666667,
  "timestamp": "2025-12-18T10:56:18.066234"
}