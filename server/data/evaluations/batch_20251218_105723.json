{
  "query": "Introduction \\label{sec:introduction} Adversarial examples, which contain perturbations to the input of a model that elicit large changes in the output, have been shown to be an effective way of assessing the robustness of models in natural language processing (NLP)<|cite_0|>. Adversarial training, in which models are trained on adversarial examples, has",
  "paper_id": "2010.01610",
  "retrieved_ids": [
    "alzantot2018generating",
    "bai2021recent",
    "goodfellow2015laceyella",
    "Goodfellow2014",
    "overview",
    "cheng2018seq2sick",
    "carlini2017towards",
    "bubeck2019adversarial",
    "tsipras2018robustness",
    "Gu2015",
    "Ebrahimi:18b",
    "acl",
    "huang2015learning",
    "zhao2017generating",
    "sur2",
    "zhang2019generating",
    "wu2018understanding",
    "yuan2019adversarial",
    "grosse2017statistical",
    "kurakin2016adversarial",
    "arnab2018robustness",
    "schmidt2018adversarially",
    "michel2019evaluation",
    "ross2018improving",
    "PMJ+16",
    "Goodfellow2016",
    "papernot2016crafting",
    "potdevin19",
    "ford2019adversarial",
    "wu2020adversarial",
    "narodytska2016simple",
    "qin2019imperceptible",
    "lu2017adversarial",
    "tramer2020fundamental",
    "li2019certified",
    "samangouei2018defense",
    "boucher2021bad",
    "pang2020mixup"
  ],
  "relevant_ids": [
    "miyato2016adversarial",
    "samanta2017towards",
    "yasunaga2018robust",
    "michel2019evaluation",
    "zhang2019generating",
    "papernot2016crafting",
    "belinkov2017synthetic",
    "tramer2017ensemble",
    "devlin2019bert",
    "goodfellow2014explaining",
    "alzantot2018generating",
    "ebrahimi2018hotflip",
    "jia2017adversarial",
    "wang2019improving",
    "hosseini2017deceiving"
  ],
  "metrics": {
    "R@5": 0.06666666666666667,
    "R@10": 0.06666666666666667,
    "R@20": 0.13333333333333333,
    "MRR": 1.0,
    "hits": 4,
    "total_relevant": 15
  },
  "score": 0.3466666666666667,
  "timestamp": "2025-12-18T10:57:23.440103"
}