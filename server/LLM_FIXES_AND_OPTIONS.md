# LLM Reranker Fixes and Configuration Options

## Issues Fixed

### 1. ‚úÖ Hugging Face Response Format Error
**Problem:** `'str' object has no attribute 'content'`

**Root Cause:** Different LLM backends return responses in different formats:
- `ChatOllama` / `ChatOpenAI`: Returns object with `.content` attribute
- `HuggingFacePipeline`: Returns string directly or dict/list with text

**Fix:** Added robust response handling in `src/agents/formulators/llm_agent.py:103-119`:
```python
# Handle different response formats
if isinstance(response, str):
    response_text = response
elif hasattr(response, 'content'):
    response_text = response.content
elif isinstance(response, dict) and 'text' in response:
    response_text = response['text']
elif isinstance(response, list) and len(response) > 0:
    if isinstance(response[0], dict) and 'generated_text' in response[0]:
        response_text = response[0]['generated_text']
    else:
        response_text = str(response)
else:
    response_text = str(response)
```

### 2. ‚úÖ Added OpenAI Support
**Added:** Full support for OpenAI models as a third inference option alongside Ollama and Hugging Face.

**Benefits:**
- No local GPU needed
- Faster inference (cloud-based)
- Access to GPT-4o, GPT-4o-mini, GPT-4-turbo
- Better quality for complex reranking tasks

---

## Configuration Options

You now have **3 ways** to run LLM reranking:

### Option 1: Ollama (Local, Free, Medium Quality)

**Best for:** Local development, privacy-sensitive work, offline usage

**Setup:**
```bash
# In .env
INFERENCE_ENGINE="ollama"
LOCAL_LLM="gemma3:4b"
```

**Requirements:**
- Ollama server running locally
- Download model: `ollama pull gemma3:4b`

**Pros:**
- ‚úÖ Free
- ‚úÖ Fast (after initial load)
- ‚úÖ Works offline
- ‚úÖ Privacy (runs locally)

**Cons:**
- ‚ö†Ô∏è Requires Ollama server running
- ‚ö†Ô∏è Medium quality compared to GPT-4
- ‚ö†Ô∏è Needs some RAM (4-8GB for 4B models)

---

### Option 2: Hugging Face (Local/Cloud, Free, Variable Quality)

**Best for:** GPU clusters, customization, research

**Setup:**
```bash
# In .env
INFERENCE_ENGINE="huggingface"
LOCAL_LLM="google/gemma-2-9b-it"
hf_key="your_hugging_face_token"
```

**Recommended Models:**
- `google/gemma-2-9b-it` - Good balance
- `meta-llama/Llama-3.1-8B-Instruct` - Better reasoning
- `mistralai/Mistral-7B-Instruct-v0.3` - Fast inference

**Requirements:**
- GPU with enough VRAM (8-16GB for 7-9B models)
- Hugging Face account (free)
- Model downloads automatically on first run

**Pros:**
- ‚úÖ Free
- ‚úÖ Full control over model
- ‚úÖ Works great on GPU clusters
- ‚úÖ Supports gated models (with token)
- ‚úÖ Cached after first load (FAST!)

**Cons:**
- ‚ö†Ô∏è Requires GPU for good performance
- ‚ö†Ô∏è First run downloads model (slow)
- ‚ö†Ô∏è VRAM requirements

---

### Option 3: OpenAI (Cloud, Paid, Best Quality)

**Best for:** Production, best quality results, no GPU available

**Setup:**
```bash
# In .env
INFERENCE_ENGINE="openai"
LOCAL_LLM="gpt-4o-mini"  # or "gpt-4o", "gpt-4-turbo"
OPENAI_API_KEY="sk-proj-..."
```

**Model Options:**
- `gpt-4o-mini` - Fast, cheap, good quality ($0.15/1M input tokens)
- `gpt-4o` - Better quality, more expensive ($5/1M input tokens)
- `gpt-4-turbo` - Highest quality ($10/1M input tokens)

**Requirements:**
- OpenAI API key (paid)
- Internet connection

**Pros:**
- ‚úÖ Best quality reranking
- ‚úÖ No GPU needed
- ‚úÖ Fast inference (cloud-based)
- ‚úÖ No model downloads
- ‚úÖ Scales easily

**Cons:**
- ‚ö†Ô∏è Costs money (though gpt-4o-mini is cheap)
- ‚ö†Ô∏è Requires internet
- ‚ö†Ô∏è Sends data to OpenAI

---

## Usage Examples

### Running with Hugging Face on GPU Cluster:

```bash
# In .env
INFERENCE_ENGINE="huggingface"
LOCAL_LLM="google/gemma-2-9b-it"

# Run evaluation
python compare_baselines_vs_system.py \
  --num-examples 500 \
  --use-dspy \
  --llm-reranker \
  --output-dir final \
  --k 20
```

**Expected output:**
```
üîÑ Loading Hugging Face model: google/gemma-2-9b-it
   This will take a few minutes on first run...
‚úÖ Hugging Face model loaded and cached!

Example 1: üöÄ Using cached LLM model: google/gemma-2-9b-it
Example 2: üöÄ Using cached LLM model: google/gemma-2-9b-it
...
```

### Running with OpenAI:

```bash
# In .env
INFERENCE_ENGINE="openai"
LOCAL_LLM="gpt-4o-mini"
OPENAI_API_KEY="sk-proj-..."

# Run evaluation
python compare_baselines_vs_system.py \
  --num-examples 500 \
  --use-dspy \
  --llm-reranker \
  --output-dir final \
  --k 20
```

**Expected output:**
```
üîÑ Initializing OpenAI with model: gpt-4o-mini
‚úÖ OpenAI ready!

Example 1: üöÄ Using cached LLM model: gpt-4o-mini
Example 2: üöÄ Using cached LLM model: gpt-4o-mini
...
```

### Running with Ollama:

```bash
# In .env
INFERENCE_ENGINE="ollama"
LOCAL_LLM="gemma3:4b"

# Make sure Ollama is running first
ollama serve  # In separate terminal

# Run evaluation
python compare_baselines_vs_system.py \
  --num-examples 500 \
  --use-dspy \
  --llm-reranker \
  --output-dir final \
  --k 20
```

---

## Cost Comparison (for 500 examples)

Assuming ~1000 tokens per reranking request:

| Method | Setup Cost | Per-Example Cost | 500 Examples Total |
|--------|-----------|------------------|-------------------|
| **Ollama** | Free (local hardware) | $0 | **$0** |
| **Hugging Face** | Free (GPU time) | $0 | **$0** |
| **OpenAI (gpt-4o-mini)** | Free | ~$0.00015 | **~$0.08** |
| **OpenAI (gpt-4o)** | Free | ~$0.005 | **~$2.50** |

---

## Recommendations

### For Development/Testing:
Use **Ollama** or **Hugging Face** (free, fast enough)

### For GPU Cluster (Your Case):
Use **Hugging Face** with cached model:
- First run: 2-5 min model load
- Subsequent examples: Fast (model stays in GPU memory)
- Total 500 examples: ~90 minutes

### For Production/Best Quality:
Use **OpenAI gpt-4o-mini**:
- No GPU needed
- Best quality/cost ratio
- Easy to scale

### For Research/Experimentation:
Use **Hugging Face** with different models:
- Full control
- Can fine-tune if needed
- Free

---

## Troubleshooting

### Hugging Face: "Out of Memory"
- Use smaller model (e.g., `google/gemma-2-2b-it`)
- Reduce `max_new_tokens` in builders.py:222
- Enable quantization (4-bit/8-bit loading)

### OpenAI: "Rate limit exceeded"
- Add delays between requests
- Use `gpt-4o-mini` (higher rate limits)
- Upgrade API tier

### Ollama: "Connection refused"
- Make sure Ollama server is running: `ollama serve`
- Check if model is downloaded: `ollama list`
- Pull model if needed: `ollama pull gemma3:4b`

---

## Performance Summary

| Metric | Ollama | Hugging Face | OpenAI |
|--------|--------|--------------|--------|
| **Quality** | ‚òÖ‚òÖ‚òÖ‚òÜ‚òÜ | ‚òÖ‚òÖ‚òÖ‚òÖ‚òÜ | ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ |
| **Speed (cached)** | ‚òÖ‚òÖ‚òÖ‚òÖ‚òÜ | ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ | ‚òÖ‚òÖ‚òÖ‚òÖ‚òÜ |
| **Cost** | ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ (Free) | ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ (Free) | ‚òÖ‚òÖ‚òÖ‚òÜ‚òÜ (Paid) |
| **GPU Required** | ‚ùå | ‚úÖ | ‚ùå |
| **Setup Complexity** | Medium | Medium | Easy |
| **Best For** | Local dev | GPU clusters | Production |
