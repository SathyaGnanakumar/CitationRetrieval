{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Single Example Test - Citation Retrieval System\n",
    "\n",
    "This notebook tests the full retrieval pipeline on a single example from the ScholarCopilot dataset.\n",
    "\n",
    "**Pipeline:**\n",
    "1. Load dataset and extract one citation context\n",
    "2. Build retrieval resources (BM25, E5, SPECTER)\n",
    "3. Run retrieval with all three methods\n",
    "4. Aggregate results using RRF\n",
    "5. Rerank with LLM (Hugging Face or Ollama)\n",
    "6. Show final ranked papers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "import json\n",
    "import re\n",
    "import sys\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Add parent directory to path\n",
    "sys.path.insert(0, str(Path.cwd()))\n",
    "\n",
    "print(\"‚úÖ Imports successful\")\n",
    "print(f\"üìÅ Working directory: {Path.cwd()}\")\n",
    "print(f\"üîß Inference Engine: {os.getenv('INFERENCE_ENGINE', 'ollama')}\")\n",
    "print(f\"ü§ñ Local LLM: {os.getenv('LOCAL_LLM', 'gemma3:4b')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Load Dataset and Extract Citation Context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from corpus_loaders.scholarcopilot import load_dataset, build_citation_corpus\n",
    "\n",
    "# Load dataset\n",
    "dataset_path = os.getenv('DATASET_DIR', 'corpus_loaders/scholarcopilot/scholar_copilot_eval_data_1k.json')\n",
    "print(f\"üìö Loading dataset from: {dataset_path}\")\n",
    "\n",
    "dataset = load_dataset(dataset_path)\n",
    "print(f\"‚úÖ Loaded {len(dataset)} papers\")\n",
    "\n",
    "# Build corpus\n",
    "print(\"\\nüî® Building citation corpus...\")\n",
    "corpus = build_citation_corpus(dataset)\n",
    "print(f\"‚úÖ Corpus: {len(corpus)} documents\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract one citation context\n",
    "cite_pattern = re.compile(r\"<\\|cite_\\d+\\|>\")\n",
    "\n",
    "test_example = None\n",
    "for paper in dataset[:50]:  # Look through first 50 papers\n",
    "    paper_text = paper.get(\"paper\", \"\")\n",
    "    if not paper_text:\n",
    "        continue\n",
    "    \n",
    "    bib_info = paper.get(\"bib_info\", {})\n",
    "    \n",
    "    # Find first citation marker\n",
    "    for match in cite_pattern.finditer(paper_text):\n",
    "        cite_token = match.group(0)\n",
    "        \n",
    "        if cite_token not in bib_info:\n",
    "            continue\n",
    "        \n",
    "        refs = bib_info[cite_token]\n",
    "        if not refs:\n",
    "            continue\n",
    "        \n",
    "        # Get ground truth IDs\n",
    "        relevant_ids = set()\n",
    "        for ref in refs:\n",
    "            ref_id = ref.get(\"citation_key\") or ref.get(\"paper_id\")\n",
    "            if ref_id:\n",
    "                relevant_ids.add(str(ref_id))\n",
    "        \n",
    "        if not relevant_ids:\n",
    "            continue\n",
    "        \n",
    "        # Extract context around citation (¬±100 words)\n",
    "        pos = match.start()\n",
    "        words_before = paper_text[:pos].split()[-100:]\n",
    "        words_after = paper_text[match.end():].split()[:100]\n",
    "        \n",
    "        context = \" \".join(words_before + words_after)\n",
    "        context = re.sub(r\"<\\|cite_\\d+\\|>\", \"\", context)\n",
    "        context = \" \".join(context.split())\n",
    "        \n",
    "        if len(context.split()) < 10:\n",
    "            continue\n",
    "        \n",
    "        test_example = {\n",
    "            \"query\": context,\n",
    "            \"relevant_ids\": relevant_ids,\n",
    "            \"ground_truth_titles\": [ref.get(\"title\", \"Unknown\") for ref in refs]\n",
    "        }\n",
    "        break\n",
    "    \n",
    "    if test_example:\n",
    "        break\n",
    "\n",
    "if not test_example:\n",
    "    raise ValueError(\"Could not find a valid citation example in dataset\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üìù TEST EXAMPLE\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\nüîç Query (first 300 chars):\\n{test_example['query'][:300]}...\")\n",
    "print(f\"\\n‚úÖ Ground Truth Citations ({len(test_example['relevant_ids'])})\")\n",
    "for i, title in enumerate(test_example['ground_truth_titles'], 1):\n",
    "    print(f\"   {i}. {title}\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Build Retrieval Resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.resources.builders import build_inmemory_resources\n",
    "\n",
    "print(\"üîß Building retrieval resources...\")\n",
    "print(\"   This may take a few minutes on first run (models will be downloaded)\")\n",
    "\n",
    "resources = build_inmemory_resources(\n",
    "    corpus,\n",
    "    enable_bm25=True,\n",
    "    enable_e5=True,\n",
    "    enable_specter=True\n",
    ")\n",
    "\n",
    "print(\"\\n‚úÖ Resources built:\")\n",
    "print(f\"   - BM25: {len(resources['bm25']['ids'])} documents indexed\")\n",
    "print(f\"   - E5: {resources['e5']['corpus_embeddings'].shape[0]} embeddings\")\n",
    "print(f\"   - SPECTER: {resources['specter']['corpus_embeddings'].shape[0]} embeddings\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Run Individual Retrievers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bm25s\n",
    "import torch\n",
    "\n",
    "k = 20  # Top-k results\n",
    "query = test_example['query']\n",
    "\n",
    "# BM25\n",
    "print(\"üîç Running BM25...\")\n",
    "bm25_res = resources['bm25']\n",
    "q_tokens = bm25s.tokenize(query, stopwords=\"en\", stemmer=bm25_res[\"stemmer\"])\n",
    "doc_indices, scores = bm25_res[\"index\"].retrieve(q_tokens, k=k)\n",
    "\n",
    "bm25_results = []\n",
    "for idx, score in zip(doc_indices[0], scores[0]):\n",
    "    bm25_results.append({\n",
    "        \"id\": bm25_res[\"ids\"][idx],\n",
    "        \"title\": bm25_res.get(\"titles\", [\"\"])[idx],\n",
    "        \"score\": float(score),\n",
    "        \"source\": \"bm25\"\n",
    "    })\n",
    "\n",
    "print(f\"   ‚úÖ Retrieved {len(bm25_results)} papers (top score: {bm25_results[0]['score']:.3f})\")\n",
    "\n",
    "# E5\n",
    "print(\"\\nüîç Running E5...\")\n",
    "e5_res = resources['e5']\n",
    "with torch.no_grad():\n",
    "    q_emb = e5_res['model'].encode(\n",
    "        [query], convert_to_tensor=True, normalize_embeddings=True, show_progress_bar=False\n",
    "    )\n",
    "\n",
    "scores = (q_emb @ e5_res['corpus_embeddings'].T)[0]\n",
    "top_k = torch.topk(scores, k=min(k, len(scores)))\n",
    "\n",
    "e5_results = []\n",
    "for idx, score in zip(top_k.indices, top_k.values):\n",
    "    e5_results.append({\n",
    "        \"id\": e5_res[\"ids\"][idx.item()],\n",
    "        \"title\": e5_res.get(\"titles\", [\"\"])[idx.item()],\n",
    "        \"score\": score.item(),\n",
    "        \"source\": \"e5\"\n",
    "    })\n",
    "\n",
    "print(f\"   ‚úÖ Retrieved {len(e5_results)} papers (top score: {e5_results[0]['score']:.3f})\")\n",
    "\n",
    "# SPECTER\n",
    "print(\"\\nüîç Running SPECTER...\")\n",
    "specter_res = resources['specter']\n",
    "device = specter_res.get('device') or str(specter_res['corpus_embeddings'].device)\n",
    "\n",
    "model = specter_res['model']\n",
    "if str(next(model.parameters()).device) != device:\n",
    "    model = model.to(device)\n",
    "\n",
    "tokenizer = specter_res['tokenizer']\n",
    "\n",
    "with torch.no_grad():\n",
    "    inputs = tokenizer([query], padding=True, truncation=True, max_length=256, return_tensors=\"pt\")\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "    q_emb = model(**inputs).last_hidden_state.mean(dim=1)\n",
    "    q_emb = torch.nn.functional.normalize(q_emb, dim=1)\n",
    "\n",
    "corpus_embs = specter_res['corpus_embeddings'].to(device)\n",
    "corpus_embs = torch.nn.functional.normalize(corpus_embs, dim=1)\n",
    "scores = (q_emb @ corpus_embs.T)[0]\n",
    "top_k = torch.topk(scores, k=min(k, len(scores)))\n",
    "\n",
    "specter_results = []\n",
    "for idx, score in zip(top_k.indices, top_k.values):\n",
    "    specter_results.append({\n",
    "        \"id\": specter_res[\"ids\"][idx.item()],\n",
    "        \"title\": specter_res.get(\"titles\", [\"\"])[idx.item()],\n",
    "        \"score\": score.item(),\n",
    "        \"source\": \"specter\"\n",
    "    })\n",
    "\n",
    "print(f\"   ‚úÖ Retrieved {len(specter_results)} papers (top score: {specter_results[0]['score']:.3f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Aggregate with Reciprocal Rank Fusion (RRF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reciprocal_rank_fusion(results_dict, k=60):\n",
    "    \"\"\"Aggregate results using RRF.\"\"\"\n",
    "    paper_scores = {}\n",
    "    \n",
    "    for source, results in results_dict.items():\n",
    "        for rank, paper in enumerate(results):\n",
    "            paper_id = paper['id']\n",
    "            rrf_score = 1.0 / (k + rank + 1)\n",
    "            \n",
    "            if paper_id not in paper_scores:\n",
    "                paper_scores[paper_id] = {\n",
    "                    'paper': paper,\n",
    "                    'rrf_score': 0,\n",
    "                    'sources': []\n",
    "                }\n",
    "            \n",
    "            paper_scores[paper_id]['rrf_score'] += rrf_score\n",
    "            paper_scores[paper_id]['sources'].append(source)\n",
    "    \n",
    "    # Sort by RRF score\n",
    "    ranked = sorted(paper_scores.values(), key=lambda x: x['rrf_score'], reverse=True)\n",
    "    return ranked\n",
    "\n",
    "print(\"üîÄ Aggregating results with RRF...\")\n",
    "aggregated = reciprocal_rank_fusion({\n",
    "    'bm25': bm25_results,\n",
    "    'e5': e5_results,\n",
    "    'specter': specter_results\n",
    "})\n",
    "\n",
    "print(f\"   ‚úÖ Aggregated into {len(aggregated)} unique papers\")\n",
    "\n",
    "# Show top 5\n",
    "print(\"\\nüìä Top 5 after RRF aggregation:\")\n",
    "for i, item in enumerate(aggregated[:5], 1):\n",
    "    paper = item['paper']\n",
    "    print(f\"\\n{i}. {paper['title'][:80]}...\")\n",
    "    print(f\"   RRF Score: {item['rrf_score']:.4f}\")\n",
    "    print(f\"   Sources: {', '.join(item['sources'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: LLM Reranking (Hugging Face or Ollama)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.prompts.llm_reranker import LLMRerankerPrompt\n",
    "from langchain_ollama import ChatOllama\n",
    "from langchain_openai import ChatOpenAI\n",
    "import logging\n",
    "\n",
    "# Suppress verbose logs\n",
    "logging.getLogger(\"httpx\").setLevel(logging.WARNING)\n",
    "\n",
    "# Prepare candidates\n",
    "candidate_papers = [item['paper'] for item in aggregated[:20]]  # Top 20 for reranking\n",
    "\n",
    "print(f\"ü§ñ LLM Reranking {len(candidate_papers)} candidates...\\n\")\n",
    "\n",
    "# Initialize LLM\n",
    "inference_engine = os.getenv(\"INFERENCE_ENGINE\", \"ollama\").lower()\n",
    "model_id = os.getenv(\"LOCAL_LLM\", \"gemma3:4b\")\n",
    "\n",
    "if inference_engine == \"ollama\":\n",
    "    print(f\"üîÑ Using Ollama with model: {model_id}\")\n",
    "    llm = ChatOllama(model=model_id, temperature=0)\n",
    "else:\n",
    "    print(f\"üîÑ Using Hugging Face model: {model_id}\")\n",
    "    from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "    from langchain_huggingface import HuggingFacePipeline\n",
    "    import torch\n",
    "    \n",
    "    print(\"   Loading model (this may take a minute)...\")\n",
    "    tok = AutoTokenizer.from_pretrained(model_id)\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_id,\n",
    "        device_map=\"auto\",\n",
    "        torch_dtype=torch.bfloat16 if torch.cuda.is_available() else torch.float32,\n",
    "    )\n",
    "    \n",
    "    gen = pipeline(\n",
    "        \"text-generation\",\n",
    "        model=model,\n",
    "        tokenizer=tok,\n",
    "        max_new_tokens=1024,\n",
    "        do_sample=False,\n",
    "    )\n",
    "    \n",
    "    llm = HuggingFacePipeline(pipeline=gen)\n",
    "    print(\"   ‚úÖ Model loaded!\")\n",
    "\n",
    "# Build prompt\n",
    "prompt = LLMRerankerPrompt(query=query, candidate_papers=candidate_papers).get_prompt()\n",
    "\n",
    "print(\"\\nüìù Invoking LLM...\")\n",
    "response = llm.invoke(prompt)\n",
    "response_text = response.content\n",
    "\n",
    "print(f\"‚úÖ LLM response received ({len(response_text)} chars)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Parse Results and Show Rankings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Parse LLM response\n",
    "try:\n",
    "    json_match = re.search(r\"\\[[\\s\\S]*\\]\", response_text)\n",
    "    if json_match:\n",
    "        json_str = json_match.group()\n",
    "        rankings = json.loads(json_str)\n",
    "    else:\n",
    "        rankings = json.loads(response_text)\n",
    "    \n",
    "    print(f\"‚úÖ Successfully parsed {len(rankings)} rankings\\n\")\n",
    "    \n",
    "    # Build final ranked list\n",
    "    final_ranked = []\n",
    "    for item in rankings:\n",
    "        idx = item['index'] - 1\n",
    "        score = item['score']\n",
    "        if 0 <= idx < len(candidate_papers):\n",
    "            final_ranked.append((candidate_papers[idx], score))\n",
    "    \n",
    "    # Display results\n",
    "    print(\"=\"*80)\n",
    "    print(\"üèÜ FINAL RANKED RESULTS (Top 10)\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    relevant_ids = test_example['relevant_ids']\n",
    "    found_count = 0\n",
    "    \n",
    "    for i, (paper, score) in enumerate(final_ranked[:10], 1):\n",
    "        is_correct = paper['id'] in relevant_ids\n",
    "        marker = \"‚úÖ GROUND TRUTH\" if is_correct else \"\"\n",
    "        \n",
    "        if is_correct:\n",
    "            found_count += 1\n",
    "        \n",
    "        print(f\"\\n{i}. {paper['title'][:100]}...\")\n",
    "        print(f\"   LLM Score: {score:.3f} {marker}\")\n",
    "        print(f\"   ID: {paper['id']}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(f\"üìä EVALUATION\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"Ground truth citations found in top 10: {found_count}/{len(relevant_ids)}\")\n",
    "    print(f\"Recall@10: {found_count/len(relevant_ids):.2%}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error parsing LLM response: {e}\")\n",
    "    print(f\"\\nRaw response (first 500 chars):\\n{response_text[:500]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook demonstrated the complete citation retrieval pipeline:\n",
    "1. ‚úÖ Loaded citation context from ScholarCopilot dataset\n",
    "2. ‚úÖ Built BM25, E5, and SPECTER retrieval indices\n",
    "3. ‚úÖ Retrieved top-k candidates from each method\n",
    "4. ‚úÖ Aggregated using Reciprocal Rank Fusion\n",
    "5. ‚úÖ Reranked with LLM (Hugging Face or Ollama)\n",
    "6. ‚úÖ Evaluated against ground truth\n",
    "\n",
    "**To run full evaluation:**\n",
    "```bash\n",
    "python compare_baselines_vs_system.py --num-examples 500 --use-dspy --llm-reranker --output-dir final --k 20\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
