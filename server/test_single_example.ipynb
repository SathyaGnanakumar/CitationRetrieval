{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Single Example Test - Citation Retrieval System\n",
    "\n",
    "This notebook tests the full retrieval pipeline on a single example from the ScholarCopilot dataset.\n",
    "\n",
    "**Pipeline:**\n",
    "1. Load dataset and extract one citation context\n",
    "2. Build retrieval resources (BM25, E5, SPECTER)\n",
    "3. Run retrieval with all three methods\n",
    "4. Aggregate results using RRF\n",
    "5. Rerank with LLM (Hugging Face or Ollama)\n",
    "6. Show final ranked papers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Imports successful\n",
      "üìÅ Working directory: /Users/ishaankalra/Dev/Retrieval/server\n",
      "üîß Inference Engine: ollama\n",
      "ü§ñ Local LLM: gemma3:4b\n"
     ]
    }
   ],
   "source": [
    "# Setup\n",
    "import json\n",
    "import re\n",
    "import sys\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Add parent directory to path\n",
    "sys.path.insert(0, str(Path.cwd()))\n",
    "\n",
    "print(\"‚úÖ Imports successful\")\n",
    "print(f\"üìÅ Working directory: {Path.cwd()}\")\n",
    "print(f\"üîß Inference Engine: {os.getenv('INFERENCE_ENGINE', 'ollama')}\")\n",
    "print(f\"ü§ñ Local LLM: {os.getenv('LOCAL_LLM', 'gemma3:4b')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Load Dataset and Extract Citation Context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìö Loading dataset from: /Users/ishaankalra/Dev/Retrieval/corpus_loaders/scholarcopilot/scholar_copilot_eval_data_1k.json\n",
      "‚úÖ Loaded 1000 papers\n",
      "\n",
      "üî® Building citation corpus...\n",
      "‚úÖ Corpus: 9740 documents\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "PROJECT_ROOT = Path.cwd().parent  # from .../CitationRetrieval/server to .../CitationRetrieval\n",
    "sys.path.insert(0, str(PROJECT_ROOT))\n",
    "from corpus_loaders.scholarcopilot import load_dataset, build_citation_corpus\n",
    "\n",
    "# Load dataset\n",
    "dataset_path = os.getenv('DATASET_DIR', 'corpus_loaders/scholarcopilot/scholar_copilot_eval_data_1k.json')\n",
    "print(f\"üìö Loading dataset from: {dataset_path}\")\n",
    "\n",
    "dataset = load_dataset(dataset_path)\n",
    "print(f\"‚úÖ Loaded {len(dataset)} papers\")\n",
    "\n",
    "# Build corpus\n",
    "print(\"\\nüî® Building citation corpus...\")\n",
    "corpus = build_citation_corpus(dataset)\n",
    "print(f\"‚úÖ Corpus: {len(corpus)} documents\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Extracting citation contexts from first 1000 papers...\n",
      "‚úÖ Found 20972 valid citation contexts\n",
      "\n",
      "================================================================================\n",
      "üìù TEST EXAMPLE #2 of 20972\n",
      "================================================================================\n",
      "\n",
      "üîç Query (first 300 chars):\n",
      "Introduction Transformer is widely used in natural language processing due to its high training efficiency and superior capability in capturing long-distance dependencies. Building on top of them, modern state-of-the-art models, such as BERT , are able to learn powerful language representations from...\n",
      "\n",
      "‚úÖ Ground Truth Citations (1):\n",
      "   1. {BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\n",
      "\n",
      "üí° To test a different query, change QUERY_INDEX (0 to 20971)\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# CONFIGURATION: Change this to test different queries\n",
    "# ============================================================================\n",
    "QUERY_INDEX = 1  # Change to 1 for 2nd query, 2 for 3rd query, etc.\n",
    "MAX_PAPERS_TO_SEARCH = 1000  # How many papers to search through\n",
    "\n",
    "# ============================================================================\n",
    "# Extract citation contexts\n",
    "# ============================================================================\n",
    "cite_pattern = re.compile(r\"<\\|cite_\\d+\\|>\")\n",
    "\n",
    "print(f\"üîç Extracting citation contexts from first {MAX_PAPERS_TO_SEARCH} papers...\")\n",
    "\n",
    "all_examples = []\n",
    "for paper in dataset[:MAX_PAPERS_TO_SEARCH]:\n",
    "    paper_text = paper.get(\"paper\", \"\")\n",
    "    if not paper_text:\n",
    "        continue\n",
    "\n",
    "    bib_info = paper.get(\"bib_info\", {})\n",
    "\n",
    "    # Find all citation markers\n",
    "    for match in cite_pattern.finditer(paper_text):\n",
    "        cite_token = match.group(0)\n",
    "\n",
    "        if cite_token not in bib_info:\n",
    "            continue\n",
    "\n",
    "        refs = bib_info[cite_token]\n",
    "        if not refs:\n",
    "            continue\n",
    "\n",
    "        # Get ground truth IDs\n",
    "        relevant_ids = set()\n",
    "        for ref in refs:\n",
    "            ref_id = ref.get(\"citation_key\") or ref.get(\"paper_id\")\n",
    "            if ref_id:\n",
    "                relevant_ids.add(str(ref_id))\n",
    "\n",
    "        if not relevant_ids:\n",
    "            continue\n",
    "\n",
    "        # Extract context around citation (¬±100 words)\n",
    "        pos = match.start()\n",
    "        words_before = paper_text[:pos].split()[-100:]\n",
    "        words_after = paper_text[match.end():].split()[:100]\n",
    "\n",
    "        context = \" \".join(words_before + words_after)\n",
    "        context = re.sub(r\"<\\|cite_\\d+\\|>\", \"\", context)\n",
    "        context = \" \".join(context.split())\n",
    "\n",
    "        if len(context.split()) < 10:\n",
    "            continue\n",
    "\n",
    "        all_examples.append({\n",
    "            \"query\": context,\n",
    "            \"relevant_ids\": relevant_ids,\n",
    "            \"ground_truth_titles\": [ref.get(\"title\", \"Unknown\") for ref in refs],\n",
    "            \"paper_id\": paper.get(\"paper_id\", \"unknown\")\n",
    "        })\n",
    "\n",
    "print(f\"‚úÖ Found {len(all_examples)} valid citation contexts\")\n",
    "\n",
    "# Select the query at QUERY_INDEX\n",
    "if QUERY_INDEX >= len(all_examples):\n",
    "    raise ValueError(f\"QUERY_INDEX={QUERY_INDEX} is too large. Only {len(all_examples)} queries available.\")\n",
    "\n",
    "test_example = all_examples[QUERY_INDEX]\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(f\"üìù TEST EXAMPLE #{QUERY_INDEX + 1} of {len(all_examples)}\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\nüîç Query (first 300 chars):\\n{test_example['query'][:300]}...\")\n",
    "print(f\"\\n‚úÖ Ground Truth Citations ({len(test_example['relevant_ids'])}):\")\n",
    "for i, title in enumerate(test_example['ground_truth_titles'], 1):\n",
    "    print(f\"   {i}. {title}\")\n",
    "print(f\"\\nüí° To test a different query, change QUERY_INDEX (0 to {len(all_examples)-1})\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Build Retrieval Resources\n",
    "\n",
    "**Resource Caching:**\n",
    "- First run builds all resources (BM25, E5, SPECTER, LLM) and saves to cache (~20-30 minutes)\n",
    "- Subsequent runs load from cache (~30 seconds)\n",
    "- Cache is stored in `.cache/` folder next to the dataset\n",
    "- Set `USE_CACHE = False` to rebuild from scratch\n",
    "- Set `ENABLE_LLM_RERANKER = True` to load LLM model for reranking\n",
    "\n",
    "**Configuration:**\n",
    "- `USE_CACHE`: Toggle caching on/off\n",
    "- `ENABLE_LLM_RERANKER`: Whether to load LLM model (uses settings from `.env`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîß Building/Loading retrieval resources...\n",
      "üì¶ Attempting to load from cache...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ishaankalra/Dev/Retrieval/server/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîß LLM reranker enabled but not in cache - loading now...\n",
      "\n",
      "‚úÖ Resources ready:\n",
      "   - BM25: 9740 documents indexed\n",
      "   - E5: 9740 embeddings\n",
      "   - SPECTER: 9740 embeddings\n",
      "   - LLM Reranker: gemma3:4b (ollama)\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# CONFIGURATION: Cache Settings\n",
    "# ============================================================================\n",
    "USE_CACHE = True  # Set to False to rebuild from scratch\n",
    "ENABLE_LLM_RERANKER = True  # Load LLM model for reranking (recommended)\n",
    "\n",
    "# ============================================================================\n",
    "# Build or Load Resources\n",
    "# ============================================================================\n",
    "from src.resources.builders import build_inmemory_resources\n",
    "from src.resources.cache import load_resources, save_resources\n",
    "\n",
    "print(\"üîß Building/Loading retrieval resources...\")\n",
    "\n",
    "resources = None\n",
    "\n",
    "# Try to load from cache\n",
    "if USE_CACHE:\n",
    "    print(f\"üì¶ Attempting to load from cache...\")\n",
    "    resources = load_resources(\n",
    "        dataset_path,\n",
    "        enable_bm25=True,\n",
    "        enable_e5=True,\n",
    "        enable_specter=True\n",
    "    )\n",
    "    \n",
    "    # Add LLM reranker if enabled and not in cache\n",
    "    if resources and ENABLE_LLM_RERANKER and \"llm_reranker\" not in resources:\n",
    "        print(f\"üîß LLM reranker enabled but not in cache - loading now...\")\n",
    "        from src.resources.builders import build_llm_reranker_resources\n",
    "        resources[\"llm_reranker\"] = build_llm_reranker_resources()\n",
    "\n",
    "# Build from scratch if cache not found or disabled\n",
    "if resources is None:\n",
    "    print(f\"üî® Building resources from scratch...\")\n",
    "    print(\"   This may take a few minutes on first run (models will be downloaded)\")\n",
    "    \n",
    "    resources = build_inmemory_resources(\n",
    "        corpus,\n",
    "        enable_bm25=True,\n",
    "        enable_e5=True,\n",
    "        enable_specter=True,\n",
    "        enable_llm_reranker=ENABLE_LLM_RERANKER\n",
    "    )\n",
    "    \n",
    "    # Save to cache for next time\n",
    "    if USE_CACHE:\n",
    "        print(f\"\\nüíæ Saving resources to cache for future use...\")\n",
    "        save_resources(resources, dataset_path)\n",
    "        print(f\"‚úÖ Cache saved! Next run will be faster.\")\n",
    "\n",
    "print(\"\\n‚úÖ Resources ready:\")\n",
    "print(f\"   - BM25: {len(resources['bm25']['ids'])} documents indexed\")\n",
    "print(f\"   - E5: {resources['e5']['corpus_embeddings'].shape[0]} embeddings\")\n",
    "print(f\"   - SPECTER: {resources['specter']['corpus_embeddings'].shape[0]} embeddings\")\n",
    "if \"llm_reranker\" in resources:\n",
    "    llm_info = resources['llm_reranker']\n",
    "    print(f\"   - LLM Reranker: {llm_info.get('model_name', 'cached')} ({llm_info.get('inference_engine', 'unknown')})\")\n",
    "else:\n",
    "    print(f\"   - LLM Reranker: Not loaded (set ENABLE_LLM_RERANKER=True to enable)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Run Individual Retrievers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Running BM25...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ‚úÖ Retrieved 30 papers (top score: 37.539)\n",
      "\n",
      "üîç Running E5...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ‚úÖ Retrieved 30 papers (top score: 0.906)\n",
      "\n",
      "üîç Running SPECTER...\n",
      "   ‚úÖ Retrieved 30 papers (top score: 0.944)\n"
     ]
    }
   ],
   "source": [
    "import bm25s\n",
    "import torch\n",
    "\n",
    "k = 30  # Top-k results\n",
    "query = test_example['query']\n",
    "\n",
    "# BM25\n",
    "print(\"üîç Running BM25...\")\n",
    "bm25_res = resources['bm25']\n",
    "q_tokens = bm25s.tokenize(query, stopwords=\"en\", stemmer=bm25_res[\"stemmer\"])\n",
    "doc_indices, scores = bm25_res[\"index\"].retrieve(q_tokens, k=k)\n",
    "\n",
    "bm25_results = []\n",
    "for idx, score in zip(doc_indices[0], scores[0]):\n",
    "    bm25_results.append({\n",
    "        \"id\": bm25_res[\"ids\"][idx],\n",
    "        \"title\": bm25_res.get(\"titles\", [\"\"])[idx],\n",
    "        \"score\": float(score),\n",
    "        \"source\": \"bm25\"\n",
    "    })\n",
    "\n",
    "print(f\"   ‚úÖ Retrieved {len(bm25_results)} papers (top score: {bm25_results[0]['score']:.3f})\")\n",
    "\n",
    "# E5\n",
    "print(\"\\nüîç Running E5...\")\n",
    "e5_res = resources['e5']\n",
    "with torch.no_grad():\n",
    "    q_emb = e5_res['model'].encode(\n",
    "        [query], convert_to_tensor=True, normalize_embeddings=True, show_progress_bar=False\n",
    "    )\n",
    "\n",
    "scores = (q_emb @ e5_res['corpus_embeddings'].T)[0]\n",
    "top_k = torch.topk(scores, k=min(k, len(scores)))\n",
    "\n",
    "e5_results = []\n",
    "for idx, score in zip(top_k.indices, top_k.values):\n",
    "    e5_results.append({\n",
    "        \"id\": e5_res[\"ids\"][idx.item()],\n",
    "        \"title\": e5_res.get(\"titles\", [\"\"])[idx.item()],\n",
    "        \"score\": score.item(),\n",
    "        \"source\": \"e5\"\n",
    "    })\n",
    "\n",
    "print(f\"   ‚úÖ Retrieved {len(e5_results)} papers (top score: {e5_results[0]['score']:.3f})\")\n",
    "\n",
    "# SPECTER\n",
    "print(\"\\nüîç Running SPECTER...\")\n",
    "specter_res = resources['specter']\n",
    "device = specter_res.get('device') or str(specter_res['corpus_embeddings'].device)\n",
    "\n",
    "model = specter_res['model']\n",
    "if str(next(model.parameters()).device) != device:\n",
    "    model = model.to(device)\n",
    "\n",
    "tokenizer = specter_res['tokenizer']\n",
    "\n",
    "with torch.no_grad():\n",
    "    inputs = tokenizer([query], padding=True, truncation=True, max_length=256, return_tensors=\"pt\")\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "    q_emb = model(**inputs).last_hidden_state.mean(dim=1)\n",
    "    q_emb = torch.nn.functional.normalize(q_emb, dim=1)\n",
    "\n",
    "corpus_embs = specter_res['corpus_embeddings'].to(device)\n",
    "corpus_embs = torch.nn.functional.normalize(corpus_embs, dim=1)\n",
    "scores = (q_emb @ corpus_embs.T)[0]\n",
    "top_k = torch.topk(scores, k=min(k, len(scores)))\n",
    "\n",
    "specter_results = []\n",
    "for idx, score in zip(top_k.indices, top_k.values):\n",
    "    specter_results.append({\n",
    "        \"id\": specter_res[\"ids\"][idx.item()],\n",
    "        \"title\": specter_res.get(\"titles\", [\"\"])[idx.item()],\n",
    "        \"score\": score.item(),\n",
    "        \"source\": \"specter\"\n",
    "    })\n",
    "\n",
    "print(f\"   ‚úÖ Retrieved {len(specter_results)} papers (top score: {specter_results[0]['score']:.3f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Aggregate with Reciprocal Rank Fusion (RRF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reciprocal_rank_fusion(results_dict, k=60):\n",
    "    \"\"\"Aggregate results using RRF.\"\"\"\n",
    "    paper_scores = {}\n",
    "    \n",
    "    for source, results in results_dict.items():\n",
    "        for rank, paper in enumerate(results):\n",
    "            paper_id = paper['id']\n",
    "            rrf_score = 1.0 / (k + rank + 1)\n",
    "            \n",
    "            if paper_id not in paper_scores:\n",
    "                paper_scores[paper_id] = {\n",
    "                    'paper': paper,\n",
    "                    'rrf_score': 0,\n",
    "                    'sources': []\n",
    "                }\n",
    "            \n",
    "            paper_scores[paper_id]['rrf_score'] += rrf_score\n",
    "            paper_scores[paper_id]['sources'].append(source)\n",
    "    \n",
    "    # Sort by RRF score\n",
    "    ranked = sorted(paper_scores.values(), key=lambda x: x['rrf_score'], reverse=True)\n",
    "    return ranked\n",
    "\n",
    "print(\"üîÄ Aggregating results with RRF...\")\n",
    "aggregated = reciprocal_rank_fusion({\n",
    "    'bm25': bm25_results,\n",
    "    'e5': e5_results,\n",
    "    'specter': specter_results\n",
    "})\n",
    "\n",
    "print(f\"   ‚úÖ Aggregated into {len(aggregated)} unique papers\")\n",
    "\n",
    "# Show top 5\n",
    "print(\"\\nüìä Top 5 after RRF aggregation:\")\n",
    "for i, item in enumerate(aggregated, 1):\n",
    "    paper = item['paper']\n",
    "    print(f\"\\n{i}. {paper['title'][:80]}...\")\n",
    "    print(f\"   RRF Score: {item['rrf_score']:.4f}\")\n",
    "    print(f\"   Sources: {', '.join(item['sources'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üî¨ Testing single retriever: E5\n",
      "================================================================================\n",
      "\n",
      "üìä Using E5-Large (Dense)\n",
      "   Retrieved: 30 papers\n",
      "\n",
      "üìã Top 5 from E5 (before LLM reranking):\n",
      "1. Lite Transformer with Long-Short Range Attention... (score: 0.9063)\n",
      "2. Autotrans: Automating transformer design via reinforced architecture search... (score: 0.9055)\n",
      "3. Understanding and Overcoming the Challenges of Efficient Transformer Quantizatio... (score: 0.8943)\n",
      "4. Compressing large-scale transformer-based models: A case study on bert... (score: 0.8937)\n",
      "5. Neural architecture search on efficient transformers and beyond... (score: 0.8921)\n",
      "\n",
      "ü§ñ Applying LLM reranking to top 20 results from E5...\n",
      "üöÄ Using cached LLM model: gemma3:4b\n",
      "\n",
      "üìù Invoking LLM...\n",
      "‚úÖ LLM response received (645 chars)\n",
      "‚úÖ Successfully parsed 20 rankings\n",
      "\n",
      "================================================================================\n",
      "üèÜ FINAL RESULTS: E5 + LLM Reranking (Top 10)\n",
      "================================================================================\n",
      "\n",
      "1. Lite Transformer with Long-Short Range Attention...\n",
      "   LLM Score: 0.980\n",
      "   Original E5 Score: 0.9063\n",
      "\n",
      "2. Autotrans: Automating transformer design via reinforced architecture search...\n",
      "   LLM Score: 0.920\n",
      "   Original E5 Score: 0.9055\n",
      "\n",
      "3. Understanding and Overcoming the Challenges of Efficient Transformer Quantization...\n",
      "   LLM Score: 0.850\n",
      "   Original E5 Score: 0.8943\n",
      "\n",
      "4. Compressing large-scale transformer-based models: A case study on bert...\n",
      "   LLM Score: 0.800\n",
      "   Original E5 Score: 0.8937\n",
      "\n",
      "5. Neural architecture search on efficient transformers and beyond...\n",
      "   LLM Score: 0.780\n",
      "   Original E5 Score: 0.8921\n",
      "\n",
      "6. The evolved transformer...\n",
      "   LLM Score: 0.750\n",
      "   Original E5 Score: 0.8920\n",
      "\n",
      "7. {The Evolved Transformer...\n",
      "   LLM Score: 0.720\n",
      "   Original E5 Score: 0.8920\n",
      "\n",
      "8. HAT: Hardware-Aware Transformers for Efficient Natural Language Processing...\n",
      "   LLM Score: 0.700\n",
      "   Original E5 Score: 0.8899\n",
      "\n",
      "9. Huggingface's transformers: State-of-the-art natural language processing...\n",
      "   LLM Score: 0.680\n",
      "   Original E5 Score: 0.8875\n",
      "\n",
      "10. Transformer in transformer...\n",
      "   LLM Score: 0.650\n",
      "   Original E5 Score: 0.8861\n",
      "\n",
      "================================================================================\n",
      "üìä EVALUATION: E5 + LLM Reranking\n",
      "================================================================================\n",
      "Ground truth found in top 10: 0/1\n",
      "Recall@10: 0.00%\n",
      "\n",
      "Comparison:\n",
      "  E5 alone (top 10): 0/1 = 0.00%\n",
      "  E5 + LLM (top 10): 0/1 = 0.00%\n",
      "  ‚û°Ô∏è  No change\n",
      "\n",
      "================================================================================\n",
      "üí° To test a different retriever, change RETRIEVER_TYPE above\n",
      "   Options: 'bm25', 'e5', 'specter'\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# CONFIGURATION: Choose Retriever\n",
    "# ============================================================================\n",
    "TEST_SINGLE_RETRIEVER = True  # Set to True to test without fusion\n",
    "RETRIEVER_TYPE = \"e5\"  # Options: \"bm25\", \"e5\", \"specter\"\n",
    "\n",
    "# ============================================================================\n",
    "# Single Retriever Test (Optional - Skip if TEST_SINGLE_RETRIEVER = False)\n",
    "# ============================================================================\n",
    "if TEST_SINGLE_RETRIEVER:\n",
    "    print(f\"üî¨ Testing single retriever: {RETRIEVER_TYPE.upper()}\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # Select results based on retriever type\n",
    "    if RETRIEVER_TYPE == \"bm25\":\n",
    "        single_retriever_results = bm25_results\n",
    "        retriever_name = \"BM25 (Sparse)\"\n",
    "    elif RETRIEVER_TYPE == \"e5\":\n",
    "        single_retriever_results = e5_results\n",
    "        retriever_name = \"E5-Large (Dense)\"\n",
    "    elif RETRIEVER_TYPE == \"specter\":\n",
    "        single_retriever_results = specter_results\n",
    "        retriever_name = \"SPECTER-2 (Scientific)\"\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown retriever type: {RETRIEVER_TYPE}\")\n",
    "    \n",
    "    print(f\"\\nüìä Using {retriever_name}\")\n",
    "    print(f\"   Retrieved: {len(single_retriever_results)} papers\")\n",
    "    \n",
    "    # Show top 5 before reranking\n",
    "    print(f\"\\nüìã Top 5 from {RETRIEVER_TYPE.upper()} (before LLM reranking):\")\n",
    "    for i, paper in enumerate(single_retriever_results[:5], 1):\n",
    "        print(f\"{i}. {paper['title'][:80]}... (score: {paper['score']:.4f})\")\n",
    "    \n",
    "    # Prepare for LLM reranking\n",
    "    single_candidate_papers = single_retriever_results[:20]  # Top 20 for reranking\n",
    "    \n",
    "    print(f\"\\nü§ñ Applying LLM reranking to top 20 results from {RETRIEVER_TYPE.upper()}...\")\n",
    "    \n",
    "    # Check if LLM is already loaded in resources\n",
    "    if \"llm_reranker\" in resources and \"llm_model\" in resources[\"llm_reranker\"]:\n",
    "        llm = resources[\"llm_reranker\"][\"llm_model\"]\n",
    "        model_id = resources[\"llm_reranker\"].get(\"model_name\", \"cached\")\n",
    "        print(f\"üöÄ Using cached LLM model: {model_id}\")\n",
    "    else:\n",
    "        print(f\"‚ö†Ô∏è  LLM not cached - loading on-the-fly\")\n",
    "        \n",
    "        inference_engine = os.getenv(\"INFERENCE_ENGINE\", \"ollama\").lower()\n",
    "        model_id = os.getenv(\"LOCAL_LLM\", \"gemma3:4b\")\n",
    "        \n",
    "        if inference_engine == \"openai\":\n",
    "            from langchain_openai import ChatOpenAI\n",
    "            api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "            if not api_key:\n",
    "                raise ValueError(\"OPENAI_API_KEY not found\")\n",
    "            llm = ChatOpenAI(model=model_id, temperature=0, api_key=api_key)\n",
    "            print(f\"ü§ñ Using OpenAI: {model_id}\")\n",
    "        elif inference_engine == \"ollama\":\n",
    "            from langchain_ollama import ChatOllama\n",
    "            llm = ChatOllama(model=model_id, temperature=0)\n",
    "            print(f\"üîÑ Using Ollama: {model_id}\")\n",
    "        else:\n",
    "            from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "            from langchain_huggingface import HuggingFacePipeline\n",
    "            import torch\n",
    "            \n",
    "            print(f\"üîÑ Loading HF model: {model_id}...\")\n",
    "            tok = AutoTokenizer.from_pretrained(model_id)\n",
    "            model = AutoModelForCausalLM.from_pretrained(\n",
    "                model_id,\n",
    "                device_map=\"auto\",\n",
    "                torch_dtype=torch.bfloat16 if torch.cuda.is_available() else torch.float32,\n",
    "            )\n",
    "            gen = pipeline(\"text-generation\", model=model, tokenizer=tok, max_new_tokens=1024, do_sample=False)\n",
    "            llm = HuggingFacePipeline(pipeline=gen)\n",
    "            print(f\"‚úÖ Model loaded!\")\n",
    "    \n",
    "    # Build prompt and invoke LLM\n",
    "    from src.prompts.llm_reranker import LLMRerankerPrompt\n",
    "    prompt = LLMRerankerPrompt(query=query, candidate_papers=single_candidate_papers).get_prompt()\n",
    "    \n",
    "    print(f\"\\nüìù Invoking LLM...\")\n",
    "    response = llm.invoke(prompt)\n",
    "    \n",
    "    # Handle different response formats\n",
    "    if isinstance(response, str):\n",
    "        response_text = response\n",
    "    elif hasattr(response, 'content'):\n",
    "        response_text = response.content\n",
    "    elif isinstance(response, dict) and 'text' in response:\n",
    "        response_text = response['text']\n",
    "    elif isinstance(response, list) and len(response) > 0:\n",
    "        if isinstance(response[0], dict) and 'generated_text' in response[0]:\n",
    "            response_text = response[0]['generated_text']\n",
    "        else:\n",
    "            response_text = str(response)\n",
    "    else:\n",
    "        response_text = str(response)\n",
    "    \n",
    "    print(f\"‚úÖ LLM response received ({len(response_text)} chars)\")\n",
    "    \n",
    "    # Parse LLM response\n",
    "    try:\n",
    "        json_match = re.search(r\"\\[[\\s\\S]*\\]\", response_text)\n",
    "        if json_match:\n",
    "            json_str = json_match.group()\n",
    "            rankings = json.loads(json_str)\n",
    "        else:\n",
    "            rankings = json.loads(response_text)\n",
    "        \n",
    "        print(f\"‚úÖ Successfully parsed {len(rankings)} rankings\")\n",
    "        \n",
    "        # Build final ranked list\n",
    "        single_final_ranked = []\n",
    "        for item in rankings:\n",
    "            idx = item['index'] - 1\n",
    "            score = item['score']\n",
    "            if 0 <= idx < len(single_candidate_papers):\n",
    "                single_final_ranked.append((single_candidate_papers[idx], score))\n",
    "        \n",
    "        # Display results\n",
    "        print(\"\\n\" + \"=\" * 80)\n",
    "        print(f\"üèÜ FINAL RESULTS: {RETRIEVER_TYPE.upper()} + LLM Reranking (Top 10)\")\n",
    "        print(\"=\" * 80)\n",
    "        \n",
    "        relevant_ids = test_example['relevant_ids']\n",
    "        found_count = 0\n",
    "        \n",
    "        for i, (paper, score) in enumerate(single_final_ranked[:10], 1):\n",
    "            is_correct = paper['id'] in relevant_ids\n",
    "            marker = \" ‚úÖ GROUND TRUTH\" if is_correct else \"\"\n",
    "            \n",
    "            if is_correct:\n",
    "                found_count += 1\n",
    "            \n",
    "            print(f\"\\n{i}. {paper['title'][:100]}...\")\n",
    "            print(f\"   LLM Score: {score:.3f}{marker}\")\n",
    "            print(f\"   Original {RETRIEVER_TYPE.upper()} Score: {paper['score']:.4f}\")\n",
    "        \n",
    "        # Evaluation\n",
    "        print(\"\\n\" + \"=\" * 80)\n",
    "        print(f\"üìä EVALUATION: {RETRIEVER_TYPE.upper()} + LLM Reranking\")\n",
    "        print(\"=\" * 80)\n",
    "        print(f\"Ground truth found in top 10: {found_count}/{len(relevant_ids)}\")\n",
    "        print(f\"Recall@10: {found_count/len(relevant_ids):.2%}\")\n",
    "        \n",
    "        # Compare to original retriever ranking\n",
    "        original_found = sum(1 for p in single_retriever_results[:10] if p['id'] in relevant_ids)\n",
    "        print(f\"\\nComparison:\")\n",
    "        print(f\"  {RETRIEVER_TYPE.upper()} alone (top 10): {original_found}/{len(relevant_ids)} = {original_found/len(relevant_ids):.2%}\")\n",
    "        print(f\"  {RETRIEVER_TYPE.upper()} + LLM (top 10): {found_count}/{len(relevant_ids)} = {found_count/len(relevant_ids):.2%}\")\n",
    "        \n",
    "        improvement = found_count - original_found\n",
    "        if improvement > 0:\n",
    "            print(f\"  ‚úÖ Improvement: +{improvement} citations found\")\n",
    "        elif improvement < 0:\n",
    "            print(f\"  ‚ö†Ô∏è  Degradation: {improvement} citations lost\")\n",
    "        else:\n",
    "            print(f\"  ‚û°Ô∏è  No change\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error parsing LLM response: {e}\")\n",
    "        print(f\"\\nRaw response (first 500 chars):\\n{response_text[:500]}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"üí° To test a different retriever, change RETRIEVER_TYPE above\")\n",
    "    print(\"   Options: 'bm25', 'e5', 'specter'\")\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "else:\n",
    "    print(\"‚è≠Ô∏è  Skipping single retriever test (TEST_SINGLE_RETRIEVER = False)\")\n",
    "    print(\"   Set TEST_SINGLE_RETRIEVER = True to test individual retrievers\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4a: OPTIONAL - Test Single Retriever Without Fusion\n",
    "\n",
    "**Ablation Study:**\n",
    "Test individual retrievers (BM25, E5, or SPECTER) with LLM reranking, skipping the fusion step.\n",
    "\n",
    "**Use Cases:**\n",
    "- Compare performance of each retriever independently\n",
    "- Understand which retriever works best for specific query types\n",
    "- Ablation studies for your paper\n",
    "\n",
    "**Skip this if you want to use the full fusion pipeline (Step 4 below)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: LLM Reranking (Hugging Face or Ollama)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'aggregated' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 10\u001b[39m\n\u001b[32m      7\u001b[39m logging.getLogger(\u001b[33m\"\u001b[39m\u001b[33mhttpx\u001b[39m\u001b[33m\"\u001b[39m).setLevel(logging.WARNING)\n\u001b[32m      9\u001b[39m \u001b[38;5;66;03m# Prepare candidates\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m candidate_papers = [item[\u001b[33m'\u001b[39m\u001b[33mpaper\u001b[39m\u001b[33m'\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m \u001b[43maggregated\u001b[49m[:\u001b[32m20\u001b[39m]]  \u001b[38;5;66;03m# Top 20 for reranking\u001b[39;00m\n\u001b[32m     12\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mü§ñ LLM Reranking \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(candidate_papers)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m candidates...\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     14\u001b[39m \u001b[38;5;66;03m# Check if LLM is already loaded in resources (from cache)\u001b[39;00m\n",
      "\u001b[31mNameError\u001b[39m: name 'aggregated' is not defined"
     ]
    }
   ],
   "source": [
    "from src.prompts.llm_reranker import LLMRerankerPrompt\n",
    "from langchain_ollama import ChatOllama\n",
    "from langchain_openai import ChatOpenAI\n",
    "import logging\n",
    "\n",
    "# Suppress verbose logs\n",
    "logging.getLogger(\"httpx\").setLevel(logging.WARNING)\n",
    "\n",
    "# Prepare candidates\n",
    "candidate_papers = [item['paper'] for item in aggregated[:20]]  # Top 20 for reranking\n",
    "\n",
    "print(f\"ü§ñ LLM Reranking {len(candidate_papers)} candidates...\\n\")\n",
    "\n",
    "# Check if LLM is already loaded in resources (from cache)\n",
    "if \"llm_reranker\" in resources and \"llm_model\" in resources[\"llm_reranker\"]:\n",
    "    # Use cached model (FAST!)\n",
    "    llm = resources[\"llm_reranker\"][\"llm_model\"]\n",
    "    model_id = resources[\"llm_reranker\"].get(\"model_name\", \"cached\")\n",
    "    print(f\"üöÄ Using cached LLM model: {model_id}\")\n",
    "else:\n",
    "    # Load model on-the-fly (slower)\n",
    "    print(f\"‚ö†Ô∏è  LLM not cached - loading on-the-fly (slower)\")\n",
    "    \n",
    "    inference_engine = os.getenv(\"INFERENCE_ENGINE\", \"ollama\").lower()\n",
    "    model_id = os.getenv(\"LOCAL_LLM\", \"gemma3:4b\")\n",
    "    \n",
    "    if inference_engine == \"openai\":\n",
    "        print(f\"üîÑ Using OpenAI with model: {model_id}\")\n",
    "        api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "        if not api_key:\n",
    "            raise ValueError(\"OPENAI_API_KEY not found in environment\")\n",
    "        llm = ChatOpenAI(model=model_id, temperature=0, api_key=api_key)\n",
    "    elif inference_engine == \"ollama\":\n",
    "        print(f\"üîÑ Using Ollama with model: {model_id}\")\n",
    "        llm = ChatOllama(model=model_id, temperature=0)\n",
    "    else:\n",
    "        print(f\"üîÑ Using Hugging Face model: {model_id}\")\n",
    "        from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "        from langchain_huggingface import HuggingFacePipeline\n",
    "        import torch\n",
    "        \n",
    "        print(\"   Loading model (this may take a minute)...\")\n",
    "        tok = AutoTokenizer.from_pretrained(model_id)\n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_id,\n",
    "            device_map=\"auto\",\n",
    "            torch_dtype=torch.bfloat16 if torch.cuda.is_available() else torch.float32,\n",
    "        )\n",
    "        \n",
    "        gen = pipeline(\n",
    "            \"text-generation\",\n",
    "            model=model,\n",
    "            tokenizer=tok,\n",
    "            max_new_tokens=1024,\n",
    "            do_sample=False,\n",
    "        )\n",
    "        \n",
    "        llm = HuggingFacePipeline(pipeline=gen)\n",
    "        print(\"   ‚úÖ Model loaded!\")\n",
    "\n",
    "# Build prompt\n",
    "prompt = LLMRerankerPrompt(query=query, candidate_papers=candidate_papers).get_prompt()\n",
    "\n",
    "print(\"\\nüìù Invoking LLM...\")\n",
    "response = llm.invoke(prompt)\n",
    "\n",
    "# Handle different response formats (Ollama/OpenAI vs HuggingFace)\n",
    "if isinstance(response, str):\n",
    "    response_text = response\n",
    "elif hasattr(response, 'content'):\n",
    "    response_text = response.content\n",
    "elif isinstance(response, dict) and 'text' in response:\n",
    "    response_text = response['text']\n",
    "elif isinstance(response, list) and len(response) > 0:\n",
    "    if isinstance(response[0], dict) and 'generated_text' in response[0]:\n",
    "        response_text = response[0]['generated_text']\n",
    "    else:\n",
    "        response_text = str(response)\n",
    "else:\n",
    "    response_text = str(response)\n",
    "\n",
    "print(f\"‚úÖ LLM response received ({len(response_text)} chars)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Parse Results and Show Rankings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Parse LLM response\n",
    "try:\n",
    "    json_match = re.search(r\"\\[[\\s\\S]*\\]\", response_text)\n",
    "    if json_match:\n",
    "        json_str = json_match.group()\n",
    "        rankings = json.loads(json_str)\n",
    "    else:\n",
    "        rankings = json.loads(response_text)\n",
    "    \n",
    "    print(f\"‚úÖ Successfully parsed {len(rankings)} rankings\\n\")\n",
    "    \n",
    "    # Build final ranked list\n",
    "    final_ranked = []\n",
    "    for item in rankings:\n",
    "        idx = item['index'] - 1\n",
    "        score = item['score']\n",
    "        if 0 <= idx < len(candidate_papers):\n",
    "            final_ranked.append((candidate_papers[idx], score))\n",
    "    \n",
    "    # Display results\n",
    "    print(\"=\"*80)\n",
    "    print(\"üèÜ FINAL RANKED RESULTS (Top 10)\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    relevant_ids = test_example['relevant_ids']\n",
    "    found_count = 0\n",
    "    \n",
    "    for i, (paper, score) in enumerate(final_ranked[:10], 1):\n",
    "        is_correct = paper['id'] in relevant_ids\n",
    "        marker = \"‚úÖ GROUND TRUTH\" if is_correct else \"\"\n",
    "        \n",
    "        if is_correct:\n",
    "            found_count += 1\n",
    "        \n",
    "        print(f\"\\n{i}. {paper['title'][:100]}...\")\n",
    "        print(f\"   LLM Score: {score:.3f} {marker}\")\n",
    "        print(f\"   ID: {paper['id']}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(f\"üìä EVALUATION\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"Ground truth citations found in top 10: {found_count}/{len(relevant_ids)}\")\n",
    "    print(f\"Recall@10: {found_count/len(relevant_ids):.2%}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error parsing LLM response: {e}\")\n",
    "    print(f\"\\nRaw response (first 500 chars):\\n{response_text[:500]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook demonstrated the complete citation retrieval pipeline:\n",
    "1. ‚úÖ Loaded citation context from ScholarCopilot dataset\n",
    "2. ‚úÖ Built BM25, E5, and SPECTER retrieval indices\n",
    "3. ‚úÖ Retrieved top-k candidates from each method\n",
    "4. ‚úÖ Aggregated using Reciprocal Rank Fusion\n",
    "5. ‚úÖ Reranked with LLM (Hugging Face or Ollama)\n",
    "6. ‚úÖ Evaluated against ground truth\n",
    "\n",
    "**To run full evaluation:**\n",
    "```bash\n",
    "python compare_baselines_vs_system.py --num-examples 500 --use-dspy --llm-reranker --output-dir final --k 20\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
