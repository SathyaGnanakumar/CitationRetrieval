{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Single Example Test - Citation Retrieval System\n",
    "\n",
    "This notebook tests the full retrieval pipeline on a single example from the ScholarCopilot dataset.\n",
    "\n",
    "**Pipeline:**\n",
    "1. Load dataset and extract one citation context\n",
    "2. Build retrieval resources (BM25, E5, SPECTER)\n",
    "3. Run retrieval with all three methods\n",
    "4. Aggregate results using RRF\n",
    "5. Rerank with LLM (Hugging Face or Ollama)\n",
    "6. Show final ranked papers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Imports successful\n",
      "üìÅ Working directory: /Users/ishaankalra/Dev/Retrieval/server\n",
      "üîß Inference Engine: ollama\n",
      "ü§ñ Local LLM: gemma3:4b\n"
     ]
    }
   ],
   "source": [
    "# Setup\n",
    "import json\n",
    "import re\n",
    "import sys\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Add parent directory to path\n",
    "sys.path.insert(0, str(Path.cwd()))\n",
    "\n",
    "print(\"‚úÖ Imports successful\")\n",
    "print(f\"üìÅ Working directory: {Path.cwd()}\")\n",
    "print(f\"üîß Inference Engine: {os.getenv('INFERENCE_ENGINE', 'ollama')}\")\n",
    "print(f\"ü§ñ Local LLM: {os.getenv('LOCAL_LLM', 'gemma3:4b')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Load Dataset and Extract Citation Context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìö Loading dataset from: /Users/ishaankalra/Dev/Retrieval/corpus_loaders/scholarcopilot/scholar_copilot_eval_data_1k.json\n",
      "‚úÖ Loaded 1000 papers\n",
      "\n",
      "üî® Building citation corpus...\n",
      "‚úÖ Corpus: 9740 documents\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "PROJECT_ROOT = Path.cwd().parent  # from .../CitationRetrieval/server to .../CitationRetrieval\n",
    "sys.path.insert(0, str(PROJECT_ROOT))\n",
    "from corpus_loaders.scholarcopilot import load_dataset, build_citation_corpus\n",
    "\n",
    "# Load dataset\n",
    "dataset_path = os.getenv('DATASET_DIR', 'corpus_loaders/scholarcopilot/scholar_copilot_eval_data_1k.json')\n",
    "print(f\"üìö Loading dataset from: {dataset_path}\")\n",
    "\n",
    "dataset = load_dataset(dataset_path)\n",
    "print(f\"‚úÖ Loaded {len(dataset)} papers\")\n",
    "\n",
    "# Build corpus\n",
    "print(\"\\nüî® Building citation corpus...\")\n",
    "corpus = build_citation_corpus(dataset)\n",
    "print(f\"‚úÖ Corpus: {len(corpus)} documents\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ============================================================================\n# CONFIGURATION: Change this to test different queries\n# ============================================================================\nQUERY_INDEX = 0  # Change to 1 for 2nd query, 2 for 3rd query, etc.\nMAX_PAPERS_TO_SEARCH = 100  # How many papers to search through\n\n# ============================================================================\n# Extract citation contexts\n# ============================================================================\ncite_pattern = re.compile(r\"<\\|cite_\\d+\\|>\")\n\nprint(f\"üîç Extracting citation contexts from first {MAX_PAPERS_TO_SEARCH} papers...\")\n\nall_examples = []\nfor paper in dataset[:MAX_PAPERS_TO_SEARCH]:\n    paper_text = paper.get(\"paper\", \"\")\n    if not paper_text:\n        continue\n\n    bib_info = paper.get(\"bib_info\", {})\n\n    # Find all citation markers\n    for match in cite_pattern.finditer(paper_text):\n        cite_token = match.group(0)\n\n        if cite_token not in bib_info:\n            continue\n\n        refs = bib_info[cite_token]\n        if not refs:\n            continue\n\n        # Get ground truth IDs\n        relevant_ids = set()\n        for ref in refs:\n            ref_id = ref.get(\"citation_key\") or ref.get(\"paper_id\")\n            if ref_id:\n                relevant_ids.add(str(ref_id))\n\n        if not relevant_ids:\n            continue\n\n        # Extract context around citation (¬±100 words)\n        pos = match.start()\n        words_before = paper_text[:pos].split()[-100:]\n        words_after = paper_text[match.end():].split()[:100]\n\n        context = \" \".join(words_before + words_after)\n        context = re.sub(r\"<\\|cite_\\d+\\|>\", \"\", context)\n        context = \" \".join(context.split())\n\n        if len(context.split()) < 10:\n            continue\n\n        all_examples.append({\n            \"query\": context,\n            \"relevant_ids\": relevant_ids,\n            \"ground_truth_titles\": [ref.get(\"title\", \"Unknown\") for ref in refs],\n            \"paper_id\": paper.get(\"paper_id\", \"unknown\")\n        })\n\nprint(f\"‚úÖ Found {len(all_examples)} valid citation contexts\")\n\n# Select the query at QUERY_INDEX\nif QUERY_INDEX >= len(all_examples):\n    raise ValueError(f\"QUERY_INDEX={QUERY_INDEX} is too large. Only {len(all_examples)} queries available.\")\n\ntest_example = all_examples[QUERY_INDEX]\n\nprint(\"\\n\" + \"=\"*80)\nprint(f\"üìù TEST EXAMPLE #{QUERY_INDEX + 1} of {len(all_examples)}\")\nprint(\"=\"*80)\nprint(f\"\\nüîç Query (first 300 chars):\\n{test_example['query'][:300]}...\")\nprint(f\"\\n‚úÖ Ground Truth Citations ({len(test_example['relevant_ids'])}):\")\nfor i, title in enumerate(test_example['ground_truth_titles'], 1):\n    print(f\"   {i}. {title}\")\nprint(f\"\\nüí° To test a different query, change QUERY_INDEX (0 to {len(all_examples)-1})\")\nprint(\"=\"*80)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Build Retrieval Resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîß Building retrieval resources...\n",
      "   This may take a few minutes on first run (models will be downloaded)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ishaankalra/Dev/Retrieval/server/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 609/609 [13:51<00:00,  1.37s/it]                  \n",
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1218/1218 [09:22<00:00,  2.16batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Resources built:\n",
      "   - BM25: 9740 documents indexed\n",
      "   - E5: 9740 embeddings\n",
      "   - SPECTER: 9740 embeddings\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from src.resources.builders import build_inmemory_resources\n",
    "\n",
    "print(\"üîß Building retrieval resources...\")\n",
    "print(\"   This may take a few minutes on first run (models will be downloaded)\")\n",
    "\n",
    "resources = build_inmemory_resources(\n",
    "    corpus,\n",
    "    enable_bm25=True,\n",
    "    enable_e5=True,\n",
    "    enable_specter=True\n",
    ")\n",
    "\n",
    "print(\"\\n‚úÖ Resources built:\")\n",
    "print(f\"   - BM25: {len(resources['bm25']['ids'])} documents indexed\")\n",
    "print(f\"   - E5: {resources['e5']['corpus_embeddings'].shape[0]} embeddings\")\n",
    "print(f\"   - SPECTER: {resources['specter']['corpus_embeddings'].shape[0]} embeddings\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Run Individual Retrievers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Running BM25...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ‚úÖ Retrieved 30 papers (top score: 26.077)\n",
      "\n",
      "üîç Running E5...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ‚úÖ Retrieved 30 papers (top score: 0.897)\n",
      "\n",
      "üîç Running SPECTER...\n",
      "   ‚úÖ Retrieved 30 papers (top score: 0.937)\n"
     ]
    }
   ],
   "source": [
    "import bm25s\n",
    "import torch\n",
    "\n",
    "k = 30  # Top-k results\n",
    "query = test_example['query']\n",
    "\n",
    "# BM25\n",
    "print(\"üîç Running BM25...\")\n",
    "bm25_res = resources['bm25']\n",
    "q_tokens = bm25s.tokenize(query, stopwords=\"en\", stemmer=bm25_res[\"stemmer\"])\n",
    "doc_indices, scores = bm25_res[\"index\"].retrieve(q_tokens, k=k)\n",
    "\n",
    "bm25_results = []\n",
    "for idx, score in zip(doc_indices[0], scores[0]):\n",
    "    bm25_results.append({\n",
    "        \"id\": bm25_res[\"ids\"][idx],\n",
    "        \"title\": bm25_res.get(\"titles\", [\"\"])[idx],\n",
    "        \"score\": float(score),\n",
    "        \"source\": \"bm25\"\n",
    "    })\n",
    "\n",
    "print(f\"   ‚úÖ Retrieved {len(bm25_results)} papers (top score: {bm25_results[0]['score']:.3f})\")\n",
    "\n",
    "# E5\n",
    "print(\"\\nüîç Running E5...\")\n",
    "e5_res = resources['e5']\n",
    "with torch.no_grad():\n",
    "    q_emb = e5_res['model'].encode(\n",
    "        [query], convert_to_tensor=True, normalize_embeddings=True, show_progress_bar=False\n",
    "    )\n",
    "\n",
    "scores = (q_emb @ e5_res['corpus_embeddings'].T)[0]\n",
    "top_k = torch.topk(scores, k=min(k, len(scores)))\n",
    "\n",
    "e5_results = []\n",
    "for idx, score in zip(top_k.indices, top_k.values):\n",
    "    e5_results.append({\n",
    "        \"id\": e5_res[\"ids\"][idx.item()],\n",
    "        \"title\": e5_res.get(\"titles\", [\"\"])[idx.item()],\n",
    "        \"score\": score.item(),\n",
    "        \"source\": \"e5\"\n",
    "    })\n",
    "\n",
    "print(f\"   ‚úÖ Retrieved {len(e5_results)} papers (top score: {e5_results[0]['score']:.3f})\")\n",
    "\n",
    "# SPECTER\n",
    "print(\"\\nüîç Running SPECTER...\")\n",
    "specter_res = resources['specter']\n",
    "device = specter_res.get('device') or str(specter_res['corpus_embeddings'].device)\n",
    "\n",
    "model = specter_res['model']\n",
    "if str(next(model.parameters()).device) != device:\n",
    "    model = model.to(device)\n",
    "\n",
    "tokenizer = specter_res['tokenizer']\n",
    "\n",
    "with torch.no_grad():\n",
    "    inputs = tokenizer([query], padding=True, truncation=True, max_length=256, return_tensors=\"pt\")\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "    q_emb = model(**inputs).last_hidden_state.mean(dim=1)\n",
    "    q_emb = torch.nn.functional.normalize(q_emb, dim=1)\n",
    "\n",
    "corpus_embs = specter_res['corpus_embeddings'].to(device)\n",
    "corpus_embs = torch.nn.functional.normalize(corpus_embs, dim=1)\n",
    "scores = (q_emb @ corpus_embs.T)[0]\n",
    "top_k = torch.topk(scores, k=min(k, len(scores)))\n",
    "\n",
    "specter_results = []\n",
    "for idx, score in zip(top_k.indices, top_k.values):\n",
    "    specter_results.append({\n",
    "        \"id\": specter_res[\"ids\"][idx.item()],\n",
    "        \"title\": specter_res.get(\"titles\", [\"\"])[idx.item()],\n",
    "        \"score\": score.item(),\n",
    "        \"source\": \"specter\"\n",
    "    })\n",
    "\n",
    "print(f\"   ‚úÖ Retrieved {len(specter_results)} papers (top score: {specter_results[0]['score']:.3f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Aggregate with Reciprocal Rank Fusion (RRF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÄ Aggregating results with RRF...\n",
      "   ‚úÖ Aggregated into 74 unique papers\n",
      "\n",
      "üìä Top 5 after RRF aggregation:\n",
      "\n",
      "1. Lite Transformer with Long-Short Range Attention...\n",
      "   RRF Score: 0.0474\n",
      "   Sources: bm25, e5, specter\n",
      "\n",
      "2. BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding...\n",
      "   RRF Score: 0.0401\n",
      "   Sources: bm25, e5, specter\n",
      "\n",
      "3. Tinybert: Distilling {BERT} for natural language understanding...\n",
      "   RRF Score: 0.0320\n",
      "   Sources: bm25, specter\n",
      "\n",
      "4. TinyBERT: Distilling BERT for Natural Language Understanding...\n",
      "   RRF Score: 0.0304\n",
      "   Sources: bm25, specter\n",
      "\n",
      "5. Funnel-transformer: Filtering out sequential redundancy for efficient language p...\n",
      "   RRF Score: 0.0301\n",
      "   Sources: bm25, specter\n",
      "\n",
      "6. Distilbert, a distilled version of BERT: smaller, faster, cheaper and lighter...\n",
      "   RRF Score: 0.0300\n",
      "   Sources: bm25, specter\n",
      "\n",
      "7. Q8bert: Quantized 8bit bert...\n",
      "   RRF Score: 0.0296\n",
      "   Sources: bm25, e5\n",
      "\n",
      "8. A survey on visual transformer...\n",
      "   RRF Score: 0.0295\n",
      "   Sources: bm25, e5\n",
      "\n",
      "9. Compressing large-scale transformer-based models: A case study on bert...\n",
      "   RRF Score: 0.0283\n",
      "   Sources: bm25, e5\n",
      "\n",
      "10. Universal transformers...\n",
      "   RRF Score: 0.0282\n",
      "   Sources: e5, specter\n",
      "\n",
      "11. Extremely Low Bit Transformer Quantization for On-Device Neural Machine Translat...\n",
      "   RRF Score: 0.0264\n",
      "   Sources: bm25, e5\n",
      "\n",
      "12. {BERT: Pre-training of Deep Bidirectional Transformers for Language Understandin...\n",
      "   RRF Score: 0.0259\n",
      "   Sources: bm25, e5\n",
      "\n",
      "13. Reducing transformer depth on demand with structured dropout...\n",
      "   RRF Score: 0.0257\n",
      "   Sources: bm25, e5\n",
      "\n",
      "14. Transformer-xl: Attentive language models beyond a fixed-length context...\n",
      "   RRF Score: 0.0252\n",
      "   Sources: e5, specter\n",
      "\n",
      "15. Recent advances in natural language processing via large pre-trained language mo...\n",
      "   RRF Score: 0.0161\n",
      "   Sources: e5\n",
      "\n",
      "16. Attention is all you need...\n",
      "   RRF Score: 0.0161\n",
      "   Sources: specter\n",
      "\n",
      "17. Understanding and Overcoming the Challenges of Efficient Transformer Quantizatio...\n",
      "   RRF Score: 0.0159\n",
      "   Sources: e5\n",
      "\n",
      "18. MobileBERT: a Compact Task-Agnostic BERT for Resource-Limited Devices...\n",
      "   RRF Score: 0.0159\n",
      "   Sources: specter\n",
      "\n",
      "19. Escaping the Big Data Paradigm with Compact Transformers...\n",
      "   RRF Score: 0.0156\n",
      "   Sources: specter\n",
      "\n",
      "20. Consert: A contrastive framework for self-supervised sentence representation tra...\n",
      "   RRF Score: 0.0154\n",
      "   Sources: bm25\n",
      "\n",
      "21. Learning Deep Transformer Models for Machine Translation...\n",
      "   RRF Score: 0.0152\n",
      "   Sources: e5\n",
      "\n",
      "22. pNLP-mixer: an efficient all-MLP architecture for language...\n",
      "   RRF Score: 0.0152\n",
      "   Sources: specter\n",
      "\n",
      "23. Huggingface's transformers: State-of-the-art natural language processing...\n",
      "   RRF Score: 0.0147\n",
      "   Sources: e5\n",
      "\n",
      "24. Colt5: Faster long-range transformers with conditional computation...\n",
      "   RRF Score: 0.0145\n",
      "   Sources: bm25\n",
      "\n",
      "25. The LAMBADA dataset: Word prediction requiring a broad discourse context...\n",
      "   RRF Score: 0.0143\n",
      "   Sources: bm25\n",
      "\n",
      "26. AMMUS : A Survey of Transformer-based Pretrained Models in Natural Language Proc...\n",
      "   RRF Score: 0.0143\n",
      "   Sources: e5\n",
      "\n",
      "27. Scaling end-to-end models for large-scale multilingual asr...\n",
      "   RRF Score: 0.0143\n",
      "   Sources: specter\n",
      "\n",
      "28. Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parall...\n",
      "   RRF Score: 0.0141\n",
      "   Sources: e5\n",
      "\n",
      "29. ST-MoE: Designing Stable and Transferable Sparse Expert Models...\n",
      "   RRF Score: 0.0139\n",
      "   Sources: bm25\n",
      "\n",
      "30. RealFormer: Transformer Likes Residual Attention...\n",
      "   RRF Score: 0.0139\n",
      "   Sources: e5\n",
      "\n",
      "31. Conformer: Convolution-augmented Transformer for Speech Recognition...\n",
      "   RRF Score: 0.0137\n",
      "   Sources: e5\n",
      "\n",
      "32. A Survey of Large Language Models...\n",
      "   RRF Score: 0.0135\n",
      "   Sources: specter\n",
      "\n",
      "33. Bp-transformer: Modelling long-range context via binary partitioning...\n",
      "   RRF Score: 0.0133\n",
      "   Sources: bm25\n",
      "\n",
      "34. AdapterDrop: On the Efficiency of Adapters in Transformers...\n",
      "   RRF Score: 0.0133\n",
      "   Sources: e5\n",
      "\n",
      "35. Deep neural machine translation with linear associative unit...\n",
      "   RRF Score: 0.0133\n",
      "   Sources: specter\n",
      "\n",
      "36. Efficientformer: Vision transformers at mobilenet speed...\n",
      "   RRF Score: 0.0132\n",
      "   Sources: bm25\n",
      "\n",
      "37. The NLP Task Effectiveness of Long-Range Transformers...\n",
      "   RRF Score: 0.0132\n",
      "   Sources: e5\n",
      "\n",
      "38. Linformer: Self-attention with linear complexity...\n",
      "   RRF Score: 0.0132\n",
      "   Sources: specter\n",
      "\n",
      "39. Bart: Denoising sequence-to-sequence pre-training for natural language generatio...\n",
      "   RRF Score: 0.0130\n",
      "   Sources: bm25\n",
      "\n",
      "40. {Pretrained Transformers as Universal Computation Engines...\n",
      "   RRF Score: 0.0130\n",
      "   Sources: e5\n",
      "\n",
      "41. A Comprehensive Evaluation of Quantization Strategies for Large Language Models...\n",
      "   RRF Score: 0.0130\n",
      "   Sources: specter\n",
      "\n",
      "42. Transformer in transformer...\n",
      "   RRF Score: 0.0128\n",
      "   Sources: bm25\n",
      "\n",
      "43. Pretrained transformers as universal computation engines...\n",
      "   RRF Score: 0.0128\n",
      "   Sources: e5\n",
      "\n",
      "44. {Language Models are Few-Shot Learners...\n",
      "   RRF Score: 0.0128\n",
      "   Sources: specter\n",
      "\n",
      "45. Competition-level code generation with alphacode...\n",
      "   RRF Score: 0.0127\n",
      "   Sources: bm25\n",
      "\n",
      "46. Language models are few-shot learners...\n",
      "   RRF Score: 0.0127\n",
      "   Sources: specter\n",
      "\n",
      "47. {Competition-level code generation with AlphaCode...\n",
      "   RRF Score: 0.0125\n",
      "   Sources: bm25\n",
      "\n",
      "48. Efficient transformers: A survey...\n",
      "   RRF Score: 0.0125\n",
      "   Sources: e5\n",
      "\n",
      "49. Outrageously large neural networks: The sparsely-gated mixture-of-experts layer...\n",
      "   RRF Score: 0.0125\n",
      "   Sources: specter\n",
      "\n",
      "50. Beyond bilinear: Generalized multimodal factorized high-order pooling for visual...\n",
      "   RRF Score: 0.0123\n",
      "   Sources: bm25\n",
      "\n",
      "51. Random feature attention...\n",
      "   RRF Score: 0.0123\n",
      "   Sources: specter\n",
      "\n",
      "52. What Makes Good In-Context Examples for GPT-$3$?...\n",
      "   RRF Score: 0.0122\n",
      "   Sources: bm25\n",
      "\n",
      "53. Modifying memories in transformer models...\n",
      "   RRF Score: 0.0122\n",
      "   Sources: e5\n",
      "\n",
      "54. Phrase-based & neural unsupervised machine translation...\n",
      "   RRF Score: 0.0122\n",
      "   Sources: specter\n",
      "\n",
      "55. Pixel-bert: Aligning image pixels with text by deep multi-modal transformers...\n",
      "   RRF Score: 0.0120\n",
      "   Sources: bm25\n",
      "\n",
      "56. HAT: Hardware-Aware Transformers for Efficient Natural Language Processing...\n",
      "   RRF Score: 0.0120\n",
      "   Sources: e5\n",
      "\n",
      "57. Incorporating convolution designs into visual transformers...\n",
      "   RRF Score: 0.0120\n",
      "   Sources: specter\n",
      "\n",
      "58. BitNet: Scaling 1-bit Transformers for Large Language Models...\n",
      "   RRF Score: 0.0119\n",
      "   Sources: e5\n",
      "\n",
      "59. Unsupervised Cross-lingual Representation Learning at Scale...\n",
      "   RRF Score: 0.0119\n",
      "   Sources: specter\n",
      "\n",
      "60. Multimodal transformer for unaligned multimodal language sequences...\n",
      "   RRF Score: 0.0118\n",
      "   Sources: bm25\n",
      "\n",
      "61. Controlling computation versus quality for neural sequence models...\n",
      "   RRF Score: 0.0118\n",
      "   Sources: e5\n",
      "\n",
      "62. Moebert: from bert to mixture-of-experts via importance-guided adaptation...\n",
      "   RRF Score: 0.0118\n",
      "   Sources: specter\n",
      "\n",
      "63. Quantizable Transformers: Removing Outliers by Helping Attention Heads Do Nothin...\n",
      "   RRF Score: 0.0116\n",
      "   Sources: bm25\n",
      "\n",
      "64. Autotrans: Automating transformer design via reinforced architecture search...\n",
      "   RRF Score: 0.0116\n",
      "   Sources: e5\n",
      "\n",
      "65. Scaling sentence embeddings with large language models...\n",
      "   RRF Score: 0.0116\n",
      "   Sources: specter\n",
      "\n",
      "66. Beyond neural scaling laws: beating power law scaling via data pruning...\n",
      "   RRF Score: 0.0115\n",
      "   Sources: bm25\n",
      "\n",
      "67. Breaking the softmax bottleneck: a high-rank rnn language model...\n",
      "   RRF Score: 0.0115\n",
      "   Sources: specter\n",
      "\n",
      "68. Structured Pruning of a BERT-based Question Answering Model...\n",
      "   RRF Score: 0.0114\n",
      "   Sources: bm25\n",
      "\n",
      "69. Scalable Transformers for Neural Machine Translation...\n",
      "   RRF Score: 0.0114\n",
      "   Sources: e5\n",
      "\n",
      "70. Prompt-prompted Adaptive Structured Pruning for Efficient LLM Generation...\n",
      "   RRF Score: 0.0114\n",
      "   Sources: specter\n",
      "\n",
      "71. Fully quantized transformer for machine translation...\n",
      "   RRF Score: 0.0112\n",
      "   Sources: e5\n",
      "\n",
      "72. Dataset Quantization...\n",
      "   RRF Score: 0.0112\n",
      "   Sources: specter\n",
      "\n",
      "73. A White Paper on Neural Network Quantization...\n",
      "   RRF Score: 0.0111\n",
      "   Sources: bm25\n",
      "\n",
      "74. Deberta: Decoding-enhanced bert with disentangled attention...\n",
      "   RRF Score: 0.0111\n",
      "   Sources: specter\n"
     ]
    }
   ],
   "source": [
    "def reciprocal_rank_fusion(results_dict, k=60):\n",
    "    \"\"\"Aggregate results using RRF.\"\"\"\n",
    "    paper_scores = {}\n",
    "    \n",
    "    for source, results in results_dict.items():\n",
    "        for rank, paper in enumerate(results):\n",
    "            paper_id = paper['id']\n",
    "            rrf_score = 1.0 / (k + rank + 1)\n",
    "            \n",
    "            if paper_id not in paper_scores:\n",
    "                paper_scores[paper_id] = {\n",
    "                    'paper': paper,\n",
    "                    'rrf_score': 0,\n",
    "                    'sources': []\n",
    "                }\n",
    "            \n",
    "            paper_scores[paper_id]['rrf_score'] += rrf_score\n",
    "            paper_scores[paper_id]['sources'].append(source)\n",
    "    \n",
    "    # Sort by RRF score\n",
    "    ranked = sorted(paper_scores.values(), key=lambda x: x['rrf_score'], reverse=True)\n",
    "    return ranked\n",
    "\n",
    "print(\"üîÄ Aggregating results with RRF...\")\n",
    "aggregated = reciprocal_rank_fusion({\n",
    "    'bm25': bm25_results,\n",
    "    'e5': e5_results,\n",
    "    'specter': specter_results\n",
    "})\n",
    "\n",
    "print(f\"   ‚úÖ Aggregated into {len(aggregated)} unique papers\")\n",
    "\n",
    "# Show top 5\n",
    "print(\"\\nüìä Top 5 after RRF aggregation:\")\n",
    "for i, item in enumerate(aggregated, 1):\n",
    "    paper = item['paper']\n",
    "    print(f\"\\n{i}. {paper['title'][:80]}...\")\n",
    "    print(f\"   RRF Score: {item['rrf_score']:.4f}\")\n",
    "    print(f\"   Sources: {', '.join(item['sources'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: LLM Reranking (Hugging Face or Ollama)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ü§ñ LLM Reranking 20 candidates...\n",
      "\n",
      "üîÑ Using Ollama with model: gemma3:4b\n",
      "\n",
      "üìù Invoking LLM...\n",
      "‚úÖ LLM response received (645 chars)\n"
     ]
    }
   ],
   "source": [
    "from src.prompts.llm_reranker import LLMRerankerPrompt\n",
    "from langchain_ollama import ChatOllama\n",
    "from langchain_openai import ChatOpenAI\n",
    "import logging\n",
    "\n",
    "# Suppress verbose logs\n",
    "logging.getLogger(\"httpx\").setLevel(logging.WARNING)\n",
    "\n",
    "# Prepare candidates\n",
    "candidate_papers = [item['paper'] for item in aggregated[:20]]  # Top 20 for reranking\n",
    "\n",
    "print(f\"ü§ñ LLM Reranking {len(candidate_papers)} candidates...\\n\")\n",
    "\n",
    "# Initialize LLM\n",
    "inference_engine = os.getenv(\"INFERENCE_ENGINE\", \"ollama\").lower()\n",
    "model_id = os.getenv(\"LOCAL_LLM\", \"gemma3:4b\")\n",
    "\n",
    "if inference_engine == \"ollama\":\n",
    "    print(f\"üîÑ Using Ollama with model: {model_id}\")\n",
    "    llm = ChatOllama(model=model_id, temperature=0)\n",
    "else:\n",
    "    print(f\"üîÑ Using Hugging Face model: {model_id}\")\n",
    "    from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "    from langchain_huggingface import HuggingFacePipeline\n",
    "    import torch\n",
    "    \n",
    "    print(\"   Loading model (this may take a minute)...\")\n",
    "    tok = AutoTokenizer.from_pretrained(model_id)\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_id,\n",
    "        device_map=\"auto\",\n",
    "        torch_dtype=torch.bfloat16 if torch.cuda.is_available() else torch.float32,\n",
    "    )\n",
    "    \n",
    "    gen = pipeline(\n",
    "        \"text-generation\",\n",
    "        model=model,\n",
    "        tokenizer=tok,\n",
    "        max_new_tokens=1024,\n",
    "        do_sample=False,\n",
    "    )\n",
    "    \n",
    "    llm = HuggingFacePipeline(pipeline=gen)\n",
    "    print(\"   ‚úÖ Model loaded!\")\n",
    "\n",
    "# Build prompt\n",
    "prompt = LLMRerankerPrompt(query=query, candidate_papers=candidate_papers).get_prompt()\n",
    "\n",
    "print(\"\\nüìù Invoking LLM...\")\n",
    "response = llm.invoke(prompt)\n",
    "response_text = response.content\n",
    "\n",
    "print(f\"‚úÖ LLM response received ({len(response_text)} chars)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Parse Results and Show Rankings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Successfully parsed 20 rankings\n",
      "\n",
      "================================================================================\n",
      "üèÜ FINAL RANKED RESULTS (Top 10)\n",
      "================================================================================\n",
      "\n",
      "1. BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding...\n",
      "   LLM Score: 0.980 \n",
      "   ID: devlin2019bert\n",
      "\n",
      "2. Lite Transformer with Long-Short Range Attention...\n",
      "   LLM Score: 0.950 \n",
      "   ID: lite_transformer\n",
      "\n",
      "3. Tinybert: Distilling {BERT} for natural language understanding...\n",
      "   LLM Score: 0.850 \n",
      "   ID: jiao2019tinybert\n",
      "\n",
      "4. TinyBERT: Distilling BERT for Natural Language Understanding...\n",
      "   LLM Score: 0.800 \n",
      "   ID: jiao20tinybert\n",
      "\n",
      "5. Funnel-transformer: Filtering out sequential redundancy for efficient language processing...\n",
      "   LLM Score: 0.750 \n",
      "   ID: funnel\n",
      "\n",
      "6. Distilbert, a distilled version of BERT: smaller, faster, cheaper and lighter...\n",
      "   LLM Score: 0.700 \n",
      "   ID: distillbert\n",
      "\n",
      "7. Q8bert: Quantized 8bit bert...\n",
      "   LLM Score: 0.650 \n",
      "   ID: zafrir2019q8bert\n",
      "\n",
      "8. A survey on visual transformer...\n",
      "   LLM Score: 0.600 \n",
      "   ID: han2020survey\n",
      "\n",
      "9. Compressing large-scale transformer-based models: A case study on bert...\n",
      "   LLM Score: 0.550 \n",
      "   ID: ganesh2020compressing\n",
      "\n",
      "10. Universal transformers...\n",
      "   LLM Score: 0.500 \n",
      "   ID: dehghani2018universal\n",
      "\n",
      "================================================================================\n",
      "üìä EVALUATION\n",
      "================================================================================\n",
      "Ground truth citations found in top 10: 0/1\n",
      "Recall@10: 0.00%\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Parse LLM response\n",
    "try:\n",
    "    json_match = re.search(r\"\\[[\\s\\S]*\\]\", response_text)\n",
    "    if json_match:\n",
    "        json_str = json_match.group()\n",
    "        rankings = json.loads(json_str)\n",
    "    else:\n",
    "        rankings = json.loads(response_text)\n",
    "    \n",
    "    print(f\"‚úÖ Successfully parsed {len(rankings)} rankings\\n\")\n",
    "    \n",
    "    # Build final ranked list\n",
    "    final_ranked = []\n",
    "    for item in rankings:\n",
    "        idx = item['index'] - 1\n",
    "        score = item['score']\n",
    "        if 0 <= idx < len(candidate_papers):\n",
    "            final_ranked.append((candidate_papers[idx], score))\n",
    "    \n",
    "    # Display results\n",
    "    print(\"=\"*80)\n",
    "    print(\"üèÜ FINAL RANKED RESULTS (Top 10)\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    relevant_ids = test_example['relevant_ids']\n",
    "    found_count = 0\n",
    "    \n",
    "    for i, (paper, score) in enumerate(final_ranked[:10], 1):\n",
    "        is_correct = paper['id'] in relevant_ids\n",
    "        marker = \"‚úÖ GROUND TRUTH\" if is_correct else \"\"\n",
    "        \n",
    "        if is_correct:\n",
    "            found_count += 1\n",
    "        \n",
    "        print(f\"\\n{i}. {paper['title'][:100]}...\")\n",
    "        print(f\"   LLM Score: {score:.3f} {marker}\")\n",
    "        print(f\"   ID: {paper['id']}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(f\"üìä EVALUATION\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"Ground truth citations found in top 10: {found_count}/{len(relevant_ids)}\")\n",
    "    print(f\"Recall@10: {found_count/len(relevant_ids):.2%}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error parsing LLM response: {e}\")\n",
    "    print(f\"\\nRaw response (first 500 chars):\\n{response_text[:500]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook demonstrated the complete citation retrieval pipeline:\n",
    "1. ‚úÖ Loaded citation context from ScholarCopilot dataset\n",
    "2. ‚úÖ Built BM25, E5, and SPECTER retrieval indices\n",
    "3. ‚úÖ Retrieved top-k candidates from each method\n",
    "4. ‚úÖ Aggregated using Reciprocal Rank Fusion\n",
    "5. ‚úÖ Reranked with LLM (Hugging Face or Ollama)\n",
    "6. ‚úÖ Evaluated against ground truth\n",
    "\n",
    "**To run full evaluation:**\n",
    "```bash\n",
    "python compare_baselines_vs_system.py --num-examples 500 --use-dspy --llm-reranker --output-dir final --k 20\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}