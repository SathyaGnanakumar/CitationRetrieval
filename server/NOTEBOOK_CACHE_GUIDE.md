# Notebook Resource Caching Guide

## What Changed

The `test_single_example.ipynb` notebook now supports **resource caching**, making it much faster to run multiple times!

## Performance Improvement

| Run | Time |
|-----|------|
| **First run** (builds and caches) | ~20-30 minutes |
| **Subsequent runs** (loads from cache) | ~30 seconds |

**Speedup: 40-60x faster!** ğŸš€

---

## How It Works

### First Run: Build & Cache

```python
# Cell configuration
USE_CACHE = True
ENABLE_LLM_RERANKER = True

# What happens:
# 1. Builds BM25 index (~1 min)
# 2. Builds E5 embeddings (~10 min)
# 3. Builds SPECTER embeddings (~10 min)
# 4. Loads LLM model (~2-5 min)
# 5. Saves everything to .cache/ folder
```

**Output:**
```
ğŸ”¨ Building resources from scratch...
   This may take a few minutes on first run (models will be downloaded)
============================================================
ğŸ”¨ Building BM25 index for 9740 documents...
âœ… BM25 index built successfully
============================================================
ğŸ”¨ Building E5 embeddings for 9740 documents...
âœ… E5 embeddings built successfully
============================================================
ğŸ”¨ Building SPECTER embeddings for 9740 documents...
âœ… SPECTER embeddings built successfully
============================================================
ğŸ”¨ Building LLM Reranker resources...
   Inference Engine: huggingface
   Model: google/gemma-2-9b-it
âœ… Hugging Face model loaded and cached!
============================================================
âœ… All retrieval resources built successfully!

ğŸ’¾ Saving resources to cache for future use...
âœ… Cache saved! Next run will be faster.
```

---

### Subsequent Runs: Load from Cache

**Output:**
```
ğŸ“¦ Attempting to load from cache...
ğŸ“¦ Loading resources from cache: .cache/a1b2c3d4/
âœ… Resources loaded from cache!

âœ… Resources ready:
   - BM25: 9740 documents indexed
   - E5: 9740 embeddings
   - SPECTER: 9740 embeddings
   - LLM Reranker: google/gemma-2-9b-it (huggingface)
```

**Total time: ~30 seconds instead of 20-30 minutes!**

---

## Configuration Options

### Option 1: Use Cache with LLM (Default - Fastest)

```python
USE_CACHE = True
ENABLE_LLM_RERANKER = True
```

- âœ… Loads everything from cache
- âœ… LLM model ready for reranking
- âœ… Fastest after first run

---

### Option 2: Use Cache without LLM

```python
USE_CACHE = True
ENABLE_LLM_RERANKER = False
```

- âœ… Loads BM25, E5, SPECTER from cache
- âš ï¸ LLM reranking step will be slower (loads on-the-fly)

---

### Option 3: Rebuild from Scratch

```python
USE_CACHE = False
ENABLE_LLM_RERANKER = True
```

- âš ï¸ Rebuilds everything (slow)
- âœ… Useful if you changed the dataset
- âœ… Will save new cache after building

---

### Option 4: No Cache, No LLM Preload

```python
USE_CACHE = False
ENABLE_LLM_RERANKER = False
```

- âš ï¸ Slowest option
- âš ï¸ Only use for debugging

---

## Cache Location

Cache is stored in:
```
corpus_loaders/scholarcopilot/.cache/<dataset_hash>/
â”œâ”€â”€ metadata.json          # Cache info
â”œâ”€â”€ corpus.pkl            # Document corpus
â”œâ”€â”€ bm25.pkl              # BM25 metadata
â”œâ”€â”€ bm25_index/           # BM25 index files
â”œâ”€â”€ e5.pkl                # E5 metadata
â”œâ”€â”€ e5_embeddings.pt      # E5 embeddings (large!)
â”œâ”€â”€ specter.pkl           # SPECTER metadata
â””â”€â”€ specter_embeddings.pt # SPECTER embeddings (large!)
```

**Note:** LLM models are cached by Hugging Face/transformers separately in `~/.cache/huggingface/`

---

## Cache Management

### Clear Cache

If you want to rebuild everything:

```python
from src.resources.cache import clear_cache

# Clear cache for current dataset
clear_cache(dataset_path)
```

Or manually delete the cache folder:
```bash
rm -rf corpus_loaders/scholarcopilot/.cache/
```

### Check Cache Status

```python
from src.resources.cache import get_cache_path
from pathlib import Path

cache_path = get_cache_path(dataset_path)
if (cache_path / "metadata.json").exists():
    print(f"âœ… Cache exists at: {cache_path}")
else:
    print(f"âŒ No cache found")
```

---

## Testing Different Queries

With caching enabled, you can quickly test different queries:

```python
# Change this to test different queries
QUERY_INDEX = 0  # Try 0, 1, 2, 3, etc.

# Run the notebook
# Resources load from cache in ~30 seconds
# Test different queries without rebuilding!
```

**Example workflow:**
1. First run: Build and cache everything (20-30 min)
2. Test query #0 (30 sec load + inference)
3. Change to `QUERY_INDEX = 1`
4. Test query #1 (30 sec load + inference)
5. Repeat for as many queries as you want!

---

## LLM Model Caching

### Hugging Face Models

When `ENABLE_LLM_RERANKER = True` and `INFERENCE_ENGINE = "huggingface"`:

**First time:**
```
ğŸ”„ Loading Hugging Face model: google/gemma-2-9b-it
   This will take a few minutes on first run...
âœ… Hugging Face model loaded and cached!
```

**Subsequent runs:**
```
ğŸ”„ Loading Hugging Face model: google/gemma-2-9b-it
   This will take a few minutes on first run...
âœ… Hugging Face model loaded and cached!
```

The model loads from HuggingFace's cache (`~/.cache/huggingface/`) which is fast (~2 min vs 5-10 min downloading).

**Then during inference:**
```
ğŸš€ Using cached LLM model: google/gemma-2-9b-it
```

No reloading between examples!

---

### OpenAI Models

When `ENABLE_LLM_RERANKER = True` and `INFERENCE_ENGINE = "openai"`:

```
ğŸ”„ Initializing OpenAI with model: gpt-4o-mini
âœ… OpenAI ready!
```

OpenAI client initializes instantly (cloud-based, no local loading).

---

### Ollama Models

When `ENABLE_LLM_RERANKER = True` and `INFERENCE_ENGINE = "ollama"`:

```
ğŸ”„ Initializing Ollama with model: gemma3:4b
âœ… Ollama ready!
```

Ollama client initializes quickly (connects to local server).

---

## Troubleshooting

### Cache Not Loading

**Symptom:** Always rebuilds even with `USE_CACHE = True`

**Causes:**
1. Dataset changed - cache is invalidated automatically
2. Cache folder deleted
3. Different Python environment

**Solution:**
- Let it rebuild once, cache will be saved
- Or check if `.cache/` folder exists

---

### Out of Memory

**Symptom:** Crash when loading E5 or SPECTER embeddings

**Cause:** Large embeddings don't fit in RAM/VRAM

**Solution:**
```python
# Load only what you need
USE_CACHE = True
ENABLE_LLM_RERANKER = True  # Or False if tight on memory

# In the load_resources call, disable heavy components:
resources = load_resources(
    dataset_path,
    enable_bm25=True,
    enable_e5=False,      # Disable if needed
    enable_specter=False  # Disable if needed
)
```

---

### LLM Not Using Cache

**Symptom:** LLM loads on every query

**Cause:** `ENABLE_LLM_RERANKER = False`

**Solution:**
```python
# Enable LLM caching
ENABLE_LLM_RERANKER = True
```

Then you'll see:
```
ğŸš€ Using cached LLM model: google/gemma-2-9b-it
```

---

## Summary

âœ… **Always use caching** for faster iterations
âœ… **Enable LLM reranker** for best performance
âœ… **First run is slow** but saves time on all future runs
âœ… **Cache is automatic** - builds and saves on first run
âœ… **Test multiple queries quickly** after initial cache build

**Time saved: 20-30 minutes â†’ 30 seconds per run!**
